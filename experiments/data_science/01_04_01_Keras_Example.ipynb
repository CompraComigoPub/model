{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f6e4f4",
   "metadata": {},
   "source": [
    "# Models with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c392112c",
   "metadata": {},
   "source": [
    "Trains models on synthetic data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1edadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d2b58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 10:54:08.356167: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-28 10:54:08.408498: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-28 10:54:08.409329: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-28 10:54:09.862539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numpy import arange\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a84cb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_compra_comigo.experimenter import Experimenter\n",
    "from model_compra_comigo.data_handler.data_simulator import DataSimulator\n",
    "from model_compra_comigo.data_handler import DataHandler\n",
    "from model_compra_comigo.data_handler.utils import plot_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d7bd993",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_simulator = DataSimulator()\n",
    "data_handler = DataHandler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278e88c",
   "metadata": {},
   "source": [
    "## Generate a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb74e019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1200x800>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_range = 365*3\n",
    "split = 365\n",
    "time_data = arange(time_range)\n",
    "\n",
    "series, parameters = data_simulator.generate(\n",
    "    time_range = time_range,\n",
    ")\n",
    "\n",
    "labels=[(f\"trend_slope = {parameters['trend_slope']}\\n\"\n",
    "         f\"seasonality_period = {parameters['seasonality_period']}\\n\"\n",
    "         f\"seasonality_amplitude = {parameters['seasonality_amplitude']}\\n\"\n",
    "         f\"seasonality_phase = {parameters['seasonality_phase']}\\n\"\n",
    "         f\"seasonality_time_threshold = {parameters['seasonality_time_threshold']}\\n\"\n",
    "         f\"seasonality_ncos = {parameters['seasonality_ncos']}\\n\"\n",
    "         f\"seasonality_nexp = {parameters['seasonality_nexp']}\\n\"\n",
    "         f\"noise_scaling_factor = {parameters['noise_scaling_factor']}\\n\"\n",
    "         f\"autocorrelation_amplitude = {parameters['autocorrelation_amplitude']}\\n\"\n",
    "         f\"autocorrelation_phi = {parameters['autocorrelation_phi']}\"\n",
    "        )]\n",
    "\n",
    "data_handler.plot_series(\n",
    "    time=time_data,\n",
    "    series=series,\n",
    "    labels=labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed6caf",
   "metadata": {},
   "source": [
    "## Creates Windowed Dataset with N forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033cf5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30\n",
    "nforecast = 20\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7bc95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=300\n",
    "dataset=series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f69b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data_handler.generate_windowed_dataset(\n",
    "    data = series,\n",
    "    window_size = window_size,\n",
    "    batch_size = batch_size,\n",
    "    shuffle_buffer_size = shuffle_buffer_size,\n",
    "    nforecast = nforecast,\n",
    "    shuffle = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90ff414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D, TimeDistributed, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bd9cecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 10:54:14.126167: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [1095]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-07-28 10:54:14.126645: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [1095]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset creation function\n",
    "def create_dataset(data, window_size, n):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.window(window_size + n, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + n))\n",
    "    dataset = dataset.map(lambda window: (window[:-n], window[-n:])).shuffle(buffer_size=100)\n",
    "    return dataset\n",
    "\n",
    "dataset = create_dataset(series, window_size, nforecast)\n",
    "\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for input_seq, target_seq in dataset:\n",
    "    inputs.append(input_seq.numpy())\n",
    "    targets.append(target_seq.numpy())\n",
    "\n",
    "inputs_ts = tf.convert_to_tensor(inputs)\n",
    "targets_ts = tf.convert_to_tensor(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "603d4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2200815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1046, 30, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = np.array(inputs)\n",
    "inp = inp.reshape(inp.shape[0], inp.shape[1], 1)\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbdb17ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1046, 20, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array(targets)\n",
    "t = t.reshape(t.shape[0], t.shape[1], 1)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9844aeff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([-11.09627168,  49.64498689,  17.47115024, -28.80375993,\n",
      "       101.72731666,  77.58717368,  65.46366834,  -4.8640219 ,\n",
      "         8.09607209,  55.7317209 ,   9.41582456,  61.83334275,\n",
      "        43.3839532 ,  42.79186531,  32.78361335, 149.68041012,\n",
      "       109.27425031,  25.89407055,  94.10193173,  29.1604104 ,\n",
      "        62.45920462, -19.259528  , -30.02332054,  54.25897839,\n",
      "       115.20075505, 104.4155928 ,  81.90616437,  66.43128026,\n",
      "         3.62500397,  16.29659574])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 42.40312683, 125.84206218, 124.86800076,  10.80211719,\n",
      "        69.67090736,  73.00015151,  49.1362523 , 106.98454433,\n",
      "       155.1638923 , 163.89514009,  76.87066266,  66.65283723,\n",
      "       105.2879106 , 152.28966719,  96.70344248,  83.77352599,\n",
      "        41.23114256,  17.99426911, 114.11325318, 184.04522052])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([150.26046624, 142.49521639, 127.19622084,  63.7869869 ,\n",
      "        88.21464394, 110.41550736,  92.38406199, 116.29604032,\n",
      "       157.91140713, 246.51495725, 196.23187143, 170.08294304,\n",
      "       152.27811061,  53.64930572, 108.24262341, 147.88143388,\n",
      "       274.83401078, 196.4136086 , 173.2665378 , 161.73243664,\n",
      "        99.10058533, 189.91445947, 217.03019629, 218.01662357,\n",
      "       135.62151463, 215.66343029, 121.17746619, 167.28600356,\n",
      "       283.28864689, 164.69125998])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([125.10364505, 159.87748372, 144.34205834,  82.29692285,\n",
      "       140.1662113 , 115.85726673, 173.41590414, 135.43190224,\n",
      "       233.88614205, 168.22234072, 149.37417651, 211.20263936,\n",
      "       134.80757158, 168.40259944, 248.81801436, 131.08170877,\n",
      "       163.32682593, 198.28054145, 230.55995397, 143.44437565])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 42.40312683, 125.84206218, 124.86800076,  10.80211719,\n",
      "        69.67090736,  73.00015151,  49.1362523 , 106.98454433,\n",
      "       155.1638923 , 163.89514009,  76.87066266,  66.65283723,\n",
      "       105.2879106 , 152.28966719,  96.70344248,  83.77352599,\n",
      "        41.23114256,  17.99426911, 114.11325318, 184.04522052,\n",
      "       131.63722049, 158.90144505, 147.21045867,  86.83575307,\n",
      "       115.00959466, 192.5315456 , 142.60208615, 193.93866008,\n",
      "        16.93172331, 103.93682066])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([129.66886997, 103.55737712, 115.40403437,  19.93344086,\n",
      "        65.42368531, 126.55694824, 199.66504437, 128.07629968,\n",
      "        76.25036135,  81.087672  , 156.95619511, 159.44456572,\n",
      "       109.77291327, 143.66763433, 143.35571794, 181.43706483,\n",
      "       117.23990033, 103.79331492, 104.97248027,  51.30083837])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 73.00015151,  49.1362523 , 106.98454433, 155.1638923 ,\n",
      "       163.89514009,  76.87066266,  66.65283723, 105.2879106 ,\n",
      "       152.28966719,  96.70344248,  83.77352599,  41.23114256,\n",
      "        17.99426911, 114.11325318, 184.04522052, 131.63722049,\n",
      "       158.90144505, 147.21045867,  86.83575307, 115.00959466,\n",
      "       192.5315456 , 142.60208615, 193.93866008,  16.93172331,\n",
      "       103.93682066, 129.66886997, 103.55737712, 115.40403437,\n",
      "        19.93344086,  65.42368531])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([126.55694824, 199.66504437, 128.07629968,  76.25036135,\n",
      "        81.087672  , 156.95619511, 159.44456572, 109.77291327,\n",
      "       143.66763433, 143.35571794, 181.43706483, 117.23990033,\n",
      "       103.79331492, 104.97248027,  51.30083837, 116.82936428,\n",
      "       150.26046624, 142.49521639, 127.19622084,  63.7869869 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 72.84388783,  41.03382835,  22.4299133 ,  55.83187576,\n",
      "       -36.14098885, -69.86728076, -13.39040036, -11.09627168,\n",
      "        49.64498689,  17.47115024, -28.80375993, 101.72731666,\n",
      "        77.58717368,  65.46366834,  -4.8640219 ,   8.09607209,\n",
      "        55.7317209 ,   9.41582456,  61.83334275,  43.3839532 ,\n",
      "        42.79186531,  32.78361335, 149.68041012, 109.27425031,\n",
      "        25.89407055,  94.10193173,  29.1604104 ,  62.45920462,\n",
      "       -19.259528  , -30.02332054])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 54.25897839, 115.20075505, 104.4155928 ,  81.90616437,\n",
      "        66.43128026,   3.62500397,  16.29659574,  42.40312683,\n",
      "       125.84206218, 124.86800076,  10.80211719,  69.67090736,\n",
      "        73.00015151,  49.1362523 , 106.98454433, 155.1638923 ,\n",
      "       163.89514009,  76.87066266,  66.65283723, 105.2879106 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([-13.39040036, -11.09627168,  49.64498689,  17.47115024,\n",
      "       -28.80375993, 101.72731666,  77.58717368,  65.46366834,\n",
      "        -4.8640219 ,   8.09607209,  55.7317209 ,   9.41582456,\n",
      "        61.83334275,  43.3839532 ,  42.79186531,  32.78361335,\n",
      "       149.68041012, 109.27425031,  25.89407055,  94.10193173,\n",
      "        29.1604104 ,  62.45920462, -19.259528  , -30.02332054,\n",
      "        54.25897839, 115.20075505, 104.4155928 ,  81.90616437,\n",
      "        66.43128026,   3.62500397])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 16.29659574,  42.40312683, 125.84206218, 124.86800076,\n",
      "        10.80211719,  69.67090736,  73.00015151,  49.1362523 ,\n",
      "       106.98454433, 155.1638923 , 163.89514009,  76.87066266,\n",
      "        66.65283723, 105.2879106 , 152.28966719,  96.70344248,\n",
      "        83.77352599,  41.23114256,  17.99426911, 114.11325318])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 62.45920462, -19.259528  , -30.02332054,  54.25897839,\n",
      "       115.20075505, 104.4155928 ,  81.90616437,  66.43128026,\n",
      "         3.62500397,  16.29659574,  42.40312683, 125.84206218,\n",
      "       124.86800076,  10.80211719,  69.67090736,  73.00015151,\n",
      "        49.1362523 , 106.98454433, 155.1638923 , 163.89514009,\n",
      "        76.87066266,  66.65283723, 105.2879106 , 152.28966719,\n",
      "        96.70344248,  83.77352599,  41.23114256,  17.99426911,\n",
      "       114.11325318, 184.04522052])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([131.63722049, 158.90144505, 147.21045867,  86.83575307,\n",
      "       115.00959466, 192.5315456 , 142.60208615, 193.93866008,\n",
      "        16.93172331, 103.93682066, 129.66886997, 103.55737712,\n",
      "       115.40403437,  19.93344086,  65.42368531, 126.55694824,\n",
      "       199.66504437, 128.07629968,  76.25036135,  81.087672  ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([-28.80375993, 101.72731666,  77.58717368,  65.46366834,\n",
      "        -4.8640219 ,   8.09607209,  55.7317209 ,   9.41582456,\n",
      "        61.83334275,  43.3839532 ,  42.79186531,  32.78361335,\n",
      "       149.68041012, 109.27425031,  25.89407055,  94.10193173,\n",
      "        29.1604104 ,  62.45920462, -19.259528  , -30.02332054,\n",
      "        54.25897839, 115.20075505, 104.4155928 ,  81.90616437,\n",
      "        66.43128026,   3.62500397,  16.29659574,  42.40312683,\n",
      "       125.84206218, 124.86800076])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 10.80211719,  69.67090736,  73.00015151,  49.1362523 ,\n",
      "       106.98454433, 155.1638923 , 163.89514009,  76.87066266,\n",
      "        66.65283723, 105.2879106 , 152.28966719,  96.70344248,\n",
      "        83.77352599,  41.23114256,  17.99426911, 114.11325318,\n",
      "       184.04522052, 131.63722049, 158.90144505, 147.21045867])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([143.35571794, 181.43706483, 117.23990033, 103.79331492,\n",
      "       104.97248027,  51.30083837, 116.82936428, 150.26046624,\n",
      "       142.49521639, 127.19622084,  63.7869869 ,  88.21464394,\n",
      "       110.41550736,  92.38406199, 116.29604032, 157.91140713,\n",
      "       246.51495725, 196.23187143, 170.08294304, 152.27811061,\n",
      "        53.64930572, 108.24262341, 147.88143388, 274.83401078,\n",
      "       196.4136086 , 173.2665378 , 161.73243664,  99.10058533,\n",
      "       189.91445947, 217.03019629])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([218.01662357, 135.62151463, 215.66343029, 121.17746619,\n",
      "       167.28600356, 283.28864689, 164.69125998, 125.10364505,\n",
      "       159.87748372, 144.34205834,  82.29692285, 140.1662113 ,\n",
      "       115.85726673, 173.41590414, 135.43190224, 233.88614205,\n",
      "       168.22234072, 149.37417651, 211.20263936, 134.80757158])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 61.83334275,  43.3839532 ,  42.79186531,  32.78361335,\n",
      "       149.68041012, 109.27425031,  25.89407055,  94.10193173,\n",
      "        29.1604104 ,  62.45920462, -19.259528  , -30.02332054,\n",
      "        54.25897839, 115.20075505, 104.4155928 ,  81.90616437,\n",
      "        66.43128026,   3.62500397,  16.29659574,  42.40312683,\n",
      "       125.84206218, 124.86800076,  10.80211719,  69.67090736,\n",
      "        73.00015151,  49.1362523 , 106.98454433, 155.1638923 ,\n",
      "       163.89514009,  76.87066266])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 66.65283723, 105.2879106 , 152.28966719,  96.70344248,\n",
      "        83.77352599,  41.23114256,  17.99426911, 114.11325318,\n",
      "       184.04522052, 131.63722049, 158.90144505, 147.21045867,\n",
      "        86.83575307, 115.00959466, 192.5315456 , 142.60208615,\n",
      "       193.93866008,  16.93172331, 103.93682066, 129.66886997])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([110.41550736,  92.38406199, 116.29604032, 157.91140713,\n",
      "       246.51495725, 196.23187143, 170.08294304, 152.27811061,\n",
      "        53.64930572, 108.24262341, 147.88143388, 274.83401078,\n",
      "       196.4136086 , 173.2665378 , 161.73243664,  99.10058533,\n",
      "       189.91445947, 217.03019629, 218.01662357, 135.62151463,\n",
      "       215.66343029, 121.17746619, 167.28600356, 283.28864689,\n",
      "       164.69125998, 125.10364505, 159.87748372, 144.34205834,\n",
      "        82.29692285, 140.1662113 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([115.85726673, 173.41590414, 135.43190224, 233.88614205,\n",
      "       168.22234072, 149.37417651, 211.20263936, 134.80757158,\n",
      "       168.40259944, 248.81801436, 131.08170877, 163.32682593,\n",
      "       198.28054145, 230.55995397, 143.44437565,  99.18040759,\n",
      "       184.29382082, 211.78528286, 211.13288375, 216.64507465])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 65.46366834,  -4.8640219 ,   8.09607209,  55.7317209 ,\n",
      "         9.41582456,  61.83334275,  43.3839532 ,  42.79186531,\n",
      "        32.78361335, 149.68041012, 109.27425031,  25.89407055,\n",
      "        94.10193173,  29.1604104 ,  62.45920462, -19.259528  ,\n",
      "       -30.02332054,  54.25897839, 115.20075505, 104.4155928 ,\n",
      "        81.90616437,  66.43128026,   3.62500397,  16.29659574,\n",
      "        42.40312683, 125.84206218, 124.86800076,  10.80211719,\n",
      "        69.67090736,  73.00015151])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 49.1362523 , 106.98454433, 155.1638923 , 163.89514009,\n",
      "        76.87066266,  66.65283723, 105.2879106 , 152.28966719,\n",
      "        96.70344248,  83.77352599,  41.23114256,  17.99426911,\n",
      "       114.11325318, 184.04522052, 131.63722049, 158.90144505,\n",
      "       147.21045867,  86.83575307, 115.00959466, 192.5315456 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([159.44456572, 109.77291327, 143.66763433, 143.35571794,\n",
      "       181.43706483, 117.23990033, 103.79331492, 104.97248027,\n",
      "        51.30083837, 116.82936428, 150.26046624, 142.49521639,\n",
      "       127.19622084,  63.7869869 ,  88.21464394, 110.41550736,\n",
      "        92.38406199, 116.29604032, 157.91140713, 246.51495725,\n",
      "       196.23187143, 170.08294304, 152.27811061,  53.64930572,\n",
      "       108.24262341, 147.88143388, 274.83401078, 196.4136086 ,\n",
      "       173.2665378 , 161.73243664])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 99.10058533, 189.91445947, 217.03019629, 218.01662357,\n",
      "       135.62151463, 215.66343029, 121.17746619, 167.28600356,\n",
      "       283.28864689, 164.69125998, 125.10364505, 159.87748372,\n",
      "       144.34205834,  82.29692285, 140.1662113 , 115.85726673,\n",
      "       173.41590414, 135.43190224, 233.88614205, 168.22234072])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([143.66763433, 143.35571794, 181.43706483, 117.23990033,\n",
      "       103.79331492, 104.97248027,  51.30083837, 116.82936428,\n",
      "       150.26046624, 142.49521639, 127.19622084,  63.7869869 ,\n",
      "        88.21464394, 110.41550736,  92.38406199, 116.29604032,\n",
      "       157.91140713, 246.51495725, 196.23187143, 170.08294304,\n",
      "       152.27811061,  53.64930572, 108.24262341, 147.88143388,\n",
      "       274.83401078, 196.4136086 , 173.2665378 , 161.73243664,\n",
      "        99.10058533, 189.91445947])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([217.03019629, 218.01662357, 135.62151463, 215.66343029,\n",
      "       121.17746619, 167.28600356, 283.28864689, 164.69125998,\n",
      "       125.10364505, 159.87748372, 144.34205834,  82.29692285,\n",
      "       140.1662113 , 115.85726673, 173.41590414, 135.43190224,\n",
      "       233.88614205, 168.22234072, 149.37417651, 211.20263936])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 32.78361335, 149.68041012, 109.27425031,  25.89407055,\n",
      "        94.10193173,  29.1604104 ,  62.45920462, -19.259528  ,\n",
      "       -30.02332054,  54.25897839, 115.20075505, 104.4155928 ,\n",
      "        81.90616437,  66.43128026,   3.62500397,  16.29659574,\n",
      "        42.40312683, 125.84206218, 124.86800076,  10.80211719,\n",
      "        69.67090736,  73.00015151,  49.1362523 , 106.98454433,\n",
      "       155.1638923 , 163.89514009,  76.87066266,  66.65283723,\n",
      "       105.2879106 , 152.28966719])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 96.70344248,  83.77352599,  41.23114256,  17.99426911,\n",
      "       114.11325318, 184.04522052, 131.63722049, 158.90144505,\n",
      "       147.21045867,  86.83575307, 115.00959466, 192.5315456 ,\n",
      "       142.60208615, 193.93866008,  16.93172331, 103.93682066,\n",
      "       129.66886997, 103.55737712, 115.40403437,  19.93344086])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([184.04522052, 131.63722049, 158.90144505, 147.21045867,\n",
      "        86.83575307, 115.00959466, 192.5315456 , 142.60208615,\n",
      "       193.93866008,  16.93172331, 103.93682066, 129.66886997,\n",
      "       103.55737712, 115.40403437,  19.93344086,  65.42368531,\n",
      "       126.55694824, 199.66504437, 128.07629968,  76.25036135,\n",
      "        81.087672  , 156.95619511, 159.44456572, 109.77291327,\n",
      "       143.66763433, 143.35571794, 181.43706483, 117.23990033,\n",
      "       103.79331492, 104.97248027])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 51.30083837, 116.82936428, 150.26046624, 142.49521639,\n",
      "       127.19622084,  63.7869869 ,  88.21464394, 110.41550736,\n",
      "        92.38406199, 116.29604032, 157.91140713, 246.51495725,\n",
      "       196.23187143, 170.08294304, 152.27811061,  53.64930572,\n",
      "       108.24262341, 147.88143388, 274.83401078, 196.4136086 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([115.20075505, 104.4155928 ,  81.90616437,  66.43128026,\n",
      "         3.62500397,  16.29659574,  42.40312683, 125.84206218,\n",
      "       124.86800076,  10.80211719,  69.67090736,  73.00015151,\n",
      "        49.1362523 , 106.98454433, 155.1638923 , 163.89514009,\n",
      "        76.87066266,  66.65283723, 105.2879106 , 152.28966719,\n",
      "        96.70344248,  83.77352599,  41.23114256,  17.99426911,\n",
      "       114.11325318, 184.04522052, 131.63722049, 158.90144505,\n",
      "       147.21045867,  86.83575307])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([115.00959466, 192.5315456 , 142.60208615, 193.93866008,\n",
      "        16.93172331, 103.93682066, 129.66886997, 103.55737712,\n",
      "       115.40403437,  19.93344086,  65.42368531, 126.55694824,\n",
      "       199.66504437, 128.07629968,  76.25036135,  81.087672  ,\n",
      "       156.95619511, 159.44456572, 109.77291327, 143.66763433])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([116.29604032, 157.91140713, 246.51495725, 196.23187143,\n",
      "       170.08294304, 152.27811061,  53.64930572, 108.24262341,\n",
      "       147.88143388, 274.83401078, 196.4136086 , 173.2665378 ,\n",
      "       161.73243664,  99.10058533, 189.91445947, 217.03019629,\n",
      "       218.01662357, 135.62151463, 215.66343029, 121.17746619,\n",
      "       167.28600356, 283.28864689, 164.69125998, 125.10364505,\n",
      "       159.87748372, 144.34205834,  82.29692285, 140.1662113 ,\n",
      "       115.85726673, 173.41590414])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([135.43190224, 233.88614205, 168.22234072, 149.37417651,\n",
      "       211.20263936, 134.80757158, 168.40259944, 248.81801436,\n",
      "       131.08170877, 163.32682593, 198.28054145, 230.55995397,\n",
      "       143.44437565,  99.18040759, 184.29382082, 211.78528286,\n",
      "       211.13288375, 216.64507465, 168.69752245, 193.57349357])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 70.871907  ,  37.47411725, 123.86372229, 121.85337381,\n",
      "        49.22925705,  72.84388783,  41.03382835,  22.4299133 ,\n",
      "        55.83187576, -36.14098885, -69.86728076, -13.39040036,\n",
      "       -11.09627168,  49.64498689,  17.47115024, -28.80375993,\n",
      "       101.72731666,  77.58717368,  65.46366834,  -4.8640219 ,\n",
      "         8.09607209,  55.7317209 ,   9.41582456,  61.83334275,\n",
      "        43.3839532 ,  42.79186531,  32.78361335, 149.68041012,\n",
      "       109.27425031,  25.89407055])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 94.10193173,  29.1604104 ,  62.45920462, -19.259528  ,\n",
      "       -30.02332054,  54.25897839, 115.20075505, 104.4155928 ,\n",
      "        81.90616437,  66.43128026,   3.62500397,  16.29659574,\n",
      "        42.40312683, 125.84206218, 124.86800076,  10.80211719,\n",
      "        69.67090736,  73.00015151,  49.1362523 , 106.98454433])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([192.5315456 , 142.60208615, 193.93866008,  16.93172331,\n",
      "       103.93682066, 129.66886997, 103.55737712, 115.40403437,\n",
      "        19.93344086,  65.42368531, 126.55694824, 199.66504437,\n",
      "       128.07629968,  76.25036135,  81.087672  , 156.95619511,\n",
      "       159.44456572, 109.77291327, 143.66763433, 143.35571794,\n",
      "       181.43706483, 117.23990033, 103.79331492, 104.97248027,\n",
      "        51.30083837, 116.82936428, 150.26046624, 142.49521639,\n",
      "       127.19622084,  63.7869869 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 88.21464394, 110.41550736,  92.38406199, 116.29604032,\n",
      "       157.91140713, 246.51495725, 196.23187143, 170.08294304,\n",
      "       152.27811061,  53.64930572, 108.24262341, 147.88143388,\n",
      "       274.83401078, 196.4136086 , 173.2665378 , 161.73243664,\n",
      "        99.10058533, 189.91445947, 217.03019629, 218.01662357])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 76.87066266,  66.65283723, 105.2879106 , 152.28966719,\n",
      "        96.70344248,  83.77352599,  41.23114256,  17.99426911,\n",
      "       114.11325318, 184.04522052, 131.63722049, 158.90144505,\n",
      "       147.21045867,  86.83575307, 115.00959466, 192.5315456 ,\n",
      "       142.60208615, 193.93866008,  16.93172331, 103.93682066,\n",
      "       129.66886997, 103.55737712, 115.40403437,  19.93344086,\n",
      "        65.42368531, 126.55694824, 199.66504437, 128.07629968,\n",
      "        76.25036135,  81.087672  ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([156.95619511, 159.44456572, 109.77291327, 143.66763433,\n",
      "       143.35571794, 181.43706483, 117.23990033, 103.79331492,\n",
      "       104.97248027,  51.30083837, 116.82936428, 150.26046624,\n",
      "       142.49521639, 127.19622084,  63.7869869 ,  88.21464394,\n",
      "       110.41550736,  92.38406199, 116.29604032, 157.91140713])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 17.47115024, -28.80375993, 101.72731666,  77.58717368,\n",
      "        65.46366834,  -4.8640219 ,   8.09607209,  55.7317209 ,\n",
      "         9.41582456,  61.83334275,  43.3839532 ,  42.79186531,\n",
      "        32.78361335, 149.68041012, 109.27425031,  25.89407055,\n",
      "        94.10193173,  29.1604104 ,  62.45920462, -19.259528  ,\n",
      "       -30.02332054,  54.25897839, 115.20075505, 104.4155928 ,\n",
      "        81.90616437,  66.43128026,   3.62500397,  16.29659574,\n",
      "        42.40312683, 125.84206218])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([124.86800076,  10.80211719,  69.67090736,  73.00015151,\n",
      "        49.1362523 , 106.98454433, 155.1638923 , 163.89514009,\n",
      "        76.87066266,  66.65283723, 105.2879106 , 152.28966719,\n",
      "        96.70344248,  83.77352599,  41.23114256,  17.99426911,\n",
      "       114.11325318, 184.04522052, 131.63722049, 158.90144505])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 37.47411725, 123.86372229, 121.85337381,  49.22925705,\n",
      "        72.84388783,  41.03382835,  22.4299133 ,  55.83187576,\n",
      "       -36.14098885, -69.86728076, -13.39040036, -11.09627168,\n",
      "        49.64498689,  17.47115024, -28.80375993, 101.72731666,\n",
      "        77.58717368,  65.46366834,  -4.8640219 ,   8.09607209,\n",
      "        55.7317209 ,   9.41582456,  61.83334275,  43.3839532 ,\n",
      "        42.79186531,  32.78361335, 149.68041012, 109.27425031,\n",
      "        25.89407055,  94.10193173])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 29.1604104 ,  62.45920462, -19.259528  , -30.02332054,\n",
      "        54.25897839, 115.20075505, 104.4155928 ,  81.90616437,\n",
      "        66.43128026,   3.62500397,  16.29659574,  42.40312683,\n",
      "       125.84206218, 124.86800076,  10.80211719,  69.67090736,\n",
      "        73.00015151,  49.1362523 , 106.98454433, 155.1638923 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([104.4155928 ,  81.90616437,  66.43128026,   3.62500397,\n",
      "        16.29659574,  42.40312683, 125.84206218, 124.86800076,\n",
      "        10.80211719,  69.67090736,  73.00015151,  49.1362523 ,\n",
      "       106.98454433, 155.1638923 , 163.89514009,  76.87066266,\n",
      "        66.65283723, 105.2879106 , 152.28966719,  96.70344248,\n",
      "        83.77352599,  41.23114256,  17.99426911, 114.11325318,\n",
      "       184.04522052, 131.63722049, 158.90144505, 147.21045867,\n",
      "        86.83575307, 115.00959466])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([192.5315456 , 142.60208615, 193.93866008,  16.93172331,\n",
      "       103.93682066, 129.66886997, 103.55737712, 115.40403437,\n",
      "        19.93344086,  65.42368531, 126.55694824, 199.66504437,\n",
      "       128.07629968,  76.25036135,  81.087672  , 156.95619511,\n",
      "       159.44456572, 109.77291327, 143.66763433, 143.35571794])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([114.11325318, 184.04522052, 131.63722049, 158.90144505,\n",
      "       147.21045867,  86.83575307, 115.00959466, 192.5315456 ,\n",
      "       142.60208615, 193.93866008,  16.93172331, 103.93682066,\n",
      "       129.66886997, 103.55737712, 115.40403437,  19.93344086,\n",
      "        65.42368531, 126.55694824, 199.66504437, 128.07629968,\n",
      "        76.25036135,  81.087672  , 156.95619511, 159.44456572,\n",
      "       109.77291327, 143.66763433, 143.35571794, 181.43706483,\n",
      "       117.23990033, 103.79331492])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([104.97248027,  51.30083837, 116.82936428, 150.26046624,\n",
      "       142.49521639, 127.19622084,  63.7869869 ,  88.21464394,\n",
      "       110.41550736,  92.38406199, 116.29604032, 157.91140713,\n",
      "       246.51495725, 196.23187143, 170.08294304, 152.27811061,\n",
      "        53.64930572, 108.24262341, 147.88143388, 274.83401078])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([-36.14098885, -69.86728076, -13.39040036, -11.09627168,\n",
      "        49.64498689,  17.47115024, -28.80375993, 101.72731666,\n",
      "        77.58717368,  65.46366834,  -4.8640219 ,   8.09607209,\n",
      "        55.7317209 ,   9.41582456,  61.83334275,  43.3839532 ,\n",
      "        42.79186531,  32.78361335, 149.68041012, 109.27425031,\n",
      "        25.89407055,  94.10193173,  29.1604104 ,  62.45920462,\n",
      "       -19.259528  , -30.02332054,  54.25897839, 115.20075505,\n",
      "       104.4155928 ,  81.90616437])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 66.43128026,   3.62500397,  16.29659574,  42.40312683,\n",
      "       125.84206218, 124.86800076,  10.80211719,  69.67090736,\n",
      "        73.00015151,  49.1362523 , 106.98454433, 155.1638923 ,\n",
      "       163.89514009,  76.87066266,  66.65283723, 105.2879106 ,\n",
      "       152.28966719,  96.70344248,  83.77352599,  41.23114256])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([181.43706483, 117.23990033, 103.79331492, 104.97248027,\n",
      "        51.30083837, 116.82936428, 150.26046624, 142.49521639,\n",
      "       127.19622084,  63.7869869 ,  88.21464394, 110.41550736,\n",
      "        92.38406199, 116.29604032, 157.91140713, 246.51495725,\n",
      "       196.23187143, 170.08294304, 152.27811061,  53.64930572,\n",
      "       108.24262341, 147.88143388, 274.83401078, 196.4136086 ,\n",
      "       173.2665378 , 161.73243664,  99.10058533, 189.91445947,\n",
      "       217.03019629, 218.01662357])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([135.62151463, 215.66343029, 121.17746619, 167.28600356,\n",
      "       283.28864689, 164.69125998, 125.10364505, 159.87748372,\n",
      "       144.34205834,  82.29692285, 140.1662113 , 115.85726673,\n",
      "       173.41590414, 135.43190224, 233.88614205, 168.22234072,\n",
      "       149.37417651, 211.20263936, 134.80757158, 168.40259944])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ -4.8640219 ,   8.09607209,  55.7317209 ,   9.41582456,\n",
      "        61.83334275,  43.3839532 ,  42.79186531,  32.78361335,\n",
      "       149.68041012, 109.27425031,  25.89407055,  94.10193173,\n",
      "        29.1604104 ,  62.45920462, -19.259528  , -30.02332054,\n",
      "        54.25897839, 115.20075505, 104.4155928 ,  81.90616437,\n",
      "        66.43128026,   3.62500397,  16.29659574,  42.40312683,\n",
      "       125.84206218, 124.86800076,  10.80211719,  69.67090736,\n",
      "        73.00015151,  49.1362523 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([106.98454433, 155.1638923 , 163.89514009,  76.87066266,\n",
      "        66.65283723, 105.2879106 , 152.28966719,  96.70344248,\n",
      "        83.77352599,  41.23114256,  17.99426911, 114.11325318,\n",
      "       184.04522052, 131.63722049, 158.90144505, 147.21045867,\n",
      "        86.83575307, 115.00959466, 192.5315456 , 142.60208615])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([103.55737712, 115.40403437,  19.93344086,  65.42368531,\n",
      "       126.55694824, 199.66504437, 128.07629968,  76.25036135,\n",
      "        81.087672  , 156.95619511, 159.44456572, 109.77291327,\n",
      "       143.66763433, 143.35571794, 181.43706483, 117.23990033,\n",
      "       103.79331492, 104.97248027,  51.30083837, 116.82936428,\n",
      "       150.26046624, 142.49521639, 127.19622084,  63.7869869 ,\n",
      "        88.21464394, 110.41550736,  92.38406199, 116.29604032,\n",
      "       157.91140713, 246.51495725])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([196.23187143, 170.08294304, 152.27811061,  53.64930572,\n",
      "       108.24262341, 147.88143388, 274.83401078, 196.4136086 ,\n",
      "       173.2665378 , 161.73243664,  99.10058533, 189.91445947,\n",
      "       217.03019629, 218.01662357, 135.62151463, 215.66343029,\n",
      "       121.17746619, 167.28600356, 283.28864689, 164.69125998])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([142.60208615, 193.93866008,  16.93172331, 103.93682066,\n",
      "       129.66886997, 103.55737712, 115.40403437,  19.93344086,\n",
      "        65.42368531, 126.55694824, 199.66504437, 128.07629968,\n",
      "        76.25036135,  81.087672  , 156.95619511, 159.44456572,\n",
      "       109.77291327, 143.66763433, 143.35571794, 181.43706483,\n",
      "       117.23990033, 103.79331492, 104.97248027,  51.30083837,\n",
      "       116.82936428, 150.26046624, 142.49521639, 127.19622084,\n",
      "        63.7869869 ,  88.21464394])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([110.41550736,  92.38406199, 116.29604032, 157.91140713,\n",
      "       246.51495725, 196.23187143, 170.08294304, 152.27811061,\n",
      "        53.64930572, 108.24262341, 147.88143388, 274.83401078,\n",
      "       196.4136086 , 173.2665378 , 161.73243664,  99.10058533,\n",
      "       189.91445947, 217.03019629, 218.01662357, 135.62151463])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([116.82936428, 150.26046624, 142.49521639, 127.19622084,\n",
      "        63.7869869 ,  88.21464394, 110.41550736,  92.38406199,\n",
      "       116.29604032, 157.91140713, 246.51495725, 196.23187143,\n",
      "       170.08294304, 152.27811061,  53.64930572, 108.24262341,\n",
      "       147.88143388, 274.83401078, 196.4136086 , 173.2665378 ,\n",
      "       161.73243664,  99.10058533, 189.91445947, 217.03019629,\n",
      "       218.01662357, 135.62151463, 215.66343029, 121.17746619,\n",
      "       167.28600356, 283.28864689])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([164.69125998, 125.10364505, 159.87748372, 144.34205834,\n",
      "        82.29692285, 140.1662113 , 115.85726673, 173.41590414,\n",
      "       135.43190224, 233.88614205, 168.22234072, 149.37417651,\n",
      "       211.20263936, 134.80757158, 168.40259944, 248.81801436,\n",
      "       131.08170877, 163.32682593, 198.28054145, 230.55995397])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 16.29659574,  42.40312683, 125.84206218, 124.86800076,\n",
      "        10.80211719,  69.67090736,  73.00015151,  49.1362523 ,\n",
      "       106.98454433, 155.1638923 , 163.89514009,  76.87066266,\n",
      "        66.65283723, 105.2879106 , 152.28966719,  96.70344248,\n",
      "        83.77352599,  41.23114256,  17.99426911, 114.11325318,\n",
      "       184.04522052, 131.63722049, 158.90144505, 147.21045867,\n",
      "        86.83575307, 115.00959466, 192.5315456 , 142.60208615,\n",
      "       193.93866008,  16.93172331])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([103.93682066, 129.66886997, 103.55737712, 115.40403437,\n",
      "        19.93344086,  65.42368531, 126.55694824, 199.66504437,\n",
      "       128.07629968,  76.25036135,  81.087672  , 156.95619511,\n",
      "       159.44456572, 109.77291327, 143.66763433, 143.35571794,\n",
      "       181.43706483, 117.23990033, 103.79331492, 104.97248027])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([121.85337381,  49.22925705,  72.84388783,  41.03382835,\n",
      "        22.4299133 ,  55.83187576, -36.14098885, -69.86728076,\n",
      "       -13.39040036, -11.09627168,  49.64498689,  17.47115024,\n",
      "       -28.80375993, 101.72731666,  77.58717368,  65.46366834,\n",
      "        -4.8640219 ,   8.09607209,  55.7317209 ,   9.41582456,\n",
      "        61.83334275,  43.3839532 ,  42.79186531,  32.78361335,\n",
      "       149.68041012, 109.27425031,  25.89407055,  94.10193173,\n",
      "        29.1604104 ,  62.45920462])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([-19.259528  , -30.02332054,  54.25897839, 115.20075505,\n",
      "       104.4155928 ,  81.90616437,  66.43128026,   3.62500397,\n",
      "        16.29659574,  42.40312683, 125.84206218, 124.86800076,\n",
      "        10.80211719,  69.67090736,  73.00015151,  49.1362523 ,\n",
      "       106.98454433, 155.1638923 , 163.89514009,  76.87066266])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([115.00959466, 192.5315456 , 142.60208615, 193.93866008,\n",
      "        16.93172331, 103.93682066, 129.66886997, 103.55737712,\n",
      "       115.40403437,  19.93344086,  65.42368531, 126.55694824,\n",
      "       199.66504437, 128.07629968,  76.25036135,  81.087672  ,\n",
      "       156.95619511, 159.44456572, 109.77291327, 143.66763433,\n",
      "       143.35571794, 181.43706483, 117.23990033, 103.79331492,\n",
      "       104.97248027,  51.30083837, 116.82936428, 150.26046624,\n",
      "       142.49521639, 127.19622084])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 63.7869869 ,  88.21464394, 110.41550736,  92.38406199,\n",
      "       116.29604032, 157.91140713, 246.51495725, 196.23187143,\n",
      "       170.08294304, 152.27811061,  53.64930572, 108.24262341,\n",
      "       147.88143388, 274.83401078, 196.4136086 , 173.2665378 ,\n",
      "       161.73243664,  99.10058533, 189.91445947, 217.03019629])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([123.86372229, 121.85337381,  49.22925705,  72.84388783,\n",
      "        41.03382835,  22.4299133 ,  55.83187576, -36.14098885,\n",
      "       -69.86728076, -13.39040036, -11.09627168,  49.64498689,\n",
      "        17.47115024, -28.80375993, 101.72731666,  77.58717368,\n",
      "        65.46366834,  -4.8640219 ,   8.09607209,  55.7317209 ,\n",
      "         9.41582456,  61.83334275,  43.3839532 ,  42.79186531,\n",
      "        32.78361335, 149.68041012, 109.27425031,  25.89407055,\n",
      "        94.10193173,  29.1604104 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 62.45920462, -19.259528  , -30.02332054,  54.25897839,\n",
      "       115.20075505, 104.4155928 ,  81.90616437,  66.43128026,\n",
      "         3.62500397,  16.29659574,  42.40312683, 125.84206218,\n",
      "       124.86800076,  10.80211719,  69.67090736,  73.00015151,\n",
      "        49.1362523 , 106.98454433, 155.1638923 , 163.89514009])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([136.71931428,  70.871907  ,  37.47411725, 123.86372229,\n",
      "       121.85337381,  49.22925705,  72.84388783,  41.03382835,\n",
      "        22.4299133 ,  55.83187576, -36.14098885, -69.86728076,\n",
      "       -13.39040036, -11.09627168,  49.64498689,  17.47115024,\n",
      "       -28.80375993, 101.72731666,  77.58717368,  65.46366834,\n",
      "        -4.8640219 ,   8.09607209,  55.7317209 ,   9.41582456,\n",
      "        61.83334275,  43.3839532 ,  42.79186531,  32.78361335,\n",
      "       149.68041012, 109.27425031])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 25.89407055,  94.10193173,  29.1604104 ,  62.45920462,\n",
      "       -19.259528  , -30.02332054,  54.25897839, 115.20075505,\n",
      "       104.4155928 ,  81.90616437,  66.43128026,   3.62500397,\n",
      "        16.29659574,  42.40312683, 125.84206218, 124.86800076,\n",
      "        10.80211719,  69.67090736,  73.00015151,  49.1362523 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([109.77291327, 143.66763433, 143.35571794, 181.43706483,\n",
      "       117.23990033, 103.79331492, 104.97248027,  51.30083837,\n",
      "       116.82936428, 150.26046624, 142.49521639, 127.19622084,\n",
      "        63.7869869 ,  88.21464394, 110.41550736,  92.38406199,\n",
      "       116.29604032, 157.91140713, 246.51495725, 196.23187143,\n",
      "       170.08294304, 152.27811061,  53.64930572, 108.24262341,\n",
      "       147.88143388, 274.83401078, 196.4136086 , 173.2665378 ,\n",
      "       161.73243664,  99.10058533])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([189.91445947, 217.03019629, 218.01662357, 135.62151463,\n",
      "       215.66343029, 121.17746619, 167.28600356, 283.28864689,\n",
      "       164.69125998, 125.10364505, 159.87748372, 144.34205834,\n",
      "        82.29692285, 140.1662113 , 115.85726673, 173.41590414,\n",
      "       135.43190224, 233.88614205, 168.22234072, 149.37417651])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 53.64930572, 108.24262341, 147.88143388, 274.83401078,\n",
      "       196.4136086 , 173.2665378 , 161.73243664,  99.10058533,\n",
      "       189.91445947, 217.03019629, 218.01662357, 135.62151463,\n",
      "       215.66343029, 121.17746619, 167.28600356, 283.28864689,\n",
      "       164.69125998, 125.10364505, 159.87748372, 144.34205834,\n",
      "        82.29692285, 140.1662113 , 115.85726673, 173.41590414,\n",
      "       135.43190224, 233.88614205, 168.22234072, 149.37417651,\n",
      "       211.20263936, 134.80757158])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([168.40259944, 248.81801436, 131.08170877, 163.32682593,\n",
      "       198.28054145, 230.55995397, 143.44437565,  99.18040759,\n",
      "       184.29382082, 211.78528286, 211.13288375, 216.64507465,\n",
      "       168.69752245, 193.57349357, 214.00622395, 168.32951   ,\n",
      "       277.04961628, 260.96146028, 156.82804628, 211.79379382])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 54.25897839, 115.20075505, 104.4155928 ,  81.90616437,\n",
      "        66.43128026,   3.62500397,  16.29659574,  42.40312683,\n",
      "       125.84206218, 124.86800076,  10.80211719,  69.67090736,\n",
      "        73.00015151,  49.1362523 , 106.98454433, 155.1638923 ,\n",
      "       163.89514009,  76.87066266,  66.65283723, 105.2879106 ,\n",
      "       152.28966719,  96.70344248,  83.77352599,  41.23114256,\n",
      "        17.99426911, 114.11325318, 184.04522052, 131.63722049,\n",
      "       158.90144505, 147.21045867])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 86.83575307, 115.00959466, 192.5315456 , 142.60208615,\n",
      "       193.93866008,  16.93172331, 103.93682066, 129.66886997,\n",
      "       103.55737712, 115.40403437,  19.93344086,  65.42368531,\n",
      "       126.55694824, 199.66504437, 128.07629968,  76.25036135,\n",
      "        81.087672  , 156.95619511, 159.44456572, 109.77291327])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([196.23187143, 170.08294304, 152.27811061,  53.64930572,\n",
      "       108.24262341, 147.88143388, 274.83401078, 196.4136086 ,\n",
      "       173.2665378 , 161.73243664,  99.10058533, 189.91445947,\n",
      "       217.03019629, 218.01662357, 135.62151463, 215.66343029,\n",
      "       121.17746619, 167.28600356, 283.28864689, 164.69125998,\n",
      "       125.10364505, 159.87748372, 144.34205834,  82.29692285,\n",
      "       140.1662113 , 115.85726673, 173.41590414, 135.43190224,\n",
      "       233.88614205, 168.22234072])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([149.37417651, 211.20263936, 134.80757158, 168.40259944,\n",
      "       248.81801436, 131.08170877, 163.32682593, 198.28054145,\n",
      "       230.55995397, 143.44437565,  99.18040759, 184.29382082,\n",
      "       211.78528286, 211.13288375, 216.64507465, 168.69752245,\n",
      "       193.57349357, 214.00622395, 168.32951   , 277.04961628])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([103.93682066, 129.66886997, 103.55737712, 115.40403437,\n",
      "        19.93344086,  65.42368531, 126.55694824, 199.66504437,\n",
      "       128.07629968,  76.25036135,  81.087672  , 156.95619511,\n",
      "       159.44456572, 109.77291327, 143.66763433, 143.35571794,\n",
      "       181.43706483, 117.23990033, 103.79331492, 104.97248027,\n",
      "        51.30083837, 116.82936428, 150.26046624, 142.49521639,\n",
      "       127.19622084,  63.7869869 ,  88.21464394, 110.41550736,\n",
      "        92.38406199, 116.29604032])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([157.91140713, 246.51495725, 196.23187143, 170.08294304,\n",
      "       152.27811061,  53.64930572, 108.24262341, 147.88143388,\n",
      "       274.83401078, 196.4136086 , 173.2665378 , 161.73243664,\n",
      "        99.10058533, 189.91445947, 217.03019629, 218.01662357,\n",
      "       135.62151463, 215.66343029, 121.17746619, 167.28600356])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([  9.41582456,  61.83334275,  43.3839532 ,  42.79186531,\n",
      "        32.78361335, 149.68041012, 109.27425031,  25.89407055,\n",
      "        94.10193173,  29.1604104 ,  62.45920462, -19.259528  ,\n",
      "       -30.02332054,  54.25897839, 115.20075505, 104.4155928 ,\n",
      "        81.90616437,  66.43128026,   3.62500397,  16.29659574,\n",
      "        42.40312683, 125.84206218, 124.86800076,  10.80211719,\n",
      "        69.67090736,  73.00015151,  49.1362523 , 106.98454433,\n",
      "       155.1638923 , 163.89514009])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 76.87066266,  66.65283723, 105.2879106 , 152.28966719,\n",
      "        96.70344248,  83.77352599,  41.23114256,  17.99426911,\n",
      "       114.11325318, 184.04522052, 131.63722049, 158.90144505,\n",
      "       147.21045867,  86.83575307, 115.00959466, 192.5315456 ,\n",
      "       142.60208615, 193.93866008,  16.93172331, 103.93682066])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([152.28966719,  96.70344248,  83.77352599,  41.23114256,\n",
      "        17.99426911, 114.11325318, 184.04522052, 131.63722049,\n",
      "       158.90144505, 147.21045867,  86.83575307, 115.00959466,\n",
      "       192.5315456 , 142.60208615, 193.93866008,  16.93172331,\n",
      "       103.93682066, 129.66886997, 103.55737712, 115.40403437,\n",
      "        19.93344086,  65.42368531, 126.55694824, 199.66504437,\n",
      "       128.07629968,  76.25036135,  81.087672  , 156.95619511,\n",
      "       159.44456572, 109.77291327])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([143.66763433, 143.35571794, 181.43706483, 117.23990033,\n",
      "       103.79331492, 104.97248027,  51.30083837, 116.82936428,\n",
      "       150.26046624, 142.49521639, 127.19622084,  63.7869869 ,\n",
      "        88.21464394, 110.41550736,  92.38406199, 116.29604032,\n",
      "       157.91140713, 246.51495725, 196.23187143, 170.08294304])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 42.79186531,  32.78361335, 149.68041012, 109.27425031,\n",
      "        25.89407055,  94.10193173,  29.1604104 ,  62.45920462,\n",
      "       -19.259528  , -30.02332054,  54.25897839, 115.20075505,\n",
      "       104.4155928 ,  81.90616437,  66.43128026,   3.62500397,\n",
      "        16.29659574,  42.40312683, 125.84206218, 124.86800076,\n",
      "        10.80211719,  69.67090736,  73.00015151,  49.1362523 ,\n",
      "       106.98454433, 155.1638923 , 163.89514009,  76.87066266,\n",
      "        66.65283723, 105.2879106 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([152.28966719,  96.70344248,  83.77352599,  41.23114256,\n",
      "        17.99426911, 114.11325318, 184.04522052, 131.63722049,\n",
      "       158.90144505, 147.21045867,  86.83575307, 115.00959466,\n",
      "       192.5315456 , 142.60208615, 193.93866008,  16.93172331,\n",
      "       103.93682066, 129.66886997, 103.55737712, 115.40403437])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([117.23990033, 103.79331492, 104.97248027,  51.30083837,\n",
      "       116.82936428, 150.26046624, 142.49521639, 127.19622084,\n",
      "        63.7869869 ,  88.21464394, 110.41550736,  92.38406199,\n",
      "       116.29604032, 157.91140713, 246.51495725, 196.23187143,\n",
      "       170.08294304, 152.27811061,  53.64930572, 108.24262341,\n",
      "       147.88143388, 274.83401078, 196.4136086 , 173.2665378 ,\n",
      "       161.73243664,  99.10058533, 189.91445947, 217.03019629,\n",
      "       218.01662357, 135.62151463])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([215.66343029, 121.17746619, 167.28600356, 283.28864689,\n",
      "       164.69125998, 125.10364505, 159.87748372, 144.34205834,\n",
      "        82.29692285, 140.1662113 , 115.85726673, 173.41590414,\n",
      "       135.43190224, 233.88614205, 168.22234072, 149.37417651,\n",
      "       211.20263936, 134.80757158, 168.40259944, 248.81801436])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 49.15704788,  77.12603962, 136.71931428,  70.871907  ,\n",
      "        37.47411725, 123.86372229, 121.85337381,  49.22925705,\n",
      "        72.84388783,  41.03382835,  22.4299133 ,  55.83187576,\n",
      "       -36.14098885, -69.86728076, -13.39040036, -11.09627168,\n",
      "        49.64498689,  17.47115024, -28.80375993, 101.72731666,\n",
      "        77.58717368,  65.46366834,  -4.8640219 ,   8.09607209,\n",
      "        55.7317209 ,   9.41582456,  61.83334275,  43.3839532 ,\n",
      "        42.79186531,  32.78361335])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([149.68041012, 109.27425031,  25.89407055,  94.10193173,\n",
      "        29.1604104 ,  62.45920462, -19.259528  , -30.02332054,\n",
      "        54.25897839, 115.20075505, 104.4155928 ,  81.90616437,\n",
      "        66.43128026,   3.62500397,  16.29659574,  42.40312683,\n",
      "       125.84206218, 124.86800076,  10.80211719,  69.67090736])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([147.21045867,  86.83575307, 115.00959466, 192.5315456 ,\n",
      "       142.60208615, 193.93866008,  16.93172331, 103.93682066,\n",
      "       129.66886997, 103.55737712, 115.40403437,  19.93344086,\n",
      "        65.42368531, 126.55694824, 199.66504437, 128.07629968,\n",
      "        76.25036135,  81.087672  , 156.95619511, 159.44456572,\n",
      "       109.77291327, 143.66763433, 143.35571794, 181.43706483,\n",
      "       117.23990033, 103.79331492, 104.97248027,  51.30083837,\n",
      "       116.82936428, 150.26046624])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([142.49521639, 127.19622084,  63.7869869 ,  88.21464394,\n",
      "       110.41550736,  92.38406199, 116.29604032, 157.91140713,\n",
      "       246.51495725, 196.23187143, 170.08294304, 152.27811061,\n",
      "        53.64930572, 108.24262341, 147.88143388, 274.83401078,\n",
      "       196.4136086 , 173.2665378 , 161.73243664,  99.10058533])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 83.77352599,  41.23114256,  17.99426911, 114.11325318,\n",
      "       184.04522052, 131.63722049, 158.90144505, 147.21045867,\n",
      "        86.83575307, 115.00959466, 192.5315456 , 142.60208615,\n",
      "       193.93866008,  16.93172331, 103.93682066, 129.66886997,\n",
      "       103.55737712, 115.40403437,  19.93344086,  65.42368531,\n",
      "       126.55694824, 199.66504437, 128.07629968,  76.25036135,\n",
      "        81.087672  , 156.95619511, 159.44456572, 109.77291327,\n",
      "       143.66763433, 143.35571794])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([181.43706483, 117.23990033, 103.79331492, 104.97248027,\n",
      "        51.30083837, 116.82936428, 150.26046624, 142.49521639,\n",
      "       127.19622084,  63.7869869 ,  88.21464394, 110.41550736,\n",
      "        92.38406199, 116.29604032, 157.91140713, 246.51495725,\n",
      "       196.23187143, 170.08294304, 152.27811061,  53.64930572])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 51.30083837, 116.82936428, 150.26046624, 142.49521639,\n",
      "       127.19622084,  63.7869869 ,  88.21464394, 110.41550736,\n",
      "        92.38406199, 116.29604032, 157.91140713, 246.51495725,\n",
      "       196.23187143, 170.08294304, 152.27811061,  53.64930572,\n",
      "       108.24262341, 147.88143388, 274.83401078, 196.4136086 ,\n",
      "       173.2665378 , 161.73243664,  99.10058533, 189.91445947,\n",
      "       217.03019629, 218.01662357, 135.62151463, 215.66343029,\n",
      "       121.17746619, 167.28600356])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([283.28864689, 164.69125998, 125.10364505, 159.87748372,\n",
      "       144.34205834,  82.29692285, 140.1662113 , 115.85726673,\n",
      "       173.41590414, 135.43190224, 233.88614205, 168.22234072,\n",
      "       149.37417651, 211.20263936, 134.80757158, 168.40259944,\n",
      "       248.81801436, 131.08170877, 163.32682593, 198.28054145])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([167.28600356, 283.28864689, 164.69125998, 125.10364505,\n",
      "       159.87748372, 144.34205834,  82.29692285, 140.1662113 ,\n",
      "       115.85726673, 173.41590414, 135.43190224, 233.88614205,\n",
      "       168.22234072, 149.37417651, 211.20263936, 134.80757158,\n",
      "       168.40259944, 248.81801436, 131.08170877, 163.32682593,\n",
      "       198.28054145, 230.55995397, 143.44437565,  99.18040759,\n",
      "       184.29382082, 211.78528286, 211.13288375, 216.64507465,\n",
      "       168.69752245, 193.57349357])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([214.00622395, 168.32951   , 277.04961628, 260.96146028,\n",
      "       156.82804628, 211.79379382, 165.25249826, 223.55048858,\n",
      "       276.7108538 , 191.76163782, 241.70231579, 247.8262009 ,\n",
      "       261.74448658, 324.40308386, 242.94621212, 176.65754742,\n",
      "       154.29002952, 153.07560126, 191.31567738, 228.94610511])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([126.55694824, 199.66504437, 128.07629968,  76.25036135,\n",
      "        81.087672  , 156.95619511, 159.44456572, 109.77291327,\n",
      "       143.66763433, 143.35571794, 181.43706483, 117.23990033,\n",
      "       103.79331492, 104.97248027,  51.30083837, 116.82936428,\n",
      "       150.26046624, 142.49521639, 127.19622084,  63.7869869 ,\n",
      "        88.21464394, 110.41550736,  92.38406199, 116.29604032,\n",
      "       157.91140713, 246.51495725, 196.23187143, 170.08294304,\n",
      "       152.27811061,  53.64930572])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([108.24262341, 147.88143388, 274.83401078, 196.4136086 ,\n",
      "       173.2665378 , 161.73243664,  99.10058533, 189.91445947,\n",
      "       217.03019629, 218.01662357, 135.62151463, 215.66343029,\n",
      "       121.17746619, 167.28600356, 283.28864689, 164.69125998,\n",
      "       125.10364505, 159.87748372, 144.34205834,  82.29692285])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([199.66504437, 128.07629968,  76.25036135,  81.087672  ,\n",
      "       156.95619511, 159.44456572, 109.77291327, 143.66763433,\n",
      "       143.35571794, 181.43706483, 117.23990033, 103.79331492,\n",
      "       104.97248027,  51.30083837, 116.82936428, 150.26046624,\n",
      "       142.49521639, 127.19622084,  63.7869869 ,  88.21464394,\n",
      "       110.41550736,  92.38406199, 116.29604032, 157.91140713,\n",
      "       246.51495725, 196.23187143, 170.08294304, 152.27811061,\n",
      "        53.64930572, 108.24262341])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([147.88143388, 274.83401078, 196.4136086 , 173.2665378 ,\n",
      "       161.73243664,  99.10058533, 189.91445947, 217.03019629,\n",
      "       218.01662357, 135.62151463, 215.66343029, 121.17746619,\n",
      "       167.28600356, 283.28864689, 164.69125998, 125.10364505,\n",
      "       159.87748372, 144.34205834,  82.29692285, 140.1662113 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([135.43190224, 233.88614205, 168.22234072, 149.37417651,\n",
      "       211.20263936, 134.80757158, 168.40259944, 248.81801436,\n",
      "       131.08170877, 163.32682593, 198.28054145, 230.55995397,\n",
      "       143.44437565,  99.18040759, 184.29382082, 211.78528286,\n",
      "       211.13288375, 216.64507465, 168.69752245, 193.57349357,\n",
      "       214.00622395, 168.32951   , 277.04961628, 260.96146028,\n",
      "       156.82804628, 211.79379382, 165.25249826, 223.55048858,\n",
      "       276.7108538 , 191.76163782])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([241.70231579, 247.8262009 , 261.74448658, 324.40308386,\n",
      "       242.94621212, 176.65754742, 154.29002952, 153.07560126,\n",
      "       191.31567738, 228.94610511, 238.09168848, 267.12643163,\n",
      "       239.73651743, 296.96554321, 241.19461979, 358.19748099,\n",
      "       314.38420433, 205.25786975, 159.52202019, 227.55789279])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([155.1638923 , 163.89514009,  76.87066266,  66.65283723,\n",
      "       105.2879106 , 152.28966719,  96.70344248,  83.77352599,\n",
      "        41.23114256,  17.99426911, 114.11325318, 184.04522052,\n",
      "       131.63722049, 158.90144505, 147.21045867,  86.83575307,\n",
      "       115.00959466, 192.5315456 , 142.60208615, 193.93866008,\n",
      "        16.93172331, 103.93682066, 129.66886997, 103.55737712,\n",
      "       115.40403437,  19.93344086,  65.42368531, 126.55694824,\n",
      "       199.66504437, 128.07629968])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 76.25036135,  81.087672  , 156.95619511, 159.44456572,\n",
      "       109.77291327, 143.66763433, 143.35571794, 181.43706483,\n",
      "       117.23990033, 103.79331492, 104.97248027,  51.30083837,\n",
      "       116.82936428, 150.26046624, 142.49521639, 127.19622084,\n",
      "        63.7869869 ,  88.21464394, 110.41550736,  92.38406199])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([158.90144505, 147.21045867,  86.83575307, 115.00959466,\n",
      "       192.5315456 , 142.60208615, 193.93866008,  16.93172331,\n",
      "       103.93682066, 129.66886997, 103.55737712, 115.40403437,\n",
      "        19.93344086,  65.42368531, 126.55694824, 199.66504437,\n",
      "       128.07629968,  76.25036135,  81.087672  , 156.95619511,\n",
      "       159.44456572, 109.77291327, 143.66763433, 143.35571794,\n",
      "       181.43706483, 117.23990033, 103.79331492, 104.97248027,\n",
      "        51.30083837, 116.82936428])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([150.26046624, 142.49521639, 127.19622084,  63.7869869 ,\n",
      "        88.21464394, 110.41550736,  92.38406199, 116.29604032,\n",
      "       157.91140713, 246.51495725, 196.23187143, 170.08294304,\n",
      "       152.27811061,  53.64930572, 108.24262341, 147.88143388,\n",
      "       274.83401078, 196.4136086 , 173.2665378 , 161.73243664])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([159.87748372, 144.34205834,  82.29692285, 140.1662113 ,\n",
      "       115.85726673, 173.41590414, 135.43190224, 233.88614205,\n",
      "       168.22234072, 149.37417651, 211.20263936, 134.80757158,\n",
      "       168.40259944, 248.81801436, 131.08170877, 163.32682593,\n",
      "       198.28054145, 230.55995397, 143.44437565,  99.18040759,\n",
      "       184.29382082, 211.78528286, 211.13288375, 216.64507465,\n",
      "       168.69752245, 193.57349357, 214.00622395, 168.32951   ,\n",
      "       277.04961628, 260.96146028])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([156.82804628, 211.79379382, 165.25249826, 223.55048858,\n",
      "       276.7108538 , 191.76163782, 241.70231579, 247.8262009 ,\n",
      "       261.74448658, 324.40308386, 242.94621212, 176.65754742,\n",
      "       154.29002952, 153.07560126, 191.31567738, 228.94610511,\n",
      "       238.09168848, 267.12643163, 239.73651743, 296.96554321])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([121.17746619, 167.28600356, 283.28864689, 164.69125998,\n",
      "       125.10364505, 159.87748372, 144.34205834,  82.29692285,\n",
      "       140.1662113 , 115.85726673, 173.41590414, 135.43190224,\n",
      "       233.88614205, 168.22234072, 149.37417651, 211.20263936,\n",
      "       134.80757158, 168.40259944, 248.81801436, 131.08170877,\n",
      "       163.32682593, 198.28054145, 230.55995397, 143.44437565,\n",
      "        99.18040759, 184.29382082, 211.78528286, 211.13288375,\n",
      "       216.64507465, 168.69752245])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([193.57349357, 214.00622395, 168.32951   , 277.04961628,\n",
      "       260.96146028, 156.82804628, 211.79379382, 165.25249826,\n",
      "       223.55048858, 276.7108538 , 191.76163782, 241.70231579,\n",
      "       247.8262009 , 261.74448658, 324.40308386, 242.94621212,\n",
      "       176.65754742, 154.29002952, 153.07560126, 191.31567738])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 96.70344248,  83.77352599,  41.23114256,  17.99426911,\n",
      "       114.11325318, 184.04522052, 131.63722049, 158.90144505,\n",
      "       147.21045867,  86.83575307, 115.00959466, 192.5315456 ,\n",
      "       142.60208615, 193.93866008,  16.93172331, 103.93682066,\n",
      "       129.66886997, 103.55737712, 115.40403437,  19.93344086,\n",
      "        65.42368531, 126.55694824, 199.66504437, 128.07629968,\n",
      "        76.25036135,  81.087672  , 156.95619511, 159.44456572,\n",
      "       109.77291327, 143.66763433])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([143.35571794, 181.43706483, 117.23990033, 103.79331492,\n",
      "       104.97248027,  51.30083837, 116.82936428, 150.26046624,\n",
      "       142.49521639, 127.19622084,  63.7869869 ,  88.21464394,\n",
      "       110.41550736,  92.38406199, 116.29604032, 157.91140713,\n",
      "       246.51495725, 196.23187143, 170.08294304, 152.27811061])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 81.90616437,  66.43128026,   3.62500397,  16.29659574,\n",
      "        42.40312683, 125.84206218, 124.86800076,  10.80211719,\n",
      "        69.67090736,  73.00015151,  49.1362523 , 106.98454433,\n",
      "       155.1638923 , 163.89514009,  76.87066266,  66.65283723,\n",
      "       105.2879106 , 152.28966719,  96.70344248,  83.77352599,\n",
      "        41.23114256,  17.99426911, 114.11325318, 184.04522052,\n",
      "       131.63722049, 158.90144505, 147.21045867,  86.83575307,\n",
      "       115.00959466, 192.5315456 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([142.60208615, 193.93866008,  16.93172331, 103.93682066,\n",
      "       129.66886997, 103.55737712, 115.40403437,  19.93344086,\n",
      "        65.42368531, 126.55694824, 199.66504437, 128.07629968,\n",
      "        76.25036135,  81.087672  , 156.95619511, 159.44456572,\n",
      "       109.77291327, 143.66763433, 143.35571794, 181.43706483])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 66.65283723, 105.2879106 , 152.28966719,  96.70344248,\n",
      "        83.77352599,  41.23114256,  17.99426911, 114.11325318,\n",
      "       184.04522052, 131.63722049, 158.90144505, 147.21045867,\n",
      "        86.83575307, 115.00959466, 192.5315456 , 142.60208615,\n",
      "       193.93866008,  16.93172331, 103.93682066, 129.66886997,\n",
      "       103.55737712, 115.40403437,  19.93344086,  65.42368531,\n",
      "       126.55694824, 199.66504437, 128.07629968,  76.25036135,\n",
      "        81.087672  , 156.95619511])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([159.44456572, 109.77291327, 143.66763433, 143.35571794,\n",
      "       181.43706483, 117.23990033, 103.79331492, 104.97248027,\n",
      "        51.30083837, 116.82936428, 150.26046624, 142.49521639,\n",
      "       127.19622084,  63.7869869 ,  88.21464394, 110.41550736,\n",
      "        92.38406199, 116.29604032, 157.91140713, 246.51495725])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([168.69752245, 193.57349357, 214.00622395, 168.32951   ,\n",
      "       277.04961628, 260.96146028, 156.82804628, 211.79379382,\n",
      "       165.25249826, 223.55048858, 276.7108538 , 191.76163782,\n",
      "       241.70231579, 247.8262009 , 261.74448658, 324.40308386,\n",
      "       242.94621212, 176.65754742, 154.29002952, 153.07560126,\n",
      "       191.31567738, 228.94610511, 238.09168848, 267.12643163,\n",
      "       239.73651743, 296.96554321, 241.19461979, 358.19748099,\n",
      "       314.38420433, 205.25786975])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([159.52202019, 227.55789279, 224.56358967, 261.6293727 ,\n",
      "       269.64979598, 241.11349397, 191.94529955, 141.51864415,\n",
      "       178.65095344, 264.60280642, 264.23905845, 183.7540556 ,\n",
      "       223.49117695, 260.65355218, 206.39405506, 233.54247196,\n",
      "       248.43310869, 190.08114942, 240.53880393, 280.09782758])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([277.04961628, 260.96146028, 156.82804628, 211.79379382,\n",
      "       165.25249826, 223.55048858, 276.7108538 , 191.76163782,\n",
      "       241.70231579, 247.8262009 , 261.74448658, 324.40308386,\n",
      "       242.94621212, 176.65754742, 154.29002952, 153.07560126,\n",
      "       191.31567738, 228.94610511, 238.09168848, 267.12643163,\n",
      "       239.73651743, 296.96554321, 241.19461979, 358.19748099,\n",
      "       314.38420433, 205.25786975, 159.52202019, 227.55789279,\n",
      "       224.56358967, 261.6293727 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([269.64979598, 241.11349397, 191.94529955, 141.51864415,\n",
      "       178.65095344, 264.60280642, 264.23905845, 183.7540556 ,\n",
      "       223.49117695, 260.65355218, 206.39405506, 233.54247196,\n",
      "       248.43310869, 190.08114942, 240.53880393, 280.09782758,\n",
      "       315.35893007, 327.78137488, 209.29589023, 180.7943296 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([143.44437565,  99.18040759, 184.29382082, 211.78528286,\n",
      "       211.13288375, 216.64507465, 168.69752245, 193.57349357,\n",
      "       214.00622395, 168.32951   , 277.04961628, 260.96146028,\n",
      "       156.82804628, 211.79379382, 165.25249826, 223.55048858,\n",
      "       276.7108538 , 191.76163782, 241.70231579, 247.8262009 ,\n",
      "       261.74448658, 324.40308386, 242.94621212, 176.65754742,\n",
      "       154.29002952, 153.07560126, 191.31567738, 228.94610511,\n",
      "       238.09168848, 267.12643163])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([239.73651743, 296.96554321, 241.19461979, 358.19748099,\n",
      "       314.38420433, 205.25786975, 159.52202019, 227.55789279,\n",
      "       224.56358967, 261.6293727 , 269.64979598, 241.11349397,\n",
      "       191.94529955, 141.51864415, 178.65095344, 264.60280642,\n",
      "       264.23905845, 183.7540556 , 223.49117695, 260.65355218])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([124.86800076,  10.80211719,  69.67090736,  73.00015151,\n",
      "        49.1362523 , 106.98454433, 155.1638923 , 163.89514009,\n",
      "        76.87066266,  66.65283723, 105.2879106 , 152.28966719,\n",
      "        96.70344248,  83.77352599,  41.23114256,  17.99426911,\n",
      "       114.11325318, 184.04522052, 131.63722049, 158.90144505,\n",
      "       147.21045867,  86.83575307, 115.00959466, 192.5315456 ,\n",
      "       142.60208615, 193.93866008,  16.93172331, 103.93682066,\n",
      "       129.66886997, 103.55737712])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([115.40403437,  19.93344086,  65.42368531, 126.55694824,\n",
      "       199.66504437, 128.07629968,  76.25036135,  81.087672  ,\n",
      "       156.95619511, 159.44456572, 109.77291327, 143.66763433,\n",
      "       143.35571794, 181.43706483, 117.23990033, 103.79331492,\n",
      "       104.97248027,  51.30083837, 116.82936428, 150.26046624])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([173.41590414, 135.43190224, 233.88614205, 168.22234072,\n",
      "       149.37417651, 211.20263936, 134.80757158, 168.40259944,\n",
      "       248.81801436, 131.08170877, 163.32682593, 198.28054145,\n",
      "       230.55995397, 143.44437565,  99.18040759, 184.29382082,\n",
      "       211.78528286, 211.13288375, 216.64507465, 168.69752245,\n",
      "       193.57349357, 214.00622395, 168.32951   , 277.04961628,\n",
      "       260.96146028, 156.82804628, 211.79379382, 165.25249826,\n",
      "       223.55048858, 276.7108538 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([191.76163782, 241.70231579, 247.8262009 , 261.74448658,\n",
      "       324.40308386, 242.94621212, 176.65754742, 154.29002952,\n",
      "       153.07560126, 191.31567738, 228.94610511, 238.09168848,\n",
      "       267.12643163, 239.73651743, 296.96554321, 241.19461979,\n",
      "       358.19748099, 314.38420433, 205.25786975, 159.52202019])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([127.19622084,  63.7869869 ,  88.21464394, 110.41550736,\n",
      "        92.38406199, 116.29604032, 157.91140713, 246.51495725,\n",
      "       196.23187143, 170.08294304, 152.27811061,  53.64930572,\n",
      "       108.24262341, 147.88143388, 274.83401078, 196.4136086 ,\n",
      "       173.2665378 , 161.73243664,  99.10058533, 189.91445947,\n",
      "       217.03019629, 218.01662357, 135.62151463, 215.66343029,\n",
      "       121.17746619, 167.28600356, 283.28864689, 164.69125998,\n",
      "       125.10364505, 159.87748372])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([144.34205834,  82.29692285, 140.1662113 , 115.85726673,\n",
      "       173.41590414, 135.43190224, 233.88614205, 168.22234072,\n",
      "       149.37417651, 211.20263936, 134.80757158, 168.40259944,\n",
      "       248.81801436, 131.08170877, 163.32682593, 198.28054145,\n",
      "       230.55995397, 143.44437565,  99.18040759, 184.29382082])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([142.49521639, 127.19622084,  63.7869869 ,  88.21464394,\n",
      "       110.41550736,  92.38406199, 116.29604032, 157.91140713,\n",
      "       246.51495725, 196.23187143, 170.08294304, 152.27811061,\n",
      "        53.64930572, 108.24262341, 147.88143388, 274.83401078,\n",
      "       196.4136086 , 173.2665378 , 161.73243664,  99.10058533,\n",
      "       189.91445947, 217.03019629, 218.01662357, 135.62151463,\n",
      "       215.66343029, 121.17746619, 167.28600356, 283.28864689,\n",
      "       164.69125998, 125.10364505])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([159.87748372, 144.34205834,  82.29692285, 140.1662113 ,\n",
      "       115.85726673, 173.41590414, 135.43190224, 233.88614205,\n",
      "       168.22234072, 149.37417651, 211.20263936, 134.80757158,\n",
      "       168.40259944, 248.81801436, 131.08170877, 163.32682593,\n",
      "       198.28054145, 230.55995397, 143.44437565,  99.18040759])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([216.64507465, 168.69752245, 193.57349357, 214.00622395,\n",
      "       168.32951   , 277.04961628, 260.96146028, 156.82804628,\n",
      "       211.79379382, 165.25249826, 223.55048858, 276.7108538 ,\n",
      "       191.76163782, 241.70231579, 247.8262009 , 261.74448658,\n",
      "       324.40308386, 242.94621212, 176.65754742, 154.29002952,\n",
      "       153.07560126, 191.31567738, 228.94610511, 238.09168848,\n",
      "       267.12643163, 239.73651743, 296.96554321, 241.19461979,\n",
      "       358.19748099, 314.38420433])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([205.25786975, 159.52202019, 227.55789279, 224.56358967,\n",
      "       261.6293727 , 269.64979598, 241.11349397, 191.94529955,\n",
      "       141.51864415, 178.65095344, 264.60280642, 264.23905845,\n",
      "       183.7540556 , 223.49117695, 260.65355218, 206.39405506,\n",
      "       233.54247196, 248.43310869, 190.08114942, 240.53880393])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([246.51495725, 196.23187143, 170.08294304, 152.27811061,\n",
      "        53.64930572, 108.24262341, 147.88143388, 274.83401078,\n",
      "       196.4136086 , 173.2665378 , 161.73243664,  99.10058533,\n",
      "       189.91445947, 217.03019629, 218.01662357, 135.62151463,\n",
      "       215.66343029, 121.17746619, 167.28600356, 283.28864689,\n",
      "       164.69125998, 125.10364505, 159.87748372, 144.34205834,\n",
      "        82.29692285, 140.1662113 , 115.85726673, 173.41590414,\n",
      "       135.43190224, 233.88614205])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([168.22234072, 149.37417651, 211.20263936, 134.80757158,\n",
      "       168.40259944, 248.81801436, 131.08170877, 163.32682593,\n",
      "       198.28054145, 230.55995397, 143.44437565,  99.18040759,\n",
      "       184.29382082, 211.78528286, 211.13288375, 216.64507465,\n",
      "       168.69752245, 193.57349357, 214.00622395, 168.32951   ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([211.79379382, 165.25249826, 223.55048858, 276.7108538 ,\n",
      "       191.76163782, 241.70231579, 247.8262009 , 261.74448658,\n",
      "       324.40308386, 242.94621212, 176.65754742, 154.29002952,\n",
      "       153.07560126, 191.31567738, 228.94610511, 238.09168848,\n",
      "       267.12643163, 239.73651743, 296.96554321, 241.19461979,\n",
      "       358.19748099, 314.38420433, 205.25786975, 159.52202019,\n",
      "       227.55789279, 224.56358967, 261.6293727 , 269.64979598,\n",
      "       241.11349397, 191.94529955])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([141.51864415, 178.65095344, 264.60280642, 264.23905845,\n",
      "       183.7540556 , 223.49117695, 260.65355218, 206.39405506,\n",
      "       233.54247196, 248.43310869, 190.08114942, 240.53880393,\n",
      "       280.09782758, 315.35893007, 327.78137488, 209.29589023,\n",
      "       180.7943296 , 256.05134529, 287.62799663, 293.2503612 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([276.7108538 , 191.76163782, 241.70231579, 247.8262009 ,\n",
      "       261.74448658, 324.40308386, 242.94621212, 176.65754742,\n",
      "       154.29002952, 153.07560126, 191.31567738, 228.94610511,\n",
      "       238.09168848, 267.12643163, 239.73651743, 296.96554321,\n",
      "       241.19461979, 358.19748099, 314.38420433, 205.25786975,\n",
      "       159.52202019, 227.55789279, 224.56358967, 261.6293727 ,\n",
      "       269.64979598, 241.11349397, 191.94529955, 141.51864415,\n",
      "       178.65095344, 264.60280642])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([264.23905845, 183.7540556 , 223.49117695, 260.65355218,\n",
      "       206.39405506, 233.54247196, 248.43310869, 190.08114942,\n",
      "       240.53880393, 280.09782758, 315.35893007, 327.78137488,\n",
      "       209.29589023, 180.7943296 , 256.05134529, 287.62799663,\n",
      "       293.2503612 , 461.66825185, 371.01366887, 340.76572611])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([163.89514009,  76.87066266,  66.65283723, 105.2879106 ,\n",
      "       152.28966719,  96.70344248,  83.77352599,  41.23114256,\n",
      "        17.99426911, 114.11325318, 184.04522052, 131.63722049,\n",
      "       158.90144505, 147.21045867,  86.83575307, 115.00959466,\n",
      "       192.5315456 , 142.60208615, 193.93866008,  16.93172331,\n",
      "       103.93682066, 129.66886997, 103.55737712, 115.40403437,\n",
      "        19.93344086,  65.42368531, 126.55694824, 199.66504437,\n",
      "       128.07629968,  76.25036135])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 81.087672  , 156.95619511, 159.44456572, 109.77291327,\n",
      "       143.66763433, 143.35571794, 181.43706483, 117.23990033,\n",
      "       103.79331492, 104.97248027,  51.30083837, 116.82936428,\n",
      "       150.26046624, 142.49521639, 127.19622084,  63.7869869 ,\n",
      "        88.21464394, 110.41550736,  92.38406199, 116.29604032])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([105.2879106 , 152.28966719,  96.70344248,  83.77352599,\n",
      "        41.23114256,  17.99426911, 114.11325318, 184.04522052,\n",
      "       131.63722049, 158.90144505, 147.21045867,  86.83575307,\n",
      "       115.00959466, 192.5315456 , 142.60208615, 193.93866008,\n",
      "        16.93172331, 103.93682066, 129.66886997, 103.55737712,\n",
      "       115.40403437,  19.93344086,  65.42368531, 126.55694824,\n",
      "       199.66504437, 128.07629968,  76.25036135,  81.087672  ,\n",
      "       156.95619511, 159.44456572])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([109.77291327, 143.66763433, 143.35571794, 181.43706483,\n",
      "       117.23990033, 103.79331492, 104.97248027,  51.30083837,\n",
      "       116.82936428, 150.26046624, 142.49521639, 127.19622084,\n",
      "        63.7869869 ,  88.21464394, 110.41550736,  92.38406199,\n",
      "       116.29604032, 157.91140713, 246.51495725, 196.23187143])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([193.93866008,  16.93172331, 103.93682066, 129.66886997,\n",
      "       103.55737712, 115.40403437,  19.93344086,  65.42368531,\n",
      "       126.55694824, 199.66504437, 128.07629968,  76.25036135,\n",
      "        81.087672  , 156.95619511, 159.44456572, 109.77291327,\n",
      "       143.66763433, 143.35571794, 181.43706483, 117.23990033,\n",
      "       103.79331492, 104.97248027,  51.30083837, 116.82936428,\n",
      "       150.26046624, 142.49521639, 127.19622084,  63.7869869 ,\n",
      "        88.21464394, 110.41550736])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 92.38406199, 116.29604032, 157.91140713, 246.51495725,\n",
      "       196.23187143, 170.08294304, 152.27811061,  53.64930572,\n",
      "       108.24262341, 147.88143388, 274.83401078, 196.4136086 ,\n",
      "       173.2665378 , 161.73243664,  99.10058533, 189.91445947,\n",
      "       217.03019629, 218.01662357, 135.62151463, 215.66343029])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([129.66886997, 103.55737712, 115.40403437,  19.93344086,\n",
      "        65.42368531, 126.55694824, 199.66504437, 128.07629968,\n",
      "        76.25036135,  81.087672  , 156.95619511, 159.44456572,\n",
      "       109.77291327, 143.66763433, 143.35571794, 181.43706483,\n",
      "       117.23990033, 103.79331492, 104.97248027,  51.30083837,\n",
      "       116.82936428, 150.26046624, 142.49521639, 127.19622084,\n",
      "        63.7869869 ,  88.21464394, 110.41550736,  92.38406199,\n",
      "       116.29604032, 157.91140713])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([246.51495725, 196.23187143, 170.08294304, 152.27811061,\n",
      "        53.64930572, 108.24262341, 147.88143388, 274.83401078,\n",
      "       196.4136086 , 173.2665378 , 161.73243664,  99.10058533,\n",
      "       189.91445947, 217.03019629, 218.01662357, 135.62151463,\n",
      "       215.66343029, 121.17746619, 167.28600356, 283.28864689])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([109.27425031,  25.89407055,  94.10193173,  29.1604104 ,\n",
      "        62.45920462, -19.259528  , -30.02332054,  54.25897839,\n",
      "       115.20075505, 104.4155928 ,  81.90616437,  66.43128026,\n",
      "         3.62500397,  16.29659574,  42.40312683, 125.84206218,\n",
      "       124.86800076,  10.80211719,  69.67090736,  73.00015151,\n",
      "        49.1362523 , 106.98454433, 155.1638923 , 163.89514009,\n",
      "        76.87066266,  66.65283723, 105.2879106 , 152.28966719,\n",
      "        96.70344248,  83.77352599])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 41.23114256,  17.99426911, 114.11325318, 184.04522052,\n",
      "       131.63722049, 158.90144505, 147.21045867,  86.83575307,\n",
      "       115.00959466, 192.5315456 , 142.60208615, 193.93866008,\n",
      "        16.93172331, 103.93682066, 129.66886997, 103.55737712,\n",
      "       115.40403437,  19.93344086,  65.42368531, 126.55694824])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([324.40308386, 242.94621212, 176.65754742, 154.29002952,\n",
      "       153.07560126, 191.31567738, 228.94610511, 238.09168848,\n",
      "       267.12643163, 239.73651743, 296.96554321, 241.19461979,\n",
      "       358.19748099, 314.38420433, 205.25786975, 159.52202019,\n",
      "       227.55789279, 224.56358967, 261.6293727 , 269.64979598,\n",
      "       241.11349397, 191.94529955, 141.51864415, 178.65095344,\n",
      "       264.60280642, 264.23905845, 183.7540556 , 223.49117695,\n",
      "       260.65355218, 206.39405506])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([233.54247196, 248.43310869, 190.08114942, 240.53880393,\n",
      "       280.09782758, 315.35893007, 327.78137488, 209.29589023,\n",
      "       180.7943296 , 256.05134529, 287.62799663, 293.2503612 ,\n",
      "       461.66825185, 371.01366887, 340.76572611, 337.21491073,\n",
      "       319.24262795, 265.1709739 , 298.00957865, 242.49635296])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([248.81801436, 131.08170877, 163.32682593, 198.28054145,\n",
      "       230.55995397, 143.44437565,  99.18040759, 184.29382082,\n",
      "       211.78528286, 211.13288375, 216.64507465, 168.69752245,\n",
      "       193.57349357, 214.00622395, 168.32951   , 277.04961628,\n",
      "       260.96146028, 156.82804628, 211.79379382, 165.25249826,\n",
      "       223.55048858, 276.7108538 , 191.76163782, 241.70231579,\n",
      "       247.8262009 , 261.74448658, 324.40308386, 242.94621212,\n",
      "       176.65754742, 154.29002952])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([153.07560126, 191.31567738, 228.94610511, 238.09168848,\n",
      "       267.12643163, 239.73651743, 296.96554321, 241.19461979,\n",
      "       358.19748099, 314.38420433, 205.25786975, 159.52202019,\n",
      "       227.55789279, 224.56358967, 261.6293727 , 269.64979598,\n",
      "       241.11349397, 191.94529955, 141.51864415, 178.65095344])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 22.4299133 ,  55.83187576, -36.14098885, -69.86728076,\n",
      "       -13.39040036, -11.09627168,  49.64498689,  17.47115024,\n",
      "       -28.80375993, 101.72731666,  77.58717368,  65.46366834,\n",
      "        -4.8640219 ,   8.09607209,  55.7317209 ,   9.41582456,\n",
      "        61.83334275,  43.3839532 ,  42.79186531,  32.78361335,\n",
      "       149.68041012, 109.27425031,  25.89407055,  94.10193173,\n",
      "        29.1604104 ,  62.45920462, -19.259528  , -30.02332054,\n",
      "        54.25897839, 115.20075505])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([104.4155928 ,  81.90616437,  66.43128026,   3.62500397,\n",
      "        16.29659574,  42.40312683, 125.84206218, 124.86800076,\n",
      "        10.80211719,  69.67090736,  73.00015151,  49.1362523 ,\n",
      "       106.98454433, 155.1638923 , 163.89514009,  76.87066266,\n",
      "        66.65283723, 105.2879106 , 152.28966719,  96.70344248])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 41.03382835,  22.4299133 ,  55.83187576, -36.14098885,\n",
      "       -69.86728076, -13.39040036, -11.09627168,  49.64498689,\n",
      "        17.47115024, -28.80375993, 101.72731666,  77.58717368,\n",
      "        65.46366834,  -4.8640219 ,   8.09607209,  55.7317209 ,\n",
      "         9.41582456,  61.83334275,  43.3839532 ,  42.79186531,\n",
      "        32.78361335, 149.68041012, 109.27425031,  25.89407055,\n",
      "        94.10193173,  29.1604104 ,  62.45920462, -19.259528  ,\n",
      "       -30.02332054,  54.25897839])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([115.20075505, 104.4155928 ,  81.90616437,  66.43128026,\n",
      "         3.62500397,  16.29659574,  42.40312683, 125.84206218,\n",
      "       124.86800076,  10.80211719,  69.67090736,  73.00015151,\n",
      "        49.1362523 , 106.98454433, 155.1638923 , 163.89514009,\n",
      "        76.87066266,  66.65283723, 105.2879106 , 152.28966719])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([-19.259528  , -30.02332054,  54.25897839, 115.20075505,\n",
      "       104.4155928 ,  81.90616437,  66.43128026,   3.62500397,\n",
      "        16.29659574,  42.40312683, 125.84206218, 124.86800076,\n",
      "        10.80211719,  69.67090736,  73.00015151,  49.1362523 ,\n",
      "       106.98454433, 155.1638923 , 163.89514009,  76.87066266,\n",
      "        66.65283723, 105.2879106 , 152.28966719,  96.70344248,\n",
      "        83.77352599,  41.23114256,  17.99426911, 114.11325318,\n",
      "       184.04522052, 131.63722049])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([158.90144505, 147.21045867,  86.83575307, 115.00959466,\n",
      "       192.5315456 , 142.60208615, 193.93866008,  16.93172331,\n",
      "       103.93682066, 129.66886997, 103.55737712, 115.40403437,\n",
      "        19.93344086,  65.42368531, 126.55694824, 199.66504437,\n",
      "       128.07629968,  76.25036135,  81.087672  , 156.95619511])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([228.94610511, 238.09168848, 267.12643163, 239.73651743,\n",
      "       296.96554321, 241.19461979, 358.19748099, 314.38420433,\n",
      "       205.25786975, 159.52202019, 227.55789279, 224.56358967,\n",
      "       261.6293727 , 269.64979598, 241.11349397, 191.94529955,\n",
      "       141.51864415, 178.65095344, 264.60280642, 264.23905845,\n",
      "       183.7540556 , 223.49117695, 260.65355218, 206.39405506,\n",
      "       233.54247196, 248.43310869, 190.08114942, 240.53880393,\n",
      "       280.09782758, 315.35893007])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([327.78137488, 209.29589023, 180.7943296 , 256.05134529,\n",
      "       287.62799663, 293.2503612 , 461.66825185, 371.01366887,\n",
      "       340.76572611, 337.21491073, 319.24262795, 265.1709739 ,\n",
      "       298.00957865, 242.49635296, 240.13836305, 236.14417874,\n",
      "       260.83591413, 384.60274617, 226.89460208, 273.27294491])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([176.65754742, 154.29002952, 153.07560126, 191.31567738,\n",
      "       228.94610511, 238.09168848, 267.12643163, 239.73651743,\n",
      "       296.96554321, 241.19461979, 358.19748099, 314.38420433,\n",
      "       205.25786975, 159.52202019, 227.55789279, 224.56358967,\n",
      "       261.6293727 , 269.64979598, 241.11349397, 191.94529955,\n",
      "       141.51864415, 178.65095344, 264.60280642, 264.23905845,\n",
      "       183.7540556 , 223.49117695, 260.65355218, 206.39405506,\n",
      "       233.54247196, 248.43310869])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([190.08114942, 240.53880393, 280.09782758, 315.35893007,\n",
      "       327.78137488, 209.29589023, 180.7943296 , 256.05134529,\n",
      "       287.62799663, 293.2503612 , 461.66825185, 371.01366887,\n",
      "       340.76572611, 337.21491073, 319.24262795, 265.1709739 ,\n",
      "       298.00957865, 242.49635296, 240.13836305, 236.14417874])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 49.1362523 , 106.98454433, 155.1638923 , 163.89514009,\n",
      "        76.87066266,  66.65283723, 105.2879106 , 152.28966719,\n",
      "        96.70344248,  83.77352599,  41.23114256,  17.99426911,\n",
      "       114.11325318, 184.04522052, 131.63722049, 158.90144505,\n",
      "       147.21045867,  86.83575307, 115.00959466, 192.5315456 ,\n",
      "       142.60208615, 193.93866008,  16.93172331, 103.93682066,\n",
      "       129.66886997, 103.55737712, 115.40403437,  19.93344086,\n",
      "        65.42368531, 126.55694824])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([199.66504437, 128.07629968,  76.25036135,  81.087672  ,\n",
      "       156.95619511, 159.44456572, 109.77291327, 143.66763433,\n",
      "       143.35571794, 181.43706483, 117.23990033, 103.79331492,\n",
      "       104.97248027,  51.30083837, 116.82936428, 150.26046624,\n",
      "       142.49521639, 127.19622084,  63.7869869 ,  88.21464394])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([154.29002952, 153.07560126, 191.31567738, 228.94610511,\n",
      "       238.09168848, 267.12643163, 239.73651743, 296.96554321,\n",
      "       241.19461979, 358.19748099, 314.38420433, 205.25786975,\n",
      "       159.52202019, 227.55789279, 224.56358967, 261.6293727 ,\n",
      "       269.64979598, 241.11349397, 191.94529955, 141.51864415,\n",
      "       178.65095344, 264.60280642, 264.23905845, 183.7540556 ,\n",
      "       223.49117695, 260.65355218, 206.39405506, 233.54247196,\n",
      "       248.43310869, 190.08114942])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([240.53880393, 280.09782758, 315.35893007, 327.78137488,\n",
      "       209.29589023, 180.7943296 , 256.05134529, 287.62799663,\n",
      "       293.2503612 , 461.66825185, 371.01366887, 340.76572611,\n",
      "       337.21491073, 319.24262795, 265.1709739 , 298.00957865,\n",
      "       242.49635296, 240.13836305, 236.14417874, 260.83591413])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([147.88143388, 274.83401078, 196.4136086 , 173.2665378 ,\n",
      "       161.73243664,  99.10058533, 189.91445947, 217.03019629,\n",
      "       218.01662357, 135.62151463, 215.66343029, 121.17746619,\n",
      "       167.28600356, 283.28864689, 164.69125998, 125.10364505,\n",
      "       159.87748372, 144.34205834,  82.29692285, 140.1662113 ,\n",
      "       115.85726673, 173.41590414, 135.43190224, 233.88614205,\n",
      "       168.22234072, 149.37417651, 211.20263936, 134.80757158,\n",
      "       168.40259944, 248.81801436])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([131.08170877, 163.32682593, 198.28054145, 230.55995397,\n",
      "       143.44437565,  99.18040759, 184.29382082, 211.78528286,\n",
      "       211.13288375, 216.64507465, 168.69752245, 193.57349357,\n",
      "       214.00622395, 168.32951   , 277.04961628, 260.96146028,\n",
      "       156.82804628, 211.79379382, 165.25249826, 223.55048858])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([269.64979598, 241.11349397, 191.94529955, 141.51864415,\n",
      "       178.65095344, 264.60280642, 264.23905845, 183.7540556 ,\n",
      "       223.49117695, 260.65355218, 206.39405506, 233.54247196,\n",
      "       248.43310869, 190.08114942, 240.53880393, 280.09782758,\n",
      "       315.35893007, 327.78137488, 209.29589023, 180.7943296 ,\n",
      "       256.05134529, 287.62799663, 293.2503612 , 461.66825185,\n",
      "       371.01366887, 340.76572611, 337.21491073, 319.24262795,\n",
      "       265.1709739 , 298.00957865])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([242.49635296, 240.13836305, 236.14417874, 260.83591413,\n",
      "       384.60274617, 226.89460208, 273.27294491, 203.17066281,\n",
      "       217.19312001, 314.43807321, 300.01414871, 227.87519988,\n",
      "       220.79051407, 295.02199764, 255.88545794, 278.47209267,\n",
      "       287.98710364, 252.91959566, 378.32322455, 361.8077385 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([261.74448658, 324.40308386, 242.94621212, 176.65754742,\n",
      "       154.29002952, 153.07560126, 191.31567738, 228.94610511,\n",
      "       238.09168848, 267.12643163, 239.73651743, 296.96554321,\n",
      "       241.19461979, 358.19748099, 314.38420433, 205.25786975,\n",
      "       159.52202019, 227.55789279, 224.56358967, 261.6293727 ,\n",
      "       269.64979598, 241.11349397, 191.94529955, 141.51864415,\n",
      "       178.65095344, 264.60280642, 264.23905845, 183.7540556 ,\n",
      "       223.49117695, 260.65355218])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([206.39405506, 233.54247196, 248.43310869, 190.08114942,\n",
      "       240.53880393, 280.09782758, 315.35893007, 327.78137488,\n",
      "       209.29589023, 180.7943296 , 256.05134529, 287.62799663,\n",
      "       293.2503612 , 461.66825185, 371.01366887, 340.76572611,\n",
      "       337.21491073, 319.24262795, 265.1709739 , 298.00957865])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 94.10193173,  29.1604104 ,  62.45920462, -19.259528  ,\n",
      "       -30.02332054,  54.25897839, 115.20075505, 104.4155928 ,\n",
      "        81.90616437,  66.43128026,   3.62500397,  16.29659574,\n",
      "        42.40312683, 125.84206218, 124.86800076,  10.80211719,\n",
      "        69.67090736,  73.00015151,  49.1362523 , 106.98454433,\n",
      "       155.1638923 , 163.89514009,  76.87066266,  66.65283723,\n",
      "       105.2879106 , 152.28966719,  96.70344248,  83.77352599,\n",
      "        41.23114256,  17.99426911])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([114.11325318, 184.04522052, 131.63722049, 158.90144505,\n",
      "       147.21045867,  86.83575307, 115.00959466, 192.5315456 ,\n",
      "       142.60208615, 193.93866008,  16.93172331, 103.93682066,\n",
      "       129.66886997, 103.55737712, 115.40403437,  19.93344086,\n",
      "        65.42368531, 126.55694824, 199.66504437, 128.07629968])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([125.10364505, 159.87748372, 144.34205834,  82.29692285,\n",
      "       140.1662113 , 115.85726673, 173.41590414, 135.43190224,\n",
      "       233.88614205, 168.22234072, 149.37417651, 211.20263936,\n",
      "       134.80757158, 168.40259944, 248.81801436, 131.08170877,\n",
      "       163.32682593, 198.28054145, 230.55995397, 143.44437565,\n",
      "        99.18040759, 184.29382082, 211.78528286, 211.13288375,\n",
      "       216.64507465, 168.69752245, 193.57349357, 214.00622395,\n",
      "       168.32951   , 277.04961628])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([260.96146028, 156.82804628, 211.79379382, 165.25249826,\n",
      "       223.55048858, 276.7108538 , 191.76163782, 241.70231579,\n",
      "       247.8262009 , 261.74448658, 324.40308386, 242.94621212,\n",
      "       176.65754742, 154.29002952, 153.07560126, 191.31567738,\n",
      "       228.94610511, 238.09168848, 267.12643163, 239.73651743])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 77.12603962, 136.71931428,  70.871907  ,  37.47411725,\n",
      "       123.86372229, 121.85337381,  49.22925705,  72.84388783,\n",
      "        41.03382835,  22.4299133 ,  55.83187576, -36.14098885,\n",
      "       -69.86728076, -13.39040036, -11.09627168,  49.64498689,\n",
      "        17.47115024, -28.80375993, 101.72731666,  77.58717368,\n",
      "        65.46366834,  -4.8640219 ,   8.09607209,  55.7317209 ,\n",
      "         9.41582456,  61.83334275,  43.3839532 ,  42.79186531,\n",
      "        32.78361335, 149.68041012])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([109.27425031,  25.89407055,  94.10193173,  29.1604104 ,\n",
      "        62.45920462, -19.259528  , -30.02332054,  54.25897839,\n",
      "       115.20075505, 104.4155928 ,  81.90616437,  66.43128026,\n",
      "         3.62500397,  16.29659574,  42.40312683, 125.84206218,\n",
      "       124.86800076,  10.80211719,  69.67090736,  73.00015151])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 49.22925705,  72.84388783,  41.03382835,  22.4299133 ,\n",
      "        55.83187576, -36.14098885, -69.86728076, -13.39040036,\n",
      "       -11.09627168,  49.64498689,  17.47115024, -28.80375993,\n",
      "       101.72731666,  77.58717368,  65.46366834,  -4.8640219 ,\n",
      "         8.09607209,  55.7317209 ,   9.41582456,  61.83334275,\n",
      "        43.3839532 ,  42.79186531,  32.78361335, 149.68041012,\n",
      "       109.27425031,  25.89407055,  94.10193173,  29.1604104 ,\n",
      "        62.45920462, -19.259528  ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([-30.02332054,  54.25897839, 115.20075505, 104.4155928 ,\n",
      "        81.90616437,  66.43128026,   3.62500397,  16.29659574,\n",
      "        42.40312683, 125.84206218, 124.86800076,  10.80211719,\n",
      "        69.67090736,  73.00015151,  49.1362523 , 106.98454433,\n",
      "       155.1638923 , 163.89514009,  76.87066266,  66.65283723])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 10.80211719,  69.67090736,  73.00015151,  49.1362523 ,\n",
      "       106.98454433, 155.1638923 , 163.89514009,  76.87066266,\n",
      "        66.65283723, 105.2879106 , 152.28966719,  96.70344248,\n",
      "        83.77352599,  41.23114256,  17.99426911, 114.11325318,\n",
      "       184.04522052, 131.63722049, 158.90144505, 147.21045867,\n",
      "        86.83575307, 115.00959466, 192.5315456 , 142.60208615,\n",
      "       193.93866008,  16.93172331, 103.93682066, 129.66886997,\n",
      "       103.55737712, 115.40403437])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 19.93344086,  65.42368531, 126.55694824, 199.66504437,\n",
      "       128.07629968,  76.25036135,  81.087672  , 156.95619511,\n",
      "       159.44456572, 109.77291327, 143.66763433, 143.35571794,\n",
      "       181.43706483, 117.23990033, 103.79331492, 104.97248027,\n",
      "        51.30083837, 116.82936428, 150.26046624, 142.49521639])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 16.93172331, 103.93682066, 129.66886997, 103.55737712,\n",
      "       115.40403437,  19.93344086,  65.42368531, 126.55694824,\n",
      "       199.66504437, 128.07629968,  76.25036135,  81.087672  ,\n",
      "       156.95619511, 159.44456572, 109.77291327, 143.66763433,\n",
      "       143.35571794, 181.43706483, 117.23990033, 103.79331492,\n",
      "       104.97248027,  51.30083837, 116.82936428, 150.26046624,\n",
      "       142.49521639, 127.19622084,  63.7869869 ,  88.21464394,\n",
      "       110.41550736,  92.38406199])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([116.29604032, 157.91140713, 246.51495725, 196.23187143,\n",
      "       170.08294304, 152.27811061,  53.64930572, 108.24262341,\n",
      "       147.88143388, 274.83401078, 196.4136086 , 173.2665378 ,\n",
      "       161.73243664,  99.10058533, 189.91445947, 217.03019629,\n",
      "       218.01662357, 135.62151463, 215.66343029, 121.17746619])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([156.95619511, 159.44456572, 109.77291327, 143.66763433,\n",
      "       143.35571794, 181.43706483, 117.23990033, 103.79331492,\n",
      "       104.97248027,  51.30083837, 116.82936428, 150.26046624,\n",
      "       142.49521639, 127.19622084,  63.7869869 ,  88.21464394,\n",
      "       110.41550736,  92.38406199, 116.29604032, 157.91140713,\n",
      "       246.51495725, 196.23187143, 170.08294304, 152.27811061,\n",
      "        53.64930572, 108.24262341, 147.88143388, 274.83401078,\n",
      "       196.4136086 , 173.2665378 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([161.73243664,  99.10058533, 189.91445947, 217.03019629,\n",
      "       218.01662357, 135.62151463, 215.66343029, 121.17746619,\n",
      "       167.28600356, 283.28864689, 164.69125998, 125.10364505,\n",
      "       159.87748372, 144.34205834,  82.29692285, 140.1662113 ,\n",
      "       115.85726673, 173.41590414, 135.43190224, 233.88614205])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([223.49117695, 260.65355218, 206.39405506, 233.54247196,\n",
      "       248.43310869, 190.08114942, 240.53880393, 280.09782758,\n",
      "       315.35893007, 327.78137488, 209.29589023, 180.7943296 ,\n",
      "       256.05134529, 287.62799663, 293.2503612 , 461.66825185,\n",
      "       371.01366887, 340.76572611, 337.21491073, 319.24262795,\n",
      "       265.1709739 , 298.00957865, 242.49635296, 240.13836305,\n",
      "       236.14417874, 260.83591413, 384.60274617, 226.89460208,\n",
      "       273.27294491, 203.17066281])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([217.19312001, 314.43807321, 300.01414871, 227.87519988,\n",
      "       220.79051407, 295.02199764, 255.88545794, 278.47209267,\n",
      "       287.98710364, 252.91959566, 378.32322455, 361.8077385 ,\n",
      "       206.60592959, 258.34179417, 255.41322766, 318.51060943,\n",
      "       268.61541097, 272.76425551, 315.14139906, 348.59105903])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([128.07629968,  76.25036135,  81.087672  , 156.95619511,\n",
      "       159.44456572, 109.77291327, 143.66763433, 143.35571794,\n",
      "       181.43706483, 117.23990033, 103.79331492, 104.97248027,\n",
      "        51.30083837, 116.82936428, 150.26046624, 142.49521639,\n",
      "       127.19622084,  63.7869869 ,  88.21464394, 110.41550736,\n",
      "        92.38406199, 116.29604032, 157.91140713, 246.51495725,\n",
      "       196.23187143, 170.08294304, 152.27811061,  53.64930572,\n",
      "       108.24262341, 147.88143388])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([274.83401078, 196.4136086 , 173.2665378 , 161.73243664,\n",
      "        99.10058533, 189.91445947, 217.03019629, 218.01662357,\n",
      "       135.62151463, 215.66343029, 121.17746619, 167.28600356,\n",
      "       283.28864689, 164.69125998, 125.10364505, 159.87748372,\n",
      "       144.34205834,  82.29692285, 140.1662113 , 115.85726673])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([247.8262009 , 261.74448658, 324.40308386, 242.94621212,\n",
      "       176.65754742, 154.29002952, 153.07560126, 191.31567738,\n",
      "       228.94610511, 238.09168848, 267.12643163, 239.73651743,\n",
      "       296.96554321, 241.19461979, 358.19748099, 314.38420433,\n",
      "       205.25786975, 159.52202019, 227.55789279, 224.56358967,\n",
      "       261.6293727 , 269.64979598, 241.11349397, 191.94529955,\n",
      "       141.51864415, 178.65095344, 264.60280642, 264.23905845,\n",
      "       183.7540556 , 223.49117695])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([260.65355218, 206.39405506, 233.54247196, 248.43310869,\n",
      "       190.08114942, 240.53880393, 280.09782758, 315.35893007,\n",
      "       327.78137488, 209.29589023, 180.7943296 , 256.05134529,\n",
      "       287.62799663, 293.2503612 , 461.66825185, 371.01366887,\n",
      "       340.76572611, 337.21491073, 319.24262795, 265.1709739 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 69.67090736,  73.00015151,  49.1362523 , 106.98454433,\n",
      "       155.1638923 , 163.89514009,  76.87066266,  66.65283723,\n",
      "       105.2879106 , 152.28966719,  96.70344248,  83.77352599,\n",
      "        41.23114256,  17.99426911, 114.11325318, 184.04522052,\n",
      "       131.63722049, 158.90144505, 147.21045867,  86.83575307,\n",
      "       115.00959466, 192.5315456 , 142.60208615, 193.93866008,\n",
      "        16.93172331, 103.93682066, 129.66886997, 103.55737712,\n",
      "       115.40403437,  19.93344086])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 65.42368531, 126.55694824, 199.66504437, 128.07629968,\n",
      "        76.25036135,  81.087672  , 156.95619511, 159.44456572,\n",
      "       109.77291327, 143.66763433, 143.35571794, 181.43706483,\n",
      "       117.23990033, 103.79331492, 104.97248027,  51.30083837,\n",
      "       116.82936428, 150.26046624, 142.49521639, 127.19622084])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([104.97248027,  51.30083837, 116.82936428, 150.26046624,\n",
      "       142.49521639, 127.19622084,  63.7869869 ,  88.21464394,\n",
      "       110.41550736,  92.38406199, 116.29604032, 157.91140713,\n",
      "       246.51495725, 196.23187143, 170.08294304, 152.27811061,\n",
      "        53.64930572, 108.24262341, 147.88143388, 274.83401078,\n",
      "       196.4136086 , 173.2665378 , 161.73243664,  99.10058533,\n",
      "       189.91445947, 217.03019629, 218.01662357, 135.62151463,\n",
      "       215.66343029, 121.17746619])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([167.28600356, 283.28864689, 164.69125998, 125.10364505,\n",
      "       159.87748372, 144.34205834,  82.29692285, 140.1662113 ,\n",
      "       115.85726673, 173.41590414, 135.43190224, 233.88614205,\n",
      "       168.22234072, 149.37417651, 211.20263936, 134.80757158,\n",
      "       168.40259944, 248.81801436, 131.08170877, 163.32682593])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([135.62151463, 215.66343029, 121.17746619, 167.28600356,\n",
      "       283.28864689, 164.69125998, 125.10364505, 159.87748372,\n",
      "       144.34205834,  82.29692285, 140.1662113 , 115.85726673,\n",
      "       173.41590414, 135.43190224, 233.88614205, 168.22234072,\n",
      "       149.37417651, 211.20263936, 134.80757158, 168.40259944,\n",
      "       248.81801436, 131.08170877, 163.32682593, 198.28054145,\n",
      "       230.55995397, 143.44437565,  99.18040759, 184.29382082,\n",
      "       211.78528286, 211.13288375])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([216.64507465, 168.69752245, 193.57349357, 214.00622395,\n",
      "       168.32951   , 277.04961628, 260.96146028, 156.82804628,\n",
      "       211.79379382, 165.25249826, 223.55048858, 276.7108538 ,\n",
      "       191.76163782, 241.70231579, 247.8262009 , 261.74448658,\n",
      "       324.40308386, 242.94621212, 176.65754742, 154.29002952])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([184.29382082, 211.78528286, 211.13288375, 216.64507465,\n",
      "       168.69752245, 193.57349357, 214.00622395, 168.32951   ,\n",
      "       277.04961628, 260.96146028, 156.82804628, 211.79379382,\n",
      "       165.25249826, 223.55048858, 276.7108538 , 191.76163782,\n",
      "       241.70231579, 247.8262009 , 261.74448658, 324.40308386,\n",
      "       242.94621212, 176.65754742, 154.29002952, 153.07560126,\n",
      "       191.31567738, 228.94610511, 238.09168848, 267.12643163,\n",
      "       239.73651743, 296.96554321])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([241.19461979, 358.19748099, 314.38420433, 205.25786975,\n",
      "       159.52202019, 227.55789279, 224.56358967, 261.6293727 ,\n",
      "       269.64979598, 241.11349397, 191.94529955, 141.51864415,\n",
      "       178.65095344, 264.60280642, 264.23905845, 183.7540556 ,\n",
      "       223.49117695, 260.65355218, 206.39405506, 233.54247196])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([240.53880393, 280.09782758, 315.35893007, 327.78137488,\n",
      "       209.29589023, 180.7943296 , 256.05134529, 287.62799663,\n",
      "       293.2503612 , 461.66825185, 371.01366887, 340.76572611,\n",
      "       337.21491073, 319.24262795, 265.1709739 , 298.00957865,\n",
      "       242.49635296, 240.13836305, 236.14417874, 260.83591413,\n",
      "       384.60274617, 226.89460208, 273.27294491, 203.17066281,\n",
      "       217.19312001, 314.43807321, 300.01414871, 227.87519988,\n",
      "       220.79051407, 295.02199764])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([255.88545794, 278.47209267, 287.98710364, 252.91959566,\n",
      "       378.32322455, 361.8077385 , 206.60592959, 258.34179417,\n",
      "       255.41322766, 318.51060943, 268.61541097, 272.76425551,\n",
      "       315.14139906, 348.59105903, 256.83218886, 258.63997842,\n",
      "       265.18095026, 255.64151055, 373.23511922, 357.72595578])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([193.57349357, 214.00622395, 168.32951   , 277.04961628,\n",
      "       260.96146028, 156.82804628, 211.79379382, 165.25249826,\n",
      "       223.55048858, 276.7108538 , 191.76163782, 241.70231579,\n",
      "       247.8262009 , 261.74448658, 324.40308386, 242.94621212,\n",
      "       176.65754742, 154.29002952, 153.07560126, 191.31567738,\n",
      "       228.94610511, 238.09168848, 267.12643163, 239.73651743,\n",
      "       296.96554321, 241.19461979, 358.19748099, 314.38420433,\n",
      "       205.25786975, 159.52202019])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([227.55789279, 224.56358967, 261.6293727 , 269.64979598,\n",
      "       241.11349397, 191.94529955, 141.51864415, 178.65095344,\n",
      "       264.60280642, 264.23905845, 183.7540556 , 223.49117695,\n",
      "       260.65355218, 206.39405506, 233.54247196, 248.43310869,\n",
      "       190.08114942, 240.53880393, 280.09782758, 315.35893007])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([159.52202019, 227.55789279, 224.56358967, 261.6293727 ,\n",
      "       269.64979598, 241.11349397, 191.94529955, 141.51864415,\n",
      "       178.65095344, 264.60280642, 264.23905845, 183.7540556 ,\n",
      "       223.49117695, 260.65355218, 206.39405506, 233.54247196,\n",
      "       248.43310869, 190.08114942, 240.53880393, 280.09782758,\n",
      "       315.35893007, 327.78137488, 209.29589023, 180.7943296 ,\n",
      "       256.05134529, 287.62799663, 293.2503612 , 461.66825185,\n",
      "       371.01366887, 340.76572611])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([337.21491073, 319.24262795, 265.1709739 , 298.00957865,\n",
      "       242.49635296, 240.13836305, 236.14417874, 260.83591413,\n",
      "       384.60274617, 226.89460208, 273.27294491, 203.17066281,\n",
      "       217.19312001, 314.43807321, 300.01414871, 227.87519988,\n",
      "       220.79051407, 295.02199764, 255.88545794, 278.47209267])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([140.1662113 , 115.85726673, 173.41590414, 135.43190224,\n",
      "       233.88614205, 168.22234072, 149.37417651, 211.20263936,\n",
      "       134.80757158, 168.40259944, 248.81801436, 131.08170877,\n",
      "       163.32682593, 198.28054145, 230.55995397, 143.44437565,\n",
      "        99.18040759, 184.29382082, 211.78528286, 211.13288375,\n",
      "       216.64507465, 168.69752245, 193.57349357, 214.00622395,\n",
      "       168.32951   , 277.04961628, 260.96146028, 156.82804628,\n",
      "       211.79379382, 165.25249826])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([223.55048858, 276.7108538 , 191.76163782, 241.70231579,\n",
      "       247.8262009 , 261.74448658, 324.40308386, 242.94621212,\n",
      "       176.65754742, 154.29002952, 153.07560126, 191.31567738,\n",
      "       228.94610511, 238.09168848, 267.12643163, 239.73651743,\n",
      "       296.96554321, 241.19461979, 358.19748099, 314.38420433])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([149.68041012, 109.27425031,  25.89407055,  94.10193173,\n",
      "        29.1604104 ,  62.45920462, -19.259528  , -30.02332054,\n",
      "        54.25897839, 115.20075505, 104.4155928 ,  81.90616437,\n",
      "        66.43128026,   3.62500397,  16.29659574,  42.40312683,\n",
      "       125.84206218, 124.86800076,  10.80211719,  69.67090736,\n",
      "        73.00015151,  49.1362523 , 106.98454433, 155.1638923 ,\n",
      "       163.89514009,  76.87066266,  66.65283723, 105.2879106 ,\n",
      "       152.28966719,  96.70344248])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 83.77352599,  41.23114256,  17.99426911, 114.11325318,\n",
      "       184.04522052, 131.63722049, 158.90144505, 147.21045867,\n",
      "        86.83575307, 115.00959466, 192.5315456 , 142.60208615,\n",
      "       193.93866008,  16.93172331, 103.93682066, 129.66886997,\n",
      "       103.55737712, 115.40403437,  19.93344086,  65.42368531])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([214.00622395, 168.32951   , 277.04961628, 260.96146028,\n",
      "       156.82804628, 211.79379382, 165.25249826, 223.55048858,\n",
      "       276.7108538 , 191.76163782, 241.70231579, 247.8262009 ,\n",
      "       261.74448658, 324.40308386, 242.94621212, 176.65754742,\n",
      "       154.29002952, 153.07560126, 191.31567738, 228.94610511,\n",
      "       238.09168848, 267.12643163, 239.73651743, 296.96554321,\n",
      "       241.19461979, 358.19748099, 314.38420433, 205.25786975,\n",
      "       159.52202019, 227.55789279])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([224.56358967, 261.6293727 , 269.64979598, 241.11349397,\n",
      "       191.94529955, 141.51864415, 178.65095344, 264.60280642,\n",
      "       264.23905845, 183.7540556 , 223.49117695, 260.65355218,\n",
      "       206.39405506, 233.54247196, 248.43310869, 190.08114942,\n",
      "       240.53880393, 280.09782758, 315.35893007, 327.78137488])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 55.83187576, -36.14098885, -69.86728076, -13.39040036,\n",
      "       -11.09627168,  49.64498689,  17.47115024, -28.80375993,\n",
      "       101.72731666,  77.58717368,  65.46366834,  -4.8640219 ,\n",
      "         8.09607209,  55.7317209 ,   9.41582456,  61.83334275,\n",
      "        43.3839532 ,  42.79186531,  32.78361335, 149.68041012,\n",
      "       109.27425031,  25.89407055,  94.10193173,  29.1604104 ,\n",
      "        62.45920462, -19.259528  , -30.02332054,  54.25897839,\n",
      "       115.20075505, 104.4155928 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 81.90616437,  66.43128026,   3.62500397,  16.29659574,\n",
      "        42.40312683, 125.84206218, 124.86800076,  10.80211719,\n",
      "        69.67090736,  73.00015151,  49.1362523 , 106.98454433,\n",
      "       155.1638923 , 163.89514009,  76.87066266,  66.65283723,\n",
      "       105.2879106 , 152.28966719,  96.70344248,  83.77352599])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([224.56358967, 261.6293727 , 269.64979598, 241.11349397,\n",
      "       191.94529955, 141.51864415, 178.65095344, 264.60280642,\n",
      "       264.23905845, 183.7540556 , 223.49117695, 260.65355218,\n",
      "       206.39405506, 233.54247196, 248.43310869, 190.08114942,\n",
      "       240.53880393, 280.09782758, 315.35893007, 327.78137488,\n",
      "       209.29589023, 180.7943296 , 256.05134529, 287.62799663,\n",
      "       293.2503612 , 461.66825185, 371.01366887, 340.76572611,\n",
      "       337.21491073, 319.24262795])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([265.1709739 , 298.00957865, 242.49635296, 240.13836305,\n",
      "       236.14417874, 260.83591413, 384.60274617, 226.89460208,\n",
      "       273.27294491, 203.17066281, 217.19312001, 314.43807321,\n",
      "       300.01414871, 227.87519988, 220.79051407, 295.02199764,\n",
      "       255.88545794, 278.47209267, 287.98710364, 252.91959566])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([180.7943296 , 256.05134529, 287.62799663, 293.2503612 ,\n",
      "       461.66825185, 371.01366887, 340.76572611, 337.21491073,\n",
      "       319.24262795, 265.1709739 , 298.00957865, 242.49635296,\n",
      "       240.13836305, 236.14417874, 260.83591413, 384.60274617,\n",
      "       226.89460208, 273.27294491, 203.17066281, 217.19312001,\n",
      "       314.43807321, 300.01414871, 227.87519988, 220.79051407,\n",
      "       295.02199764, 255.88545794, 278.47209267, 287.98710364,\n",
      "       252.91959566, 378.32322455])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([361.8077385 , 206.60592959, 258.34179417, 255.41322766,\n",
      "       318.51060943, 268.61541097, 272.76425551, 315.14139906,\n",
      "       348.59105903, 256.83218886, 258.63997842, 265.18095026,\n",
      "       255.64151055, 373.23511922, 357.72595578, 254.37809822,\n",
      "       325.97651154, 427.7355496 , 405.77162346, 261.78093214])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([125.84206218, 124.86800076,  10.80211719,  69.67090736,\n",
      "        73.00015151,  49.1362523 , 106.98454433, 155.1638923 ,\n",
      "       163.89514009,  76.87066266,  66.65283723, 105.2879106 ,\n",
      "       152.28966719,  96.70344248,  83.77352599,  41.23114256,\n",
      "        17.99426911, 114.11325318, 184.04522052, 131.63722049,\n",
      "       158.90144505, 147.21045867,  86.83575307, 115.00959466,\n",
      "       192.5315456 , 142.60208615, 193.93866008,  16.93172331,\n",
      "       103.93682066, 129.66886997])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([103.55737712, 115.40403437,  19.93344086,  65.42368531,\n",
      "       126.55694824, 199.66504437, 128.07629968,  76.25036135,\n",
      "        81.087672  , 156.95619511, 159.44456572, 109.77291327,\n",
      "       143.66763433, 143.35571794, 181.43706483, 117.23990033,\n",
      "       103.79331492, 104.97248027,  51.30083837, 116.82936428])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([261.6293727 , 269.64979598, 241.11349397, 191.94529955,\n",
      "       141.51864415, 178.65095344, 264.60280642, 264.23905845,\n",
      "       183.7540556 , 223.49117695, 260.65355218, 206.39405506,\n",
      "       233.54247196, 248.43310869, 190.08114942, 240.53880393,\n",
      "       280.09782758, 315.35893007, 327.78137488, 209.29589023,\n",
      "       180.7943296 , 256.05134529, 287.62799663, 293.2503612 ,\n",
      "       461.66825185, 371.01366887, 340.76572611, 337.21491073,\n",
      "       319.24262795, 265.1709739 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([298.00957865, 242.49635296, 240.13836305, 236.14417874,\n",
      "       260.83591413, 384.60274617, 226.89460208, 273.27294491,\n",
      "       203.17066281, 217.19312001, 314.43807321, 300.01414871,\n",
      "       227.87519988, 220.79051407, 295.02199764, 255.88545794,\n",
      "       278.47209267, 287.98710364, 252.91959566, 378.32322455])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 99.18040759, 184.29382082, 211.78528286, 211.13288375,\n",
      "       216.64507465, 168.69752245, 193.57349357, 214.00622395,\n",
      "       168.32951   , 277.04961628, 260.96146028, 156.82804628,\n",
      "       211.79379382, 165.25249826, 223.55048858, 276.7108538 ,\n",
      "       191.76163782, 241.70231579, 247.8262009 , 261.74448658,\n",
      "       324.40308386, 242.94621212, 176.65754742, 154.29002952,\n",
      "       153.07560126, 191.31567738, 228.94610511, 238.09168848,\n",
      "       267.12643163, 239.73651743])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([296.96554321, 241.19461979, 358.19748099, 314.38420433,\n",
      "       205.25786975, 159.52202019, 227.55789279, 224.56358967,\n",
      "       261.6293727 , 269.64979598, 241.11349397, 191.94529955,\n",
      "       141.51864415, 178.65095344, 264.60280642, 264.23905845,\n",
      "       183.7540556 , 223.49117695, 260.65355218, 206.39405506])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([131.08170877, 163.32682593, 198.28054145, 230.55995397,\n",
      "       143.44437565,  99.18040759, 184.29382082, 211.78528286,\n",
      "       211.13288375, 216.64507465, 168.69752245, 193.57349357,\n",
      "       214.00622395, 168.32951   , 277.04961628, 260.96146028,\n",
      "       156.82804628, 211.79379382, 165.25249826, 223.55048858,\n",
      "       276.7108538 , 191.76163782, 241.70231579, 247.8262009 ,\n",
      "       261.74448658, 324.40308386, 242.94621212, 176.65754742,\n",
      "       154.29002952, 153.07560126])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([191.31567738, 228.94610511, 238.09168848, 267.12643163,\n",
      "       239.73651743, 296.96554321, 241.19461979, 358.19748099,\n",
      "       314.38420433, 205.25786975, 159.52202019, 227.55789279,\n",
      "       224.56358967, 261.6293727 , 269.64979598, 241.11349397,\n",
      "       191.94529955, 141.51864415, 178.65095344, 264.60280642])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([233.54247196, 248.43310869, 190.08114942, 240.53880393,\n",
      "       280.09782758, 315.35893007, 327.78137488, 209.29589023,\n",
      "       180.7943296 , 256.05134529, 287.62799663, 293.2503612 ,\n",
      "       461.66825185, 371.01366887, 340.76572611, 337.21491073,\n",
      "       319.24262795, 265.1709739 , 298.00957865, 242.49635296,\n",
      "       240.13836305, 236.14417874, 260.83591413, 384.60274617,\n",
      "       226.89460208, 273.27294491, 203.17066281, 217.19312001,\n",
      "       314.43807321, 300.01414871])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([227.87519988, 220.79051407, 295.02199764, 255.88545794,\n",
      "       278.47209267, 287.98710364, 252.91959566, 378.32322455,\n",
      "       361.8077385 , 206.60592959, 258.34179417, 255.41322766,\n",
      "       318.51060943, 268.61541097, 272.76425551, 315.14139906,\n",
      "       348.59105903, 256.83218886, 258.63997842, 265.18095026])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([168.40259944, 248.81801436, 131.08170877, 163.32682593,\n",
      "       198.28054145, 230.55995397, 143.44437565,  99.18040759,\n",
      "       184.29382082, 211.78528286, 211.13288375, 216.64507465,\n",
      "       168.69752245, 193.57349357, 214.00622395, 168.32951   ,\n",
      "       277.04961628, 260.96146028, 156.82804628, 211.79379382,\n",
      "       165.25249826, 223.55048858, 276.7108538 , 191.76163782,\n",
      "       241.70231579, 247.8262009 , 261.74448658, 324.40308386,\n",
      "       242.94621212, 176.65754742])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([154.29002952, 153.07560126, 191.31567738, 228.94610511,\n",
      "       238.09168848, 267.12643163, 239.73651743, 296.96554321,\n",
      "       241.19461979, 358.19748099, 314.38420433, 205.25786975,\n",
      "       159.52202019, 227.55789279, 224.56358967, 261.6293727 ,\n",
      "       269.64979598, 241.11349397, 191.94529955, 141.51864415])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([265.1709739 , 298.00957865, 242.49635296, 240.13836305,\n",
      "       236.14417874, 260.83591413, 384.60274617, 226.89460208,\n",
      "       273.27294491, 203.17066281, 217.19312001, 314.43807321,\n",
      "       300.01414871, 227.87519988, 220.79051407, 295.02199764,\n",
      "       255.88545794, 278.47209267, 287.98710364, 252.91959566,\n",
      "       378.32322455, 361.8077385 , 206.60592959, 258.34179417,\n",
      "       255.41322766, 318.51060943, 268.61541097, 272.76425551,\n",
      "       315.14139906, 348.59105903])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([256.83218886, 258.63997842, 265.18095026, 255.64151055,\n",
      "       373.23511922, 357.72595578, 254.37809822, 325.97651154,\n",
      "       427.7355496 , 405.77162346, 261.78093214, 257.79632772,\n",
      "       359.61398809, 300.97891002, 323.07814374, 360.51164299,\n",
      "       286.45978574, 296.09527034, 152.42525801, 197.44811645])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([227.55789279, 224.56358967, 261.6293727 , 269.64979598,\n",
      "       241.11349397, 191.94529955, 141.51864415, 178.65095344,\n",
      "       264.60280642, 264.23905845, 183.7540556 , 223.49117695,\n",
      "       260.65355218, 206.39405506, 233.54247196, 248.43310869,\n",
      "       190.08114942, 240.53880393, 280.09782758, 315.35893007,\n",
      "       327.78137488, 209.29589023, 180.7943296 , 256.05134529,\n",
      "       287.62799663, 293.2503612 , 461.66825185, 371.01366887,\n",
      "       340.76572611, 337.21491073])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([319.24262795, 265.1709739 , 298.00957865, 242.49635296,\n",
      "       240.13836305, 236.14417874, 260.83591413, 384.60274617,\n",
      "       226.89460208, 273.27294491, 203.17066281, 217.19312001,\n",
      "       314.43807321, 300.01414871, 227.87519988, 220.79051407,\n",
      "       295.02199764, 255.88545794, 278.47209267, 287.98710364])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([218.01662357, 135.62151463, 215.66343029, 121.17746619,\n",
      "       167.28600356, 283.28864689, 164.69125998, 125.10364505,\n",
      "       159.87748372, 144.34205834,  82.29692285, 140.1662113 ,\n",
      "       115.85726673, 173.41590414, 135.43190224, 233.88614205,\n",
      "       168.22234072, 149.37417651, 211.20263936, 134.80757158,\n",
      "       168.40259944, 248.81801436, 131.08170877, 163.32682593,\n",
      "       198.28054145, 230.55995397, 143.44437565,  99.18040759,\n",
      "       184.29382082, 211.78528286])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([211.13288375, 216.64507465, 168.69752245, 193.57349357,\n",
      "       214.00622395, 168.32951   , 277.04961628, 260.96146028,\n",
      "       156.82804628, 211.79379382, 165.25249826, 223.55048858,\n",
      "       276.7108538 , 191.76163782, 241.70231579, 247.8262009 ,\n",
      "       261.74448658, 324.40308386, 242.94621212, 176.65754742])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([239.73651743, 296.96554321, 241.19461979, 358.19748099,\n",
      "       314.38420433, 205.25786975, 159.52202019, 227.55789279,\n",
      "       224.56358967, 261.6293727 , 269.64979598, 241.11349397,\n",
      "       191.94529955, 141.51864415, 178.65095344, 264.60280642,\n",
      "       264.23905845, 183.7540556 , 223.49117695, 260.65355218,\n",
      "       206.39405506, 233.54247196, 248.43310869, 190.08114942,\n",
      "       240.53880393, 280.09782758, 315.35893007, 327.78137488,\n",
      "       209.29589023, 180.7943296 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([256.05134529, 287.62799663, 293.2503612 , 461.66825185,\n",
      "       371.01366887, 340.76572611, 337.21491073, 319.24262795,\n",
      "       265.1709739 , 298.00957865, 242.49635296, 240.13836305,\n",
      "       236.14417874, 260.83591413, 384.60274617, 226.89460208,\n",
      "       273.27294491, 203.17066281, 217.19312001, 314.43807321])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([  8.09607209,  55.7317209 ,   9.41582456,  61.83334275,\n",
      "        43.3839532 ,  42.79186531,  32.78361335, 149.68041012,\n",
      "       109.27425031,  25.89407055,  94.10193173,  29.1604104 ,\n",
      "        62.45920462, -19.259528  , -30.02332054,  54.25897839,\n",
      "       115.20075505, 104.4155928 ,  81.90616437,  66.43128026,\n",
      "         3.62500397,  16.29659574,  42.40312683, 125.84206218,\n",
      "       124.86800076,  10.80211719,  69.67090736,  73.00015151,\n",
      "        49.1362523 , 106.98454433])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([155.1638923 , 163.89514009,  76.87066266,  66.65283723,\n",
      "       105.2879106 , 152.28966719,  96.70344248,  83.77352599,\n",
      "        41.23114256,  17.99426911, 114.11325318, 184.04522052,\n",
      "       131.63722049, 158.90144505, 147.21045867,  86.83575307,\n",
      "       115.00959466, 192.5315456 , 142.60208615, 193.93866008])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([170.08294304, 152.27811061,  53.64930572, 108.24262341,\n",
      "       147.88143388, 274.83401078, 196.4136086 , 173.2665378 ,\n",
      "       161.73243664,  99.10058533, 189.91445947, 217.03019629,\n",
      "       218.01662357, 135.62151463, 215.66343029, 121.17746619,\n",
      "       167.28600356, 283.28864689, 164.69125998, 125.10364505,\n",
      "       159.87748372, 144.34205834,  82.29692285, 140.1662113 ,\n",
      "       115.85726673, 173.41590414, 135.43190224, 233.88614205,\n",
      "       168.22234072, 149.37417651])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([211.20263936, 134.80757158, 168.40259944, 248.81801436,\n",
      "       131.08170877, 163.32682593, 198.28054145, 230.55995397,\n",
      "       143.44437565,  99.18040759, 184.29382082, 211.78528286,\n",
      "       211.13288375, 216.64507465, 168.69752245, 193.57349357,\n",
      "       214.00622395, 168.32951   , 277.04961628, 260.96146028])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([153.07560126, 191.31567738, 228.94610511, 238.09168848,\n",
      "       267.12643163, 239.73651743, 296.96554321, 241.19461979,\n",
      "       358.19748099, 314.38420433, 205.25786975, 159.52202019,\n",
      "       227.55789279, 224.56358967, 261.6293727 , 269.64979598,\n",
      "       241.11349397, 191.94529955, 141.51864415, 178.65095344,\n",
      "       264.60280642, 264.23905845, 183.7540556 , 223.49117695,\n",
      "       260.65355218, 206.39405506, 233.54247196, 248.43310869,\n",
      "       190.08114942, 240.53880393])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([280.09782758, 315.35893007, 327.78137488, 209.29589023,\n",
      "       180.7943296 , 256.05134529, 287.62799663, 293.2503612 ,\n",
      "       461.66825185, 371.01366887, 340.76572611, 337.21491073,\n",
      "       319.24262795, 265.1709739 , 298.00957865, 242.49635296,\n",
      "       240.13836305, 236.14417874, 260.83591413, 384.60274617])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 19.93344086,  65.42368531, 126.55694824, 199.66504437,\n",
      "       128.07629968,  76.25036135,  81.087672  , 156.95619511,\n",
      "       159.44456572, 109.77291327, 143.66763433, 143.35571794,\n",
      "       181.43706483, 117.23990033, 103.79331492, 104.97248027,\n",
      "        51.30083837, 116.82936428, 150.26046624, 142.49521639,\n",
      "       127.19622084,  63.7869869 ,  88.21464394, 110.41550736,\n",
      "        92.38406199, 116.29604032, 157.91140713, 246.51495725,\n",
      "       196.23187143, 170.08294304])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([152.27811061,  53.64930572, 108.24262341, 147.88143388,\n",
      "       274.83401078, 196.4136086 , 173.2665378 , 161.73243664,\n",
      "        99.10058533, 189.91445947, 217.03019629, 218.01662357,\n",
      "       135.62151463, 215.66343029, 121.17746619, 167.28600356,\n",
      "       283.28864689, 164.69125998, 125.10364505, 159.87748372])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 92.38406199, 116.29604032, 157.91140713, 246.51495725,\n",
      "       196.23187143, 170.08294304, 152.27811061,  53.64930572,\n",
      "       108.24262341, 147.88143388, 274.83401078, 196.4136086 ,\n",
      "       173.2665378 , 161.73243664,  99.10058533, 189.91445947,\n",
      "       217.03019629, 218.01662357, 135.62151463, 215.66343029,\n",
      "       121.17746619, 167.28600356, 283.28864689, 164.69125998,\n",
      "       125.10364505, 159.87748372, 144.34205834,  82.29692285,\n",
      "       140.1662113 , 115.85726673])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([173.41590414, 135.43190224, 233.88614205, 168.22234072,\n",
      "       149.37417651, 211.20263936, 134.80757158, 168.40259944,\n",
      "       248.81801436, 131.08170877, 163.32682593, 198.28054145,\n",
      "       230.55995397, 143.44437565,  99.18040759, 184.29382082,\n",
      "       211.78528286, 211.13288375, 216.64507465, 168.69752245])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([211.13288375, 216.64507465, 168.69752245, 193.57349357,\n",
      "       214.00622395, 168.32951   , 277.04961628, 260.96146028,\n",
      "       156.82804628, 211.79379382, 165.25249826, 223.55048858,\n",
      "       276.7108538 , 191.76163782, 241.70231579, 247.8262009 ,\n",
      "       261.74448658, 324.40308386, 242.94621212, 176.65754742,\n",
      "       154.29002952, 153.07560126, 191.31567738, 228.94610511,\n",
      "       238.09168848, 267.12643163, 239.73651743, 296.96554321,\n",
      "       241.19461979, 358.19748099])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([314.38420433, 205.25786975, 159.52202019, 227.55789279,\n",
      "       224.56358967, 261.6293727 , 269.64979598, 241.11349397,\n",
      "       191.94529955, 141.51864415, 178.65095344, 264.60280642,\n",
      "       264.23905845, 183.7540556 , 223.49117695, 260.65355218,\n",
      "       206.39405506, 233.54247196, 248.43310869, 190.08114942])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([165.25249826, 223.55048858, 276.7108538 , 191.76163782,\n",
      "       241.70231579, 247.8262009 , 261.74448658, 324.40308386,\n",
      "       242.94621212, 176.65754742, 154.29002952, 153.07560126,\n",
      "       191.31567738, 228.94610511, 238.09168848, 267.12643163,\n",
      "       239.73651743, 296.96554321, 241.19461979, 358.19748099,\n",
      "       314.38420433, 205.25786975, 159.52202019, 227.55789279,\n",
      "       224.56358967, 261.6293727 , 269.64979598, 241.11349397,\n",
      "       191.94529955, 141.51864415])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([178.65095344, 264.60280642, 264.23905845, 183.7540556 ,\n",
      "       223.49117695, 260.65355218, 206.39405506, 233.54247196,\n",
      "       248.43310869, 190.08114942, 240.53880393, 280.09782758,\n",
      "       315.35893007, 327.78137488, 209.29589023, 180.7943296 ,\n",
      "       256.05134529, 287.62799663, 293.2503612 , 461.66825185])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([223.55048858, 276.7108538 , 191.76163782, 241.70231579,\n",
      "       247.8262009 , 261.74448658, 324.40308386, 242.94621212,\n",
      "       176.65754742, 154.29002952, 153.07560126, 191.31567738,\n",
      "       228.94610511, 238.09168848, 267.12643163, 239.73651743,\n",
      "       296.96554321, 241.19461979, 358.19748099, 314.38420433,\n",
      "       205.25786975, 159.52202019, 227.55789279, 224.56358967,\n",
      "       261.6293727 , 269.64979598, 241.11349397, 191.94529955,\n",
      "       141.51864415, 178.65095344])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([264.60280642, 264.23905845, 183.7540556 , 223.49117695,\n",
      "       260.65355218, 206.39405506, 233.54247196, 248.43310869,\n",
      "       190.08114942, 240.53880393, 280.09782758, 315.35893007,\n",
      "       327.78137488, 209.29589023, 180.7943296 , 256.05134529,\n",
      "       287.62799663, 293.2503612 , 461.66825185, 371.01366887])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([242.94621212, 176.65754742, 154.29002952, 153.07560126,\n",
      "       191.31567738, 228.94610511, 238.09168848, 267.12643163,\n",
      "       239.73651743, 296.96554321, 241.19461979, 358.19748099,\n",
      "       314.38420433, 205.25786975, 159.52202019, 227.55789279,\n",
      "       224.56358967, 261.6293727 , 269.64979598, 241.11349397,\n",
      "       191.94529955, 141.51864415, 178.65095344, 264.60280642,\n",
      "       264.23905845, 183.7540556 , 223.49117695, 260.65355218,\n",
      "       206.39405506, 233.54247196])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([248.43310869, 190.08114942, 240.53880393, 280.09782758,\n",
      "       315.35893007, 327.78137488, 209.29589023, 180.7943296 ,\n",
      "       256.05134529, 287.62799663, 293.2503612 , 461.66825185,\n",
      "       371.01366887, 340.76572611, 337.21491073, 319.24262795,\n",
      "       265.1709739 , 298.00957865, 242.49635296, 240.13836305])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([134.80757158, 168.40259944, 248.81801436, 131.08170877,\n",
      "       163.32682593, 198.28054145, 230.55995397, 143.44437565,\n",
      "        99.18040759, 184.29382082, 211.78528286, 211.13288375,\n",
      "       216.64507465, 168.69752245, 193.57349357, 214.00622395,\n",
      "       168.32951   , 277.04961628, 260.96146028, 156.82804628,\n",
      "       211.79379382, 165.25249826, 223.55048858, 276.7108538 ,\n",
      "       191.76163782, 241.70231579, 247.8262009 , 261.74448658,\n",
      "       324.40308386, 242.94621212])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([176.65754742, 154.29002952, 153.07560126, 191.31567738,\n",
      "       228.94610511, 238.09168848, 267.12643163, 239.73651743,\n",
      "       296.96554321, 241.19461979, 358.19748099, 314.38420433,\n",
      "       205.25786975, 159.52202019, 227.55789279, 224.56358967,\n",
      "       261.6293727 , 269.64979598, 241.11349397, 191.94529955])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([264.60280642, 264.23905845, 183.7540556 , 223.49117695,\n",
      "       260.65355218, 206.39405506, 233.54247196, 248.43310869,\n",
      "       190.08114942, 240.53880393, 280.09782758, 315.35893007,\n",
      "       327.78137488, 209.29589023, 180.7943296 , 256.05134529,\n",
      "       287.62799663, 293.2503612 , 461.66825185, 371.01366887,\n",
      "       340.76572611, 337.21491073, 319.24262795, 265.1709739 ,\n",
      "       298.00957865, 242.49635296, 240.13836305, 236.14417874,\n",
      "       260.83591413, 384.60274617])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([226.89460208, 273.27294491, 203.17066281, 217.19312001,\n",
      "       314.43807321, 300.01414871, 227.87519988, 220.79051407,\n",
      "       295.02199764, 255.88545794, 278.47209267, 287.98710364,\n",
      "       252.91959566, 378.32322455, 361.8077385 , 206.60592959,\n",
      "       258.34179417, 255.41322766, 318.51060943, 268.61541097])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([168.22234072, 149.37417651, 211.20263936, 134.80757158,\n",
      "       168.40259944, 248.81801436, 131.08170877, 163.32682593,\n",
      "       198.28054145, 230.55995397, 143.44437565,  99.18040759,\n",
      "       184.29382082, 211.78528286, 211.13288375, 216.64507465,\n",
      "       168.69752245, 193.57349357, 214.00622395, 168.32951   ,\n",
      "       277.04961628, 260.96146028, 156.82804628, 211.79379382,\n",
      "       165.25249826, 223.55048858, 276.7108538 , 191.76163782,\n",
      "       241.70231579, 247.8262009 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([261.74448658, 324.40308386, 242.94621212, 176.65754742,\n",
      "       154.29002952, 153.07560126, 191.31567738, 228.94610511,\n",
      "       238.09168848, 267.12643163, 239.73651743, 296.96554321,\n",
      "       241.19461979, 358.19748099, 314.38420433, 205.25786975,\n",
      "       159.52202019, 227.55789279, 224.56358967, 261.6293727 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([203.17066281, 217.19312001, 314.43807321, 300.01414871,\n",
      "       227.87519988, 220.79051407, 295.02199764, 255.88545794,\n",
      "       278.47209267, 287.98710364, 252.91959566, 378.32322455,\n",
      "       361.8077385 , 206.60592959, 258.34179417, 255.41322766,\n",
      "       318.51060943, 268.61541097, 272.76425551, 315.14139906,\n",
      "       348.59105903, 256.83218886, 258.63997842, 265.18095026,\n",
      "       255.64151055, 373.23511922, 357.72595578, 254.37809822,\n",
      "       325.97651154, 427.7355496 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([405.77162346, 261.78093214, 257.79632772, 359.61398809,\n",
      "       300.97891002, 323.07814374, 360.51164299, 286.45978574,\n",
      "       296.09527034, 152.42525801, 197.44811645, 274.68819015,\n",
      "       247.35305589, 374.20690796, 282.19155374, 275.74591981,\n",
      "       318.92733395, 398.74035381, 286.78178474, 360.52363199])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([198.28054145, 230.55995397, 143.44437565,  99.18040759,\n",
      "       184.29382082, 211.78528286, 211.13288375, 216.64507465,\n",
      "       168.69752245, 193.57349357, 214.00622395, 168.32951   ,\n",
      "       277.04961628, 260.96146028, 156.82804628, 211.79379382,\n",
      "       165.25249826, 223.55048858, 276.7108538 , 191.76163782,\n",
      "       241.70231579, 247.8262009 , 261.74448658, 324.40308386,\n",
      "       242.94621212, 176.65754742, 154.29002952, 153.07560126,\n",
      "       191.31567738, 228.94610511])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([238.09168848, 267.12643163, 239.73651743, 296.96554321,\n",
      "       241.19461979, 358.19748099, 314.38420433, 205.25786975,\n",
      "       159.52202019, 227.55789279, 224.56358967, 261.6293727 ,\n",
      "       269.64979598, 241.11349397, 191.94529955, 141.51864415,\n",
      "       178.65095344, 264.60280642, 264.23905845, 183.7540556 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([283.28864689, 164.69125998, 125.10364505, 159.87748372,\n",
      "       144.34205834,  82.29692285, 140.1662113 , 115.85726673,\n",
      "       173.41590414, 135.43190224, 233.88614205, 168.22234072,\n",
      "       149.37417651, 211.20263936, 134.80757158, 168.40259944,\n",
      "       248.81801436, 131.08170877, 163.32682593, 198.28054145,\n",
      "       230.55995397, 143.44437565,  99.18040759, 184.29382082,\n",
      "       211.78528286, 211.13288375, 216.64507465, 168.69752245,\n",
      "       193.57349357, 214.00622395])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([168.32951   , 277.04961628, 260.96146028, 156.82804628,\n",
      "       211.79379382, 165.25249826, 223.55048858, 276.7108538 ,\n",
      "       191.76163782, 241.70231579, 247.8262009 , 261.74448658,\n",
      "       324.40308386, 242.94621212, 176.65754742, 154.29002952,\n",
      "       153.07560126, 191.31567738, 228.94610511, 238.09168848])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 86.83575307, 115.00959466, 192.5315456 , 142.60208615,\n",
      "       193.93866008,  16.93172331, 103.93682066, 129.66886997,\n",
      "       103.55737712, 115.40403437,  19.93344086,  65.42368531,\n",
      "       126.55694824, 199.66504437, 128.07629968,  76.25036135,\n",
      "        81.087672  , 156.95619511, 159.44456572, 109.77291327,\n",
      "       143.66763433, 143.35571794, 181.43706483, 117.23990033,\n",
      "       103.79331492, 104.97248027,  51.30083837, 116.82936428,\n",
      "       150.26046624, 142.49521639])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([127.19622084,  63.7869869 ,  88.21464394, 110.41550736,\n",
      "        92.38406199, 116.29604032, 157.91140713, 246.51495725,\n",
      "       196.23187143, 170.08294304, 152.27811061,  53.64930572,\n",
      "       108.24262341, 147.88143388, 274.83401078, 196.4136086 ,\n",
      "       173.2665378 , 161.73243664,  99.10058533, 189.91445947])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 66.43128026,   3.62500397,  16.29659574,  42.40312683,\n",
      "       125.84206218, 124.86800076,  10.80211719,  69.67090736,\n",
      "        73.00015151,  49.1362523 , 106.98454433, 155.1638923 ,\n",
      "       163.89514009,  76.87066266,  66.65283723, 105.2879106 ,\n",
      "       152.28966719,  96.70344248,  83.77352599,  41.23114256,\n",
      "        17.99426911, 114.11325318, 184.04522052, 131.63722049,\n",
      "       158.90144505, 147.21045867,  86.83575307, 115.00959466,\n",
      "       192.5315456 , 142.60208615])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([193.93866008,  16.93172331, 103.93682066, 129.66886997,\n",
      "       103.55737712, 115.40403437,  19.93344086,  65.42368531,\n",
      "       126.55694824, 199.66504437, 128.07629968,  76.25036135,\n",
      "        81.087672  , 156.95619511, 159.44456572, 109.77291327,\n",
      "       143.66763433, 143.35571794, 181.43706483, 117.23990033])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 65.42368531, 126.55694824, 199.66504437, 128.07629968,\n",
      "        76.25036135,  81.087672  , 156.95619511, 159.44456572,\n",
      "       109.77291327, 143.66763433, 143.35571794, 181.43706483,\n",
      "       117.23990033, 103.79331492, 104.97248027,  51.30083837,\n",
      "       116.82936428, 150.26046624, 142.49521639, 127.19622084,\n",
      "        63.7869869 ,  88.21464394, 110.41550736,  92.38406199,\n",
      "       116.29604032, 157.91140713, 246.51495725, 196.23187143,\n",
      "       170.08294304, 152.27811061])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 53.64930572, 108.24262341, 147.88143388, 274.83401078,\n",
      "       196.4136086 , 173.2665378 , 161.73243664,  99.10058533,\n",
      "       189.91445947, 217.03019629, 218.01662357, 135.62151463,\n",
      "       215.66343029, 121.17746619, 167.28600356, 283.28864689,\n",
      "       164.69125998, 125.10364505, 159.87748372, 144.34205834])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 55.7317209 ,   9.41582456,  61.83334275,  43.3839532 ,\n",
      "        42.79186531,  32.78361335, 149.68041012, 109.27425031,\n",
      "        25.89407055,  94.10193173,  29.1604104 ,  62.45920462,\n",
      "       -19.259528  , -30.02332054,  54.25897839, 115.20075505,\n",
      "       104.4155928 ,  81.90616437,  66.43128026,   3.62500397,\n",
      "        16.29659574,  42.40312683, 125.84206218, 124.86800076,\n",
      "        10.80211719,  69.67090736,  73.00015151,  49.1362523 ,\n",
      "       106.98454433, 155.1638923 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([163.89514009,  76.87066266,  66.65283723, 105.2879106 ,\n",
      "       152.28966719,  96.70344248,  83.77352599,  41.23114256,\n",
      "        17.99426911, 114.11325318, 184.04522052, 131.63722049,\n",
      "       158.90144505, 147.21045867,  86.83575307, 115.00959466,\n",
      "       192.5315456 , 142.60208615, 193.93866008,  16.93172331])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([183.7540556 , 223.49117695, 260.65355218, 206.39405506,\n",
      "       233.54247196, 248.43310869, 190.08114942, 240.53880393,\n",
      "       280.09782758, 315.35893007, 327.78137488, 209.29589023,\n",
      "       180.7943296 , 256.05134529, 287.62799663, 293.2503612 ,\n",
      "       461.66825185, 371.01366887, 340.76572611, 337.21491073,\n",
      "       319.24262795, 265.1709739 , 298.00957865, 242.49635296,\n",
      "       240.13836305, 236.14417874, 260.83591413, 384.60274617,\n",
      "       226.89460208, 273.27294491])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([203.17066281, 217.19312001, 314.43807321, 300.01414871,\n",
      "       227.87519988, 220.79051407, 295.02199764, 255.88545794,\n",
      "       278.47209267, 287.98710364, 252.91959566, 378.32322455,\n",
      "       361.8077385 , 206.60592959, 258.34179417, 255.41322766,\n",
      "       318.51060943, 268.61541097, 272.76425551, 315.14139906])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([280.09782758, 315.35893007, 327.78137488, 209.29589023,\n",
      "       180.7943296 , 256.05134529, 287.62799663, 293.2503612 ,\n",
      "       461.66825185, 371.01366887, 340.76572611, 337.21491073,\n",
      "       319.24262795, 265.1709739 , 298.00957865, 242.49635296,\n",
      "       240.13836305, 236.14417874, 260.83591413, 384.60274617,\n",
      "       226.89460208, 273.27294491, 203.17066281, 217.19312001,\n",
      "       314.43807321, 300.01414871, 227.87519988, 220.79051407,\n",
      "       295.02199764, 255.88545794])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([278.47209267, 287.98710364, 252.91959566, 378.32322455,\n",
      "       361.8077385 , 206.60592959, 258.34179417, 255.41322766,\n",
      "       318.51060943, 268.61541097, 272.76425551, 315.14139906,\n",
      "       348.59105903, 256.83218886, 258.63997842, 265.18095026,\n",
      "       255.64151055, 373.23511922, 357.72595578, 254.37809822])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([190.08114942, 240.53880393, 280.09782758, 315.35893007,\n",
      "       327.78137488, 209.29589023, 180.7943296 , 256.05134529,\n",
      "       287.62799663, 293.2503612 , 461.66825185, 371.01366887,\n",
      "       340.76572611, 337.21491073, 319.24262795, 265.1709739 ,\n",
      "       298.00957865, 242.49635296, 240.13836305, 236.14417874,\n",
      "       260.83591413, 384.60274617, 226.89460208, 273.27294491,\n",
      "       203.17066281, 217.19312001, 314.43807321, 300.01414871,\n",
      "       227.87519988, 220.79051407])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([295.02199764, 255.88545794, 278.47209267, 287.98710364,\n",
      "       252.91959566, 378.32322455, 361.8077385 , 206.60592959,\n",
      "       258.34179417, 255.41322766, 318.51060943, 268.61541097,\n",
      "       272.76425551, 315.14139906, 348.59105903, 256.83218886,\n",
      "       258.63997842, 265.18095026, 255.64151055, 373.23511922])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([358.19748099, 314.38420433, 205.25786975, 159.52202019,\n",
      "       227.55789279, 224.56358967, 261.6293727 , 269.64979598,\n",
      "       241.11349397, 191.94529955, 141.51864415, 178.65095344,\n",
      "       264.60280642, 264.23905845, 183.7540556 , 223.49117695,\n",
      "       260.65355218, 206.39405506, 233.54247196, 248.43310869,\n",
      "       190.08114942, 240.53880393, 280.09782758, 315.35893007,\n",
      "       327.78137488, 209.29589023, 180.7943296 , 256.05134529,\n",
      "       287.62799663, 293.2503612 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([461.66825185, 371.01366887, 340.76572611, 337.21491073,\n",
      "       319.24262795, 265.1709739 , 298.00957865, 242.49635296,\n",
      "       240.13836305, 236.14417874, 260.83591413, 384.60274617,\n",
      "       226.89460208, 273.27294491, 203.17066281, 217.19312001,\n",
      "       314.43807321, 300.01414871, 227.87519988, 220.79051407])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([211.20263936, 134.80757158, 168.40259944, 248.81801436,\n",
      "       131.08170877, 163.32682593, 198.28054145, 230.55995397,\n",
      "       143.44437565,  99.18040759, 184.29382082, 211.78528286,\n",
      "       211.13288375, 216.64507465, 168.69752245, 193.57349357,\n",
      "       214.00622395, 168.32951   , 277.04961628, 260.96146028,\n",
      "       156.82804628, 211.79379382, 165.25249826, 223.55048858,\n",
      "       276.7108538 , 191.76163782, 241.70231579, 247.8262009 ,\n",
      "       261.74448658, 324.40308386])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([242.94621212, 176.65754742, 154.29002952, 153.07560126,\n",
      "       191.31567738, 228.94610511, 238.09168848, 267.12643163,\n",
      "       239.73651743, 296.96554321, 241.19461979, 358.19748099,\n",
      "       314.38420433, 205.25786975, 159.52202019, 227.55789279,\n",
      "       224.56358967, 261.6293727 , 269.64979598, 241.11349397])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([267.12643163, 239.73651743, 296.96554321, 241.19461979,\n",
      "       358.19748099, 314.38420433, 205.25786975, 159.52202019,\n",
      "       227.55789279, 224.56358967, 261.6293727 , 269.64979598,\n",
      "       241.11349397, 191.94529955, 141.51864415, 178.65095344,\n",
      "       264.60280642, 264.23905845, 183.7540556 , 223.49117695,\n",
      "       260.65355218, 206.39405506, 233.54247196, 248.43310869,\n",
      "       190.08114942, 240.53880393, 280.09782758, 315.35893007,\n",
      "       327.78137488, 209.29589023])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([180.7943296 , 256.05134529, 287.62799663, 293.2503612 ,\n",
      "       461.66825185, 371.01366887, 340.76572611, 337.21491073,\n",
      "       319.24262795, 265.1709739 , 298.00957865, 242.49635296,\n",
      "       240.13836305, 236.14417874, 260.83591413, 384.60274617,\n",
      "       226.89460208, 273.27294491, 203.17066281, 217.19312001])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([173.2665378 , 161.73243664,  99.10058533, 189.91445947,\n",
      "       217.03019629, 218.01662357, 135.62151463, 215.66343029,\n",
      "       121.17746619, 167.28600356, 283.28864689, 164.69125998,\n",
      "       125.10364505, 159.87748372, 144.34205834,  82.29692285,\n",
      "       140.1662113 , 115.85726673, 173.41590414, 135.43190224,\n",
      "       233.88614205, 168.22234072, 149.37417651, 211.20263936,\n",
      "       134.80757158, 168.40259944, 248.81801436, 131.08170877,\n",
      "       163.32682593, 198.28054145])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([230.55995397, 143.44437565,  99.18040759, 184.29382082,\n",
      "       211.78528286, 211.13288375, 216.64507465, 168.69752245,\n",
      "       193.57349357, 214.00622395, 168.32951   , 277.04961628,\n",
      "       260.96146028, 156.82804628, 211.79379382, 165.25249826,\n",
      "       223.55048858, 276.7108538 , 191.76163782, 241.70231579])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([319.24262795, 265.1709739 , 298.00957865, 242.49635296,\n",
      "       240.13836305, 236.14417874, 260.83591413, 384.60274617,\n",
      "       226.89460208, 273.27294491, 203.17066281, 217.19312001,\n",
      "       314.43807321, 300.01414871, 227.87519988, 220.79051407,\n",
      "       295.02199764, 255.88545794, 278.47209267, 287.98710364,\n",
      "       252.91959566, 378.32322455, 361.8077385 , 206.60592959,\n",
      "       258.34179417, 255.41322766, 318.51060943, 268.61541097,\n",
      "       272.76425551, 315.14139906])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([348.59105903, 256.83218886, 258.63997842, 265.18095026,\n",
      "       255.64151055, 373.23511922, 357.72595578, 254.37809822,\n",
      "       325.97651154, 427.7355496 , 405.77162346, 261.78093214,\n",
      "       257.79632772, 359.61398809, 300.97891002, 323.07814374,\n",
      "       360.51164299, 286.45978574, 296.09527034, 152.42525801])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([241.70231579, 247.8262009 , 261.74448658, 324.40308386,\n",
      "       242.94621212, 176.65754742, 154.29002952, 153.07560126,\n",
      "       191.31567738, 228.94610511, 238.09168848, 267.12643163,\n",
      "       239.73651743, 296.96554321, 241.19461979, 358.19748099,\n",
      "       314.38420433, 205.25786975, 159.52202019, 227.55789279,\n",
      "       224.56358967, 261.6293727 , 269.64979598, 241.11349397,\n",
      "       191.94529955, 141.51864415, 178.65095344, 264.60280642,\n",
      "       264.23905845, 183.7540556 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([223.49117695, 260.65355218, 206.39405506, 233.54247196,\n",
      "       248.43310869, 190.08114942, 240.53880393, 280.09782758,\n",
      "       315.35893007, 327.78137488, 209.29589023, 180.7943296 ,\n",
      "       256.05134529, 287.62799663, 293.2503612 , 461.66825185,\n",
      "       371.01366887, 340.76572611, 337.21491073, 319.24262795])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([258.63997842, 265.18095026, 255.64151055, 373.23511922,\n",
      "       357.72595578, 254.37809822, 325.97651154, 427.7355496 ,\n",
      "       405.77162346, 261.78093214, 257.79632772, 359.61398809,\n",
      "       300.97891002, 323.07814374, 360.51164299, 286.45978574,\n",
      "       296.09527034, 152.42525801, 197.44811645, 274.68819015,\n",
      "       247.35305589, 374.20690796, 282.19155374, 275.74591981,\n",
      "       318.92733395, 398.74035381, 286.78178474, 360.52363199,\n",
      "       351.88463431, 285.26129367])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([335.01022329, 350.24709861, 309.63551757, 327.15226648,\n",
      "       317.45700632, 335.20605714, 373.26065529, 433.43357323,\n",
      "       315.82157975, 427.89093968, 289.63949291, 301.77052434,\n",
      "       367.48165357, 372.13463675, 324.83322363, 327.12874953,\n",
      "       320.05395004, 310.90629308, 380.879735  , 387.76152403])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([268.61541097, 272.76425551, 315.14139906, 348.59105903,\n",
      "       256.83218886, 258.63997842, 265.18095026, 255.64151055,\n",
      "       373.23511922, 357.72595578, 254.37809822, 325.97651154,\n",
      "       427.7355496 , 405.77162346, 261.78093214, 257.79632772,\n",
      "       359.61398809, 300.97891002, 323.07814374, 360.51164299,\n",
      "       286.45978574, 296.09527034, 152.42525801, 197.44811645,\n",
      "       274.68819015, 247.35305589, 374.20690796, 282.19155374,\n",
      "       275.74591981, 318.92733395])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([398.74035381, 286.78178474, 360.52363199, 351.88463431,\n",
      "       285.26129367, 335.01022329, 350.24709861, 309.63551757,\n",
      "       327.15226648, 317.45700632, 335.20605714, 373.26065529,\n",
      "       433.43357323, 315.82157975, 427.89093968, 289.63949291,\n",
      "       301.77052434, 367.48165357, 372.13463675, 324.83322363])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([252.91959566, 378.32322455, 361.8077385 , 206.60592959,\n",
      "       258.34179417, 255.41322766, 318.51060943, 268.61541097,\n",
      "       272.76425551, 315.14139906, 348.59105903, 256.83218886,\n",
      "       258.63997842, 265.18095026, 255.64151055, 373.23511922,\n",
      "       357.72595578, 254.37809822, 325.97651154, 427.7355496 ,\n",
      "       405.77162346, 261.78093214, 257.79632772, 359.61398809,\n",
      "       300.97891002, 323.07814374, 360.51164299, 286.45978574,\n",
      "       296.09527034, 152.42525801])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([197.44811645, 274.68819015, 247.35305589, 374.20690796,\n",
      "       282.19155374, 275.74591981, 318.92733395, 398.74035381,\n",
      "       286.78178474, 360.52363199, 351.88463431, 285.26129367,\n",
      "       335.01022329, 350.24709861, 309.63551757, 327.15226648,\n",
      "       317.45700632, 335.20605714, 373.26065529, 433.43357323])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([255.88545794, 278.47209267, 287.98710364, 252.91959566,\n",
      "       378.32322455, 361.8077385 , 206.60592959, 258.34179417,\n",
      "       255.41322766, 318.51060943, 268.61541097, 272.76425551,\n",
      "       315.14139906, 348.59105903, 256.83218886, 258.63997842,\n",
      "       265.18095026, 255.64151055, 373.23511922, 357.72595578,\n",
      "       254.37809822, 325.97651154, 427.7355496 , 405.77162346,\n",
      "       261.78093214, 257.79632772, 359.61398809, 300.97891002,\n",
      "       323.07814374, 360.51164299])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([286.45978574, 296.09527034, 152.42525801, 197.44811645,\n",
      "       274.68819015, 247.35305589, 374.20690796, 282.19155374,\n",
      "       275.74591981, 318.92733395, 398.74035381, 286.78178474,\n",
      "       360.52363199, 351.88463431, 285.26129367, 335.01022329,\n",
      "       350.24709861, 309.63551757, 327.15226648, 317.45700632])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([227.87519988, 220.79051407, 295.02199764, 255.88545794,\n",
      "       278.47209267, 287.98710364, 252.91959566, 378.32322455,\n",
      "       361.8077385 , 206.60592959, 258.34179417, 255.41322766,\n",
      "       318.51060943, 268.61541097, 272.76425551, 315.14139906,\n",
      "       348.59105903, 256.83218886, 258.63997842, 265.18095026,\n",
      "       255.64151055, 373.23511922, 357.72595578, 254.37809822,\n",
      "       325.97651154, 427.7355496 , 405.77162346, 261.78093214,\n",
      "       257.79632772, 359.61398809])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([300.97891002, 323.07814374, 360.51164299, 286.45978574,\n",
      "       296.09527034, 152.42525801, 197.44811645, 274.68819015,\n",
      "       247.35305589, 374.20690796, 282.19155374, 275.74591981,\n",
      "       318.92733395, 398.74035381, 286.78178474, 360.52363199,\n",
      "       351.88463431, 285.26129367, 335.01022329, 350.24709861])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([318.51060943, 268.61541097, 272.76425551, 315.14139906,\n",
      "       348.59105903, 256.83218886, 258.63997842, 265.18095026,\n",
      "       255.64151055, 373.23511922, 357.72595578, 254.37809822,\n",
      "       325.97651154, 427.7355496 , 405.77162346, 261.78093214,\n",
      "       257.79632772, 359.61398809, 300.97891002, 323.07814374,\n",
      "       360.51164299, 286.45978574, 296.09527034, 152.42525801,\n",
      "       197.44811645, 274.68819015, 247.35305589, 374.20690796,\n",
      "       282.19155374, 275.74591981])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([318.92733395, 398.74035381, 286.78178474, 360.52363199,\n",
      "       351.88463431, 285.26129367, 335.01022329, 350.24709861,\n",
      "       309.63551757, 327.15226648, 317.45700632, 335.20605714,\n",
      "       373.26065529, 433.43357323, 315.82157975, 427.89093968,\n",
      "       289.63949291, 301.77052434, 367.48165357, 372.13463675])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([103.79331492, 104.97248027,  51.30083837, 116.82936428,\n",
      "       150.26046624, 142.49521639, 127.19622084,  63.7869869 ,\n",
      "        88.21464394, 110.41550736,  92.38406199, 116.29604032,\n",
      "       157.91140713, 246.51495725, 196.23187143, 170.08294304,\n",
      "       152.27811061,  53.64930572, 108.24262341, 147.88143388,\n",
      "       274.83401078, 196.4136086 , 173.2665378 , 161.73243664,\n",
      "        99.10058533, 189.91445947, 217.03019629, 218.01662357,\n",
      "       135.62151463, 215.66343029])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([121.17746619, 167.28600356, 283.28864689, 164.69125998,\n",
      "       125.10364505, 159.87748372, 144.34205834,  82.29692285,\n",
      "       140.1662113 , 115.85726673, 173.41590414, 135.43190224,\n",
      "       233.88614205, 168.22234072, 149.37417651, 211.20263936,\n",
      "       134.80757158, 168.40259944, 248.81801436, 131.08170877])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([164.69125998, 125.10364505, 159.87748372, 144.34205834,\n",
      "        82.29692285, 140.1662113 , 115.85726673, 173.41590414,\n",
      "       135.43190224, 233.88614205, 168.22234072, 149.37417651,\n",
      "       211.20263936, 134.80757158, 168.40259944, 248.81801436,\n",
      "       131.08170877, 163.32682593, 198.28054145, 230.55995397,\n",
      "       143.44437565,  99.18040759, 184.29382082, 211.78528286,\n",
      "       211.13288375, 216.64507465, 168.69752245, 193.57349357,\n",
      "       214.00622395, 168.32951   ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([277.04961628, 260.96146028, 156.82804628, 211.79379382,\n",
      "       165.25249826, 223.55048858, 276.7108538 , 191.76163782,\n",
      "       241.70231579, 247.8262009 , 261.74448658, 324.40308386,\n",
      "       242.94621212, 176.65754742, 154.29002952, 153.07560126,\n",
      "       191.31567738, 228.94610511, 238.09168848, 267.12643163])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([156.82804628, 211.79379382, 165.25249826, 223.55048858,\n",
      "       276.7108538 , 191.76163782, 241.70231579, 247.8262009 ,\n",
      "       261.74448658, 324.40308386, 242.94621212, 176.65754742,\n",
      "       154.29002952, 153.07560126, 191.31567738, 228.94610511,\n",
      "       238.09168848, 267.12643163, 239.73651743, 296.96554321,\n",
      "       241.19461979, 358.19748099, 314.38420433, 205.25786975,\n",
      "       159.52202019, 227.55789279, 224.56358967, 261.6293727 ,\n",
      "       269.64979598, 241.11349397])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([191.94529955, 141.51864415, 178.65095344, 264.60280642,\n",
      "       264.23905845, 183.7540556 , 223.49117695, 260.65355218,\n",
      "       206.39405506, 233.54247196, 248.43310869, 190.08114942,\n",
      "       240.53880393, 280.09782758, 315.35893007, 327.78137488,\n",
      "       209.29589023, 180.7943296 , 256.05134529, 287.62799663])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([209.29589023, 180.7943296 , 256.05134529, 287.62799663,\n",
      "       293.2503612 , 461.66825185, 371.01366887, 340.76572611,\n",
      "       337.21491073, 319.24262795, 265.1709739 , 298.00957865,\n",
      "       242.49635296, 240.13836305, 236.14417874, 260.83591413,\n",
      "       384.60274617, 226.89460208, 273.27294491, 203.17066281,\n",
      "       217.19312001, 314.43807321, 300.01414871, 227.87519988,\n",
      "       220.79051407, 295.02199764, 255.88545794, 278.47209267,\n",
      "       287.98710364, 252.91959566])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([378.32322455, 361.8077385 , 206.60592959, 258.34179417,\n",
      "       255.41322766, 318.51060943, 268.61541097, 272.76425551,\n",
      "       315.14139906, 348.59105903, 256.83218886, 258.63997842,\n",
      "       265.18095026, 255.64151055, 373.23511922, 357.72595578,\n",
      "       254.37809822, 325.97651154, 427.7355496 , 405.77162346])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([178.65095344, 264.60280642, 264.23905845, 183.7540556 ,\n",
      "       223.49117695, 260.65355218, 206.39405506, 233.54247196,\n",
      "       248.43310869, 190.08114942, 240.53880393, 280.09782758,\n",
      "       315.35893007, 327.78137488, 209.29589023, 180.7943296 ,\n",
      "       256.05134529, 287.62799663, 293.2503612 , 461.66825185,\n",
      "       371.01366887, 340.76572611, 337.21491073, 319.24262795,\n",
      "       265.1709739 , 298.00957865, 242.49635296, 240.13836305,\n",
      "       236.14417874, 260.83591413])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([384.60274617, 226.89460208, 273.27294491, 203.17066281,\n",
      "       217.19312001, 314.43807321, 300.01414871, 227.87519988,\n",
      "       220.79051407, 295.02199764, 255.88545794, 278.47209267,\n",
      "       287.98710364, 252.91959566, 378.32322455, 361.8077385 ,\n",
      "       206.60592959, 258.34179417, 255.41322766, 318.51060943])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([287.62799663, 293.2503612 , 461.66825185, 371.01366887,\n",
      "       340.76572611, 337.21491073, 319.24262795, 265.1709739 ,\n",
      "       298.00957865, 242.49635296, 240.13836305, 236.14417874,\n",
      "       260.83591413, 384.60274617, 226.89460208, 273.27294491,\n",
      "       203.17066281, 217.19312001, 314.43807321, 300.01414871,\n",
      "       227.87519988, 220.79051407, 295.02199764, 255.88545794,\n",
      "       278.47209267, 287.98710364, 252.91959566, 378.32322455,\n",
      "       361.8077385 , 206.60592959])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([258.34179417, 255.41322766, 318.51060943, 268.61541097,\n",
      "       272.76425551, 315.14139906, 348.59105903, 256.83218886,\n",
      "       258.63997842, 265.18095026, 255.64151055, 373.23511922,\n",
      "       357.72595578, 254.37809822, 325.97651154, 427.7355496 ,\n",
      "       405.77162346, 261.78093214, 257.79632772, 359.61398809])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([191.76163782, 241.70231579, 247.8262009 , 261.74448658,\n",
      "       324.40308386, 242.94621212, 176.65754742, 154.29002952,\n",
      "       153.07560126, 191.31567738, 228.94610511, 238.09168848,\n",
      "       267.12643163, 239.73651743, 296.96554321, 241.19461979,\n",
      "       358.19748099, 314.38420433, 205.25786975, 159.52202019,\n",
      "       227.55789279, 224.56358967, 261.6293727 , 269.64979598,\n",
      "       241.11349397, 191.94529955, 141.51864415, 178.65095344,\n",
      "       264.60280642, 264.23905845])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([183.7540556 , 223.49117695, 260.65355218, 206.39405506,\n",
      "       233.54247196, 248.43310869, 190.08114942, 240.53880393,\n",
      "       280.09782758, 315.35893007, 327.78137488, 209.29589023,\n",
      "       180.7943296 , 256.05134529, 287.62799663, 293.2503612 ,\n",
      "       461.66825185, 371.01366887, 340.76572611, 337.21491073])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([359.61398809, 300.97891002, 323.07814374, 360.51164299,\n",
      "       286.45978574, 296.09527034, 152.42525801, 197.44811645,\n",
      "       274.68819015, 247.35305589, 374.20690796, 282.19155374,\n",
      "       275.74591981, 318.92733395, 398.74035381, 286.78178474,\n",
      "       360.52363199, 351.88463431, 285.26129367, 335.01022329,\n",
      "       350.24709861, 309.63551757, 327.15226648, 317.45700632,\n",
      "       335.20605714, 373.26065529, 433.43357323, 315.82157975,\n",
      "       427.89093968, 289.63949291])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([301.77052434, 367.48165357, 372.13463675, 324.83322363,\n",
      "       327.12874953, 320.05395004, 310.90629308, 380.879735  ,\n",
      "       387.76152403, 330.557742  , 388.11743607, 390.37958736,\n",
      "       408.2225002 , 410.23820866, 336.26542706, 319.41677054,\n",
      "       386.73270881, 408.68045252, 379.54368873, 374.68737687])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([108.24262341, 147.88143388, 274.83401078, 196.4136086 ,\n",
      "       173.2665378 , 161.73243664,  99.10058533, 189.91445947,\n",
      "       217.03019629, 218.01662357, 135.62151463, 215.66343029,\n",
      "       121.17746619, 167.28600356, 283.28864689, 164.69125998,\n",
      "       125.10364505, 159.87748372, 144.34205834,  82.29692285,\n",
      "       140.1662113 , 115.85726673, 173.41590414, 135.43190224,\n",
      "       233.88614205, 168.22234072, 149.37417651, 211.20263936,\n",
      "       134.80757158, 168.40259944])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([248.81801436, 131.08170877, 163.32682593, 198.28054145,\n",
      "       230.55995397, 143.44437565,  99.18040759, 184.29382082,\n",
      "       211.78528286, 211.13288375, 216.64507465, 168.69752245,\n",
      "       193.57349357, 214.00622395, 168.32951   , 277.04961628,\n",
      "       260.96146028, 156.82804628, 211.79379382, 165.25249826])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([242.49635296, 240.13836305, 236.14417874, 260.83591413,\n",
      "       384.60274617, 226.89460208, 273.27294491, 203.17066281,\n",
      "       217.19312001, 314.43807321, 300.01414871, 227.87519988,\n",
      "       220.79051407, 295.02199764, 255.88545794, 278.47209267,\n",
      "       287.98710364, 252.91959566, 378.32322455, 361.8077385 ,\n",
      "       206.60592959, 258.34179417, 255.41322766, 318.51060943,\n",
      "       268.61541097, 272.76425551, 315.14139906, 348.59105903,\n",
      "       256.83218886, 258.63997842])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([265.18095026, 255.64151055, 373.23511922, 357.72595578,\n",
      "       254.37809822, 325.97651154, 427.7355496 , 405.77162346,\n",
      "       261.78093214, 257.79632772, 359.61398809, 300.97891002,\n",
      "       323.07814374, 360.51164299, 286.45978574, 296.09527034,\n",
      "       152.42525801, 197.44811645, 274.68819015, 247.35305589])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([296.96554321, 241.19461979, 358.19748099, 314.38420433,\n",
      "       205.25786975, 159.52202019, 227.55789279, 224.56358967,\n",
      "       261.6293727 , 269.64979598, 241.11349397, 191.94529955,\n",
      "       141.51864415, 178.65095344, 264.60280642, 264.23905845,\n",
      "       183.7540556 , 223.49117695, 260.65355218, 206.39405506,\n",
      "       233.54247196, 248.43310869, 190.08114942, 240.53880393,\n",
      "       280.09782758, 315.35893007, 327.78137488, 209.29589023,\n",
      "       180.7943296 , 256.05134529])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([287.62799663, 293.2503612 , 461.66825185, 371.01366887,\n",
      "       340.76572611, 337.21491073, 319.24262795, 265.1709739 ,\n",
      "       298.00957865, 242.49635296, 240.13836305, 236.14417874,\n",
      "       260.83591413, 384.60274617, 226.89460208, 273.27294491,\n",
      "       203.17066281, 217.19312001, 314.43807321, 300.01414871])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([314.38420433, 205.25786975, 159.52202019, 227.55789279,\n",
      "       224.56358967, 261.6293727 , 269.64979598, 241.11349397,\n",
      "       191.94529955, 141.51864415, 178.65095344, 264.60280642,\n",
      "       264.23905845, 183.7540556 , 223.49117695, 260.65355218,\n",
      "       206.39405506, 233.54247196, 248.43310869, 190.08114942,\n",
      "       240.53880393, 280.09782758, 315.35893007, 327.78137488,\n",
      "       209.29589023, 180.7943296 , 256.05134529, 287.62799663,\n",
      "       293.2503612 , 461.66825185])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([371.01366887, 340.76572611, 337.21491073, 319.24262795,\n",
      "       265.1709739 , 298.00957865, 242.49635296, 240.13836305,\n",
      "       236.14417874, 260.83591413, 384.60274617, 226.89460208,\n",
      "       273.27294491, 203.17066281, 217.19312001, 314.43807321,\n",
      "       300.01414871, 227.87519988, 220.79051407, 295.02199764])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([248.43310869, 190.08114942, 240.53880393, 280.09782758,\n",
      "       315.35893007, 327.78137488, 209.29589023, 180.7943296 ,\n",
      "       256.05134529, 287.62799663, 293.2503612 , 461.66825185,\n",
      "       371.01366887, 340.76572611, 337.21491073, 319.24262795,\n",
      "       265.1709739 , 298.00957865, 242.49635296, 240.13836305,\n",
      "       236.14417874, 260.83591413, 384.60274617, 226.89460208,\n",
      "       273.27294491, 203.17066281, 217.19312001, 314.43807321,\n",
      "       300.01414871, 227.87519988])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([220.79051407, 295.02199764, 255.88545794, 278.47209267,\n",
      "       287.98710364, 252.91959566, 378.32322455, 361.8077385 ,\n",
      "       206.60592959, 258.34179417, 255.41322766, 318.51060943,\n",
      "       268.61541097, 272.76425551, 315.14139906, 348.59105903,\n",
      "       256.83218886, 258.63997842, 265.18095026, 255.64151055])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([191.94529955, 141.51864415, 178.65095344, 264.60280642,\n",
      "       264.23905845, 183.7540556 , 223.49117695, 260.65355218,\n",
      "       206.39405506, 233.54247196, 248.43310869, 190.08114942,\n",
      "       240.53880393, 280.09782758, 315.35893007, 327.78137488,\n",
      "       209.29589023, 180.7943296 , 256.05134529, 287.62799663,\n",
      "       293.2503612 , 461.66825185, 371.01366887, 340.76572611,\n",
      "       337.21491073, 319.24262795, 265.1709739 , 298.00957865,\n",
      "       242.49635296, 240.13836305])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([236.14417874, 260.83591413, 384.60274617, 226.89460208,\n",
      "       273.27294491, 203.17066281, 217.19312001, 314.43807321,\n",
      "       300.01414871, 227.87519988, 220.79051407, 295.02199764,\n",
      "       255.88545794, 278.47209267, 287.98710364, 252.91959566,\n",
      "       378.32322455, 361.8077385 , 206.60592959, 258.34179417])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([254.37809822, 325.97651154, 427.7355496 , 405.77162346,\n",
      "       261.78093214, 257.79632772, 359.61398809, 300.97891002,\n",
      "       323.07814374, 360.51164299, 286.45978574, 296.09527034,\n",
      "       152.42525801, 197.44811645, 274.68819015, 247.35305589,\n",
      "       374.20690796, 282.19155374, 275.74591981, 318.92733395,\n",
      "       398.74035381, 286.78178474, 360.52363199, 351.88463431,\n",
      "       285.26129367, 335.01022329, 350.24709861, 309.63551757,\n",
      "       327.15226648, 317.45700632])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([335.20605714, 373.26065529, 433.43357323, 315.82157975,\n",
      "       427.89093968, 289.63949291, 301.77052434, 367.48165357,\n",
      "       372.13463675, 324.83322363, 327.12874953, 320.05395004,\n",
      "       310.90629308, 380.879735  , 387.76152403, 330.557742  ,\n",
      "       388.11743607, 390.37958736, 408.2225002 , 410.23820866])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([163.32682593, 198.28054145, 230.55995397, 143.44437565,\n",
      "        99.18040759, 184.29382082, 211.78528286, 211.13288375,\n",
      "       216.64507465, 168.69752245, 193.57349357, 214.00622395,\n",
      "       168.32951   , 277.04961628, 260.96146028, 156.82804628,\n",
      "       211.79379382, 165.25249826, 223.55048858, 276.7108538 ,\n",
      "       191.76163782, 241.70231579, 247.8262009 , 261.74448658,\n",
      "       324.40308386, 242.94621212, 176.65754742, 154.29002952,\n",
      "       153.07560126, 191.31567738])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([228.94610511, 238.09168848, 267.12643163, 239.73651743,\n",
      "       296.96554321, 241.19461979, 358.19748099, 314.38420433,\n",
      "       205.25786975, 159.52202019, 227.55789279, 224.56358967,\n",
      "       261.6293727 , 269.64979598, 241.11349397, 191.94529955,\n",
      "       141.51864415, 178.65095344, 264.60280642, 264.23905845])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([461.66825185, 371.01366887, 340.76572611, 337.21491073,\n",
      "       319.24262795, 265.1709739 , 298.00957865, 242.49635296,\n",
      "       240.13836305, 236.14417874, 260.83591413, 384.60274617,\n",
      "       226.89460208, 273.27294491, 203.17066281, 217.19312001,\n",
      "       314.43807321, 300.01414871, 227.87519988, 220.79051407,\n",
      "       295.02199764, 255.88545794, 278.47209267, 287.98710364,\n",
      "       252.91959566, 378.32322455, 361.8077385 , 206.60592959,\n",
      "       258.34179417, 255.41322766])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([318.51060943, 268.61541097, 272.76425551, 315.14139906,\n",
      "       348.59105903, 256.83218886, 258.63997842, 265.18095026,\n",
      "       255.64151055, 373.23511922, 357.72595578, 254.37809822,\n",
      "       325.97651154, 427.7355496 , 405.77162346, 261.78093214,\n",
      "       257.79632772, 359.61398809, 300.97891002, 323.07814374])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([106.98454433, 155.1638923 , 163.89514009,  76.87066266,\n",
      "        66.65283723, 105.2879106 , 152.28966719,  96.70344248,\n",
      "        83.77352599,  41.23114256,  17.99426911, 114.11325318,\n",
      "       184.04522052, 131.63722049, 158.90144505, 147.21045867,\n",
      "        86.83575307, 115.00959466, 192.5315456 , 142.60208615,\n",
      "       193.93866008,  16.93172331, 103.93682066, 129.66886997,\n",
      "       103.55737712, 115.40403437,  19.93344086,  65.42368531,\n",
      "       126.55694824, 199.66504437])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([128.07629968,  76.25036135,  81.087672  , 156.95619511,\n",
      "       159.44456572, 109.77291327, 143.66763433, 143.35571794,\n",
      "       181.43706483, 117.23990033, 103.79331492, 104.97248027,\n",
      "        51.30083837, 116.82936428, 150.26046624, 142.49521639,\n",
      "       127.19622084,  63.7869869 ,  88.21464394, 110.41550736])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([152.27811061,  53.64930572, 108.24262341, 147.88143388,\n",
      "       274.83401078, 196.4136086 , 173.2665378 , 161.73243664,\n",
      "        99.10058533, 189.91445947, 217.03019629, 218.01662357,\n",
      "       135.62151463, 215.66343029, 121.17746619, 167.28600356,\n",
      "       283.28864689, 164.69125998, 125.10364505, 159.87748372,\n",
      "       144.34205834,  82.29692285, 140.1662113 , 115.85726673,\n",
      "       173.41590414, 135.43190224, 233.88614205, 168.22234072,\n",
      "       149.37417651, 211.20263936])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([134.80757158, 168.40259944, 248.81801436, 131.08170877,\n",
      "       163.32682593, 198.28054145, 230.55995397, 143.44437565,\n",
      "        99.18040759, 184.29382082, 211.78528286, 211.13288375,\n",
      "       216.64507465, 168.69752245, 193.57349357, 214.00622395,\n",
      "       168.32951   , 277.04961628, 260.96146028, 156.82804628])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([264.23905845, 183.7540556 , 223.49117695, 260.65355218,\n",
      "       206.39405506, 233.54247196, 248.43310869, 190.08114942,\n",
      "       240.53880393, 280.09782758, 315.35893007, 327.78137488,\n",
      "       209.29589023, 180.7943296 , 256.05134529, 287.62799663,\n",
      "       293.2503612 , 461.66825185, 371.01366887, 340.76572611,\n",
      "       337.21491073, 319.24262795, 265.1709739 , 298.00957865,\n",
      "       242.49635296, 240.13836305, 236.14417874, 260.83591413,\n",
      "       384.60274617, 226.89460208])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([273.27294491, 203.17066281, 217.19312001, 314.43807321,\n",
      "       300.01414871, 227.87519988, 220.79051407, 295.02199764,\n",
      "       255.88545794, 278.47209267, 287.98710364, 252.91959566,\n",
      "       378.32322455, 361.8077385 , 206.60592959, 258.34179417,\n",
      "       255.41322766, 318.51060943, 268.61541097, 272.76425551])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([287.98710364, 252.91959566, 378.32322455, 361.8077385 ,\n",
      "       206.60592959, 258.34179417, 255.41322766, 318.51060943,\n",
      "       268.61541097, 272.76425551, 315.14139906, 348.59105903,\n",
      "       256.83218886, 258.63997842, 265.18095026, 255.64151055,\n",
      "       373.23511922, 357.72595578, 254.37809822, 325.97651154,\n",
      "       427.7355496 , 405.77162346, 261.78093214, 257.79632772,\n",
      "       359.61398809, 300.97891002, 323.07814374, 360.51164299,\n",
      "       286.45978574, 296.09527034])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([152.42525801, 197.44811645, 274.68819015, 247.35305589,\n",
      "       374.20690796, 282.19155374, 275.74591981, 318.92733395,\n",
      "       398.74035381, 286.78178474, 360.52363199, 351.88463431,\n",
      "       285.26129367, 335.01022329, 350.24709861, 309.63551757,\n",
      "       327.15226648, 317.45700632, 335.20605714, 373.26065529])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([-69.86728076, -13.39040036, -11.09627168,  49.64498689,\n",
      "        17.47115024, -28.80375993, 101.72731666,  77.58717368,\n",
      "        65.46366834,  -4.8640219 ,   8.09607209,  55.7317209 ,\n",
      "         9.41582456,  61.83334275,  43.3839532 ,  42.79186531,\n",
      "        32.78361335, 149.68041012, 109.27425031,  25.89407055,\n",
      "        94.10193173,  29.1604104 ,  62.45920462, -19.259528  ,\n",
      "       -30.02332054,  54.25897839, 115.20075505, 104.4155928 ,\n",
      "        81.90616437,  66.43128026])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([  3.62500397,  16.29659574,  42.40312683, 125.84206218,\n",
      "       124.86800076,  10.80211719,  69.67090736,  73.00015151,\n",
      "        49.1362523 , 106.98454433, 155.1638923 , 163.89514009,\n",
      "        76.87066266,  66.65283723, 105.2879106 , 152.28966719,\n",
      "        96.70344248,  83.77352599,  41.23114256,  17.99426911])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 88.21464394, 110.41550736,  92.38406199, 116.29604032,\n",
      "       157.91140713, 246.51495725, 196.23187143, 170.08294304,\n",
      "       152.27811061,  53.64930572, 108.24262341, 147.88143388,\n",
      "       274.83401078, 196.4136086 , 173.2665378 , 161.73243664,\n",
      "        99.10058533, 189.91445947, 217.03019629, 218.01662357,\n",
      "       135.62151463, 215.66343029, 121.17746619, 167.28600356,\n",
      "       283.28864689, 164.69125998, 125.10364505, 159.87748372,\n",
      "       144.34205834,  82.29692285])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([140.1662113 , 115.85726673, 173.41590414, 135.43190224,\n",
      "       233.88614205, 168.22234072, 149.37417651, 211.20263936,\n",
      "       134.80757158, 168.40259944, 248.81801436, 131.08170877,\n",
      "       163.32682593, 198.28054145, 230.55995397, 143.44437565,\n",
      "        99.18040759, 184.29382082, 211.78528286, 211.13288375])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([144.34205834,  82.29692285, 140.1662113 , 115.85726673,\n",
      "       173.41590414, 135.43190224, 233.88614205, 168.22234072,\n",
      "       149.37417651, 211.20263936, 134.80757158, 168.40259944,\n",
      "       248.81801436, 131.08170877, 163.32682593, 198.28054145,\n",
      "       230.55995397, 143.44437565,  99.18040759, 184.29382082,\n",
      "       211.78528286, 211.13288375, 216.64507465, 168.69752245,\n",
      "       193.57349357, 214.00622395, 168.32951   , 277.04961628,\n",
      "       260.96146028, 156.82804628])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([211.79379382, 165.25249826, 223.55048858, 276.7108538 ,\n",
      "       191.76163782, 241.70231579, 247.8262009 , 261.74448658,\n",
      "       324.40308386, 242.94621212, 176.65754742, 154.29002952,\n",
      "       153.07560126, 191.31567738, 228.94610511, 238.09168848,\n",
      "       267.12643163, 239.73651743, 296.96554321, 241.19461979])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([115.40403437,  19.93344086,  65.42368531, 126.55694824,\n",
      "       199.66504437, 128.07629968,  76.25036135,  81.087672  ,\n",
      "       156.95619511, 159.44456572, 109.77291327, 143.66763433,\n",
      "       143.35571794, 181.43706483, 117.23990033, 103.79331492,\n",
      "       104.97248027,  51.30083837, 116.82936428, 150.26046624,\n",
      "       142.49521639, 127.19622084,  63.7869869 ,  88.21464394,\n",
      "       110.41550736,  92.38406199, 116.29604032, 157.91140713,\n",
      "       246.51495725, 196.23187143])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([170.08294304, 152.27811061,  53.64930572, 108.24262341,\n",
      "       147.88143388, 274.83401078, 196.4136086 , 173.2665378 ,\n",
      "       161.73243664,  99.10058533, 189.91445947, 217.03019629,\n",
      "       218.01662357, 135.62151463, 215.66343029, 121.17746619,\n",
      "       167.28600356, 283.28864689, 164.69125998, 125.10364505])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([327.78137488, 209.29589023, 180.7943296 , 256.05134529,\n",
      "       287.62799663, 293.2503612 , 461.66825185, 371.01366887,\n",
      "       340.76572611, 337.21491073, 319.24262795, 265.1709739 ,\n",
      "       298.00957865, 242.49635296, 240.13836305, 236.14417874,\n",
      "       260.83591413, 384.60274617, 226.89460208, 273.27294491,\n",
      "       203.17066281, 217.19312001, 314.43807321, 300.01414871,\n",
      "       227.87519988, 220.79051407, 295.02199764, 255.88545794,\n",
      "       278.47209267, 287.98710364])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([252.91959566, 378.32322455, 361.8077385 , 206.60592959,\n",
      "       258.34179417, 255.41322766, 318.51060943, 268.61541097,\n",
      "       272.76425551, 315.14139906, 348.59105903, 256.83218886,\n",
      "       258.63997842, 265.18095026, 255.64151055, 373.23511922,\n",
      "       357.72595578, 254.37809822, 325.97651154, 427.7355496 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([230.55995397, 143.44437565,  99.18040759, 184.29382082,\n",
      "       211.78528286, 211.13288375, 216.64507465, 168.69752245,\n",
      "       193.57349357, 214.00622395, 168.32951   , 277.04961628,\n",
      "       260.96146028, 156.82804628, 211.79379382, 165.25249826,\n",
      "       223.55048858, 276.7108538 , 191.76163782, 241.70231579,\n",
      "       247.8262009 , 261.74448658, 324.40308386, 242.94621212,\n",
      "       176.65754742, 154.29002952, 153.07560126, 191.31567738,\n",
      "       228.94610511, 238.09168848])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([267.12643163, 239.73651743, 296.96554321, 241.19461979,\n",
      "       358.19748099, 314.38420433, 205.25786975, 159.52202019,\n",
      "       227.55789279, 224.56358967, 261.6293727 , 269.64979598,\n",
      "       241.11349397, 191.94529955, 141.51864415, 178.65095344,\n",
      "       264.60280642, 264.23905845, 183.7540556 , 223.49117695])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([360.52363199, 351.88463431, 285.26129367, 335.01022329,\n",
      "       350.24709861, 309.63551757, 327.15226648, 317.45700632,\n",
      "       335.20605714, 373.26065529, 433.43357323, 315.82157975,\n",
      "       427.89093968, 289.63949291, 301.77052434, 367.48165357,\n",
      "       372.13463675, 324.83322363, 327.12874953, 320.05395004,\n",
      "       310.90629308, 380.879735  , 387.76152403, 330.557742  ,\n",
      "       388.11743607, 390.37958736, 408.2225002 , 410.23820866,\n",
      "       336.26542706, 319.41677054])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([386.73270881, 408.68045252, 379.54368873, 374.68737687,\n",
      "       434.71295722, 367.58405072, 389.15715379, 372.35580464,\n",
      "       359.08299146, 423.66672844, 438.84670817, 437.61444311,\n",
      "       463.16543088, 410.89882285, 419.07923581, 381.30008063,\n",
      "       394.0134984 , 383.27079237, 387.32550047, 417.0279523 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([205.25786975, 159.52202019, 227.55789279, 224.56358967,\n",
      "       261.6293727 , 269.64979598, 241.11349397, 191.94529955,\n",
      "       141.51864415, 178.65095344, 264.60280642, 264.23905845,\n",
      "       183.7540556 , 223.49117695, 260.65355218, 206.39405506,\n",
      "       233.54247196, 248.43310869, 190.08114942, 240.53880393,\n",
      "       280.09782758, 315.35893007, 327.78137488, 209.29589023,\n",
      "       180.7943296 , 256.05134529, 287.62799663, 293.2503612 ,\n",
      "       461.66825185, 371.01366887])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([340.76572611, 337.21491073, 319.24262795, 265.1709739 ,\n",
      "       298.00957865, 242.49635296, 240.13836305, 236.14417874,\n",
      "       260.83591413, 384.60274617, 226.89460208, 273.27294491,\n",
      "       203.17066281, 217.19312001, 314.43807321, 300.01414871,\n",
      "       227.87519988, 220.79051407, 295.02199764, 255.88545794])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([101.72731666,  77.58717368,  65.46366834,  -4.8640219 ,\n",
      "         8.09607209,  55.7317209 ,   9.41582456,  61.83334275,\n",
      "        43.3839532 ,  42.79186531,  32.78361335, 149.68041012,\n",
      "       109.27425031,  25.89407055,  94.10193173,  29.1604104 ,\n",
      "        62.45920462, -19.259528  , -30.02332054,  54.25897839,\n",
      "       115.20075505, 104.4155928 ,  81.90616437,  66.43128026,\n",
      "         3.62500397,  16.29659574,  42.40312683, 125.84206218,\n",
      "       124.86800076,  10.80211719])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 69.67090736,  73.00015151,  49.1362523 , 106.98454433,\n",
      "       155.1638923 , 163.89514009,  76.87066266,  66.65283723,\n",
      "       105.2879106 , 152.28966719,  96.70344248,  83.77352599,\n",
      "        41.23114256,  17.99426911, 114.11325318, 184.04522052,\n",
      "       131.63722049, 158.90144505, 147.21045867,  86.83575307])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([373.26065529, 433.43357323, 315.82157975, 427.89093968,\n",
      "       289.63949291, 301.77052434, 367.48165357, 372.13463675,\n",
      "       324.83322363, 327.12874953, 320.05395004, 310.90629308,\n",
      "       380.879735  , 387.76152403, 330.557742  , 388.11743607,\n",
      "       390.37958736, 408.2225002 , 410.23820866, 336.26542706,\n",
      "       319.41677054, 386.73270881, 408.68045252, 379.54368873,\n",
      "       374.68737687, 434.71295722, 367.58405072, 389.15715379,\n",
      "       372.35580464, 359.08299146])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([423.66672844, 438.84670817, 437.61444311, 463.16543088,\n",
      "       410.89882285, 419.07923581, 381.30008063, 394.0134984 ,\n",
      "       383.27079237, 387.32550047, 417.0279523 , 358.91573723,\n",
      "       476.45969901, 381.52225147, 314.31964945, 421.39247611,\n",
      "       452.38691493, 444.22925323, 442.59260105, 411.5736148 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([220.79051407, 295.02199764, 255.88545794, 278.47209267,\n",
      "       287.98710364, 252.91959566, 378.32322455, 361.8077385 ,\n",
      "       206.60592959, 258.34179417, 255.41322766, 318.51060943,\n",
      "       268.61541097, 272.76425551, 315.14139906, 348.59105903,\n",
      "       256.83218886, 258.63997842, 265.18095026, 255.64151055,\n",
      "       373.23511922, 357.72595578, 254.37809822, 325.97651154,\n",
      "       427.7355496 , 405.77162346, 261.78093214, 257.79632772,\n",
      "       359.61398809, 300.97891002])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([323.07814374, 360.51164299, 286.45978574, 296.09527034,\n",
      "       152.42525801, 197.44811645, 274.68819015, 247.35305589,\n",
      "       374.20690796, 282.19155374, 275.74591981, 318.92733395,\n",
      "       398.74035381, 286.78178474, 360.52363199, 351.88463431,\n",
      "       285.26129367, 335.01022329, 350.24709861, 309.63551757])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([189.91445947, 217.03019629, 218.01662357, 135.62151463,\n",
      "       215.66343029, 121.17746619, 167.28600356, 283.28864689,\n",
      "       164.69125998, 125.10364505, 159.87748372, 144.34205834,\n",
      "        82.29692285, 140.1662113 , 115.85726673, 173.41590414,\n",
      "       135.43190224, 233.88614205, 168.22234072, 149.37417651,\n",
      "       211.20263936, 134.80757158, 168.40259944, 248.81801436,\n",
      "       131.08170877, 163.32682593, 198.28054145, 230.55995397,\n",
      "       143.44437565,  99.18040759])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([184.29382082, 211.78528286, 211.13288375, 216.64507465,\n",
      "       168.69752245, 193.57349357, 214.00622395, 168.32951   ,\n",
      "       277.04961628, 260.96146028, 156.82804628, 211.79379382,\n",
      "       165.25249826, 223.55048858, 276.7108538 , 191.76163782,\n",
      "       241.70231579, 247.8262009 , 261.74448658, 324.40308386])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 77.58717368,  65.46366834,  -4.8640219 ,   8.09607209,\n",
      "        55.7317209 ,   9.41582456,  61.83334275,  43.3839532 ,\n",
      "        42.79186531,  32.78361335, 149.68041012, 109.27425031,\n",
      "        25.89407055,  94.10193173,  29.1604104 ,  62.45920462,\n",
      "       -19.259528  , -30.02332054,  54.25897839, 115.20075505,\n",
      "       104.4155928 ,  81.90616437,  66.43128026,   3.62500397,\n",
      "        16.29659574,  42.40312683, 125.84206218, 124.86800076,\n",
      "        10.80211719,  69.67090736])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 73.00015151,  49.1362523 , 106.98454433, 155.1638923 ,\n",
      "       163.89514009,  76.87066266,  66.65283723, 105.2879106 ,\n",
      "       152.28966719,  96.70344248,  83.77352599,  41.23114256,\n",
      "        17.99426911, 114.11325318, 184.04522052, 131.63722049,\n",
      "       158.90144505, 147.21045867,  86.83575307, 115.00959466])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([161.73243664,  99.10058533, 189.91445947, 217.03019629,\n",
      "       218.01662357, 135.62151463, 215.66343029, 121.17746619,\n",
      "       167.28600356, 283.28864689, 164.69125998, 125.10364505,\n",
      "       159.87748372, 144.34205834,  82.29692285, 140.1662113 ,\n",
      "       115.85726673, 173.41590414, 135.43190224, 233.88614205,\n",
      "       168.22234072, 149.37417651, 211.20263936, 134.80757158,\n",
      "       168.40259944, 248.81801436, 131.08170877, 163.32682593,\n",
      "       198.28054145, 230.55995397])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([143.44437565,  99.18040759, 184.29382082, 211.78528286,\n",
      "       211.13288375, 216.64507465, 168.69752245, 193.57349357,\n",
      "       214.00622395, 168.32951   , 277.04961628, 260.96146028,\n",
      "       156.82804628, 211.79379382, 165.25249826, 223.55048858,\n",
      "       276.7108538 , 191.76163782, 241.70231579, 247.8262009 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([327.12874953, 320.05395004, 310.90629308, 380.879735  ,\n",
      "       387.76152403, 330.557742  , 388.11743607, 390.37958736,\n",
      "       408.2225002 , 410.23820866, 336.26542706, 319.41677054,\n",
      "       386.73270881, 408.68045252, 379.54368873, 374.68737687,\n",
      "       434.71295722, 367.58405072, 389.15715379, 372.35580464,\n",
      "       359.08299146, 423.66672844, 438.84670817, 437.61444311,\n",
      "       463.16543088, 410.89882285, 419.07923581, 381.30008063,\n",
      "       394.0134984 , 383.27079237])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([387.32550047, 417.0279523 , 358.91573723, 476.45969901,\n",
      "       381.52225147, 314.31964945, 421.39247611, 452.38691493,\n",
      "       444.22925323, 442.59260105, 411.5736148 , 354.91586408,\n",
      "       383.56673024, 364.72357312, 434.29575526, 413.08282734,\n",
      "       361.27286765, 370.69375928, 416.44682588, 385.58899509])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([115.85726673, 173.41590414, 135.43190224, 233.88614205,\n",
      "       168.22234072, 149.37417651, 211.20263936, 134.80757158,\n",
      "       168.40259944, 248.81801436, 131.08170877, 163.32682593,\n",
      "       198.28054145, 230.55995397, 143.44437565,  99.18040759,\n",
      "       184.29382082, 211.78528286, 211.13288375, 216.64507465,\n",
      "       168.69752245, 193.57349357, 214.00622395, 168.32951   ,\n",
      "       277.04961628, 260.96146028, 156.82804628, 211.79379382,\n",
      "       165.25249826, 223.55048858])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([276.7108538 , 191.76163782, 241.70231579, 247.8262009 ,\n",
      "       261.74448658, 324.40308386, 242.94621212, 176.65754742,\n",
      "       154.29002952, 153.07560126, 191.31567738, 228.94610511,\n",
      "       238.09168848, 267.12643163, 239.73651743, 296.96554321,\n",
      "       241.19461979, 358.19748099, 314.38420433, 205.25786975])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([211.78528286, 211.13288375, 216.64507465, 168.69752245,\n",
      "       193.57349357, 214.00622395, 168.32951   , 277.04961628,\n",
      "       260.96146028, 156.82804628, 211.79379382, 165.25249826,\n",
      "       223.55048858, 276.7108538 , 191.76163782, 241.70231579,\n",
      "       247.8262009 , 261.74448658, 324.40308386, 242.94621212,\n",
      "       176.65754742, 154.29002952, 153.07560126, 191.31567738,\n",
      "       228.94610511, 238.09168848, 267.12643163, 239.73651743,\n",
      "       296.96554321, 241.19461979])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([358.19748099, 314.38420433, 205.25786975, 159.52202019,\n",
      "       227.55789279, 224.56358967, 261.6293727 , 269.64979598,\n",
      "       241.11349397, 191.94529955, 141.51864415, 178.65095344,\n",
      "       264.60280642, 264.23905845, 183.7540556 , 223.49117695,\n",
      "       260.65355218, 206.39405506, 233.54247196, 248.43310869])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([168.32951   , 277.04961628, 260.96146028, 156.82804628,\n",
      "       211.79379382, 165.25249826, 223.55048858, 276.7108538 ,\n",
      "       191.76163782, 241.70231579, 247.8262009 , 261.74448658,\n",
      "       324.40308386, 242.94621212, 176.65754742, 154.29002952,\n",
      "       153.07560126, 191.31567738, 228.94610511, 238.09168848,\n",
      "       267.12643163, 239.73651743, 296.96554321, 241.19461979,\n",
      "       358.19748099, 314.38420433, 205.25786975, 159.52202019,\n",
      "       227.55789279, 224.56358967])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([261.6293727 , 269.64979598, 241.11349397, 191.94529955,\n",
      "       141.51864415, 178.65095344, 264.60280642, 264.23905845,\n",
      "       183.7540556 , 223.49117695, 260.65355218, 206.39405506,\n",
      "       233.54247196, 248.43310869, 190.08114942, 240.53880393,\n",
      "       280.09782758, 315.35893007, 327.78137488, 209.29589023])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([433.43357323, 315.82157975, 427.89093968, 289.63949291,\n",
      "       301.77052434, 367.48165357, 372.13463675, 324.83322363,\n",
      "       327.12874953, 320.05395004, 310.90629308, 380.879735  ,\n",
      "       387.76152403, 330.557742  , 388.11743607, 390.37958736,\n",
      "       408.2225002 , 410.23820866, 336.26542706, 319.41677054,\n",
      "       386.73270881, 408.68045252, 379.54368873, 374.68737687,\n",
      "       434.71295722, 367.58405072, 389.15715379, 372.35580464,\n",
      "       359.08299146, 423.66672844])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([438.84670817, 437.61444311, 463.16543088, 410.89882285,\n",
      "       419.07923581, 381.30008063, 394.0134984 , 383.27079237,\n",
      "       387.32550047, 417.0279523 , 358.91573723, 476.45969901,\n",
      "       381.52225147, 314.31964945, 421.39247611, 452.38691493,\n",
      "       444.22925323, 442.59260105, 411.5736148 , 354.91586408])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([255.41322766, 318.51060943, 268.61541097, 272.76425551,\n",
      "       315.14139906, 348.59105903, 256.83218886, 258.63997842,\n",
      "       265.18095026, 255.64151055, 373.23511922, 357.72595578,\n",
      "       254.37809822, 325.97651154, 427.7355496 , 405.77162346,\n",
      "       261.78093214, 257.79632772, 359.61398809, 300.97891002,\n",
      "       323.07814374, 360.51164299, 286.45978574, 296.09527034,\n",
      "       152.42525801, 197.44811645, 274.68819015, 247.35305589,\n",
      "       374.20690796, 282.19155374])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([275.74591981, 318.92733395, 398.74035381, 286.78178474,\n",
      "       360.52363199, 351.88463431, 285.26129367, 335.01022329,\n",
      "       350.24709861, 309.63551757, 327.15226648, 317.45700632,\n",
      "       335.20605714, 373.26065529, 433.43357323, 315.82157975,\n",
      "       427.89093968, 289.63949291, 301.77052434, 367.48165357])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([335.20605714, 373.26065529, 433.43357323, 315.82157975,\n",
      "       427.89093968, 289.63949291, 301.77052434, 367.48165357,\n",
      "       372.13463675, 324.83322363, 327.12874953, 320.05395004,\n",
      "       310.90629308, 380.879735  , 387.76152403, 330.557742  ,\n",
      "       388.11743607, 390.37958736, 408.2225002 , 410.23820866,\n",
      "       336.26542706, 319.41677054, 386.73270881, 408.68045252,\n",
      "       379.54368873, 374.68737687, 434.71295722, 367.58405072,\n",
      "       389.15715379, 372.35580464])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([359.08299146, 423.66672844, 438.84670817, 437.61444311,\n",
      "       463.16543088, 410.89882285, 419.07923581, 381.30008063,\n",
      "       394.0134984 , 383.27079237, 387.32550047, 417.0279523 ,\n",
      "       358.91573723, 476.45969901, 381.52225147, 314.31964945,\n",
      "       421.39247611, 452.38691493, 444.22925323, 442.59260105])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([384.60274617, 226.89460208, 273.27294491, 203.17066281,\n",
      "       217.19312001, 314.43807321, 300.01414871, 227.87519988,\n",
      "       220.79051407, 295.02199764, 255.88545794, 278.47209267,\n",
      "       287.98710364, 252.91959566, 378.32322455, 361.8077385 ,\n",
      "       206.60592959, 258.34179417, 255.41322766, 318.51060943,\n",
      "       268.61541097, 272.76425551, 315.14139906, 348.59105903,\n",
      "       256.83218886, 258.63997842, 265.18095026, 255.64151055,\n",
      "       373.23511922, 357.72595578])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([254.37809822, 325.97651154, 427.7355496 , 405.77162346,\n",
      "       261.78093214, 257.79632772, 359.61398809, 300.97891002,\n",
      "       323.07814374, 360.51164299, 286.45978574, 296.09527034,\n",
      "       152.42525801, 197.44811645, 274.68819015, 247.35305589,\n",
      "       374.20690796, 282.19155374, 275.74591981, 318.92733395])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([335.01022329, 350.24709861, 309.63551757, 327.15226648,\n",
      "       317.45700632, 335.20605714, 373.26065529, 433.43357323,\n",
      "       315.82157975, 427.89093968, 289.63949291, 301.77052434,\n",
      "       367.48165357, 372.13463675, 324.83322363, 327.12874953,\n",
      "       320.05395004, 310.90629308, 380.879735  , 387.76152403,\n",
      "       330.557742  , 388.11743607, 390.37958736, 408.2225002 ,\n",
      "       410.23820866, 336.26542706, 319.41677054, 386.73270881,\n",
      "       408.68045252, 379.54368873])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([374.68737687, 434.71295722, 367.58405072, 389.15715379,\n",
      "       372.35580464, 359.08299146, 423.66672844, 438.84670817,\n",
      "       437.61444311, 463.16543088, 410.89882285, 419.07923581,\n",
      "       381.30008063, 394.0134984 , 383.27079237, 387.32550047,\n",
      "       417.0279523 , 358.91573723, 476.45969901, 381.52225147])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 82.29692285, 140.1662113 , 115.85726673, 173.41590414,\n",
      "       135.43190224, 233.88614205, 168.22234072, 149.37417651,\n",
      "       211.20263936, 134.80757158, 168.40259944, 248.81801436,\n",
      "       131.08170877, 163.32682593, 198.28054145, 230.55995397,\n",
      "       143.44437565,  99.18040759, 184.29382082, 211.78528286,\n",
      "       211.13288375, 216.64507465, 168.69752245, 193.57349357,\n",
      "       214.00622395, 168.32951   , 277.04961628, 260.96146028,\n",
      "       156.82804628, 211.79379382])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([165.25249826, 223.55048858, 276.7108538 , 191.76163782,\n",
      "       241.70231579, 247.8262009 , 261.74448658, 324.40308386,\n",
      "       242.94621212, 176.65754742, 154.29002952, 153.07560126,\n",
      "       191.31567738, 228.94610511, 238.09168848, 267.12643163,\n",
      "       239.73651743, 296.96554321, 241.19461979, 358.19748099])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([373.23511922, 357.72595578, 254.37809822, 325.97651154,\n",
      "       427.7355496 , 405.77162346, 261.78093214, 257.79632772,\n",
      "       359.61398809, 300.97891002, 323.07814374, 360.51164299,\n",
      "       286.45978574, 296.09527034, 152.42525801, 197.44811645,\n",
      "       274.68819015, 247.35305589, 374.20690796, 282.19155374,\n",
      "       275.74591981, 318.92733395, 398.74035381, 286.78178474,\n",
      "       360.52363199, 351.88463431, 285.26129367, 335.01022329,\n",
      "       350.24709861, 309.63551757])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([327.15226648, 317.45700632, 335.20605714, 373.26065529,\n",
      "       433.43357323, 315.82157975, 427.89093968, 289.63949291,\n",
      "       301.77052434, 367.48165357, 372.13463675, 324.83322363,\n",
      "       327.12874953, 320.05395004, 310.90629308, 380.879735  ,\n",
      "       387.76152403, 330.557742  , 388.11743607, 390.37958736])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([378.32322455, 361.8077385 , 206.60592959, 258.34179417,\n",
      "       255.41322766, 318.51060943, 268.61541097, 272.76425551,\n",
      "       315.14139906, 348.59105903, 256.83218886, 258.63997842,\n",
      "       265.18095026, 255.64151055, 373.23511922, 357.72595578,\n",
      "       254.37809822, 325.97651154, 427.7355496 , 405.77162346,\n",
      "       261.78093214, 257.79632772, 359.61398809, 300.97891002,\n",
      "       323.07814374, 360.51164299, 286.45978574, 296.09527034,\n",
      "       152.42525801, 197.44811645])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([274.68819015, 247.35305589, 374.20690796, 282.19155374,\n",
      "       275.74591981, 318.92733395, 398.74035381, 286.78178474,\n",
      "       360.52363199, 351.88463431, 285.26129367, 335.01022329,\n",
      "       350.24709861, 309.63551757, 327.15226648, 317.45700632,\n",
      "       335.20605714, 373.26065529, 433.43357323, 315.82157975])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([260.65355218, 206.39405506, 233.54247196, 248.43310869,\n",
      "       190.08114942, 240.53880393, 280.09782758, 315.35893007,\n",
      "       327.78137488, 209.29589023, 180.7943296 , 256.05134529,\n",
      "       287.62799663, 293.2503612 , 461.66825185, 371.01366887,\n",
      "       340.76572611, 337.21491073, 319.24262795, 265.1709739 ,\n",
      "       298.00957865, 242.49635296, 240.13836305, 236.14417874,\n",
      "       260.83591413, 384.60274617, 226.89460208, 273.27294491,\n",
      "       203.17066281, 217.19312001])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([314.43807321, 300.01414871, 227.87519988, 220.79051407,\n",
      "       295.02199764, 255.88545794, 278.47209267, 287.98710364,\n",
      "       252.91959566, 378.32322455, 361.8077385 , 206.60592959,\n",
      "       258.34179417, 255.41322766, 318.51060943, 268.61541097,\n",
      "       272.76425551, 315.14139906, 348.59105903, 256.83218886])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([275.74591981, 318.92733395, 398.74035381, 286.78178474,\n",
      "       360.52363199, 351.88463431, 285.26129367, 335.01022329,\n",
      "       350.24709861, 309.63551757, 327.15226648, 317.45700632,\n",
      "       335.20605714, 373.26065529, 433.43357323, 315.82157975,\n",
      "       427.89093968, 289.63949291, 301.77052434, 367.48165357,\n",
      "       372.13463675, 324.83322363, 327.12874953, 320.05395004,\n",
      "       310.90629308, 380.879735  , 387.76152403, 330.557742  ,\n",
      "       388.11743607, 390.37958736])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([408.2225002 , 410.23820866, 336.26542706, 319.41677054,\n",
      "       386.73270881, 408.68045252, 379.54368873, 374.68737687,\n",
      "       434.71295722, 367.58405072, 389.15715379, 372.35580464,\n",
      "       359.08299146, 423.66672844, 438.84670817, 437.61444311,\n",
      "       463.16543088, 410.89882285, 419.07923581, 381.30008063])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([300.97891002, 323.07814374, 360.51164299, 286.45978574,\n",
      "       296.09527034, 152.42525801, 197.44811645, 274.68819015,\n",
      "       247.35305589, 374.20690796, 282.19155374, 275.74591981,\n",
      "       318.92733395, 398.74035381, 286.78178474, 360.52363199,\n",
      "       351.88463431, 285.26129367, 335.01022329, 350.24709861,\n",
      "       309.63551757, 327.15226648, 317.45700632, 335.20605714,\n",
      "       373.26065529, 433.43357323, 315.82157975, 427.89093968,\n",
      "       289.63949291, 301.77052434])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([367.48165357, 372.13463675, 324.83322363, 327.12874953,\n",
      "       320.05395004, 310.90629308, 380.879735  , 387.76152403,\n",
      "       330.557742  , 388.11743607, 390.37958736, 408.2225002 ,\n",
      "       410.23820866, 336.26542706, 319.41677054, 386.73270881,\n",
      "       408.68045252, 379.54368873, 374.68737687, 434.71295722])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([379.54368873, 374.68737687, 434.71295722, 367.58405072,\n",
      "       389.15715379, 372.35580464, 359.08299146, 423.66672844,\n",
      "       438.84670817, 437.61444311, 463.16543088, 410.89882285,\n",
      "       419.07923581, 381.30008063, 394.0134984 , 383.27079237,\n",
      "       387.32550047, 417.0279523 , 358.91573723, 476.45969901,\n",
      "       381.52225147, 314.31964945, 421.39247611, 452.38691493,\n",
      "       444.22925323, 442.59260105, 411.5736148 , 354.91586408,\n",
      "       383.56673024, 364.72357312])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([434.29575526, 413.08282734, 361.27286765, 370.69375928,\n",
      "       416.44682588, 385.58899509, 355.57554051, 401.83538562,\n",
      "       424.40409635, 391.1802466 , 378.83606312, 413.5852798 ,\n",
      "       345.70142055, 315.11381903, 346.66881669, 386.61950519,\n",
      "       426.58713648, 499.00199322, 496.48184556, 438.03423281])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([357.72595578, 254.37809822, 325.97651154, 427.7355496 ,\n",
      "       405.77162346, 261.78093214, 257.79632772, 359.61398809,\n",
      "       300.97891002, 323.07814374, 360.51164299, 286.45978574,\n",
      "       296.09527034, 152.42525801, 197.44811645, 274.68819015,\n",
      "       247.35305589, 374.20690796, 282.19155374, 275.74591981,\n",
      "       318.92733395, 398.74035381, 286.78178474, 360.52363199,\n",
      "       351.88463431, 285.26129367, 335.01022329, 350.24709861,\n",
      "       309.63551757, 327.15226648])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([317.45700632, 335.20605714, 373.26065529, 433.43357323,\n",
      "       315.82157975, 427.89093968, 289.63949291, 301.77052434,\n",
      "       367.48165357, 372.13463675, 324.83322363, 327.12874953,\n",
      "       320.05395004, 310.90629308, 380.879735  , 387.76152403,\n",
      "       330.557742  , 388.11743607, 390.37958736, 408.2225002 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([  3.62500397,  16.29659574,  42.40312683, 125.84206218,\n",
      "       124.86800076,  10.80211719,  69.67090736,  73.00015151,\n",
      "        49.1362523 , 106.98454433, 155.1638923 , 163.89514009,\n",
      "        76.87066266,  66.65283723, 105.2879106 , 152.28966719,\n",
      "        96.70344248,  83.77352599,  41.23114256,  17.99426911,\n",
      "       114.11325318, 184.04522052, 131.63722049, 158.90144505,\n",
      "       147.21045867,  86.83575307, 115.00959466, 192.5315456 ,\n",
      "       142.60208615, 193.93866008])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 16.93172331, 103.93682066, 129.66886997, 103.55737712,\n",
      "       115.40403437,  19.93344086,  65.42368531, 126.55694824,\n",
      "       199.66504437, 128.07629968,  76.25036135,  81.087672  ,\n",
      "       156.95619511, 159.44456572, 109.77291327, 143.66763433,\n",
      "       143.35571794, 181.43706483, 117.23990033, 103.79331492])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([319.41677054, 386.73270881, 408.68045252, 379.54368873,\n",
      "       374.68737687, 434.71295722, 367.58405072, 389.15715379,\n",
      "       372.35580464, 359.08299146, 423.66672844, 438.84670817,\n",
      "       437.61444311, 463.16543088, 410.89882285, 419.07923581,\n",
      "       381.30008063, 394.0134984 , 383.27079237, 387.32550047,\n",
      "       417.0279523 , 358.91573723, 476.45969901, 381.52225147,\n",
      "       314.31964945, 421.39247611, 452.38691493, 444.22925323,\n",
      "       442.59260105, 411.5736148 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([354.91586408, 383.56673024, 364.72357312, 434.29575526,\n",
      "       413.08282734, 361.27286765, 370.69375928, 416.44682588,\n",
      "       385.58899509, 355.57554051, 401.83538562, 424.40409635,\n",
      "       391.1802466 , 378.83606312, 413.5852798 , 345.70142055,\n",
      "       315.11381903, 346.66881669, 386.61950519, 426.58713648])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([327.15226648, 317.45700632, 335.20605714, 373.26065529,\n",
      "       433.43357323, 315.82157975, 427.89093968, 289.63949291,\n",
      "       301.77052434, 367.48165357, 372.13463675, 324.83322363,\n",
      "       327.12874953, 320.05395004, 310.90629308, 380.879735  ,\n",
      "       387.76152403, 330.557742  , 388.11743607, 390.37958736,\n",
      "       408.2225002 , 410.23820866, 336.26542706, 319.41677054,\n",
      "       386.73270881, 408.68045252, 379.54368873, 374.68737687,\n",
      "       434.71295722, 367.58405072])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([389.15715379, 372.35580464, 359.08299146, 423.66672844,\n",
      "       438.84670817, 437.61444311, 463.16543088, 410.89882285,\n",
      "       419.07923581, 381.30008063, 394.0134984 , 383.27079237,\n",
      "       387.32550047, 417.0279523 , 358.91573723, 476.45969901,\n",
      "       381.52225147, 314.31964945, 421.39247611, 452.38691493])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 29.1604104 ,  62.45920462, -19.259528  , -30.02332054,\n",
      "        54.25897839, 115.20075505, 104.4155928 ,  81.90616437,\n",
      "        66.43128026,   3.62500397,  16.29659574,  42.40312683,\n",
      "       125.84206218, 124.86800076,  10.80211719,  69.67090736,\n",
      "        73.00015151,  49.1362523 , 106.98454433, 155.1638923 ,\n",
      "       163.89514009,  76.87066266,  66.65283723, 105.2879106 ,\n",
      "       152.28966719,  96.70344248,  83.77352599,  41.23114256,\n",
      "        17.99426911, 114.11325318])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([184.04522052, 131.63722049, 158.90144505, 147.21045867,\n",
      "        86.83575307, 115.00959466, 192.5315456 , 142.60208615,\n",
      "       193.93866008,  16.93172331, 103.93682066, 129.66886997,\n",
      "       103.55737712, 115.40403437,  19.93344086,  65.42368531,\n",
      "       126.55694824, 199.66504437, 128.07629968,  76.25036135])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([289.63949291, 301.77052434, 367.48165357, 372.13463675,\n",
      "       324.83322363, 327.12874953, 320.05395004, 310.90629308,\n",
      "       380.879735  , 387.76152403, 330.557742  , 388.11743607,\n",
      "       390.37958736, 408.2225002 , 410.23820866, 336.26542706,\n",
      "       319.41677054, 386.73270881, 408.68045252, 379.54368873,\n",
      "       374.68737687, 434.71295722, 367.58405072, 389.15715379,\n",
      "       372.35580464, 359.08299146, 423.66672844, 438.84670817,\n",
      "       437.61444311, 463.16543088])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([410.89882285, 419.07923581, 381.30008063, 394.0134984 ,\n",
      "       383.27079237, 387.32550047, 417.0279523 , 358.91573723,\n",
      "       476.45969901, 381.52225147, 314.31964945, 421.39247611,\n",
      "       452.38691493, 444.22925323, 442.59260105, 411.5736148 ,\n",
      "       354.91586408, 383.56673024, 364.72357312, 434.29575526])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([260.83591413, 384.60274617, 226.89460208, 273.27294491,\n",
      "       203.17066281, 217.19312001, 314.43807321, 300.01414871,\n",
      "       227.87519988, 220.79051407, 295.02199764, 255.88545794,\n",
      "       278.47209267, 287.98710364, 252.91959566, 378.32322455,\n",
      "       361.8077385 , 206.60592959, 258.34179417, 255.41322766,\n",
      "       318.51060943, 268.61541097, 272.76425551, 315.14139906,\n",
      "       348.59105903, 256.83218886, 258.63997842, 265.18095026,\n",
      "       255.64151055, 373.23511922])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([357.72595578, 254.37809822, 325.97651154, 427.7355496 ,\n",
      "       405.77162346, 261.78093214, 257.79632772, 359.61398809,\n",
      "       300.97891002, 323.07814374, 360.51164299, 286.45978574,\n",
      "       296.09527034, 152.42525801, 197.44811645, 274.68819015,\n",
      "       247.35305589, 374.20690796, 282.19155374, 275.74591981])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([236.14417874, 260.83591413, 384.60274617, 226.89460208,\n",
      "       273.27294491, 203.17066281, 217.19312001, 314.43807321,\n",
      "       300.01414871, 227.87519988, 220.79051407, 295.02199764,\n",
      "       255.88545794, 278.47209267, 287.98710364, 252.91959566,\n",
      "       378.32322455, 361.8077385 , 206.60592959, 258.34179417,\n",
      "       255.41322766, 318.51060943, 268.61541097, 272.76425551,\n",
      "       315.14139906, 348.59105903, 256.83218886, 258.63997842,\n",
      "       265.18095026, 255.64151055])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([373.23511922, 357.72595578, 254.37809822, 325.97651154,\n",
      "       427.7355496 , 405.77162346, 261.78093214, 257.79632772,\n",
      "       359.61398809, 300.97891002, 323.07814374, 360.51164299,\n",
      "       286.45978574, 296.09527034, 152.42525801, 197.44811645,\n",
      "       274.68819015, 247.35305589, 374.20690796, 282.19155374])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([427.7355496 , 405.77162346, 261.78093214, 257.79632772,\n",
      "       359.61398809, 300.97891002, 323.07814374, 360.51164299,\n",
      "       286.45978574, 296.09527034, 152.42525801, 197.44811645,\n",
      "       274.68819015, 247.35305589, 374.20690796, 282.19155374,\n",
      "       275.74591981, 318.92733395, 398.74035381, 286.78178474,\n",
      "       360.52363199, 351.88463431, 285.26129367, 335.01022329,\n",
      "       350.24709861, 309.63551757, 327.15226648, 317.45700632,\n",
      "       335.20605714, 373.26065529])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([433.43357323, 315.82157975, 427.89093968, 289.63949291,\n",
      "       301.77052434, 367.48165357, 372.13463675, 324.83322363,\n",
      "       327.12874953, 320.05395004, 310.90629308, 380.879735  ,\n",
      "       387.76152403, 330.557742  , 388.11743607, 390.37958736,\n",
      "       408.2225002 , 410.23820866, 336.26542706, 319.41677054])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([241.11349397, 191.94529955, 141.51864415, 178.65095344,\n",
      "       264.60280642, 264.23905845, 183.7540556 , 223.49117695,\n",
      "       260.65355218, 206.39405506, 233.54247196, 248.43310869,\n",
      "       190.08114942, 240.53880393, 280.09782758, 315.35893007,\n",
      "       327.78137488, 209.29589023, 180.7943296 , 256.05134529,\n",
      "       287.62799663, 293.2503612 , 461.66825185, 371.01366887,\n",
      "       340.76572611, 337.21491073, 319.24262795, 265.1709739 ,\n",
      "       298.00957865, 242.49635296])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([240.13836305, 236.14417874, 260.83591413, 384.60274617,\n",
      "       226.89460208, 273.27294491, 203.17066281, 217.19312001,\n",
      "       314.43807321, 300.01414871, 227.87519988, 220.79051407,\n",
      "       295.02199764, 255.88545794, 278.47209267, 287.98710364,\n",
      "       252.91959566, 378.32322455, 361.8077385 , 206.60592959])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([141.51864415, 178.65095344, 264.60280642, 264.23905845,\n",
      "       183.7540556 , 223.49117695, 260.65355218, 206.39405506,\n",
      "       233.54247196, 248.43310869, 190.08114942, 240.53880393,\n",
      "       280.09782758, 315.35893007, 327.78137488, 209.29589023,\n",
      "       180.7943296 , 256.05134529, 287.62799663, 293.2503612 ,\n",
      "       461.66825185, 371.01366887, 340.76572611, 337.21491073,\n",
      "       319.24262795, 265.1709739 , 298.00957865, 242.49635296,\n",
      "       240.13836305, 236.14417874])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([260.83591413, 384.60274617, 226.89460208, 273.27294491,\n",
      "       203.17066281, 217.19312001, 314.43807321, 300.01414871,\n",
      "       227.87519988, 220.79051407, 295.02199764, 255.88545794,\n",
      "       278.47209267, 287.98710364, 252.91959566, 378.32322455,\n",
      "       361.8077385 , 206.60592959, 258.34179417, 255.41322766])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([197.44811645, 274.68819015, 247.35305589, 374.20690796,\n",
      "       282.19155374, 275.74591981, 318.92733395, 398.74035381,\n",
      "       286.78178474, 360.52363199, 351.88463431, 285.26129367,\n",
      "       335.01022329, 350.24709861, 309.63551757, 327.15226648,\n",
      "       317.45700632, 335.20605714, 373.26065529, 433.43357323,\n",
      "       315.82157975, 427.89093968, 289.63949291, 301.77052434,\n",
      "       367.48165357, 372.13463675, 324.83322363, 327.12874953,\n",
      "       320.05395004, 310.90629308])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([380.879735  , 387.76152403, 330.557742  , 388.11743607,\n",
      "       390.37958736, 408.2225002 , 410.23820866, 336.26542706,\n",
      "       319.41677054, 386.73270881, 408.68045252, 379.54368873,\n",
      "       374.68737687, 434.71295722, 367.58405072, 389.15715379,\n",
      "       372.35580464, 359.08299146, 423.66672844, 438.84670817])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([398.74035381, 286.78178474, 360.52363199, 351.88463431,\n",
      "       285.26129367, 335.01022329, 350.24709861, 309.63551757,\n",
      "       327.15226648, 317.45700632, 335.20605714, 373.26065529,\n",
      "       433.43357323, 315.82157975, 427.89093968, 289.63949291,\n",
      "       301.77052434, 367.48165357, 372.13463675, 324.83322363,\n",
      "       327.12874953, 320.05395004, 310.90629308, 380.879735  ,\n",
      "       387.76152403, 330.557742  , 388.11743607, 390.37958736,\n",
      "       408.2225002 , 410.23820866])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([336.26542706, 319.41677054, 386.73270881, 408.68045252,\n",
      "       379.54368873, 374.68737687, 434.71295722, 367.58405072,\n",
      "       389.15715379, 372.35580464, 359.08299146, 423.66672844,\n",
      "       438.84670817, 437.61444311, 463.16543088, 410.89882285,\n",
      "       419.07923581, 381.30008063, 394.0134984 , 383.27079237])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([206.60592959, 258.34179417, 255.41322766, 318.51060943,\n",
      "       268.61541097, 272.76425551, 315.14139906, 348.59105903,\n",
      "       256.83218886, 258.63997842, 265.18095026, 255.64151055,\n",
      "       373.23511922, 357.72595578, 254.37809822, 325.97651154,\n",
      "       427.7355496 , 405.77162346, 261.78093214, 257.79632772,\n",
      "       359.61398809, 300.97891002, 323.07814374, 360.51164299,\n",
      "       286.45978574, 296.09527034, 152.42525801, 197.44811645,\n",
      "       274.68819015, 247.35305589])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([374.20690796, 282.19155374, 275.74591981, 318.92733395,\n",
      "       398.74035381, 286.78178474, 360.52363199, 351.88463431,\n",
      "       285.26129367, 335.01022329, 350.24709861, 309.63551757,\n",
      "       327.15226648, 317.45700632, 335.20605714, 373.26065529,\n",
      "       433.43357323, 315.82157975, 427.89093968, 289.63949291])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([434.71295722, 367.58405072, 389.15715379, 372.35580464,\n",
      "       359.08299146, 423.66672844, 438.84670817, 437.61444311,\n",
      "       463.16543088, 410.89882285, 419.07923581, 381.30008063,\n",
      "       394.0134984 , 383.27079237, 387.32550047, 417.0279523 ,\n",
      "       358.91573723, 476.45969901, 381.52225147, 314.31964945,\n",
      "       421.39247611, 452.38691493, 444.22925323, 442.59260105,\n",
      "       411.5736148 , 354.91586408, 383.56673024, 364.72357312,\n",
      "       434.29575526, 413.08282734])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([361.27286765, 370.69375928, 416.44682588, 385.58899509,\n",
      "       355.57554051, 401.83538562, 424.40409635, 391.1802466 ,\n",
      "       378.83606312, 413.5852798 , 345.70142055, 315.11381903,\n",
      "       346.66881669, 386.61950519, 426.58713648, 499.00199322,\n",
      "       496.48184556, 438.03423281, 423.59440741, 375.37548583])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([374.68737687, 434.71295722, 367.58405072, 389.15715379,\n",
      "       372.35580464, 359.08299146, 423.66672844, 438.84670817,\n",
      "       437.61444311, 463.16543088, 410.89882285, 419.07923581,\n",
      "       381.30008063, 394.0134984 , 383.27079237, 387.32550047,\n",
      "       417.0279523 , 358.91573723, 476.45969901, 381.52225147,\n",
      "       314.31964945, 421.39247611, 452.38691493, 444.22925323,\n",
      "       442.59260105, 411.5736148 , 354.91586408, 383.56673024,\n",
      "       364.72357312, 434.29575526])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([413.08282734, 361.27286765, 370.69375928, 416.44682588,\n",
      "       385.58899509, 355.57554051, 401.83538562, 424.40409635,\n",
      "       391.1802466 , 378.83606312, 413.5852798 , 345.70142055,\n",
      "       315.11381903, 346.66881669, 386.61950519, 426.58713648,\n",
      "       499.00199322, 496.48184556, 438.03423281, 423.59440741])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([427.89093968, 289.63949291, 301.77052434, 367.48165357,\n",
      "       372.13463675, 324.83322363, 327.12874953, 320.05395004,\n",
      "       310.90629308, 380.879735  , 387.76152403, 330.557742  ,\n",
      "       388.11743607, 390.37958736, 408.2225002 , 410.23820866,\n",
      "       336.26542706, 319.41677054, 386.73270881, 408.68045252,\n",
      "       379.54368873, 374.68737687, 434.71295722, 367.58405072,\n",
      "       389.15715379, 372.35580464, 359.08299146, 423.66672844,\n",
      "       438.84670817, 437.61444311])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([463.16543088, 410.89882285, 419.07923581, 381.30008063,\n",
      "       394.0134984 , 383.27079237, 387.32550047, 417.0279523 ,\n",
      "       358.91573723, 476.45969901, 381.52225147, 314.31964945,\n",
      "       421.39247611, 452.38691493, 444.22925323, 442.59260105,\n",
      "       411.5736148 , 354.91586408, 383.56673024, 364.72357312])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([-30.02332054,  54.25897839, 115.20075505, 104.4155928 ,\n",
      "        81.90616437,  66.43128026,   3.62500397,  16.29659574,\n",
      "        42.40312683, 125.84206218, 124.86800076,  10.80211719,\n",
      "        69.67090736,  73.00015151,  49.1362523 , 106.98454433,\n",
      "       155.1638923 , 163.89514009,  76.87066266,  66.65283723,\n",
      "       105.2879106 , 152.28966719,  96.70344248,  83.77352599,\n",
      "        41.23114256,  17.99426911, 114.11325318, 184.04522052,\n",
      "       131.63722049, 158.90144505])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([147.21045867,  86.83575307, 115.00959466, 192.5315456 ,\n",
      "       142.60208615, 193.93866008,  16.93172331, 103.93682066,\n",
      "       129.66886997, 103.55737712, 115.40403437,  19.93344086,\n",
      "        65.42368531, 126.55694824, 199.66504437, 128.07629968,\n",
      "        76.25036135,  81.087672  , 156.95619511, 159.44456572])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([325.97651154, 427.7355496 , 405.77162346, 261.78093214,\n",
      "       257.79632772, 359.61398809, 300.97891002, 323.07814374,\n",
      "       360.51164299, 286.45978574, 296.09527034, 152.42525801,\n",
      "       197.44811645, 274.68819015, 247.35305589, 374.20690796,\n",
      "       282.19155374, 275.74591981, 318.92733395, 398.74035381,\n",
      "       286.78178474, 360.52363199, 351.88463431, 285.26129367,\n",
      "       335.01022329, 350.24709861, 309.63551757, 327.15226648,\n",
      "       317.45700632, 335.20605714])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([373.26065529, 433.43357323, 315.82157975, 427.89093968,\n",
      "       289.63949291, 301.77052434, 367.48165357, 372.13463675,\n",
      "       324.83322363, 327.12874953, 320.05395004, 310.90629308,\n",
      "       380.879735  , 387.76152403, 330.557742  , 388.11743607,\n",
      "       390.37958736, 408.2225002 , 410.23820866, 336.26542706])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([371.01366887, 340.76572611, 337.21491073, 319.24262795,\n",
      "       265.1709739 , 298.00957865, 242.49635296, 240.13836305,\n",
      "       236.14417874, 260.83591413, 384.60274617, 226.89460208,\n",
      "       273.27294491, 203.17066281, 217.19312001, 314.43807321,\n",
      "       300.01414871, 227.87519988, 220.79051407, 295.02199764,\n",
      "       255.88545794, 278.47209267, 287.98710364, 252.91959566,\n",
      "       378.32322455, 361.8077385 , 206.60592959, 258.34179417,\n",
      "       255.41322766, 318.51060943])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([268.61541097, 272.76425551, 315.14139906, 348.59105903,\n",
      "       256.83218886, 258.63997842, 265.18095026, 255.64151055,\n",
      "       373.23511922, 357.72595578, 254.37809822, 325.97651154,\n",
      "       427.7355496 , 405.77162346, 261.78093214, 257.79632772,\n",
      "       359.61398809, 300.97891002, 323.07814374, 360.51164299])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([423.66672844, 438.84670817, 437.61444311, 463.16543088,\n",
      "       410.89882285, 419.07923581, 381.30008063, 394.0134984 ,\n",
      "       383.27079237, 387.32550047, 417.0279523 , 358.91573723,\n",
      "       476.45969901, 381.52225147, 314.31964945, 421.39247611,\n",
      "       452.38691493, 444.22925323, 442.59260105, 411.5736148 ,\n",
      "       354.91586408, 383.56673024, 364.72357312, 434.29575526,\n",
      "       413.08282734, 361.27286765, 370.69375928, 416.44682588,\n",
      "       385.58899509, 355.57554051])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([401.83538562, 424.40409635, 391.1802466 , 378.83606312,\n",
      "       413.5852798 , 345.70142055, 315.11381903, 346.66881669,\n",
      "       386.61950519, 426.58713648, 499.00199322, 496.48184556,\n",
      "       438.03423281, 423.59440741, 375.37548583, 404.09737706,\n",
      "       409.62535098, 437.76060348, 394.47503303, 439.97630195])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([372.13463675, 324.83322363, 327.12874953, 320.05395004,\n",
      "       310.90629308, 380.879735  , 387.76152403, 330.557742  ,\n",
      "       388.11743607, 390.37958736, 408.2225002 , 410.23820866,\n",
      "       336.26542706, 319.41677054, 386.73270881, 408.68045252,\n",
      "       379.54368873, 374.68737687, 434.71295722, 367.58405072,\n",
      "       389.15715379, 372.35580464, 359.08299146, 423.66672844,\n",
      "       438.84670817, 437.61444311, 463.16543088, 410.89882285,\n",
      "       419.07923581, 381.30008063])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([394.0134984 , 383.27079237, 387.32550047, 417.0279523 ,\n",
      "       358.91573723, 476.45969901, 381.52225147, 314.31964945,\n",
      "       421.39247611, 452.38691493, 444.22925323, 442.59260105,\n",
      "       411.5736148 , 354.91586408, 383.56673024, 364.72357312,\n",
      "       434.29575526, 413.08282734, 361.27286765, 370.69375928])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([315.14139906, 348.59105903, 256.83218886, 258.63997842,\n",
      "       265.18095026, 255.64151055, 373.23511922, 357.72595578,\n",
      "       254.37809822, 325.97651154, 427.7355496 , 405.77162346,\n",
      "       261.78093214, 257.79632772, 359.61398809, 300.97891002,\n",
      "       323.07814374, 360.51164299, 286.45978574, 296.09527034,\n",
      "       152.42525801, 197.44811645, 274.68819015, 247.35305589,\n",
      "       374.20690796, 282.19155374, 275.74591981, 318.92733395,\n",
      "       398.74035381, 286.78178474])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([360.52363199, 351.88463431, 285.26129367, 335.01022329,\n",
      "       350.24709861, 309.63551757, 327.15226648, 317.45700632,\n",
      "       335.20605714, 373.26065529, 433.43357323, 315.82157975,\n",
      "       427.89093968, 289.63949291, 301.77052434, 367.48165357,\n",
      "       372.13463675, 324.83322363, 327.12874953, 320.05395004])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([318.92733395, 398.74035381, 286.78178474, 360.52363199,\n",
      "       351.88463431, 285.26129367, 335.01022329, 350.24709861,\n",
      "       309.63551757, 327.15226648, 317.45700632, 335.20605714,\n",
      "       373.26065529, 433.43357323, 315.82157975, 427.89093968,\n",
      "       289.63949291, 301.77052434, 367.48165357, 372.13463675,\n",
      "       324.83322363, 327.12874953, 320.05395004, 310.90629308,\n",
      "       380.879735  , 387.76152403, 330.557742  , 388.11743607,\n",
      "       390.37958736, 408.2225002 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([410.23820866, 336.26542706, 319.41677054, 386.73270881,\n",
      "       408.68045252, 379.54368873, 374.68737687, 434.71295722,\n",
      "       367.58405072, 389.15715379, 372.35580464, 359.08299146,\n",
      "       423.66672844, 438.84670817, 437.61444311, 463.16543088,\n",
      "       410.89882285, 419.07923581, 381.30008063, 394.0134984 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([314.43807321, 300.01414871, 227.87519988, 220.79051407,\n",
      "       295.02199764, 255.88545794, 278.47209267, 287.98710364,\n",
      "       252.91959566, 378.32322455, 361.8077385 , 206.60592959,\n",
      "       258.34179417, 255.41322766, 318.51060943, 268.61541097,\n",
      "       272.76425551, 315.14139906, 348.59105903, 256.83218886,\n",
      "       258.63997842, 265.18095026, 255.64151055, 373.23511922,\n",
      "       357.72595578, 254.37809822, 325.97651154, 427.7355496 ,\n",
      "       405.77162346, 261.78093214])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([257.79632772, 359.61398809, 300.97891002, 323.07814374,\n",
      "       360.51164299, 286.45978574, 296.09527034, 152.42525801,\n",
      "       197.44811645, 274.68819015, 247.35305589, 374.20690796,\n",
      "       282.19155374, 275.74591981, 318.92733395, 398.74035381,\n",
      "       286.78178474, 360.52363199, 351.88463431, 285.26129367])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([381.52225147, 314.31964945, 421.39247611, 452.38691493,\n",
      "       444.22925323, 442.59260105, 411.5736148 , 354.91586408,\n",
      "       383.56673024, 364.72357312, 434.29575526, 413.08282734,\n",
      "       361.27286765, 370.69375928, 416.44682588, 385.58899509,\n",
      "       355.57554051, 401.83538562, 424.40409635, 391.1802466 ,\n",
      "       378.83606312, 413.5852798 , 345.70142055, 315.11381903,\n",
      "       346.66881669, 386.61950519, 426.58713648, 499.00199322,\n",
      "       496.48184556, 438.03423281])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([423.59440741, 375.37548583, 404.09737706, 409.62535098,\n",
      "       437.76060348, 394.47503303, 439.97630195, 517.19346552,\n",
      "       461.76251444, 457.355711  , 479.62716775, 433.19914647,\n",
      "       443.23925318, 444.05635782, 446.12106416, 405.39701126,\n",
      "       427.84486792, 466.91572959, 527.49026645, 526.50191535])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([226.89460208, 273.27294491, 203.17066281, 217.19312001,\n",
      "       314.43807321, 300.01414871, 227.87519988, 220.79051407,\n",
      "       295.02199764, 255.88545794, 278.47209267, 287.98710364,\n",
      "       252.91959566, 378.32322455, 361.8077385 , 206.60592959,\n",
      "       258.34179417, 255.41322766, 318.51060943, 268.61541097,\n",
      "       272.76425551, 315.14139906, 348.59105903, 256.83218886,\n",
      "       258.63997842, 265.18095026, 255.64151055, 373.23511922,\n",
      "       357.72595578, 254.37809822])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([325.97651154, 427.7355496 , 405.77162346, 261.78093214,\n",
      "       257.79632772, 359.61398809, 300.97891002, 323.07814374,\n",
      "       360.51164299, 286.45978574, 296.09527034, 152.42525801,\n",
      "       197.44811645, 274.68819015, 247.35305589, 374.20690796,\n",
      "       282.19155374, 275.74591981, 318.92733395, 398.74035381])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([476.45969901, 381.52225147, 314.31964945, 421.39247611,\n",
      "       452.38691493, 444.22925323, 442.59260105, 411.5736148 ,\n",
      "       354.91586408, 383.56673024, 364.72357312, 434.29575526,\n",
      "       413.08282734, 361.27286765, 370.69375928, 416.44682588,\n",
      "       385.58899509, 355.57554051, 401.83538562, 424.40409635,\n",
      "       391.1802466 , 378.83606312, 413.5852798 , 345.70142055,\n",
      "       315.11381903, 346.66881669, 386.61950519, 426.58713648,\n",
      "       499.00199322, 496.48184556])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([438.03423281, 423.59440741, 375.37548583, 404.09737706,\n",
      "       409.62535098, 437.76060348, 394.47503303, 439.97630195,\n",
      "       517.19346552, 461.76251444, 457.355711  , 479.62716775,\n",
      "       433.19914647, 443.23925318, 444.05635782, 446.12106416,\n",
      "       405.39701126, 427.84486792, 466.91572959, 527.49026645])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([354.91586408, 383.56673024, 364.72357312, 434.29575526,\n",
      "       413.08282734, 361.27286765, 370.69375928, 416.44682588,\n",
      "       385.58899509, 355.57554051, 401.83538562, 424.40409635,\n",
      "       391.1802466 , 378.83606312, 413.5852798 , 345.70142055,\n",
      "       315.11381903, 346.66881669, 386.61950519, 426.58713648,\n",
      "       499.00199322, 496.48184556, 438.03423281, 423.59440741,\n",
      "       375.37548583, 404.09737706, 409.62535098, 437.76060348,\n",
      "       394.47503303, 439.97630195])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([517.19346552, 461.76251444, 457.355711  , 479.62716775,\n",
      "       433.19914647, 443.23925318, 444.05635782, 446.12106416,\n",
      "       405.39701126, 427.84486792, 466.91572959, 527.49026645,\n",
      "       526.50191535, 580.10855533, 460.73538646, 484.74676984,\n",
      "       478.32468609, 567.74701886, 461.14155259, 402.6968913 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([389.15715379, 372.35580464, 359.08299146, 423.66672844,\n",
      "       438.84670817, 437.61444311, 463.16543088, 410.89882285,\n",
      "       419.07923581, 381.30008063, 394.0134984 , 383.27079237,\n",
      "       387.32550047, 417.0279523 , 358.91573723, 476.45969901,\n",
      "       381.52225147, 314.31964945, 421.39247611, 452.38691493,\n",
      "       444.22925323, 442.59260105, 411.5736148 , 354.91586408,\n",
      "       383.56673024, 364.72357312, 434.29575526, 413.08282734,\n",
      "       361.27286765, 370.69375928])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([416.44682588, 385.58899509, 355.57554051, 401.83538562,\n",
      "       424.40409635, 391.1802466 , 378.83606312, 413.5852798 ,\n",
      "       345.70142055, 315.11381903, 346.66881669, 386.61950519,\n",
      "       426.58713648, 499.00199322, 496.48184556, 438.03423281,\n",
      "       423.59440741, 375.37548583, 404.09737706, 409.62535098])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([217.03019629, 218.01662357, 135.62151463, 215.66343029,\n",
      "       121.17746619, 167.28600356, 283.28864689, 164.69125998,\n",
      "       125.10364505, 159.87748372, 144.34205834,  82.29692285,\n",
      "       140.1662113 , 115.85726673, 173.41590414, 135.43190224,\n",
      "       233.88614205, 168.22234072, 149.37417651, 211.20263936,\n",
      "       134.80757158, 168.40259944, 248.81801436, 131.08170877,\n",
      "       163.32682593, 198.28054145, 230.55995397, 143.44437565,\n",
      "        99.18040759, 184.29382082])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([211.78528286, 211.13288375, 216.64507465, 168.69752245,\n",
      "       193.57349357, 214.00622395, 168.32951   , 277.04961628,\n",
      "       260.96146028, 156.82804628, 211.79379382, 165.25249826,\n",
      "       223.55048858, 276.7108538 , 191.76163782, 241.70231579,\n",
      "       247.8262009 , 261.74448658, 324.40308386, 242.94621212])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([351.88463431, 285.26129367, 335.01022329, 350.24709861,\n",
      "       309.63551757, 327.15226648, 317.45700632, 335.20605714,\n",
      "       373.26065529, 433.43357323, 315.82157975, 427.89093968,\n",
      "       289.63949291, 301.77052434, 367.48165357, 372.13463675,\n",
      "       324.83322363, 327.12874953, 320.05395004, 310.90629308,\n",
      "       380.879735  , 387.76152403, 330.557742  , 388.11743607,\n",
      "       390.37958736, 408.2225002 , 410.23820866, 336.26542706,\n",
      "       319.41677054, 386.73270881])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([408.68045252, 379.54368873, 374.68737687, 434.71295722,\n",
      "       367.58405072, 389.15715379, 372.35580464, 359.08299146,\n",
      "       423.66672844, 438.84670817, 437.61444311, 463.16543088,\n",
      "       410.89882285, 419.07923581, 381.30008063, 394.0134984 ,\n",
      "       383.27079237, 387.32550047, 417.0279523 , 358.91573723])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([272.76425551, 315.14139906, 348.59105903, 256.83218886,\n",
      "       258.63997842, 265.18095026, 255.64151055, 373.23511922,\n",
      "       357.72595578, 254.37809822, 325.97651154, 427.7355496 ,\n",
      "       405.77162346, 261.78093214, 257.79632772, 359.61398809,\n",
      "       300.97891002, 323.07814374, 360.51164299, 286.45978574,\n",
      "       296.09527034, 152.42525801, 197.44811645, 274.68819015,\n",
      "       247.35305589, 374.20690796, 282.19155374, 275.74591981,\n",
      "       318.92733395, 398.74035381])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([286.78178474, 360.52363199, 351.88463431, 285.26129367,\n",
      "       335.01022329, 350.24709861, 309.63551757, 327.15226648,\n",
      "       317.45700632, 335.20605714, 373.26065529, 433.43357323,\n",
      "       315.82157975, 427.89093968, 289.63949291, 301.77052434,\n",
      "       367.48165357, 372.13463675, 324.83322363, 327.12874953])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([390.37958736, 408.2225002 , 410.23820866, 336.26542706,\n",
      "       319.41677054, 386.73270881, 408.68045252, 379.54368873,\n",
      "       374.68737687, 434.71295722, 367.58405072, 389.15715379,\n",
      "       372.35580464, 359.08299146, 423.66672844, 438.84670817,\n",
      "       437.61444311, 463.16543088, 410.89882285, 419.07923581,\n",
      "       381.30008063, 394.0134984 , 383.27079237, 387.32550047,\n",
      "       417.0279523 , 358.91573723, 476.45969901, 381.52225147,\n",
      "       314.31964945, 421.39247611])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([452.38691493, 444.22925323, 442.59260105, 411.5736148 ,\n",
      "       354.91586408, 383.56673024, 364.72357312, 434.29575526,\n",
      "       413.08282734, 361.27286765, 370.69375928, 416.44682588,\n",
      "       385.58899509, 355.57554051, 401.83538562, 424.40409635,\n",
      "       391.1802466 , 378.83606312, 413.5852798 , 345.70142055])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([359.08299146, 423.66672844, 438.84670817, 437.61444311,\n",
      "       463.16543088, 410.89882285, 419.07923581, 381.30008063,\n",
      "       394.0134984 , 383.27079237, 387.32550047, 417.0279523 ,\n",
      "       358.91573723, 476.45969901, 381.52225147, 314.31964945,\n",
      "       421.39247611, 452.38691493, 444.22925323, 442.59260105,\n",
      "       411.5736148 , 354.91586408, 383.56673024, 364.72357312,\n",
      "       434.29575526, 413.08282734, 361.27286765, 370.69375928,\n",
      "       416.44682588, 385.58899509])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([355.57554051, 401.83538562, 424.40409635, 391.1802466 ,\n",
      "       378.83606312, 413.5852798 , 345.70142055, 315.11381903,\n",
      "       346.66881669, 386.61950519, 426.58713648, 499.00199322,\n",
      "       496.48184556, 438.03423281, 423.59440741, 375.37548583,\n",
      "       404.09737706, 409.62535098, 437.76060348, 394.47503303])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([314.31964945, 421.39247611, 452.38691493, 444.22925323,\n",
      "       442.59260105, 411.5736148 , 354.91586408, 383.56673024,\n",
      "       364.72357312, 434.29575526, 413.08282734, 361.27286765,\n",
      "       370.69375928, 416.44682588, 385.58899509, 355.57554051,\n",
      "       401.83538562, 424.40409635, 391.1802466 , 378.83606312,\n",
      "       413.5852798 , 345.70142055, 315.11381903, 346.66881669,\n",
      "       386.61950519, 426.58713648, 499.00199322, 496.48184556,\n",
      "       438.03423281, 423.59440741])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([375.37548583, 404.09737706, 409.62535098, 437.76060348,\n",
      "       394.47503303, 439.97630195, 517.19346552, 461.76251444,\n",
      "       457.355711  , 479.62716775, 433.19914647, 443.23925318,\n",
      "       444.05635782, 446.12106416, 405.39701126, 427.84486792,\n",
      "       466.91572959, 527.49026645, 526.50191535, 580.10855533])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([278.47209267, 287.98710364, 252.91959566, 378.32322455,\n",
      "       361.8077385 , 206.60592959, 258.34179417, 255.41322766,\n",
      "       318.51060943, 268.61541097, 272.76425551, 315.14139906,\n",
      "       348.59105903, 256.83218886, 258.63997842, 265.18095026,\n",
      "       255.64151055, 373.23511922, 357.72595578, 254.37809822,\n",
      "       325.97651154, 427.7355496 , 405.77162346, 261.78093214,\n",
      "       257.79632772, 359.61398809, 300.97891002, 323.07814374,\n",
      "       360.51164299, 286.45978574])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([296.09527034, 152.42525801, 197.44811645, 274.68819015,\n",
      "       247.35305589, 374.20690796, 282.19155374, 275.74591981,\n",
      "       318.92733395, 398.74035381, 286.78178474, 360.52363199,\n",
      "       351.88463431, 285.26129367, 335.01022329, 350.24709861,\n",
      "       309.63551757, 327.15226648, 317.45700632, 335.20605714])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([387.76152403, 330.557742  , 388.11743607, 390.37958736,\n",
      "       408.2225002 , 410.23820866, 336.26542706, 319.41677054,\n",
      "       386.73270881, 408.68045252, 379.54368873, 374.68737687,\n",
      "       434.71295722, 367.58405072, 389.15715379, 372.35580464,\n",
      "       359.08299146, 423.66672844, 438.84670817, 437.61444311,\n",
      "       463.16543088, 410.89882285, 419.07923581, 381.30008063,\n",
      "       394.0134984 , 383.27079237, 387.32550047, 417.0279523 ,\n",
      "       358.91573723, 476.45969901])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([381.52225147, 314.31964945, 421.39247611, 452.38691493,\n",
      "       444.22925323, 442.59260105, 411.5736148 , 354.91586408,\n",
      "       383.56673024, 364.72357312, 434.29575526, 413.08282734,\n",
      "       361.27286765, 370.69375928, 416.44682588, 385.58899509,\n",
      "       355.57554051, 401.83538562, 424.40409635, 391.1802466 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([442.59260105, 411.5736148 , 354.91586408, 383.56673024,\n",
      "       364.72357312, 434.29575526, 413.08282734, 361.27286765,\n",
      "       370.69375928, 416.44682588, 385.58899509, 355.57554051,\n",
      "       401.83538562, 424.40409635, 391.1802466 , 378.83606312,\n",
      "       413.5852798 , 345.70142055, 315.11381903, 346.66881669,\n",
      "       386.61950519, 426.58713648, 499.00199322, 496.48184556,\n",
      "       438.03423281, 423.59440741, 375.37548583, 404.09737706,\n",
      "       409.62535098, 437.76060348])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([394.47503303, 439.97630195, 517.19346552, 461.76251444,\n",
      "       457.355711  , 479.62716775, 433.19914647, 443.23925318,\n",
      "       444.05635782, 446.12106416, 405.39701126, 427.84486792,\n",
      "       466.91572959, 527.49026645, 526.50191535, 580.10855533,\n",
      "       460.73538646, 484.74676984, 478.32468609, 567.74701886])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([300.01414871, 227.87519988, 220.79051407, 295.02199764,\n",
      "       255.88545794, 278.47209267, 287.98710364, 252.91959566,\n",
      "       378.32322455, 361.8077385 , 206.60592959, 258.34179417,\n",
      "       255.41322766, 318.51060943, 268.61541097, 272.76425551,\n",
      "       315.14139906, 348.59105903, 256.83218886, 258.63997842,\n",
      "       265.18095026, 255.64151055, 373.23511922, 357.72595578,\n",
      "       254.37809822, 325.97651154, 427.7355496 , 405.77162346,\n",
      "       261.78093214, 257.79632772])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([359.61398809, 300.97891002, 323.07814374, 360.51164299,\n",
      "       286.45978574, 296.09527034, 152.42525801, 197.44811645,\n",
      "       274.68819015, 247.35305589, 374.20690796, 282.19155374,\n",
      "       275.74591981, 318.92733395, 398.74035381, 286.78178474,\n",
      "       360.52363199, 351.88463431, 285.26129367, 335.01022329])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([233.88614205, 168.22234072, 149.37417651, 211.20263936,\n",
      "       134.80757158, 168.40259944, 248.81801436, 131.08170877,\n",
      "       163.32682593, 198.28054145, 230.55995397, 143.44437565,\n",
      "        99.18040759, 184.29382082, 211.78528286, 211.13288375,\n",
      "       216.64507465, 168.69752245, 193.57349357, 214.00622395,\n",
      "       168.32951   , 277.04961628, 260.96146028, 156.82804628,\n",
      "       211.79379382, 165.25249826, 223.55048858, 276.7108538 ,\n",
      "       191.76163782, 241.70231579])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([247.8262009 , 261.74448658, 324.40308386, 242.94621212,\n",
      "       176.65754742, 154.29002952, 153.07560126, 191.31567738,\n",
      "       228.94610511, 238.09168848, 267.12643163, 239.73651743,\n",
      "       296.96554321, 241.19461979, 358.19748099, 314.38420433,\n",
      "       205.25786975, 159.52202019, 227.55789279, 224.56358967])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([274.68819015, 247.35305589, 374.20690796, 282.19155374,\n",
      "       275.74591981, 318.92733395, 398.74035381, 286.78178474,\n",
      "       360.52363199, 351.88463431, 285.26129367, 335.01022329,\n",
      "       350.24709861, 309.63551757, 327.15226648, 317.45700632,\n",
      "       335.20605714, 373.26065529, 433.43357323, 315.82157975,\n",
      "       427.89093968, 289.63949291, 301.77052434, 367.48165357,\n",
      "       372.13463675, 324.83322363, 327.12874953, 320.05395004,\n",
      "       310.90629308, 380.879735  ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([387.76152403, 330.557742  , 388.11743607, 390.37958736,\n",
      "       408.2225002 , 410.23820866, 336.26542706, 319.41677054,\n",
      "       386.73270881, 408.68045252, 379.54368873, 374.68737687,\n",
      "       434.71295722, 367.58405072, 389.15715379, 372.35580464,\n",
      "       359.08299146, 423.66672844, 438.84670817, 437.61444311])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([152.42525801, 197.44811645, 274.68819015, 247.35305589,\n",
      "       374.20690796, 282.19155374, 275.74591981, 318.92733395,\n",
      "       398.74035381, 286.78178474, 360.52363199, 351.88463431,\n",
      "       285.26129367, 335.01022329, 350.24709861, 309.63551757,\n",
      "       327.15226648, 317.45700632, 335.20605714, 373.26065529,\n",
      "       433.43357323, 315.82157975, 427.89093968, 289.63949291,\n",
      "       301.77052434, 367.48165357, 372.13463675, 324.83322363,\n",
      "       327.12874953, 320.05395004])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([310.90629308, 380.879735  , 387.76152403, 330.557742  ,\n",
      "       388.11743607, 390.37958736, 408.2225002 , 410.23820866,\n",
      "       336.26542706, 319.41677054, 386.73270881, 408.68045252,\n",
      "       379.54368873, 374.68737687, 434.71295722, 367.58405072,\n",
      "       389.15715379, 372.35580464, 359.08299146, 423.66672844])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([411.5736148 , 354.91586408, 383.56673024, 364.72357312,\n",
      "       434.29575526, 413.08282734, 361.27286765, 370.69375928,\n",
      "       416.44682588, 385.58899509, 355.57554051, 401.83538562,\n",
      "       424.40409635, 391.1802466 , 378.83606312, 413.5852798 ,\n",
      "       345.70142055, 315.11381903, 346.66881669, 386.61950519,\n",
      "       426.58713648, 499.00199322, 496.48184556, 438.03423281,\n",
      "       423.59440741, 375.37548583, 404.09737706, 409.62535098,\n",
      "       437.76060348, 394.47503303])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([439.97630195, 517.19346552, 461.76251444, 457.355711  ,\n",
      "       479.62716775, 433.19914647, 443.23925318, 444.05635782,\n",
      "       446.12106416, 405.39701126, 427.84486792, 466.91572959,\n",
      "       527.49026645, 526.50191535, 580.10855533, 460.73538646,\n",
      "       484.74676984, 478.32468609, 567.74701886, 461.14155259])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 17.99426911, 114.11325318, 184.04522052, 131.63722049,\n",
      "       158.90144505, 147.21045867,  86.83575307, 115.00959466,\n",
      "       192.5315456 , 142.60208615, 193.93866008,  16.93172331,\n",
      "       103.93682066, 129.66886997, 103.55737712, 115.40403437,\n",
      "        19.93344086,  65.42368531, 126.55694824, 199.66504437,\n",
      "       128.07629968,  76.25036135,  81.087672  , 156.95619511,\n",
      "       159.44456572, 109.77291327, 143.66763433, 143.35571794,\n",
      "       181.43706483, 117.23990033])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([103.79331492, 104.97248027,  51.30083837, 116.82936428,\n",
      "       150.26046624, 142.49521639, 127.19622084,  63.7869869 ,\n",
      "        88.21464394, 110.41550736,  92.38406199, 116.29604032,\n",
      "       157.91140713, 246.51495725, 196.23187143, 170.08294304,\n",
      "       152.27811061,  53.64930572, 108.24262341, 147.88143388])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([360.51164299, 286.45978574, 296.09527034, 152.42525801,\n",
      "       197.44811645, 274.68819015, 247.35305589, 374.20690796,\n",
      "       282.19155374, 275.74591981, 318.92733395, 398.74035381,\n",
      "       286.78178474, 360.52363199, 351.88463431, 285.26129367,\n",
      "       335.01022329, 350.24709861, 309.63551757, 327.15226648,\n",
      "       317.45700632, 335.20605714, 373.26065529, 433.43357323,\n",
      "       315.82157975, 427.89093968, 289.63949291, 301.77052434,\n",
      "       367.48165357, 372.13463675])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([324.83322363, 327.12874953, 320.05395004, 310.90629308,\n",
      "       380.879735  , 387.76152403, 330.557742  , 388.11743607,\n",
      "       390.37958736, 408.2225002 , 410.23820866, 336.26542706,\n",
      "       319.41677054, 386.73270881, 408.68045252, 379.54368873,\n",
      "       374.68737687, 434.71295722, 367.58405072, 389.15715379])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([378.83606312, 413.5852798 , 345.70142055, 315.11381903,\n",
      "       346.66881669, 386.61950519, 426.58713648, 499.00199322,\n",
      "       496.48184556, 438.03423281, 423.59440741, 375.37548583,\n",
      "       404.09737706, 409.62535098, 437.76060348, 394.47503303,\n",
      "       439.97630195, 517.19346552, 461.76251444, 457.355711  ,\n",
      "       479.62716775, 433.19914647, 443.23925318, 444.05635782,\n",
      "       446.12106416, 405.39701126, 427.84486792, 466.91572959,\n",
      "       527.49026645, 526.50191535])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([580.10855533, 460.73538646, 484.74676984, 478.32468609,\n",
      "       567.74701886, 461.14155259, 402.6968913 , 407.19234085,\n",
      "       335.50445663, 383.89448025, 402.829297  , 448.14808423,\n",
      "       478.51272274, 563.09347579, 552.31094708, 462.87334599,\n",
      "       413.79781564, 472.07663971, 410.40991406, 533.57006609])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([410.89882285, 419.07923581, 381.30008063, 394.0134984 ,\n",
      "       383.27079237, 387.32550047, 417.0279523 , 358.91573723,\n",
      "       476.45969901, 381.52225147, 314.31964945, 421.39247611,\n",
      "       452.38691493, 444.22925323, 442.59260105, 411.5736148 ,\n",
      "       354.91586408, 383.56673024, 364.72357312, 434.29575526,\n",
      "       413.08282734, 361.27286765, 370.69375928, 416.44682588,\n",
      "       385.58899509, 355.57554051, 401.83538562, 424.40409635,\n",
      "       391.1802466 , 378.83606312])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([413.5852798 , 345.70142055, 315.11381903, 346.66881669,\n",
      "       386.61950519, 426.58713648, 499.00199322, 496.48184556,\n",
      "       438.03423281, 423.59440741, 375.37548583, 404.09737706,\n",
      "       409.62535098, 437.76060348, 394.47503303, 439.97630195,\n",
      "       517.19346552, 461.76251444, 457.355711  , 479.62716775])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([256.83218886, 258.63997842, 265.18095026, 255.64151055,\n",
      "       373.23511922, 357.72595578, 254.37809822, 325.97651154,\n",
      "       427.7355496 , 405.77162346, 261.78093214, 257.79632772,\n",
      "       359.61398809, 300.97891002, 323.07814374, 360.51164299,\n",
      "       286.45978574, 296.09527034, 152.42525801, 197.44811645,\n",
      "       274.68819015, 247.35305589, 374.20690796, 282.19155374,\n",
      "       275.74591981, 318.92733395, 398.74035381, 286.78178474,\n",
      "       360.52363199, 351.88463431])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([285.26129367, 335.01022329, 350.24709861, 309.63551757,\n",
      "       327.15226648, 317.45700632, 335.20605714, 373.26065529,\n",
      "       433.43357323, 315.82157975, 427.89093968, 289.63949291,\n",
      "       301.77052434, 367.48165357, 372.13463675, 324.83322363,\n",
      "       327.12874953, 320.05395004, 310.90629308, 380.879735  ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([358.91573723, 476.45969901, 381.52225147, 314.31964945,\n",
      "       421.39247611, 452.38691493, 444.22925323, 442.59260105,\n",
      "       411.5736148 , 354.91586408, 383.56673024, 364.72357312,\n",
      "       434.29575526, 413.08282734, 361.27286765, 370.69375928,\n",
      "       416.44682588, 385.58899509, 355.57554051, 401.83538562,\n",
      "       424.40409635, 391.1802466 , 378.83606312, 413.5852798 ,\n",
      "       345.70142055, 315.11381903, 346.66881669, 386.61950519,\n",
      "       426.58713648, 499.00199322])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([496.48184556, 438.03423281, 423.59440741, 375.37548583,\n",
      "       404.09737706, 409.62535098, 437.76060348, 394.47503303,\n",
      "       439.97630195, 517.19346552, 461.76251444, 457.355711  ,\n",
      "       479.62716775, 433.19914647, 443.23925318, 444.05635782,\n",
      "       446.12106416, 405.39701126, 427.84486792, 466.91572959])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([417.0279523 , 358.91573723, 476.45969901, 381.52225147,\n",
      "       314.31964945, 421.39247611, 452.38691493, 444.22925323,\n",
      "       442.59260105, 411.5736148 , 354.91586408, 383.56673024,\n",
      "       364.72357312, 434.29575526, 413.08282734, 361.27286765,\n",
      "       370.69375928, 416.44682588, 385.58899509, 355.57554051,\n",
      "       401.83538562, 424.40409635, 391.1802466 , 378.83606312,\n",
      "       413.5852798 , 345.70142055, 315.11381903, 346.66881669,\n",
      "       386.61950519, 426.58713648])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([499.00199322, 496.48184556, 438.03423281, 423.59440741,\n",
      "       375.37548583, 404.09737706, 409.62535098, 437.76060348,\n",
      "       394.47503303, 439.97630195, 517.19346552, 461.76251444,\n",
      "       457.355711  , 479.62716775, 433.19914647, 443.23925318,\n",
      "       444.05635782, 446.12106416, 405.39701126, 427.84486792])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([346.66881669, 386.61950519, 426.58713648, 499.00199322,\n",
      "       496.48184556, 438.03423281, 423.59440741, 375.37548583,\n",
      "       404.09737706, 409.62535098, 437.76060348, 394.47503303,\n",
      "       439.97630195, 517.19346552, 461.76251444, 457.355711  ,\n",
      "       479.62716775, 433.19914647, 443.23925318, 444.05635782,\n",
      "       446.12106416, 405.39701126, 427.84486792, 466.91572959,\n",
      "       527.49026645, 526.50191535, 580.10855533, 460.73538646,\n",
      "       484.74676984, 478.32468609])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([567.74701886, 461.14155259, 402.6968913 , 407.19234085,\n",
      "       335.50445663, 383.89448025, 402.829297  , 448.14808423,\n",
      "       478.51272274, 563.09347579, 552.31094708, 462.87334599,\n",
      "       413.79781564, 472.07663971, 410.40991406, 533.57006609,\n",
      "       564.84733814, 478.79369029, 382.97566996, 505.07405096])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([437.76060348, 394.47503303, 439.97630195, 517.19346552,\n",
      "       461.76251444, 457.355711  , 479.62716775, 433.19914647,\n",
      "       443.23925318, 444.05635782, 446.12106416, 405.39701126,\n",
      "       427.84486792, 466.91572959, 527.49026645, 526.50191535,\n",
      "       580.10855533, 460.73538646, 484.74676984, 478.32468609,\n",
      "       567.74701886, 461.14155259, 402.6968913 , 407.19234085,\n",
      "       335.50445663, 383.89448025, 402.829297  , 448.14808423,\n",
      "       478.51272274, 563.09347579])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([552.31094708, 462.87334599, 413.79781564, 472.07663971,\n",
      "       410.40991406, 533.57006609, 564.84733814, 478.79369029,\n",
      "       382.97566996, 505.07405096, 493.43569082, 538.94192392,\n",
      "       425.49096814, 419.33548104, 463.8273169 , 481.11941758,\n",
      "       460.64102572, 505.29460783, 443.84386519, 458.00712312])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 76.25036135,  81.087672  , 156.95619511, 159.44456572,\n",
      "       109.77291327, 143.66763433, 143.35571794, 181.43706483,\n",
      "       117.23990033, 103.79331492, 104.97248027,  51.30083837,\n",
      "       116.82936428, 150.26046624, 142.49521639, 127.19622084,\n",
      "        63.7869869 ,  88.21464394, 110.41550736,  92.38406199,\n",
      "       116.29604032, 157.91140713, 246.51495725, 196.23187143,\n",
      "       170.08294304, 152.27811061,  53.64930572, 108.24262341,\n",
      "       147.88143388, 274.83401078])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([196.4136086 , 173.2665378 , 161.73243664,  99.10058533,\n",
      "       189.91445947, 217.03019629, 218.01662357, 135.62151463,\n",
      "       215.66343029, 121.17746619, 167.28600356, 283.28864689,\n",
      "       164.69125998, 125.10364505, 159.87748372, 144.34205834,\n",
      "        82.29692285, 140.1662113 , 115.85726673, 173.41590414])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([386.73270881, 408.68045252, 379.54368873, 374.68737687,\n",
      "       434.71295722, 367.58405072, 389.15715379, 372.35580464,\n",
      "       359.08299146, 423.66672844, 438.84670817, 437.61444311,\n",
      "       463.16543088, 410.89882285, 419.07923581, 381.30008063,\n",
      "       394.0134984 , 383.27079237, 387.32550047, 417.0279523 ,\n",
      "       358.91573723, 476.45969901, 381.52225147, 314.31964945,\n",
      "       421.39247611, 452.38691493, 444.22925323, 442.59260105,\n",
      "       411.5736148 , 354.91586408])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([383.56673024, 364.72357312, 434.29575526, 413.08282734,\n",
      "       361.27286765, 370.69375928, 416.44682588, 385.58899509,\n",
      "       355.57554051, 401.83538562, 424.40409635, 391.1802466 ,\n",
      "       378.83606312, 413.5852798 , 345.70142055, 315.11381903,\n",
      "       346.66881669, 386.61950519, 426.58713648, 499.00199322])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([409.62535098, 437.76060348, 394.47503303, 439.97630195,\n",
      "       517.19346552, 461.76251444, 457.355711  , 479.62716775,\n",
      "       433.19914647, 443.23925318, 444.05635782, 446.12106416,\n",
      "       405.39701126, 427.84486792, 466.91572959, 527.49026645,\n",
      "       526.50191535, 580.10855533, 460.73538646, 484.74676984,\n",
      "       478.32468609, 567.74701886, 461.14155259, 402.6968913 ,\n",
      "       407.19234085, 335.50445663, 383.89448025, 402.829297  ,\n",
      "       448.14808423, 478.51272274])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([563.09347579, 552.31094708, 462.87334599, 413.79781564,\n",
      "       472.07663971, 410.40991406, 533.57006609, 564.84733814,\n",
      "       478.79369029, 382.97566996, 505.07405096, 493.43569082,\n",
      "       538.94192392, 425.49096814, 419.33548104, 463.8273169 ,\n",
      "       481.11941758, 460.64102572, 505.29460783, 443.84386519])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([296.09527034, 152.42525801, 197.44811645, 274.68819015,\n",
      "       247.35305589, 374.20690796, 282.19155374, 275.74591981,\n",
      "       318.92733395, 398.74035381, 286.78178474, 360.52363199,\n",
      "       351.88463431, 285.26129367, 335.01022329, 350.24709861,\n",
      "       309.63551757, 327.15226648, 317.45700632, 335.20605714,\n",
      "       373.26065529, 433.43357323, 315.82157975, 427.89093968,\n",
      "       289.63949291, 301.77052434, 367.48165357, 372.13463675,\n",
      "       324.83322363, 327.12874953])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([320.05395004, 310.90629308, 380.879735  , 387.76152403,\n",
      "       330.557742  , 388.11743607, 390.37958736, 408.2225002 ,\n",
      "       410.23820866, 336.26542706, 319.41677054, 386.73270881,\n",
      "       408.68045252, 379.54368873, 374.68737687, 434.71295722,\n",
      "       367.58405072, 389.15715379, 372.35580464, 359.08299146])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([255.64151055, 373.23511922, 357.72595578, 254.37809822,\n",
      "       325.97651154, 427.7355496 , 405.77162346, 261.78093214,\n",
      "       257.79632772, 359.61398809, 300.97891002, 323.07814374,\n",
      "       360.51164299, 286.45978574, 296.09527034, 152.42525801,\n",
      "       197.44811645, 274.68819015, 247.35305589, 374.20690796,\n",
      "       282.19155374, 275.74591981, 318.92733395, 398.74035381,\n",
      "       286.78178474, 360.52363199, 351.88463431, 285.26129367,\n",
      "       335.01022329, 350.24709861])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([309.63551757, 327.15226648, 317.45700632, 335.20605714,\n",
      "       373.26065529, 433.43357323, 315.82157975, 427.89093968,\n",
      "       289.63949291, 301.77052434, 367.48165357, 372.13463675,\n",
      "       324.83322363, 327.12874953, 320.05395004, 310.90629308,\n",
      "       380.879735  , 387.76152403, 330.557742  , 388.11743607])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([217.19312001, 314.43807321, 300.01414871, 227.87519988,\n",
      "       220.79051407, 295.02199764, 255.88545794, 278.47209267,\n",
      "       287.98710364, 252.91959566, 378.32322455, 361.8077385 ,\n",
      "       206.60592959, 258.34179417, 255.41322766, 318.51060943,\n",
      "       268.61541097, 272.76425551, 315.14139906, 348.59105903,\n",
      "       256.83218886, 258.63997842, 265.18095026, 255.64151055,\n",
      "       373.23511922, 357.72595578, 254.37809822, 325.97651154,\n",
      "       427.7355496 , 405.77162346])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([261.78093214, 257.79632772, 359.61398809, 300.97891002,\n",
      "       323.07814374, 360.51164299, 286.45978574, 296.09527034,\n",
      "       152.42525801, 197.44811645, 274.68819015, 247.35305589,\n",
      "       374.20690796, 282.19155374, 275.74591981, 318.92733395,\n",
      "       398.74035381, 286.78178474, 360.52363199, 351.88463431])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([423.59440741, 375.37548583, 404.09737706, 409.62535098,\n",
      "       437.76060348, 394.47503303, 439.97630195, 517.19346552,\n",
      "       461.76251444, 457.355711  , 479.62716775, 433.19914647,\n",
      "       443.23925318, 444.05635782, 446.12106416, 405.39701126,\n",
      "       427.84486792, 466.91572959, 527.49026645, 526.50191535,\n",
      "       580.10855533, 460.73538646, 484.74676984, 478.32468609,\n",
      "       567.74701886, 461.14155259, 402.6968913 , 407.19234085,\n",
      "       335.50445663, 383.89448025])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([402.829297  , 448.14808423, 478.51272274, 563.09347579,\n",
      "       552.31094708, 462.87334599, 413.79781564, 472.07663971,\n",
      "       410.40991406, 533.57006609, 564.84733814, 478.79369029,\n",
      "       382.97566996, 505.07405096, 493.43569082, 538.94192392,\n",
      "       425.49096814, 419.33548104, 463.8273169 , 481.11941758])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([247.35305589, 374.20690796, 282.19155374, 275.74591981,\n",
      "       318.92733395, 398.74035381, 286.78178474, 360.52363199,\n",
      "       351.88463431, 285.26129367, 335.01022329, 350.24709861,\n",
      "       309.63551757, 327.15226648, 317.45700632, 335.20605714,\n",
      "       373.26065529, 433.43357323, 315.82157975, 427.89093968,\n",
      "       289.63949291, 301.77052434, 367.48165357, 372.13463675,\n",
      "       324.83322363, 327.12874953, 320.05395004, 310.90629308,\n",
      "       380.879735  , 387.76152403])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([330.557742  , 388.11743607, 390.37958736, 408.2225002 ,\n",
      "       410.23820866, 336.26542706, 319.41677054, 386.73270881,\n",
      "       408.68045252, 379.54368873, 374.68737687, 434.71295722,\n",
      "       367.58405072, 389.15715379, 372.35580464, 359.08299146,\n",
      "       423.66672844, 438.84670817, 437.61444311, 463.16543088])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([370.69375928, 416.44682588, 385.58899509, 355.57554051,\n",
      "       401.83538562, 424.40409635, 391.1802466 , 378.83606312,\n",
      "       413.5852798 , 345.70142055, 315.11381903, 346.66881669,\n",
      "       386.61950519, 426.58713648, 499.00199322, 496.48184556,\n",
      "       438.03423281, 423.59440741, 375.37548583, 404.09737706,\n",
      "       409.62535098, 437.76060348, 394.47503303, 439.97630195,\n",
      "       517.19346552, 461.76251444, 457.355711  , 479.62716775,\n",
      "       433.19914647, 443.23925318])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([444.05635782, 446.12106416, 405.39701126, 427.84486792,\n",
      "       466.91572959, 527.49026645, 526.50191535, 580.10855533,\n",
      "       460.73538646, 484.74676984, 478.32468609, 567.74701886,\n",
      "       461.14155259, 402.6968913 , 407.19234085, 335.50445663,\n",
      "       383.89448025, 402.829297  , 448.14808423, 478.51272274])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([461.76251444, 457.355711  , 479.62716775, 433.19914647,\n",
      "       443.23925318, 444.05635782, 446.12106416, 405.39701126,\n",
      "       427.84486792, 466.91572959, 527.49026645, 526.50191535,\n",
      "       580.10855533, 460.73538646, 484.74676984, 478.32468609,\n",
      "       567.74701886, 461.14155259, 402.6968913 , 407.19234085,\n",
      "       335.50445663, 383.89448025, 402.829297  , 448.14808423,\n",
      "       478.51272274, 563.09347579, 552.31094708, 462.87334599,\n",
      "       413.79781564, 472.07663971])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([410.40991406, 533.57006609, 564.84733814, 478.79369029,\n",
      "       382.97566996, 505.07405096, 493.43569082, 538.94192392,\n",
      "       425.49096814, 419.33548104, 463.8273169 , 481.11941758,\n",
      "       460.64102572, 505.29460783, 443.84386519, 458.00712312,\n",
      "       487.44691126, 515.62869948, 536.31984124, 451.87212301])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 43.3839532 ,  42.79186531,  32.78361335, 149.68041012,\n",
      "       109.27425031,  25.89407055,  94.10193173,  29.1604104 ,\n",
      "        62.45920462, -19.259528  , -30.02332054,  54.25897839,\n",
      "       115.20075505, 104.4155928 ,  81.90616437,  66.43128026,\n",
      "         3.62500397,  16.29659574,  42.40312683, 125.84206218,\n",
      "       124.86800076,  10.80211719,  69.67090736,  73.00015151,\n",
      "        49.1362523 , 106.98454433, 155.1638923 , 163.89514009,\n",
      "        76.87066266,  66.65283723])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([105.2879106 , 152.28966719,  96.70344248,  83.77352599,\n",
      "        41.23114256,  17.99426911, 114.11325318, 184.04522052,\n",
      "       131.63722049, 158.90144505, 147.21045867,  86.83575307,\n",
      "       115.00959466, 192.5315456 , 142.60208615, 193.93866008,\n",
      "        16.93172331, 103.93682066, 129.66886997, 103.55737712])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([386.61950519, 426.58713648, 499.00199322, 496.48184556,\n",
      "       438.03423281, 423.59440741, 375.37548583, 404.09737706,\n",
      "       409.62535098, 437.76060348, 394.47503303, 439.97630195,\n",
      "       517.19346552, 461.76251444, 457.355711  , 479.62716775,\n",
      "       433.19914647, 443.23925318, 444.05635782, 446.12106416,\n",
      "       405.39701126, 427.84486792, 466.91572959, 527.49026645,\n",
      "       526.50191535, 580.10855533, 460.73538646, 484.74676984,\n",
      "       478.32468609, 567.74701886])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([461.14155259, 402.6968913 , 407.19234085, 335.50445663,\n",
      "       383.89448025, 402.829297  , 448.14808423, 478.51272274,\n",
      "       563.09347579, 552.31094708, 462.87334599, 413.79781564,\n",
      "       472.07663971, 410.40991406, 533.57006609, 564.84733814,\n",
      "       478.79369029, 382.97566996, 505.07405096, 493.43569082])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([274.83401078, 196.4136086 , 173.2665378 , 161.73243664,\n",
      "        99.10058533, 189.91445947, 217.03019629, 218.01662357,\n",
      "       135.62151463, 215.66343029, 121.17746619, 167.28600356,\n",
      "       283.28864689, 164.69125998, 125.10364505, 159.87748372,\n",
      "       144.34205834,  82.29692285, 140.1662113 , 115.85726673,\n",
      "       173.41590414, 135.43190224, 233.88614205, 168.22234072,\n",
      "       149.37417651, 211.20263936, 134.80757158, 168.40259944,\n",
      "       248.81801436, 131.08170877])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([163.32682593, 198.28054145, 230.55995397, 143.44437565,\n",
      "        99.18040759, 184.29382082, 211.78528286, 211.13288375,\n",
      "       216.64507465, 168.69752245, 193.57349357, 214.00622395,\n",
      "       168.32951   , 277.04961628, 260.96146028, 156.82804628,\n",
      "       211.79379382, 165.25249826, 223.55048858, 276.7108538 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([439.97630195, 517.19346552, 461.76251444, 457.355711  ,\n",
      "       479.62716775, 433.19914647, 443.23925318, 444.05635782,\n",
      "       446.12106416, 405.39701126, 427.84486792, 466.91572959,\n",
      "       527.49026645, 526.50191535, 580.10855533, 460.73538646,\n",
      "       484.74676984, 478.32468609, 567.74701886, 461.14155259,\n",
      "       402.6968913 , 407.19234085, 335.50445663, 383.89448025,\n",
      "       402.829297  , 448.14808423, 478.51272274, 563.09347579,\n",
      "       552.31094708, 462.87334599])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([413.79781564, 472.07663971, 410.40991406, 533.57006609,\n",
      "       564.84733814, 478.79369029, 382.97566996, 505.07405096,\n",
      "       493.43569082, 538.94192392, 425.49096814, 419.33548104,\n",
      "       463.8273169 , 481.11941758, 460.64102572, 505.29460783,\n",
      "       443.84386519, 458.00712312, 487.44691126, 515.62869948])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([372.35580464, 359.08299146, 423.66672844, 438.84670817,\n",
      "       437.61444311, 463.16543088, 410.89882285, 419.07923581,\n",
      "       381.30008063, 394.0134984 , 383.27079237, 387.32550047,\n",
      "       417.0279523 , 358.91573723, 476.45969901, 381.52225147,\n",
      "       314.31964945, 421.39247611, 452.38691493, 444.22925323,\n",
      "       442.59260105, 411.5736148 , 354.91586408, 383.56673024,\n",
      "       364.72357312, 434.29575526, 413.08282734, 361.27286765,\n",
      "       370.69375928, 416.44682588])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([385.58899509, 355.57554051, 401.83538562, 424.40409635,\n",
      "       391.1802466 , 378.83606312, 413.5852798 , 345.70142055,\n",
      "       315.11381903, 346.66881669, 386.61950519, 426.58713648,\n",
      "       499.00199322, 496.48184556, 438.03423281, 423.59440741,\n",
      "       375.37548583, 404.09737706, 409.62535098, 437.76060348])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([315.35893007, 327.78137488, 209.29589023, 180.7943296 ,\n",
      "       256.05134529, 287.62799663, 293.2503612 , 461.66825185,\n",
      "       371.01366887, 340.76572611, 337.21491073, 319.24262795,\n",
      "       265.1709739 , 298.00957865, 242.49635296, 240.13836305,\n",
      "       236.14417874, 260.83591413, 384.60274617, 226.89460208,\n",
      "       273.27294491, 203.17066281, 217.19312001, 314.43807321,\n",
      "       300.01414871, 227.87519988, 220.79051407, 295.02199764,\n",
      "       255.88545794, 278.47209267])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([287.98710364, 252.91959566, 378.32322455, 361.8077385 ,\n",
      "       206.60592959, 258.34179417, 255.41322766, 318.51060943,\n",
      "       268.61541097, 272.76425551, 315.14139906, 348.59105903,\n",
      "       256.83218886, 258.63997842, 265.18095026, 255.64151055,\n",
      "       373.23511922, 357.72595578, 254.37809822, 325.97651154])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([374.20690796, 282.19155374, 275.74591981, 318.92733395,\n",
      "       398.74035381, 286.78178474, 360.52363199, 351.88463431,\n",
      "       285.26129367, 335.01022329, 350.24709861, 309.63551757,\n",
      "       327.15226648, 317.45700632, 335.20605714, 373.26065529,\n",
      "       433.43357323, 315.82157975, 427.89093968, 289.63949291,\n",
      "       301.77052434, 367.48165357, 372.13463675, 324.83322363,\n",
      "       327.12874953, 320.05395004, 310.90629308, 380.879735  ,\n",
      "       387.76152403, 330.557742  ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([388.11743607, 390.37958736, 408.2225002 , 410.23820866,\n",
      "       336.26542706, 319.41677054, 386.73270881, 408.68045252,\n",
      "       379.54368873, 374.68737687, 434.71295722, 367.58405072,\n",
      "       389.15715379, 372.35580464, 359.08299146, 423.66672844,\n",
      "       438.84670817, 437.61444311, 463.16543088, 410.89882285])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([361.27286765, 370.69375928, 416.44682588, 385.58899509,\n",
      "       355.57554051, 401.83538562, 424.40409635, 391.1802466 ,\n",
      "       378.83606312, 413.5852798 , 345.70142055, 315.11381903,\n",
      "       346.66881669, 386.61950519, 426.58713648, 499.00199322,\n",
      "       496.48184556, 438.03423281, 423.59440741, 375.37548583,\n",
      "       404.09737706, 409.62535098, 437.76060348, 394.47503303,\n",
      "       439.97630195, 517.19346552, 461.76251444, 457.355711  ,\n",
      "       479.62716775, 433.19914647])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([443.23925318, 444.05635782, 446.12106416, 405.39701126,\n",
      "       427.84486792, 466.91572959, 527.49026645, 526.50191535,\n",
      "       580.10855533, 460.73538646, 484.74676984, 478.32468609,\n",
      "       567.74701886, 461.14155259, 402.6968913 , 407.19234085,\n",
      "       335.50445663, 383.89448025, 402.829297  , 448.14808423])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([131.63722049, 158.90144505, 147.21045867,  86.83575307,\n",
      "       115.00959466, 192.5315456 , 142.60208615, 193.93866008,\n",
      "        16.93172331, 103.93682066, 129.66886997, 103.55737712,\n",
      "       115.40403437,  19.93344086,  65.42368531, 126.55694824,\n",
      "       199.66504437, 128.07629968,  76.25036135,  81.087672  ,\n",
      "       156.95619511, 159.44456572, 109.77291327, 143.66763433,\n",
      "       143.35571794, 181.43706483, 117.23990033, 103.79331492,\n",
      "       104.97248027,  51.30083837])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([116.82936428, 150.26046624, 142.49521639, 127.19622084,\n",
      "        63.7869869 ,  88.21464394, 110.41550736,  92.38406199,\n",
      "       116.29604032, 157.91140713, 246.51495725, 196.23187143,\n",
      "       170.08294304, 152.27811061,  53.64930572, 108.24262341,\n",
      "       147.88143388, 274.83401078, 196.4136086 , 173.2665378 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([419.07923581, 381.30008063, 394.0134984 , 383.27079237,\n",
      "       387.32550047, 417.0279523 , 358.91573723, 476.45969901,\n",
      "       381.52225147, 314.31964945, 421.39247611, 452.38691493,\n",
      "       444.22925323, 442.59260105, 411.5736148 , 354.91586408,\n",
      "       383.56673024, 364.72357312, 434.29575526, 413.08282734,\n",
      "       361.27286765, 370.69375928, 416.44682588, 385.58899509,\n",
      "       355.57554051, 401.83538562, 424.40409635, 391.1802466 ,\n",
      "       378.83606312, 413.5852798 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([345.70142055, 315.11381903, 346.66881669, 386.61950519,\n",
      "       426.58713648, 499.00199322, 496.48184556, 438.03423281,\n",
      "       423.59440741, 375.37548583, 404.09737706, 409.62535098,\n",
      "       437.76060348, 394.47503303, 439.97630195, 517.19346552,\n",
      "       461.76251444, 457.355711  , 479.62716775, 433.19914647])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([567.74701886, 461.14155259, 402.6968913 , 407.19234085,\n",
      "       335.50445663, 383.89448025, 402.829297  , 448.14808423,\n",
      "       478.51272274, 563.09347579, 552.31094708, 462.87334599,\n",
      "       413.79781564, 472.07663971, 410.40991406, 533.57006609,\n",
      "       564.84733814, 478.79369029, 382.97566996, 505.07405096,\n",
      "       493.43569082, 538.94192392, 425.49096814, 419.33548104,\n",
      "       463.8273169 , 481.11941758, 460.64102572, 505.29460783,\n",
      "       443.84386519, 458.00712312])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([487.44691126, 515.62869948, 536.31984124, 451.87212301,\n",
      "       394.45919165, 521.30830427, 532.80160653, 468.19758845,\n",
      "       559.84221298, 535.04153282, 564.85427982, 530.47387032,\n",
      "       609.74709298, 635.44991269, 535.91731397, 555.47851634,\n",
      "       560.26732238, 592.77245291, 492.78699422, 528.47301518])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([298.00957865, 242.49635296, 240.13836305, 236.14417874,\n",
      "       260.83591413, 384.60274617, 226.89460208, 273.27294491,\n",
      "       203.17066281, 217.19312001, 314.43807321, 300.01414871,\n",
      "       227.87519988, 220.79051407, 295.02199764, 255.88545794,\n",
      "       278.47209267, 287.98710364, 252.91959566, 378.32322455,\n",
      "       361.8077385 , 206.60592959, 258.34179417, 255.41322766,\n",
      "       318.51060943, 268.61541097, 272.76425551, 315.14139906,\n",
      "       348.59105903, 256.83218886])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([258.63997842, 265.18095026, 255.64151055, 373.23511922,\n",
      "       357.72595578, 254.37809822, 325.97651154, 427.7355496 ,\n",
      "       405.77162346, 261.78093214, 257.79632772, 359.61398809,\n",
      "       300.97891002, 323.07814374, 360.51164299, 286.45978574,\n",
      "       296.09527034, 152.42525801, 197.44811645, 274.68819015])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([426.58713648, 499.00199322, 496.48184556, 438.03423281,\n",
      "       423.59440741, 375.37548583, 404.09737706, 409.62535098,\n",
      "       437.76060348, 394.47503303, 439.97630195, 517.19346552,\n",
      "       461.76251444, 457.355711  , 479.62716775, 433.19914647,\n",
      "       443.23925318, 444.05635782, 446.12106416, 405.39701126,\n",
      "       427.84486792, 466.91572959, 527.49026645, 526.50191535,\n",
      "       580.10855533, 460.73538646, 484.74676984, 478.32468609,\n",
      "       567.74701886, 461.14155259])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([402.6968913 , 407.19234085, 335.50445663, 383.89448025,\n",
      "       402.829297  , 448.14808423, 478.51272274, 563.09347579,\n",
      "       552.31094708, 462.87334599, 413.79781564, 472.07663971,\n",
      "       410.40991406, 533.57006609, 564.84733814, 478.79369029,\n",
      "       382.97566996, 505.07405096, 493.43569082, 538.94192392])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([149.37417651, 211.20263936, 134.80757158, 168.40259944,\n",
      "       248.81801436, 131.08170877, 163.32682593, 198.28054145,\n",
      "       230.55995397, 143.44437565,  99.18040759, 184.29382082,\n",
      "       211.78528286, 211.13288375, 216.64507465, 168.69752245,\n",
      "       193.57349357, 214.00622395, 168.32951   , 277.04961628,\n",
      "       260.96146028, 156.82804628, 211.79379382, 165.25249826,\n",
      "       223.55048858, 276.7108538 , 191.76163782, 241.70231579,\n",
      "       247.8262009 , 261.74448658])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([324.40308386, 242.94621212, 176.65754742, 154.29002952,\n",
      "       153.07560126, 191.31567738, 228.94610511, 238.09168848,\n",
      "       267.12643163, 239.73651743, 296.96554321, 241.19461979,\n",
      "       358.19748099, 314.38420433, 205.25786975, 159.52202019,\n",
      "       227.55789279, 224.56358967, 261.6293727 , 269.64979598])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([383.27079237, 387.32550047, 417.0279523 , 358.91573723,\n",
      "       476.45969901, 381.52225147, 314.31964945, 421.39247611,\n",
      "       452.38691493, 444.22925323, 442.59260105, 411.5736148 ,\n",
      "       354.91586408, 383.56673024, 364.72357312, 434.29575526,\n",
      "       413.08282734, 361.27286765, 370.69375928, 416.44682588,\n",
      "       385.58899509, 355.57554051, 401.83538562, 424.40409635,\n",
      "       391.1802466 , 378.83606312, 413.5852798 , 345.70142055,\n",
      "       315.11381903, 346.66881669])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([386.61950519, 426.58713648, 499.00199322, 496.48184556,\n",
      "       438.03423281, 423.59440741, 375.37548583, 404.09737706,\n",
      "       409.62535098, 437.76060348, 394.47503303, 439.97630195,\n",
      "       517.19346552, 461.76251444, 457.355711  , 479.62716775,\n",
      "       433.19914647, 443.23925318, 444.05635782, 446.12106416])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([413.08282734, 361.27286765, 370.69375928, 416.44682588,\n",
      "       385.58899509, 355.57554051, 401.83538562, 424.40409635,\n",
      "       391.1802466 , 378.83606312, 413.5852798 , 345.70142055,\n",
      "       315.11381903, 346.66881669, 386.61950519, 426.58713648,\n",
      "       499.00199322, 496.48184556, 438.03423281, 423.59440741,\n",
      "       375.37548583, 404.09737706, 409.62535098, 437.76060348,\n",
      "       394.47503303, 439.97630195, 517.19346552, 461.76251444,\n",
      "       457.355711  , 479.62716775])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([433.19914647, 443.23925318, 444.05635782, 446.12106416,\n",
      "       405.39701126, 427.84486792, 466.91572959, 527.49026645,\n",
      "       526.50191535, 580.10855533, 460.73538646, 484.74676984,\n",
      "       478.32468609, 567.74701886, 461.14155259, 402.6968913 ,\n",
      "       407.19234085, 335.50445663, 383.89448025, 402.829297  ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([517.19346552, 461.76251444, 457.355711  , 479.62716775,\n",
      "       433.19914647, 443.23925318, 444.05635782, 446.12106416,\n",
      "       405.39701126, 427.84486792, 466.91572959, 527.49026645,\n",
      "       526.50191535, 580.10855533, 460.73538646, 484.74676984,\n",
      "       478.32468609, 567.74701886, 461.14155259, 402.6968913 ,\n",
      "       407.19234085, 335.50445663, 383.89448025, 402.829297  ,\n",
      "       448.14808423, 478.51272274, 563.09347579, 552.31094708,\n",
      "       462.87334599, 413.79781564])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([472.07663971, 410.40991406, 533.57006609, 564.84733814,\n",
      "       478.79369029, 382.97566996, 505.07405096, 493.43569082,\n",
      "       538.94192392, 425.49096814, 419.33548104, 463.8273169 ,\n",
      "       481.11941758, 460.64102572, 505.29460783, 443.84386519,\n",
      "       458.00712312, 487.44691126, 515.62869948, 536.31984124])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([527.49026645, 526.50191535, 580.10855533, 460.73538646,\n",
      "       484.74676984, 478.32468609, 567.74701886, 461.14155259,\n",
      "       402.6968913 , 407.19234085, 335.50445663, 383.89448025,\n",
      "       402.829297  , 448.14808423, 478.51272274, 563.09347579,\n",
      "       552.31094708, 462.87334599, 413.79781564, 472.07663971,\n",
      "       410.40991406, 533.57006609, 564.84733814, 478.79369029,\n",
      "       382.97566996, 505.07405096, 493.43569082, 538.94192392,\n",
      "       425.49096814, 419.33548104])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([463.8273169 , 481.11941758, 460.64102572, 505.29460783,\n",
      "       443.84386519, 458.00712312, 487.44691126, 515.62869948,\n",
      "       536.31984124, 451.87212301, 394.45919165, 521.30830427,\n",
      "       532.80160653, 468.19758845, 559.84221298, 535.04153282,\n",
      "       564.85427982, 530.47387032, 609.74709298, 635.44991269])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([336.26542706, 319.41677054, 386.73270881, 408.68045252,\n",
      "       379.54368873, 374.68737687, 434.71295722, 367.58405072,\n",
      "       389.15715379, 372.35580464, 359.08299146, 423.66672844,\n",
      "       438.84670817, 437.61444311, 463.16543088, 410.89882285,\n",
      "       419.07923581, 381.30008063, 394.0134984 , 383.27079237,\n",
      "       387.32550047, 417.0279523 , 358.91573723, 476.45969901,\n",
      "       381.52225147, 314.31964945, 421.39247611, 452.38691493,\n",
      "       444.22925323, 442.59260105])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([411.5736148 , 354.91586408, 383.56673024, 364.72357312,\n",
      "       434.29575526, 413.08282734, 361.27286765, 370.69375928,\n",
      "       416.44682588, 385.58899509, 355.57554051, 401.83538562,\n",
      "       424.40409635, 391.1802466 , 378.83606312, 413.5852798 ,\n",
      "       345.70142055, 315.11381903, 346.66881669, 386.61950519])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([215.66343029, 121.17746619, 167.28600356, 283.28864689,\n",
      "       164.69125998, 125.10364505, 159.87748372, 144.34205834,\n",
      "        82.29692285, 140.1662113 , 115.85726673, 173.41590414,\n",
      "       135.43190224, 233.88614205, 168.22234072, 149.37417651,\n",
      "       211.20263936, 134.80757158, 168.40259944, 248.81801436,\n",
      "       131.08170877, 163.32682593, 198.28054145, 230.55995397,\n",
      "       143.44437565,  99.18040759, 184.29382082, 211.78528286,\n",
      "       211.13288375, 216.64507465])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([168.69752245, 193.57349357, 214.00622395, 168.32951   ,\n",
      "       277.04961628, 260.96146028, 156.82804628, 211.79379382,\n",
      "       165.25249826, 223.55048858, 276.7108538 , 191.76163782,\n",
      "       241.70231579, 247.8262009 , 261.74448658, 324.40308386,\n",
      "       242.94621212, 176.65754742, 154.29002952, 153.07560126])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([407.19234085, 335.50445663, 383.89448025, 402.829297  ,\n",
      "       448.14808423, 478.51272274, 563.09347579, 552.31094708,\n",
      "       462.87334599, 413.79781564, 472.07663971, 410.40991406,\n",
      "       533.57006609, 564.84733814, 478.79369029, 382.97566996,\n",
      "       505.07405096, 493.43569082, 538.94192392, 425.49096814,\n",
      "       419.33548104, 463.8273169 , 481.11941758, 460.64102572,\n",
      "       505.29460783, 443.84386519, 458.00712312, 487.44691126,\n",
      "       515.62869948, 536.31984124])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([451.87212301, 394.45919165, 521.30830427, 532.80160653,\n",
      "       468.19758845, 559.84221298, 535.04153282, 564.85427982,\n",
      "       530.47387032, 609.74709298, 635.44991269, 535.91731397,\n",
      "       555.47851634, 560.26732238, 592.77245291, 492.78699422,\n",
      "       528.47301518, 576.62673148, 449.02573821, 420.73057322])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([257.79632772, 359.61398809, 300.97891002, 323.07814374,\n",
      "       360.51164299, 286.45978574, 296.09527034, 152.42525801,\n",
      "       197.44811645, 274.68819015, 247.35305589, 374.20690796,\n",
      "       282.19155374, 275.74591981, 318.92733395, 398.74035381,\n",
      "       286.78178474, 360.52363199, 351.88463431, 285.26129367,\n",
      "       335.01022329, 350.24709861, 309.63551757, 327.15226648,\n",
      "       317.45700632, 335.20605714, 373.26065529, 433.43357323,\n",
      "       315.82157975, 427.89093968])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([289.63949291, 301.77052434, 367.48165357, 372.13463675,\n",
      "       324.83322363, 327.12874953, 320.05395004, 310.90629308,\n",
      "       380.879735  , 387.76152403, 330.557742  , 388.11743607,\n",
      "       390.37958736, 408.2225002 , 410.23820866, 336.26542706,\n",
      "       319.41677054, 386.73270881, 408.68045252, 379.54368873])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([404.09737706, 409.62535098, 437.76060348, 394.47503303,\n",
      "       439.97630195, 517.19346552, 461.76251444, 457.355711  ,\n",
      "       479.62716775, 433.19914647, 443.23925318, 444.05635782,\n",
      "       446.12106416, 405.39701126, 427.84486792, 466.91572959,\n",
      "       527.49026645, 526.50191535, 580.10855533, 460.73538646,\n",
      "       484.74676984, 478.32468609, 567.74701886, 461.14155259,\n",
      "       402.6968913 , 407.19234085, 335.50445663, 383.89448025,\n",
      "       402.829297  , 448.14808423])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([478.51272274, 563.09347579, 552.31094708, 462.87334599,\n",
      "       413.79781564, 472.07663971, 410.40991406, 533.57006609,\n",
      "       564.84733814, 478.79369029, 382.97566996, 505.07405096,\n",
      "       493.43569082, 538.94192392, 425.49096814, 419.33548104,\n",
      "       463.8273169 , 481.11941758, 460.64102572, 505.29460783])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([394.47503303, 439.97630195, 517.19346552, 461.76251444,\n",
      "       457.355711  , 479.62716775, 433.19914647, 443.23925318,\n",
      "       444.05635782, 446.12106416, 405.39701126, 427.84486792,\n",
      "       466.91572959, 527.49026645, 526.50191535, 580.10855533,\n",
      "       460.73538646, 484.74676984, 478.32468609, 567.74701886,\n",
      "       461.14155259, 402.6968913 , 407.19234085, 335.50445663,\n",
      "       383.89448025, 402.829297  , 448.14808423, 478.51272274,\n",
      "       563.09347579, 552.31094708])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([462.87334599, 413.79781564, 472.07663971, 410.40991406,\n",
      "       533.57006609, 564.84733814, 478.79369029, 382.97566996,\n",
      "       505.07405096, 493.43569082, 538.94192392, 425.49096814,\n",
      "       419.33548104, 463.8273169 , 481.11941758, 460.64102572,\n",
      "       505.29460783, 443.84386519, 458.00712312, 487.44691126])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([472.07663971, 410.40991406, 533.57006609, 564.84733814,\n",
      "       478.79369029, 382.97566996, 505.07405096, 493.43569082,\n",
      "       538.94192392, 425.49096814, 419.33548104, 463.8273169 ,\n",
      "       481.11941758, 460.64102572, 505.29460783, 443.84386519,\n",
      "       458.00712312, 487.44691126, 515.62869948, 536.31984124,\n",
      "       451.87212301, 394.45919165, 521.30830427, 532.80160653,\n",
      "       468.19758845, 559.84221298, 535.04153282, 564.85427982,\n",
      "       530.47387032, 609.74709298])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([635.44991269, 535.91731397, 555.47851634, 560.26732238,\n",
      "       592.77245291, 492.78699422, 528.47301518, 576.62673148,\n",
      "       449.02573821, 420.73057322, 383.2050939 , 453.90534066,\n",
      "       539.24259306, 605.47121043, 556.67616859, 608.30828456,\n",
      "       489.31267053, 414.13221475, 481.92922581, 537.03024237])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([402.6968913 , 407.19234085, 335.50445663, 383.89448025,\n",
      "       402.829297  , 448.14808423, 478.51272274, 563.09347579,\n",
      "       552.31094708, 462.87334599, 413.79781564, 472.07663971,\n",
      "       410.40991406, 533.57006609, 564.84733814, 478.79369029,\n",
      "       382.97566996, 505.07405096, 493.43569082, 538.94192392,\n",
      "       425.49096814, 419.33548104, 463.8273169 , 481.11941758,\n",
      "       460.64102572, 505.29460783, 443.84386519, 458.00712312,\n",
      "       487.44691126, 515.62869948])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([536.31984124, 451.87212301, 394.45919165, 521.30830427,\n",
      "       532.80160653, 468.19758845, 559.84221298, 535.04153282,\n",
      "       564.85427982, 530.47387032, 609.74709298, 635.44991269,\n",
      "       535.91731397, 555.47851634, 560.26732238, 592.77245291,\n",
      "       492.78699422, 528.47301518, 576.62673148, 449.02573821])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([564.84733814, 478.79369029, 382.97566996, 505.07405096,\n",
      "       493.43569082, 538.94192392, 425.49096814, 419.33548104,\n",
      "       463.8273169 , 481.11941758, 460.64102572, 505.29460783,\n",
      "       443.84386519, 458.00712312, 487.44691126, 515.62869948,\n",
      "       536.31984124, 451.87212301, 394.45919165, 521.30830427,\n",
      "       532.80160653, 468.19758845, 559.84221298, 535.04153282,\n",
      "       564.85427982, 530.47387032, 609.74709298, 635.44991269,\n",
      "       535.91731397, 555.47851634])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([560.26732238, 592.77245291, 492.78699422, 528.47301518,\n",
      "       576.62673148, 449.02573821, 420.73057322, 383.2050939 ,\n",
      "       453.90534066, 539.24259306, 605.47121043, 556.67616859,\n",
      "       608.30828456, 489.31267053, 414.13221475, 481.92922581,\n",
      "       537.03024237, 531.27077547, 424.76439816, 480.27019493])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([438.03423281, 423.59440741, 375.37548583, 404.09737706,\n",
      "       409.62535098, 437.76060348, 394.47503303, 439.97630195,\n",
      "       517.19346552, 461.76251444, 457.355711  , 479.62716775,\n",
      "       433.19914647, 443.23925318, 444.05635782, 446.12106416,\n",
      "       405.39701126, 427.84486792, 466.91572959, 527.49026645,\n",
      "       526.50191535, 580.10855533, 460.73538646, 484.74676984,\n",
      "       478.32468609, 567.74701886, 461.14155259, 402.6968913 ,\n",
      "       407.19234085, 335.50445663])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([383.89448025, 402.829297  , 448.14808423, 478.51272274,\n",
      "       563.09347579, 552.31094708, 462.87334599, 413.79781564,\n",
      "       472.07663971, 410.40991406, 533.57006609, 564.84733814,\n",
      "       478.79369029, 382.97566996, 505.07405096, 493.43569082,\n",
      "       538.94192392, 425.49096814, 419.33548104, 463.8273169 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([452.38691493, 444.22925323, 442.59260105, 411.5736148 ,\n",
      "       354.91586408, 383.56673024, 364.72357312, 434.29575526,\n",
      "       413.08282734, 361.27286765, 370.69375928, 416.44682588,\n",
      "       385.58899509, 355.57554051, 401.83538562, 424.40409635,\n",
      "       391.1802466 , 378.83606312, 413.5852798 , 345.70142055,\n",
      "       315.11381903, 346.66881669, 386.61950519, 426.58713648,\n",
      "       499.00199322, 496.48184556, 438.03423281, 423.59440741,\n",
      "       375.37548583, 404.09737706])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([409.62535098, 437.76060348, 394.47503303, 439.97630195,\n",
      "       517.19346552, 461.76251444, 457.355711  , 479.62716775,\n",
      "       433.19914647, 443.23925318, 444.05635782, 446.12106416,\n",
      "       405.39701126, 427.84486792, 466.91572959, 527.49026645,\n",
      "       526.50191535, 580.10855533, 460.73538646, 484.74676984])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([315.11381903, 346.66881669, 386.61950519, 426.58713648,\n",
      "       499.00199322, 496.48184556, 438.03423281, 423.59440741,\n",
      "       375.37548583, 404.09737706, 409.62535098, 437.76060348,\n",
      "       394.47503303, 439.97630195, 517.19346552, 461.76251444,\n",
      "       457.355711  , 479.62716775, 433.19914647, 443.23925318,\n",
      "       444.05635782, 446.12106416, 405.39701126, 427.84486792,\n",
      "       466.91572959, 527.49026645, 526.50191535, 580.10855533,\n",
      "       460.73538646, 484.74676984])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([478.32468609, 567.74701886, 461.14155259, 402.6968913 ,\n",
      "       407.19234085, 335.50445663, 383.89448025, 402.829297  ,\n",
      "       448.14808423, 478.51272274, 563.09347579, 552.31094708,\n",
      "       462.87334599, 413.79781564, 472.07663971, 410.40991406,\n",
      "       533.57006609, 564.84733814, 478.79369029, 382.97566996])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([444.05635782, 446.12106416, 405.39701126, 427.84486792,\n",
      "       466.91572959, 527.49026645, 526.50191535, 580.10855533,\n",
      "       460.73538646, 484.74676984, 478.32468609, 567.74701886,\n",
      "       461.14155259, 402.6968913 , 407.19234085, 335.50445663,\n",
      "       383.89448025, 402.829297  , 448.14808423, 478.51272274,\n",
      "       563.09347579, 552.31094708, 462.87334599, 413.79781564,\n",
      "       472.07663971, 410.40991406, 533.57006609, 564.84733814,\n",
      "       478.79369029, 382.97566996])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([505.07405096, 493.43569082, 538.94192392, 425.49096814,\n",
      "       419.33548104, 463.8273169 , 481.11941758, 460.64102572,\n",
      "       505.29460783, 443.84386519, 458.00712312, 487.44691126,\n",
      "       515.62869948, 536.31984124, 451.87212301, 394.45919165,\n",
      "       521.30830427, 532.80160653, 468.19758845, 559.84221298])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([261.78093214, 257.79632772, 359.61398809, 300.97891002,\n",
      "       323.07814374, 360.51164299, 286.45978574, 296.09527034,\n",
      "       152.42525801, 197.44811645, 274.68819015, 247.35305589,\n",
      "       374.20690796, 282.19155374, 275.74591981, 318.92733395,\n",
      "       398.74035381, 286.78178474, 360.52363199, 351.88463431,\n",
      "       285.26129367, 335.01022329, 350.24709861, 309.63551757,\n",
      "       327.15226648, 317.45700632, 335.20605714, 373.26065529,\n",
      "       433.43357323, 315.82157975])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([427.89093968, 289.63949291, 301.77052434, 367.48165357,\n",
      "       372.13463675, 324.83322363, 327.12874953, 320.05395004,\n",
      "       310.90629308, 380.879735  , 387.76152403, 330.557742  ,\n",
      "       388.11743607, 390.37958736, 408.2225002 , 410.23820866,\n",
      "       336.26542706, 319.41677054, 386.73270881, 408.68045252])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([240.13836305, 236.14417874, 260.83591413, 384.60274617,\n",
      "       226.89460208, 273.27294491, 203.17066281, 217.19312001,\n",
      "       314.43807321, 300.01414871, 227.87519988, 220.79051407,\n",
      "       295.02199764, 255.88545794, 278.47209267, 287.98710364,\n",
      "       252.91959566, 378.32322455, 361.8077385 , 206.60592959,\n",
      "       258.34179417, 255.41322766, 318.51060943, 268.61541097,\n",
      "       272.76425551, 315.14139906, 348.59105903, 256.83218886,\n",
      "       258.63997842, 265.18095026])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([255.64151055, 373.23511922, 357.72595578, 254.37809822,\n",
      "       325.97651154, 427.7355496 , 405.77162346, 261.78093214,\n",
      "       257.79632772, 359.61398809, 300.97891002, 323.07814374,\n",
      "       360.51164299, 286.45978574, 296.09527034, 152.42525801,\n",
      "       197.44811645, 274.68819015, 247.35305589, 374.20690796])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([484.74676984, 478.32468609, 567.74701886, 461.14155259,\n",
      "       402.6968913 , 407.19234085, 335.50445663, 383.89448025,\n",
      "       402.829297  , 448.14808423, 478.51272274, 563.09347579,\n",
      "       552.31094708, 462.87334599, 413.79781564, 472.07663971,\n",
      "       410.40991406, 533.57006609, 564.84733814, 478.79369029,\n",
      "       382.97566996, 505.07405096, 493.43569082, 538.94192392,\n",
      "       425.49096814, 419.33548104, 463.8273169 , 481.11941758,\n",
      "       460.64102572, 505.29460783])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([443.84386519, 458.00712312, 487.44691126, 515.62869948,\n",
      "       536.31984124, 451.87212301, 394.45919165, 521.30830427,\n",
      "       532.80160653, 468.19758845, 559.84221298, 535.04153282,\n",
      "       564.85427982, 530.47387032, 609.74709298, 635.44991269,\n",
      "       535.91731397, 555.47851634, 560.26732238, 592.77245291])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([462.87334599, 413.79781564, 472.07663971, 410.40991406,\n",
      "       533.57006609, 564.84733814, 478.79369029, 382.97566996,\n",
      "       505.07405096, 493.43569082, 538.94192392, 425.49096814,\n",
      "       419.33548104, 463.8273169 , 481.11941758, 460.64102572,\n",
      "       505.29460783, 443.84386519, 458.00712312, 487.44691126,\n",
      "       515.62869948, 536.31984124, 451.87212301, 394.45919165,\n",
      "       521.30830427, 532.80160653, 468.19758845, 559.84221298,\n",
      "       535.04153282, 564.85427982])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([530.47387032, 609.74709298, 635.44991269, 535.91731397,\n",
      "       555.47851634, 560.26732238, 592.77245291, 492.78699422,\n",
      "       528.47301518, 576.62673148, 449.02573821, 420.73057322,\n",
      "       383.2050939 , 453.90534066, 539.24259306, 605.47121043,\n",
      "       556.67616859, 608.30828456, 489.31267053, 414.13221475])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([320.05395004, 310.90629308, 380.879735  , 387.76152403,\n",
      "       330.557742  , 388.11743607, 390.37958736, 408.2225002 ,\n",
      "       410.23820866, 336.26542706, 319.41677054, 386.73270881,\n",
      "       408.68045252, 379.54368873, 374.68737687, 434.71295722,\n",
      "       367.58405072, 389.15715379, 372.35580464, 359.08299146,\n",
      "       423.66672844, 438.84670817, 437.61444311, 463.16543088,\n",
      "       410.89882285, 419.07923581, 381.30008063, 394.0134984 ,\n",
      "       383.27079237, 387.32550047])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([417.0279523 , 358.91573723, 476.45969901, 381.52225147,\n",
      "       314.31964945, 421.39247611, 452.38691493, 444.22925323,\n",
      "       442.59260105, 411.5736148 , 354.91586408, 383.56673024,\n",
      "       364.72357312, 434.29575526, 413.08282734, 361.27286765,\n",
      "       370.69375928, 416.44682588, 385.58899509, 355.57554051])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([410.23820866, 336.26542706, 319.41677054, 386.73270881,\n",
      "       408.68045252, 379.54368873, 374.68737687, 434.71295722,\n",
      "       367.58405072, 389.15715379, 372.35580464, 359.08299146,\n",
      "       423.66672844, 438.84670817, 437.61444311, 463.16543088,\n",
      "       410.89882285, 419.07923581, 381.30008063, 394.0134984 ,\n",
      "       383.27079237, 387.32550047, 417.0279523 , 358.91573723,\n",
      "       476.45969901, 381.52225147, 314.31964945, 421.39247611,\n",
      "       452.38691493, 444.22925323])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([442.59260105, 411.5736148 , 354.91586408, 383.56673024,\n",
      "       364.72357312, 434.29575526, 413.08282734, 361.27286765,\n",
      "       370.69375928, 416.44682588, 385.58899509, 355.57554051,\n",
      "       401.83538562, 424.40409635, 391.1802466 , 378.83606312,\n",
      "       413.5852798 , 345.70142055, 315.11381903, 346.66881669])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([364.72357312, 434.29575526, 413.08282734, 361.27286765,\n",
      "       370.69375928, 416.44682588, 385.58899509, 355.57554051,\n",
      "       401.83538562, 424.40409635, 391.1802466 , 378.83606312,\n",
      "       413.5852798 , 345.70142055, 315.11381903, 346.66881669,\n",
      "       386.61950519, 426.58713648, 499.00199322, 496.48184556,\n",
      "       438.03423281, 423.59440741, 375.37548583, 404.09737706,\n",
      "       409.62535098, 437.76060348, 394.47503303, 439.97630195,\n",
      "       517.19346552, 461.76251444])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([457.355711  , 479.62716775, 433.19914647, 443.23925318,\n",
      "       444.05635782, 446.12106416, 405.39701126, 427.84486792,\n",
      "       466.91572959, 527.49026645, 526.50191535, 580.10855533,\n",
      "       460.73538646, 484.74676984, 478.32468609, 567.74701886,\n",
      "       461.14155259, 402.6968913 , 407.19234085, 335.50445663])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([350.24709861, 309.63551757, 327.15226648, 317.45700632,\n",
      "       335.20605714, 373.26065529, 433.43357323, 315.82157975,\n",
      "       427.89093968, 289.63949291, 301.77052434, 367.48165357,\n",
      "       372.13463675, 324.83322363, 327.12874953, 320.05395004,\n",
      "       310.90629308, 380.879735  , 387.76152403, 330.557742  ,\n",
      "       388.11743607, 390.37958736, 408.2225002 , 410.23820866,\n",
      "       336.26542706, 319.41677054, 386.73270881, 408.68045252,\n",
      "       379.54368873, 374.68737687])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([434.71295722, 367.58405072, 389.15715379, 372.35580464,\n",
      "       359.08299146, 423.66672844, 438.84670817, 437.61444311,\n",
      "       463.16543088, 410.89882285, 419.07923581, 381.30008063,\n",
      "       394.0134984 , 383.27079237, 387.32550047, 417.0279523 ,\n",
      "       358.91573723, 476.45969901, 381.52225147, 314.31964945])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([383.89448025, 402.829297  , 448.14808423, 478.51272274,\n",
      "       563.09347579, 552.31094708, 462.87334599, 413.79781564,\n",
      "       472.07663971, 410.40991406, 533.57006609, 564.84733814,\n",
      "       478.79369029, 382.97566996, 505.07405096, 493.43569082,\n",
      "       538.94192392, 425.49096814, 419.33548104, 463.8273169 ,\n",
      "       481.11941758, 460.64102572, 505.29460783, 443.84386519,\n",
      "       458.00712312, 487.44691126, 515.62869948, 536.31984124,\n",
      "       451.87212301, 394.45919165])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([521.30830427, 532.80160653, 468.19758845, 559.84221298,\n",
      "       535.04153282, 564.85427982, 530.47387032, 609.74709298,\n",
      "       635.44991269, 535.91731397, 555.47851634, 560.26732238,\n",
      "       592.77245291, 492.78699422, 528.47301518, 576.62673148,\n",
      "       449.02573821, 420.73057322, 383.2050939 , 453.90534066])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([463.8273169 , 481.11941758, 460.64102572, 505.29460783,\n",
      "       443.84386519, 458.00712312, 487.44691126, 515.62869948,\n",
      "       536.31984124, 451.87212301, 394.45919165, 521.30830427,\n",
      "       532.80160653, 468.19758845, 559.84221298, 535.04153282,\n",
      "       564.85427982, 530.47387032, 609.74709298, 635.44991269,\n",
      "       535.91731397, 555.47851634, 560.26732238, 592.77245291,\n",
      "       492.78699422, 528.47301518, 576.62673148, 449.02573821,\n",
      "       420.73057322, 383.2050939 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([453.90534066, 539.24259306, 605.47121043, 556.67616859,\n",
      "       608.30828456, 489.31267053, 414.13221475, 481.92922581,\n",
      "       537.03024237, 531.27077547, 424.76439816, 480.27019493,\n",
      "       456.60722947, 534.68799708, 560.27001124, 495.50885821,\n",
      "       490.43221638, 469.54868894, 509.0216617 , 580.5186044 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([413.5852798 , 345.70142055, 315.11381903, 346.66881669,\n",
      "       386.61950519, 426.58713648, 499.00199322, 496.48184556,\n",
      "       438.03423281, 423.59440741, 375.37548583, 404.09737706,\n",
      "       409.62535098, 437.76060348, 394.47503303, 439.97630195,\n",
      "       517.19346552, 461.76251444, 457.355711  , 479.62716775,\n",
      "       433.19914647, 443.23925318, 444.05635782, 446.12106416,\n",
      "       405.39701126, 427.84486792, 466.91572959, 527.49026645,\n",
      "       526.50191535, 580.10855533])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([460.73538646, 484.74676984, 478.32468609, 567.74701886,\n",
      "       461.14155259, 402.6968913 , 407.19234085, 335.50445663,\n",
      "       383.89448025, 402.829297  , 448.14808423, 478.51272274,\n",
      "       563.09347579, 552.31094708, 462.87334599, 413.79781564,\n",
      "       472.07663971, 410.40991406, 533.57006609, 564.84733814])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([463.16543088, 410.89882285, 419.07923581, 381.30008063,\n",
      "       394.0134984 , 383.27079237, 387.32550047, 417.0279523 ,\n",
      "       358.91573723, 476.45969901, 381.52225147, 314.31964945,\n",
      "       421.39247611, 452.38691493, 444.22925323, 442.59260105,\n",
      "       411.5736148 , 354.91586408, 383.56673024, 364.72357312,\n",
      "       434.29575526, 413.08282734, 361.27286765, 370.69375928,\n",
      "       416.44682588, 385.58899509, 355.57554051, 401.83538562,\n",
      "       424.40409635, 391.1802466 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([378.83606312, 413.5852798 , 345.70142055, 315.11381903,\n",
      "       346.66881669, 386.61950519, 426.58713648, 499.00199322,\n",
      "       496.48184556, 438.03423281, 423.59440741, 375.37548583,\n",
      "       404.09737706, 409.62535098, 437.76060348, 394.47503303,\n",
      "       439.97630195, 517.19346552, 461.76251444, 457.355711  ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([367.58405072, 389.15715379, 372.35580464, 359.08299146,\n",
      "       423.66672844, 438.84670817, 437.61444311, 463.16543088,\n",
      "       410.89882285, 419.07923581, 381.30008063, 394.0134984 ,\n",
      "       383.27079237, 387.32550047, 417.0279523 , 358.91573723,\n",
      "       476.45969901, 381.52225147, 314.31964945, 421.39247611,\n",
      "       452.38691493, 444.22925323, 442.59260105, 411.5736148 ,\n",
      "       354.91586408, 383.56673024, 364.72357312, 434.29575526,\n",
      "       413.08282734, 361.27286765])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([370.69375928, 416.44682588, 385.58899509, 355.57554051,\n",
      "       401.83538562, 424.40409635, 391.1802466 , 378.83606312,\n",
      "       413.5852798 , 345.70142055, 315.11381903, 346.66881669,\n",
      "       386.61950519, 426.58713648, 499.00199322, 496.48184556,\n",
      "       438.03423281, 423.59440741, 375.37548583, 404.09737706])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([260.96146028, 156.82804628, 211.79379382, 165.25249826,\n",
      "       223.55048858, 276.7108538 , 191.76163782, 241.70231579,\n",
      "       247.8262009 , 261.74448658, 324.40308386, 242.94621212,\n",
      "       176.65754742, 154.29002952, 153.07560126, 191.31567738,\n",
      "       228.94610511, 238.09168848, 267.12643163, 239.73651743,\n",
      "       296.96554321, 241.19461979, 358.19748099, 314.38420433,\n",
      "       205.25786975, 159.52202019, 227.55789279, 224.56358967,\n",
      "       261.6293727 , 269.64979598])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([241.11349397, 191.94529955, 141.51864415, 178.65095344,\n",
      "       264.60280642, 264.23905845, 183.7540556 , 223.49117695,\n",
      "       260.65355218, 206.39405506, 233.54247196, 248.43310869,\n",
      "       190.08114942, 240.53880393, 280.09782758, 315.35893007,\n",
      "       327.78137488, 209.29589023, 180.7943296 , 256.05134529])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([282.19155374, 275.74591981, 318.92733395, 398.74035381,\n",
      "       286.78178474, 360.52363199, 351.88463431, 285.26129367,\n",
      "       335.01022329, 350.24709861, 309.63551757, 327.15226648,\n",
      "       317.45700632, 335.20605714, 373.26065529, 433.43357323,\n",
      "       315.82157975, 427.89093968, 289.63949291, 301.77052434,\n",
      "       367.48165357, 372.13463675, 324.83322363, 327.12874953,\n",
      "       320.05395004, 310.90629308, 380.879735  , 387.76152403,\n",
      "       330.557742  , 388.11743607])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([390.37958736, 408.2225002 , 410.23820866, 336.26542706,\n",
      "       319.41677054, 386.73270881, 408.68045252, 379.54368873,\n",
      "       374.68737687, 434.71295722, 367.58405072, 389.15715379,\n",
      "       372.35580464, 359.08299146, 423.66672844, 438.84670817,\n",
      "       437.61444311, 463.16543088, 410.89882285, 419.07923581])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([410.40991406, 533.57006609, 564.84733814, 478.79369029,\n",
      "       382.97566996, 505.07405096, 493.43569082, 538.94192392,\n",
      "       425.49096814, 419.33548104, 463.8273169 , 481.11941758,\n",
      "       460.64102572, 505.29460783, 443.84386519, 458.00712312,\n",
      "       487.44691126, 515.62869948, 536.31984124, 451.87212301,\n",
      "       394.45919165, 521.30830427, 532.80160653, 468.19758845,\n",
      "       559.84221298, 535.04153282, 564.85427982, 530.47387032,\n",
      "       609.74709298, 635.44991269])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([535.91731397, 555.47851634, 560.26732238, 592.77245291,\n",
      "       492.78699422, 528.47301518, 576.62673148, 449.02573821,\n",
      "       420.73057322, 383.2050939 , 453.90534066, 539.24259306,\n",
      "       605.47121043, 556.67616859, 608.30828456, 489.31267053,\n",
      "       414.13221475, 481.92922581, 537.03024237, 531.27077547])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([499.00199322, 496.48184556, 438.03423281, 423.59440741,\n",
      "       375.37548583, 404.09737706, 409.62535098, 437.76060348,\n",
      "       394.47503303, 439.97630195, 517.19346552, 461.76251444,\n",
      "       457.355711  , 479.62716775, 433.19914647, 443.23925318,\n",
      "       444.05635782, 446.12106416, 405.39701126, 427.84486792,\n",
      "       466.91572959, 527.49026645, 526.50191535, 580.10855533,\n",
      "       460.73538646, 484.74676984, 478.32468609, 567.74701886,\n",
      "       461.14155259, 402.6968913 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([407.19234085, 335.50445663, 383.89448025, 402.829297  ,\n",
      "       448.14808423, 478.51272274, 563.09347579, 552.31094708,\n",
      "       462.87334599, 413.79781564, 472.07663971, 410.40991406,\n",
      "       533.57006609, 564.84733814, 478.79369029, 382.97566996,\n",
      "       505.07405096, 493.43569082, 538.94192392, 425.49096814])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([478.79369029, 382.97566996, 505.07405096, 493.43569082,\n",
      "       538.94192392, 425.49096814, 419.33548104, 463.8273169 ,\n",
      "       481.11941758, 460.64102572, 505.29460783, 443.84386519,\n",
      "       458.00712312, 487.44691126, 515.62869948, 536.31984124,\n",
      "       451.87212301, 394.45919165, 521.30830427, 532.80160653,\n",
      "       468.19758845, 559.84221298, 535.04153282, 564.85427982,\n",
      "       530.47387032, 609.74709298, 635.44991269, 535.91731397,\n",
      "       555.47851634, 560.26732238])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([592.77245291, 492.78699422, 528.47301518, 576.62673148,\n",
      "       449.02573821, 420.73057322, 383.2050939 , 453.90534066,\n",
      "       539.24259306, 605.47121043, 556.67616859, 608.30828456,\n",
      "       489.31267053, 414.13221475, 481.92922581, 537.03024237,\n",
      "       531.27077547, 424.76439816, 480.27019493, 456.60722947])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([444.22925323, 442.59260105, 411.5736148 , 354.91586408,\n",
      "       383.56673024, 364.72357312, 434.29575526, 413.08282734,\n",
      "       361.27286765, 370.69375928, 416.44682588, 385.58899509,\n",
      "       355.57554051, 401.83538562, 424.40409635, 391.1802466 ,\n",
      "       378.83606312, 413.5852798 , 345.70142055, 315.11381903,\n",
      "       346.66881669, 386.61950519, 426.58713648, 499.00199322,\n",
      "       496.48184556, 438.03423281, 423.59440741, 375.37548583,\n",
      "       404.09737706, 409.62535098])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([437.76060348, 394.47503303, 439.97630195, 517.19346552,\n",
      "       461.76251444, 457.355711  , 479.62716775, 433.19914647,\n",
      "       443.23925318, 444.05635782, 446.12106416, 405.39701126,\n",
      "       427.84486792, 466.91572959, 527.49026645, 526.50191535,\n",
      "       580.10855533, 460.73538646, 484.74676984, 478.32468609])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([521.30830427, 532.80160653, 468.19758845, 559.84221298,\n",
      "       535.04153282, 564.85427982, 530.47387032, 609.74709298,\n",
      "       635.44991269, 535.91731397, 555.47851634, 560.26732238,\n",
      "       592.77245291, 492.78699422, 528.47301518, 576.62673148,\n",
      "       449.02573821, 420.73057322, 383.2050939 , 453.90534066,\n",
      "       539.24259306, 605.47121043, 556.67616859, 608.30828456,\n",
      "       489.31267053, 414.13221475, 481.92922581, 537.03024237,\n",
      "       531.27077547, 424.76439816])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([480.27019493, 456.60722947, 534.68799708, 560.27001124,\n",
      "       495.50885821, 490.43221638, 469.54868894, 509.0216617 ,\n",
      "       580.5186044 , 509.56832961, 547.35738325, 523.72622733,\n",
      "       493.15355426, 520.45598519, 488.64675133, 495.78048545,\n",
      "       472.70249657, 618.45257776, 589.3399963 , 522.26342527])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([310.90629308, 380.879735  , 387.76152403, 330.557742  ,\n",
      "       388.11743607, 390.37958736, 408.2225002 , 410.23820866,\n",
      "       336.26542706, 319.41677054, 386.73270881, 408.68045252,\n",
      "       379.54368873, 374.68737687, 434.71295722, 367.58405072,\n",
      "       389.15715379, 372.35580464, 359.08299146, 423.66672844,\n",
      "       438.84670817, 437.61444311, 463.16543088, 410.89882285,\n",
      "       419.07923581, 381.30008063, 394.0134984 , 383.27079237,\n",
      "       387.32550047, 417.0279523 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([358.91573723, 476.45969901, 381.52225147, 314.31964945,\n",
      "       421.39247611, 452.38691493, 444.22925323, 442.59260105,\n",
      "       411.5736148 , 354.91586408, 383.56673024, 364.72357312,\n",
      "       434.29575526, 413.08282734, 361.27286765, 370.69375928,\n",
      "       416.44682588, 385.58899509, 355.57554051, 401.83538562])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([387.32550047, 417.0279523 , 358.91573723, 476.45969901,\n",
      "       381.52225147, 314.31964945, 421.39247611, 452.38691493,\n",
      "       444.22925323, 442.59260105, 411.5736148 , 354.91586408,\n",
      "       383.56673024, 364.72357312, 434.29575526, 413.08282734,\n",
      "       361.27286765, 370.69375928, 416.44682588, 385.58899509,\n",
      "       355.57554051, 401.83538562, 424.40409635, 391.1802466 ,\n",
      "       378.83606312, 413.5852798 , 345.70142055, 315.11381903,\n",
      "       346.66881669, 386.61950519])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([426.58713648, 499.00199322, 496.48184556, 438.03423281,\n",
      "       423.59440741, 375.37548583, 404.09737706, 409.62535098,\n",
      "       437.76060348, 394.47503303, 439.97630195, 517.19346552,\n",
      "       461.76251444, 457.355711  , 479.62716775, 433.19914647,\n",
      "       443.23925318, 444.05635782, 446.12106416, 405.39701126])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([448.14808423, 478.51272274, 563.09347579, 552.31094708,\n",
      "       462.87334599, 413.79781564, 472.07663971, 410.40991406,\n",
      "       533.57006609, 564.84733814, 478.79369029, 382.97566996,\n",
      "       505.07405096, 493.43569082, 538.94192392, 425.49096814,\n",
      "       419.33548104, 463.8273169 , 481.11941758, 460.64102572,\n",
      "       505.29460783, 443.84386519, 458.00712312, 487.44691126,\n",
      "       515.62869948, 536.31984124, 451.87212301, 394.45919165,\n",
      "       521.30830427, 532.80160653])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([468.19758845, 559.84221298, 535.04153282, 564.85427982,\n",
      "       530.47387032, 609.74709298, 635.44991269, 535.91731397,\n",
      "       555.47851634, 560.26732238, 592.77245291, 492.78699422,\n",
      "       528.47301518, 576.62673148, 449.02573821, 420.73057322,\n",
      "       383.2050939 , 453.90534066, 539.24259306, 605.47121043])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([457.355711  , 479.62716775, 433.19914647, 443.23925318,\n",
      "       444.05635782, 446.12106416, 405.39701126, 427.84486792,\n",
      "       466.91572959, 527.49026645, 526.50191535, 580.10855533,\n",
      "       460.73538646, 484.74676984, 478.32468609, 567.74701886,\n",
      "       461.14155259, 402.6968913 , 407.19234085, 335.50445663,\n",
      "       383.89448025, 402.829297  , 448.14808423, 478.51272274,\n",
      "       563.09347579, 552.31094708, 462.87334599, 413.79781564,\n",
      "       472.07663971, 410.40991406])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([533.57006609, 564.84733814, 478.79369029, 382.97566996,\n",
      "       505.07405096, 493.43569082, 538.94192392, 425.49096814,\n",
      "       419.33548104, 463.8273169 , 481.11941758, 460.64102572,\n",
      "       505.29460783, 443.84386519, 458.00712312, 487.44691126,\n",
      "       515.62869948, 536.31984124, 451.87212301, 394.45919165])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([419.33548104, 463.8273169 , 481.11941758, 460.64102572,\n",
      "       505.29460783, 443.84386519, 458.00712312, 487.44691126,\n",
      "       515.62869948, 536.31984124, 451.87212301, 394.45919165,\n",
      "       521.30830427, 532.80160653, 468.19758845, 559.84221298,\n",
      "       535.04153282, 564.85427982, 530.47387032, 609.74709298,\n",
      "       635.44991269, 535.91731397, 555.47851634, 560.26732238,\n",
      "       592.77245291, 492.78699422, 528.47301518, 576.62673148,\n",
      "       449.02573821, 420.73057322])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([383.2050939 , 453.90534066, 539.24259306, 605.47121043,\n",
      "       556.67616859, 608.30828456, 489.31267053, 414.13221475,\n",
      "       481.92922581, 537.03024237, 531.27077547, 424.76439816,\n",
      "       480.27019493, 456.60722947, 534.68799708, 560.27001124,\n",
      "       495.50885821, 490.43221638, 469.54868894, 509.0216617 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([375.37548583, 404.09737706, 409.62535098, 437.76060348,\n",
      "       394.47503303, 439.97630195, 517.19346552, 461.76251444,\n",
      "       457.355711  , 479.62716775, 433.19914647, 443.23925318,\n",
      "       444.05635782, 446.12106416, 405.39701126, 427.84486792,\n",
      "       466.91572959, 527.49026645, 526.50191535, 580.10855533,\n",
      "       460.73538646, 484.74676984, 478.32468609, 567.74701886,\n",
      "       461.14155259, 402.6968913 , 407.19234085, 335.50445663,\n",
      "       383.89448025, 402.829297  ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([448.14808423, 478.51272274, 563.09347579, 552.31094708,\n",
      "       462.87334599, 413.79781564, 472.07663971, 410.40991406,\n",
      "       533.57006609, 564.84733814, 478.79369029, 382.97566996,\n",
      "       505.07405096, 493.43569082, 538.94192392, 425.49096814,\n",
      "       419.33548104, 463.8273169 , 481.11941758, 460.64102572])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([301.77052434, 367.48165357, 372.13463675, 324.83322363,\n",
      "       327.12874953, 320.05395004, 310.90629308, 380.879735  ,\n",
      "       387.76152403, 330.557742  , 388.11743607, 390.37958736,\n",
      "       408.2225002 , 410.23820866, 336.26542706, 319.41677054,\n",
      "       386.73270881, 408.68045252, 379.54368873, 374.68737687,\n",
      "       434.71295722, 367.58405072, 389.15715379, 372.35580464,\n",
      "       359.08299146, 423.66672844, 438.84670817, 437.61444311,\n",
      "       463.16543088, 410.89882285])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([419.07923581, 381.30008063, 394.0134984 , 383.27079237,\n",
      "       387.32550047, 417.0279523 , 358.91573723, 476.45969901,\n",
      "       381.52225147, 314.31964945, 421.39247611, 452.38691493,\n",
      "       444.22925323, 442.59260105, 411.5736148 , 354.91586408,\n",
      "       383.56673024, 364.72357312, 434.29575526, 413.08282734])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([286.45978574, 296.09527034, 152.42525801, 197.44811645,\n",
      "       274.68819015, 247.35305589, 374.20690796, 282.19155374,\n",
      "       275.74591981, 318.92733395, 398.74035381, 286.78178474,\n",
      "       360.52363199, 351.88463431, 285.26129367, 335.01022329,\n",
      "       350.24709861, 309.63551757, 327.15226648, 317.45700632,\n",
      "       335.20605714, 373.26065529, 433.43357323, 315.82157975,\n",
      "       427.89093968, 289.63949291, 301.77052434, 367.48165357,\n",
      "       372.13463675, 324.83322363])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([327.12874953, 320.05395004, 310.90629308, 380.879735  ,\n",
      "       387.76152403, 330.557742  , 388.11743607, 390.37958736,\n",
      "       408.2225002 , 410.23820866, 336.26542706, 319.41677054,\n",
      "       386.73270881, 408.68045252, 379.54368873, 374.68737687,\n",
      "       434.71295722, 367.58405072, 389.15715379, 372.35580464])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([538.94192392, 425.49096814, 419.33548104, 463.8273169 ,\n",
      "       481.11941758, 460.64102572, 505.29460783, 443.84386519,\n",
      "       458.00712312, 487.44691126, 515.62869948, 536.31984124,\n",
      "       451.87212301, 394.45919165, 521.30830427, 532.80160653,\n",
      "       468.19758845, 559.84221298, 535.04153282, 564.85427982,\n",
      "       530.47387032, 609.74709298, 635.44991269, 535.91731397,\n",
      "       555.47851634, 560.26732238, 592.77245291, 492.78699422,\n",
      "       528.47301518, 576.62673148])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([449.02573821, 420.73057322, 383.2050939 , 453.90534066,\n",
      "       539.24259306, 605.47121043, 556.67616859, 608.30828456,\n",
      "       489.31267053, 414.13221475, 481.92922581, 537.03024237,\n",
      "       531.27077547, 424.76439816, 480.27019493, 456.60722947,\n",
      "       534.68799708, 560.27001124, 495.50885821, 490.43221638])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([340.76572611, 337.21491073, 319.24262795, 265.1709739 ,\n",
      "       298.00957865, 242.49635296, 240.13836305, 236.14417874,\n",
      "       260.83591413, 384.60274617, 226.89460208, 273.27294491,\n",
      "       203.17066281, 217.19312001, 314.43807321, 300.01414871,\n",
      "       227.87519988, 220.79051407, 295.02199764, 255.88545794,\n",
      "       278.47209267, 287.98710364, 252.91959566, 378.32322455,\n",
      "       361.8077385 , 206.60592959, 258.34179417, 255.41322766,\n",
      "       318.51060943, 268.61541097])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([272.76425551, 315.14139906, 348.59105903, 256.83218886,\n",
      "       258.63997842, 265.18095026, 255.64151055, 373.23511922,\n",
      "       357.72595578, 254.37809822, 325.97651154, 427.7355496 ,\n",
      "       405.77162346, 261.78093214, 257.79632772, 359.61398809,\n",
      "       300.97891002, 323.07814374, 360.51164299, 286.45978574])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([330.557742  , 388.11743607, 390.37958736, 408.2225002 ,\n",
      "       410.23820866, 336.26542706, 319.41677054, 386.73270881,\n",
      "       408.68045252, 379.54368873, 374.68737687, 434.71295722,\n",
      "       367.58405072, 389.15715379, 372.35580464, 359.08299146,\n",
      "       423.66672844, 438.84670817, 437.61444311, 463.16543088,\n",
      "       410.89882285, 419.07923581, 381.30008063, 394.0134984 ,\n",
      "       383.27079237, 387.32550047, 417.0279523 , 358.91573723,\n",
      "       476.45969901, 381.52225147])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([314.31964945, 421.39247611, 452.38691493, 444.22925323,\n",
      "       442.59260105, 411.5736148 , 354.91586408, 383.56673024,\n",
      "       364.72357312, 434.29575526, 413.08282734, 361.27286765,\n",
      "       370.69375928, 416.44682588, 385.58899509, 355.57554051,\n",
      "       401.83538562, 424.40409635, 391.1802466 , 378.83606312])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([265.18095026, 255.64151055, 373.23511922, 357.72595578,\n",
      "       254.37809822, 325.97651154, 427.7355496 , 405.77162346,\n",
      "       261.78093214, 257.79632772, 359.61398809, 300.97891002,\n",
      "       323.07814374, 360.51164299, 286.45978574, 296.09527034,\n",
      "       152.42525801, 197.44811645, 274.68819015, 247.35305589,\n",
      "       374.20690796, 282.19155374, 275.74591981, 318.92733395,\n",
      "       398.74035381, 286.78178474, 360.52363199, 351.88463431,\n",
      "       285.26129367, 335.01022329])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([350.24709861, 309.63551757, 327.15226648, 317.45700632,\n",
      "       335.20605714, 373.26065529, 433.43357323, 315.82157975,\n",
      "       427.89093968, 289.63949291, 301.77052434, 367.48165357,\n",
      "       372.13463675, 324.83322363, 327.12874953, 320.05395004,\n",
      "       310.90629308, 380.879735  , 387.76152403, 330.557742  ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([382.97566996, 505.07405096, 493.43569082, 538.94192392,\n",
      "       425.49096814, 419.33548104, 463.8273169 , 481.11941758,\n",
      "       460.64102572, 505.29460783, 443.84386519, 458.00712312,\n",
      "       487.44691126, 515.62869948, 536.31984124, 451.87212301,\n",
      "       394.45919165, 521.30830427, 532.80160653, 468.19758845,\n",
      "       559.84221298, 535.04153282, 564.85427982, 530.47387032,\n",
      "       609.74709298, 635.44991269, 535.91731397, 555.47851634,\n",
      "       560.26732238, 592.77245291])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([492.78699422, 528.47301518, 576.62673148, 449.02573821,\n",
      "       420.73057322, 383.2050939 , 453.90534066, 539.24259306,\n",
      "       605.47121043, 556.67616859, 608.30828456, 489.31267053,\n",
      "       414.13221475, 481.92922581, 537.03024237, 531.27077547,\n",
      "       424.76439816, 480.27019493, 456.60722947, 534.68799708])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([295.02199764, 255.88545794, 278.47209267, 287.98710364,\n",
      "       252.91959566, 378.32322455, 361.8077385 , 206.60592959,\n",
      "       258.34179417, 255.41322766, 318.51060943, 268.61541097,\n",
      "       272.76425551, 315.14139906, 348.59105903, 256.83218886,\n",
      "       258.63997842, 265.18095026, 255.64151055, 373.23511922,\n",
      "       357.72595578, 254.37809822, 325.97651154, 427.7355496 ,\n",
      "       405.77162346, 261.78093214, 257.79632772, 359.61398809,\n",
      "       300.97891002, 323.07814374])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([360.51164299, 286.45978574, 296.09527034, 152.42525801,\n",
      "       197.44811645, 274.68819015, 247.35305589, 374.20690796,\n",
      "       282.19155374, 275.74591981, 318.92733395, 398.74035381,\n",
      "       286.78178474, 360.52363199, 351.88463431, 285.26129367,\n",
      "       335.01022329, 350.24709861, 309.63551757, 327.15226648])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([323.07814374, 360.51164299, 286.45978574, 296.09527034,\n",
      "       152.42525801, 197.44811645, 274.68819015, 247.35305589,\n",
      "       374.20690796, 282.19155374, 275.74591981, 318.92733395,\n",
      "       398.74035381, 286.78178474, 360.52363199, 351.88463431,\n",
      "       285.26129367, 335.01022329, 350.24709861, 309.63551757,\n",
      "       327.15226648, 317.45700632, 335.20605714, 373.26065529,\n",
      "       433.43357323, 315.82157975, 427.89093968, 289.63949291,\n",
      "       301.77052434, 367.48165357])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([372.13463675, 324.83322363, 327.12874953, 320.05395004,\n",
      "       310.90629308, 380.879735  , 387.76152403, 330.557742  ,\n",
      "       388.11743607, 390.37958736, 408.2225002 , 410.23820866,\n",
      "       336.26542706, 319.41677054, 386.73270881, 408.68045252,\n",
      "       379.54368873, 374.68737687, 434.71295722, 367.58405072])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([535.91731397, 555.47851634, 560.26732238, 592.77245291,\n",
      "       492.78699422, 528.47301518, 576.62673148, 449.02573821,\n",
      "       420.73057322, 383.2050939 , 453.90534066, 539.24259306,\n",
      "       605.47121043, 556.67616859, 608.30828456, 489.31267053,\n",
      "       414.13221475, 481.92922581, 537.03024237, 531.27077547,\n",
      "       424.76439816, 480.27019493, 456.60722947, 534.68799708,\n",
      "       560.27001124, 495.50885821, 490.43221638, 469.54868894,\n",
      "       509.0216617 , 580.5186044 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([509.56832961, 547.35738325, 523.72622733, 493.15355426,\n",
      "       520.45598519, 488.64675133, 495.78048545, 472.70249657,\n",
      "       618.45257776, 589.3399963 , 522.26342527, 549.53276579,\n",
      "       550.7361558 , 541.6098775 , 581.53451462, 607.22224773,\n",
      "       549.87462813, 522.37636622, 533.76513254, 439.37788128])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 63.7869869 ,  88.21464394, 110.41550736,  92.38406199,\n",
      "       116.29604032, 157.91140713, 246.51495725, 196.23187143,\n",
      "       170.08294304, 152.27811061,  53.64930572, 108.24262341,\n",
      "       147.88143388, 274.83401078, 196.4136086 , 173.2665378 ,\n",
      "       161.73243664,  99.10058533, 189.91445947, 217.03019629,\n",
      "       218.01662357, 135.62151463, 215.66343029, 121.17746619,\n",
      "       167.28600356, 283.28864689, 164.69125998, 125.10364505,\n",
      "       159.87748372, 144.34205834])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 82.29692285, 140.1662113 , 115.85726673, 173.41590414,\n",
      "       135.43190224, 233.88614205, 168.22234072, 149.37417651,\n",
      "       211.20263936, 134.80757158, 168.40259944, 248.81801436,\n",
      "       131.08170877, 163.32682593, 198.28054145, 230.55995397,\n",
      "       143.44437565,  99.18040759, 184.29382082, 211.78528286])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 41.23114256,  17.99426911, 114.11325318, 184.04522052,\n",
      "       131.63722049, 158.90144505, 147.21045867,  86.83575307,\n",
      "       115.00959466, 192.5315456 , 142.60208615, 193.93866008,\n",
      "        16.93172331, 103.93682066, 129.66886997, 103.55737712,\n",
      "       115.40403437,  19.93344086,  65.42368531, 126.55694824,\n",
      "       199.66504437, 128.07629968,  76.25036135,  81.087672  ,\n",
      "       156.95619511, 159.44456572, 109.77291327, 143.66763433,\n",
      "       143.35571794, 181.43706483])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([117.23990033, 103.79331492, 104.97248027,  51.30083837,\n",
      "       116.82936428, 150.26046624, 142.49521639, 127.19622084,\n",
      "        63.7869869 ,  88.21464394, 110.41550736,  92.38406199,\n",
      "       116.29604032, 157.91140713, 246.51495725, 196.23187143,\n",
      "       170.08294304, 152.27811061,  53.64930572, 108.24262341])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([258.34179417, 255.41322766, 318.51060943, 268.61541097,\n",
      "       272.76425551, 315.14139906, 348.59105903, 256.83218886,\n",
      "       258.63997842, 265.18095026, 255.64151055, 373.23511922,\n",
      "       357.72595578, 254.37809822, 325.97651154, 427.7355496 ,\n",
      "       405.77162346, 261.78093214, 257.79632772, 359.61398809,\n",
      "       300.97891002, 323.07814374, 360.51164299, 286.45978574,\n",
      "       296.09527034, 152.42525801, 197.44811645, 274.68819015,\n",
      "       247.35305589, 374.20690796])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([282.19155374, 275.74591981, 318.92733395, 398.74035381,\n",
      "       286.78178474, 360.52363199, 351.88463431, 285.26129367,\n",
      "       335.01022329, 350.24709861, 309.63551757, 327.15226648,\n",
      "       317.45700632, 335.20605714, 373.26065529, 433.43357323,\n",
      "       315.82157975, 427.89093968, 289.63949291, 301.77052434])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([492.78699422, 528.47301518, 576.62673148, 449.02573821,\n",
      "       420.73057322, 383.2050939 , 453.90534066, 539.24259306,\n",
      "       605.47121043, 556.67616859, 608.30828456, 489.31267053,\n",
      "       414.13221475, 481.92922581, 537.03024237, 531.27077547,\n",
      "       424.76439816, 480.27019493, 456.60722947, 534.68799708,\n",
      "       560.27001124, 495.50885821, 490.43221638, 469.54868894,\n",
      "       509.0216617 , 580.5186044 , 509.56832961, 547.35738325,\n",
      "       523.72622733, 493.15355426])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([520.45598519, 488.64675133, 495.78048545, 472.70249657,\n",
      "       618.45257776, 589.3399963 , 522.26342527, 549.53276579,\n",
      "       550.7361558 , 541.6098775 , 581.53451462, 607.22224773,\n",
      "       549.87462813, 522.37636622, 533.76513254, 439.37788128,\n",
      "       437.09888259, 592.31584485, 669.82746742, 592.01663525])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([427.84486792, 466.91572959, 527.49026645, 526.50191535,\n",
      "       580.10855533, 460.73538646, 484.74676984, 478.32468609,\n",
      "       567.74701886, 461.14155259, 402.6968913 , 407.19234085,\n",
      "       335.50445663, 383.89448025, 402.829297  , 448.14808423,\n",
      "       478.51272274, 563.09347579, 552.31094708, 462.87334599,\n",
      "       413.79781564, 472.07663971, 410.40991406, 533.57006609,\n",
      "       564.84733814, 478.79369029, 382.97566996, 505.07405096,\n",
      "       493.43569082, 538.94192392])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([425.49096814, 419.33548104, 463.8273169 , 481.11941758,\n",
      "       460.64102572, 505.29460783, 443.84386519, 458.00712312,\n",
      "       487.44691126, 515.62869948, 536.31984124, 451.87212301,\n",
      "       394.45919165, 521.30830427, 532.80160653, 468.19758845,\n",
      "       559.84221298, 535.04153282, 564.85427982, 530.47387032])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([355.57554051, 401.83538562, 424.40409635, 391.1802466 ,\n",
      "       378.83606312, 413.5852798 , 345.70142055, 315.11381903,\n",
      "       346.66881669, 386.61950519, 426.58713648, 499.00199322,\n",
      "       496.48184556, 438.03423281, 423.59440741, 375.37548583,\n",
      "       404.09737706, 409.62535098, 437.76060348, 394.47503303,\n",
      "       439.97630195, 517.19346552, 461.76251444, 457.355711  ,\n",
      "       479.62716775, 433.19914647, 443.23925318, 444.05635782,\n",
      "       446.12106416, 405.39701126])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([427.84486792, 466.91572959, 527.49026645, 526.50191535,\n",
      "       580.10855533, 460.73538646, 484.74676984, 478.32468609,\n",
      "       567.74701886, 461.14155259, 402.6968913 , 407.19234085,\n",
      "       335.50445663, 383.89448025, 402.829297  , 448.14808423,\n",
      "       478.51272274, 563.09347579, 552.31094708, 462.87334599])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([238.09168848, 267.12643163, 239.73651743, 296.96554321,\n",
      "       241.19461979, 358.19748099, 314.38420433, 205.25786975,\n",
      "       159.52202019, 227.55789279, 224.56358967, 261.6293727 ,\n",
      "       269.64979598, 241.11349397, 191.94529955, 141.51864415,\n",
      "       178.65095344, 264.60280642, 264.23905845, 183.7540556 ,\n",
      "       223.49117695, 260.65355218, 206.39405506, 233.54247196,\n",
      "       248.43310869, 190.08114942, 240.53880393, 280.09782758,\n",
      "       315.35893007, 327.78137488])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([209.29589023, 180.7943296 , 256.05134529, 287.62799663,\n",
      "       293.2503612 , 461.66825185, 371.01366887, 340.76572611,\n",
      "       337.21491073, 319.24262795, 265.1709739 , 298.00957865,\n",
      "       242.49635296, 240.13836305, 236.14417874, 260.83591413,\n",
      "       384.60274617, 226.89460208, 273.27294491, 203.17066281])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([635.44991269, 535.91731397, 555.47851634, 560.26732238,\n",
      "       592.77245291, 492.78699422, 528.47301518, 576.62673148,\n",
      "       449.02573821, 420.73057322, 383.2050939 , 453.90534066,\n",
      "       539.24259306, 605.47121043, 556.67616859, 608.30828456,\n",
      "       489.31267053, 414.13221475, 481.92922581, 537.03024237,\n",
      "       531.27077547, 424.76439816, 480.27019493, 456.60722947,\n",
      "       534.68799708, 560.27001124, 495.50885821, 490.43221638,\n",
      "       469.54868894, 509.0216617 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([580.5186044 , 509.56832961, 547.35738325, 523.72622733,\n",
      "       493.15355426, 520.45598519, 488.64675133, 495.78048545,\n",
      "       472.70249657, 618.45257776, 589.3399963 , 522.26342527,\n",
      "       549.53276579, 550.7361558 , 541.6098775 , 581.53451462,\n",
      "       607.22224773, 549.87462813, 522.37636622, 533.76513254])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([401.83538562, 424.40409635, 391.1802466 , 378.83606312,\n",
      "       413.5852798 , 345.70142055, 315.11381903, 346.66881669,\n",
      "       386.61950519, 426.58713648, 499.00199322, 496.48184556,\n",
      "       438.03423281, 423.59440741, 375.37548583, 404.09737706,\n",
      "       409.62535098, 437.76060348, 394.47503303, 439.97630195,\n",
      "       517.19346552, 461.76251444, 457.355711  , 479.62716775,\n",
      "       433.19914647, 443.23925318, 444.05635782, 446.12106416,\n",
      "       405.39701126, 427.84486792])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([466.91572959, 527.49026645, 526.50191535, 580.10855533,\n",
      "       460.73538646, 484.74676984, 478.32468609, 567.74701886,\n",
      "       461.14155259, 402.6968913 , 407.19234085, 335.50445663,\n",
      "       383.89448025, 402.829297  , 448.14808423, 478.51272274,\n",
      "       563.09347579, 552.31094708, 462.87334599, 413.79781564])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 49.64498689,  17.47115024, -28.80375993, 101.72731666,\n",
      "        77.58717368,  65.46366834,  -4.8640219 ,   8.09607209,\n",
      "        55.7317209 ,   9.41582456,  61.83334275,  43.3839532 ,\n",
      "        42.79186531,  32.78361335, 149.68041012, 109.27425031,\n",
      "        25.89407055,  94.10193173,  29.1604104 ,  62.45920462,\n",
      "       -19.259528  , -30.02332054,  54.25897839, 115.20075505,\n",
      "       104.4155928 ,  81.90616437,  66.43128026,   3.62500397,\n",
      "        16.29659574,  42.40312683])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([125.84206218, 124.86800076,  10.80211719,  69.67090736,\n",
      "        73.00015151,  49.1362523 , 106.98454433, 155.1638923 ,\n",
      "       163.89514009,  76.87066266,  66.65283723, 105.2879106 ,\n",
      "       152.28966719,  96.70344248,  83.77352599,  41.23114256,\n",
      "        17.99426911, 114.11325318, 184.04522052, 131.63722049])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([446.12106416, 405.39701126, 427.84486792, 466.91572959,\n",
      "       527.49026645, 526.50191535, 580.10855533, 460.73538646,\n",
      "       484.74676984, 478.32468609, 567.74701886, 461.14155259,\n",
      "       402.6968913 , 407.19234085, 335.50445663, 383.89448025,\n",
      "       402.829297  , 448.14808423, 478.51272274, 563.09347579,\n",
      "       552.31094708, 462.87334599, 413.79781564, 472.07663971,\n",
      "       410.40991406, 533.57006609, 564.84733814, 478.79369029,\n",
      "       382.97566996, 505.07405096])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([493.43569082, 538.94192392, 425.49096814, 419.33548104,\n",
      "       463.8273169 , 481.11941758, 460.64102572, 505.29460783,\n",
      "       443.84386519, 458.00712312, 487.44691126, 515.62869948,\n",
      "       536.31984124, 451.87212301, 394.45919165, 521.30830427,\n",
      "       532.80160653, 468.19758845, 559.84221298, 535.04153282])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([348.59105903, 256.83218886, 258.63997842, 265.18095026,\n",
      "       255.64151055, 373.23511922, 357.72595578, 254.37809822,\n",
      "       325.97651154, 427.7355496 , 405.77162346, 261.78093214,\n",
      "       257.79632772, 359.61398809, 300.97891002, 323.07814374,\n",
      "       360.51164299, 286.45978574, 296.09527034, 152.42525801,\n",
      "       197.44811645, 274.68819015, 247.35305589, 374.20690796,\n",
      "       282.19155374, 275.74591981, 318.92733395, 398.74035381,\n",
      "       286.78178474, 360.52363199])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([351.88463431, 285.26129367, 335.01022329, 350.24709861,\n",
      "       309.63551757, 327.15226648, 317.45700632, 335.20605714,\n",
      "       373.26065529, 433.43357323, 315.82157975, 427.89093968,\n",
      "       289.63949291, 301.77052434, 367.48165357, 372.13463675,\n",
      "       324.83322363, 327.12874953, 320.05395004, 310.90629308])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([443.84386519, 458.00712312, 487.44691126, 515.62869948,\n",
      "       536.31984124, 451.87212301, 394.45919165, 521.30830427,\n",
      "       532.80160653, 468.19758845, 559.84221298, 535.04153282,\n",
      "       564.85427982, 530.47387032, 609.74709298, 635.44991269,\n",
      "       535.91731397, 555.47851634, 560.26732238, 592.77245291,\n",
      "       492.78699422, 528.47301518, 576.62673148, 449.02573821,\n",
      "       420.73057322, 383.2050939 , 453.90534066, 539.24259306,\n",
      "       605.47121043, 556.67616859])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([608.30828456, 489.31267053, 414.13221475, 481.92922581,\n",
      "       537.03024237, 531.27077547, 424.76439816, 480.27019493,\n",
      "       456.60722947, 534.68799708, 560.27001124, 495.50885821,\n",
      "       490.43221638, 469.54868894, 509.0216617 , 580.5186044 ,\n",
      "       509.56832961, 547.35738325, 523.72622733, 493.15355426])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([495.50885821, 490.43221638, 469.54868894, 509.0216617 ,\n",
      "       580.5186044 , 509.56832961, 547.35738325, 523.72622733,\n",
      "       493.15355426, 520.45598519, 488.64675133, 495.78048545,\n",
      "       472.70249657, 618.45257776, 589.3399963 , 522.26342527,\n",
      "       549.53276579, 550.7361558 , 541.6098775 , 581.53451462,\n",
      "       607.22224773, 549.87462813, 522.37636622, 533.76513254,\n",
      "       439.37788128, 437.09888259, 592.31584485, 669.82746742,\n",
      "       592.01663525, 597.07230042])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([597.51240164, 732.10605797, 694.37287539, 600.22696169,\n",
      "       529.54083307, 475.68450536, 549.19229931, 538.22986094,\n",
      "       490.72208051, 514.5088088 , 508.05592927, 639.58053533,\n",
      "       658.89585885, 607.11396555, 662.96302643, 623.20154476,\n",
      "       551.67016265, 648.06221241, 647.35774585, 555.63289089])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([345.70142055, 315.11381903, 346.66881669, 386.61950519,\n",
      "       426.58713648, 499.00199322, 496.48184556, 438.03423281,\n",
      "       423.59440741, 375.37548583, 404.09737706, 409.62535098,\n",
      "       437.76060348, 394.47503303, 439.97630195, 517.19346552,\n",
      "       461.76251444, 457.355711  , 479.62716775, 433.19914647,\n",
      "       443.23925318, 444.05635782, 446.12106416, 405.39701126,\n",
      "       427.84486792, 466.91572959, 527.49026645, 526.50191535,\n",
      "       580.10855533, 460.73538646])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([484.74676984, 478.32468609, 567.74701886, 461.14155259,\n",
      "       402.6968913 , 407.19234085, 335.50445663, 383.89448025,\n",
      "       402.829297  , 448.14808423, 478.51272274, 563.09347579,\n",
      "       552.31094708, 462.87334599, 413.79781564, 472.07663971,\n",
      "       410.40991406, 533.57006609, 564.84733814, 478.79369029])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([469.54868894, 509.0216617 , 580.5186044 , 509.56832961,\n",
      "       547.35738325, 523.72622733, 493.15355426, 520.45598519,\n",
      "       488.64675133, 495.78048545, 472.70249657, 618.45257776,\n",
      "       589.3399963 , 522.26342527, 549.53276579, 550.7361558 ,\n",
      "       541.6098775 , 581.53451462, 607.22224773, 549.87462813,\n",
      "       522.37636622, 533.76513254, 439.37788128, 437.09888259,\n",
      "       592.31584485, 669.82746742, 592.01663525, 597.07230042,\n",
      "       597.51240164, 732.10605797])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([694.37287539, 600.22696169, 529.54083307, 475.68450536,\n",
      "       549.19229931, 538.22986094, 490.72208051, 514.5088088 ,\n",
      "       508.05592927, 639.58053533, 658.89585885, 607.11396555,\n",
      "       662.96302643, 623.20154476, 551.67016265, 648.06221241,\n",
      "       647.35774585, 555.63289089, 563.85025964, 543.66405584])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([489.31267053, 414.13221475, 481.92922581, 537.03024237,\n",
      "       531.27077547, 424.76439816, 480.27019493, 456.60722947,\n",
      "       534.68799708, 560.27001124, 495.50885821, 490.43221638,\n",
      "       469.54868894, 509.0216617 , 580.5186044 , 509.56832961,\n",
      "       547.35738325, 523.72622733, 493.15355426, 520.45598519,\n",
      "       488.64675133, 495.78048545, 472.70249657, 618.45257776,\n",
      "       589.3399963 , 522.26342527, 549.53276579, 550.7361558 ,\n",
      "       541.6098775 , 581.53451462])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([607.22224773, 549.87462813, 522.37636622, 533.76513254,\n",
      "       439.37788128, 437.09888259, 592.31584485, 669.82746742,\n",
      "       592.01663525, 597.07230042, 597.51240164, 732.10605797,\n",
      "       694.37287539, 600.22696169, 529.54083307, 475.68450536,\n",
      "       549.19229931, 538.22986094, 490.72208051, 514.5088088 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([490.43221638, 469.54868894, 509.0216617 , 580.5186044 ,\n",
      "       509.56832961, 547.35738325, 523.72622733, 493.15355426,\n",
      "       520.45598519, 488.64675133, 495.78048545, 472.70249657,\n",
      "       618.45257776, 589.3399963 , 522.26342527, 549.53276579,\n",
      "       550.7361558 , 541.6098775 , 581.53451462, 607.22224773,\n",
      "       549.87462813, 522.37636622, 533.76513254, 439.37788128,\n",
      "       437.09888259, 592.31584485, 669.82746742, 592.01663525,\n",
      "       597.07230042, 597.51240164])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([732.10605797, 694.37287539, 600.22696169, 529.54083307,\n",
      "       475.68450536, 549.19229931, 538.22986094, 490.72208051,\n",
      "       514.5088088 , 508.05592927, 639.58053533, 658.89585885,\n",
      "       607.11396555, 662.96302643, 623.20154476, 551.67016265,\n",
      "       648.06221241, 647.35774585, 555.63289089, 563.85025964])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([394.45919165, 521.30830427, 532.80160653, 468.19758845,\n",
      "       559.84221298, 535.04153282, 564.85427982, 530.47387032,\n",
      "       609.74709298, 635.44991269, 535.91731397, 555.47851634,\n",
      "       560.26732238, 592.77245291, 492.78699422, 528.47301518,\n",
      "       576.62673148, 449.02573821, 420.73057322, 383.2050939 ,\n",
      "       453.90534066, 539.24259306, 605.47121043, 556.67616859,\n",
      "       608.30828456, 489.31267053, 414.13221475, 481.92922581,\n",
      "       537.03024237, 531.27077547])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([424.76439816, 480.27019493, 456.60722947, 534.68799708,\n",
      "       560.27001124, 495.50885821, 490.43221638, 469.54868894,\n",
      "       509.0216617 , 580.5186044 , 509.56832961, 547.35738325,\n",
      "       523.72622733, 493.15355426, 520.45598519, 488.64675133,\n",
      "       495.78048545, 472.70249657, 618.45257776, 589.3399963 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([286.78178474, 360.52363199, 351.88463431, 285.26129367,\n",
      "       335.01022329, 350.24709861, 309.63551757, 327.15226648,\n",
      "       317.45700632, 335.20605714, 373.26065529, 433.43357323,\n",
      "       315.82157975, 427.89093968, 289.63949291, 301.77052434,\n",
      "       367.48165357, 372.13463675, 324.83322363, 327.12874953,\n",
      "       320.05395004, 310.90629308, 380.879735  , 387.76152403,\n",
      "       330.557742  , 388.11743607, 390.37958736, 408.2225002 ,\n",
      "       410.23820866, 336.26542706])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([319.41677054, 386.73270881, 408.68045252, 379.54368873,\n",
      "       374.68737687, 434.71295722, 367.58405072, 389.15715379,\n",
      "       372.35580464, 359.08299146, 423.66672844, 438.84670817,\n",
      "       437.61444311, 463.16543088, 410.89882285, 419.07923581,\n",
      "       381.30008063, 394.0134984 , 383.27079237, 387.32550047])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([361.8077385 , 206.60592959, 258.34179417, 255.41322766,\n",
      "       318.51060943, 268.61541097, 272.76425551, 315.14139906,\n",
      "       348.59105903, 256.83218886, 258.63997842, 265.18095026,\n",
      "       255.64151055, 373.23511922, 357.72595578, 254.37809822,\n",
      "       325.97651154, 427.7355496 , 405.77162346, 261.78093214,\n",
      "       257.79632772, 359.61398809, 300.97891002, 323.07814374,\n",
      "       360.51164299, 286.45978574, 296.09527034, 152.42525801,\n",
      "       197.44811645, 274.68819015])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([247.35305589, 374.20690796, 282.19155374, 275.74591981,\n",
      "       318.92733395, 398.74035381, 286.78178474, 360.52363199,\n",
      "       351.88463431, 285.26129367, 335.01022329, 350.24709861,\n",
      "       309.63551757, 327.15226648, 317.45700632, 335.20605714,\n",
      "       373.26065529, 433.43357323, 315.82157975, 427.89093968])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([394.0134984 , 383.27079237, 387.32550047, 417.0279523 ,\n",
      "       358.91573723, 476.45969901, 381.52225147, 314.31964945,\n",
      "       421.39247611, 452.38691493, 444.22925323, 442.59260105,\n",
      "       411.5736148 , 354.91586408, 383.56673024, 364.72357312,\n",
      "       434.29575526, 413.08282734, 361.27286765, 370.69375928,\n",
      "       416.44682588, 385.58899509, 355.57554051, 401.83538562,\n",
      "       424.40409635, 391.1802466 , 378.83606312, 413.5852798 ,\n",
      "       345.70142055, 315.11381903])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([346.66881669, 386.61950519, 426.58713648, 499.00199322,\n",
      "       496.48184556, 438.03423281, 423.59440741, 375.37548583,\n",
      "       404.09737706, 409.62535098, 437.76060348, 394.47503303,\n",
      "       439.97630195, 517.19346552, 461.76251444, 457.355711  ,\n",
      "       479.62716775, 433.19914647, 443.23925318, 444.05635782])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([451.87212301, 394.45919165, 521.30830427, 532.80160653,\n",
      "       468.19758845, 559.84221298, 535.04153282, 564.85427982,\n",
      "       530.47387032, 609.74709298, 635.44991269, 535.91731397,\n",
      "       555.47851634, 560.26732238, 592.77245291, 492.78699422,\n",
      "       528.47301518, 576.62673148, 449.02573821, 420.73057322,\n",
      "       383.2050939 , 453.90534066, 539.24259306, 605.47121043,\n",
      "       556.67616859, 608.30828456, 489.31267053, 414.13221475,\n",
      "       481.92922581, 537.03024237])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([531.27077547, 424.76439816, 480.27019493, 456.60722947,\n",
      "       534.68799708, 560.27001124, 495.50885821, 490.43221638,\n",
      "       469.54868894, 509.0216617 , 580.5186044 , 509.56832961,\n",
      "       547.35738325, 523.72622733, 493.15355426, 520.45598519,\n",
      "       488.64675133, 495.78048545, 472.70249657, 618.45257776])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([443.23925318, 444.05635782, 446.12106416, 405.39701126,\n",
      "       427.84486792, 466.91572959, 527.49026645, 526.50191535,\n",
      "       580.10855533, 460.73538646, 484.74676984, 478.32468609,\n",
      "       567.74701886, 461.14155259, 402.6968913 , 407.19234085,\n",
      "       335.50445663, 383.89448025, 402.829297  , 448.14808423,\n",
      "       478.51272274, 563.09347579, 552.31094708, 462.87334599,\n",
      "       413.79781564, 472.07663971, 410.40991406, 533.57006609,\n",
      "       564.84733814, 478.79369029])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([382.97566996, 505.07405096, 493.43569082, 538.94192392,\n",
      "       425.49096814, 419.33548104, 463.8273169 , 481.11941758,\n",
      "       460.64102572, 505.29460783, 443.84386519, 458.00712312,\n",
      "       487.44691126, 515.62869948, 536.31984124, 451.87212301,\n",
      "       394.45919165, 521.30830427, 532.80160653, 468.19758845])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([317.45700632, 335.20605714, 373.26065529, 433.43357323,\n",
      "       315.82157975, 427.89093968, 289.63949291, 301.77052434,\n",
      "       367.48165357, 372.13463675, 324.83322363, 327.12874953,\n",
      "       320.05395004, 310.90629308, 380.879735  , 387.76152403,\n",
      "       330.557742  , 388.11743607, 390.37958736, 408.2225002 ,\n",
      "       410.23820866, 336.26542706, 319.41677054, 386.73270881,\n",
      "       408.68045252, 379.54368873, 374.68737687, 434.71295722,\n",
      "       367.58405072, 389.15715379])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([372.35580464, 359.08299146, 423.66672844, 438.84670817,\n",
      "       437.61444311, 463.16543088, 410.89882285, 419.07923581,\n",
      "       381.30008063, 394.0134984 , 383.27079237, 387.32550047,\n",
      "       417.0279523 , 358.91573723, 476.45969901, 381.52225147,\n",
      "       314.31964945, 421.39247611, 452.38691493, 444.22925323])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([241.19461979, 358.19748099, 314.38420433, 205.25786975,\n",
      "       159.52202019, 227.55789279, 224.56358967, 261.6293727 ,\n",
      "       269.64979598, 241.11349397, 191.94529955, 141.51864415,\n",
      "       178.65095344, 264.60280642, 264.23905845, 183.7540556 ,\n",
      "       223.49117695, 260.65355218, 206.39405506, 233.54247196,\n",
      "       248.43310869, 190.08114942, 240.53880393, 280.09782758,\n",
      "       315.35893007, 327.78137488, 209.29589023, 180.7943296 ,\n",
      "       256.05134529, 287.62799663])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([293.2503612 , 461.66825185, 371.01366887, 340.76572611,\n",
      "       337.21491073, 319.24262795, 265.1709739 , 298.00957865,\n",
      "       242.49635296, 240.13836305, 236.14417874, 260.83591413,\n",
      "       384.60274617, 226.89460208, 273.27294491, 203.17066281,\n",
      "       217.19312001, 314.43807321, 300.01414871, 227.87519988])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([605.47121043, 556.67616859, 608.30828456, 489.31267053,\n",
      "       414.13221475, 481.92922581, 537.03024237, 531.27077547,\n",
      "       424.76439816, 480.27019493, 456.60722947, 534.68799708,\n",
      "       560.27001124, 495.50885821, 490.43221638, 469.54868894,\n",
      "       509.0216617 , 580.5186044 , 509.56832961, 547.35738325,\n",
      "       523.72622733, 493.15355426, 520.45598519, 488.64675133,\n",
      "       495.78048545, 472.70249657, 618.45257776, 589.3399963 ,\n",
      "       522.26342527, 549.53276579])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([550.7361558 , 541.6098775 , 581.53451462, 607.22224773,\n",
      "       549.87462813, 522.37636622, 533.76513254, 439.37788128,\n",
      "       437.09888259, 592.31584485, 669.82746742, 592.01663525,\n",
      "       597.07230042, 597.51240164, 732.10605797, 694.37287539,\n",
      "       600.22696169, 529.54083307, 475.68450536, 549.19229931])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 81.087672  , 156.95619511, 159.44456572, 109.77291327,\n",
      "       143.66763433, 143.35571794, 181.43706483, 117.23990033,\n",
      "       103.79331492, 104.97248027,  51.30083837, 116.82936428,\n",
      "       150.26046624, 142.49521639, 127.19622084,  63.7869869 ,\n",
      "        88.21464394, 110.41550736,  92.38406199, 116.29604032,\n",
      "       157.91140713, 246.51495725, 196.23187143, 170.08294304,\n",
      "       152.27811061,  53.64930572, 108.24262341, 147.88143388,\n",
      "       274.83401078, 196.4136086 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([173.2665378 , 161.73243664,  99.10058533, 189.91445947,\n",
      "       217.03019629, 218.01662357, 135.62151463, 215.66343029,\n",
      "       121.17746619, 167.28600356, 283.28864689, 164.69125998,\n",
      "       125.10364505, 159.87748372, 144.34205834,  82.29692285,\n",
      "       140.1662113 , 115.85726673, 173.41590414, 135.43190224])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([420.73057322, 383.2050939 , 453.90534066, 539.24259306,\n",
      "       605.47121043, 556.67616859, 608.30828456, 489.31267053,\n",
      "       414.13221475, 481.92922581, 537.03024237, 531.27077547,\n",
      "       424.76439816, 480.27019493, 456.60722947, 534.68799708,\n",
      "       560.27001124, 495.50885821, 490.43221638, 469.54868894,\n",
      "       509.0216617 , 580.5186044 , 509.56832961, 547.35738325,\n",
      "       523.72622733, 493.15355426, 520.45598519, 488.64675133,\n",
      "       495.78048545, 472.70249657])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([618.45257776, 589.3399963 , 522.26342527, 549.53276579,\n",
      "       550.7361558 , 541.6098775 , 581.53451462, 607.22224773,\n",
      "       549.87462813, 522.37636622, 533.76513254, 439.37788128,\n",
      "       437.09888259, 592.31584485, 669.82746742, 592.01663525,\n",
      "       597.07230042, 597.51240164, 732.10605797, 694.37287539])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([560.26732238, 592.77245291, 492.78699422, 528.47301518,\n",
      "       576.62673148, 449.02573821, 420.73057322, 383.2050939 ,\n",
      "       453.90534066, 539.24259306, 605.47121043, 556.67616859,\n",
      "       608.30828456, 489.31267053, 414.13221475, 481.92922581,\n",
      "       537.03024237, 531.27077547, 424.76439816, 480.27019493,\n",
      "       456.60722947, 534.68799708, 560.27001124, 495.50885821,\n",
      "       490.43221638, 469.54868894, 509.0216617 , 580.5186044 ,\n",
      "       509.56832961, 547.35738325])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([523.72622733, 493.15355426, 520.45598519, 488.64675133,\n",
      "       495.78048545, 472.70249657, 618.45257776, 589.3399963 ,\n",
      "       522.26342527, 549.53276579, 550.7361558 , 541.6098775 ,\n",
      "       581.53451462, 607.22224773, 549.87462813, 522.37636622,\n",
      "       533.76513254, 439.37788128, 437.09888259, 592.31584485])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([520.45598519, 488.64675133, 495.78048545, 472.70249657,\n",
      "       618.45257776, 589.3399963 , 522.26342527, 549.53276579,\n",
      "       550.7361558 , 541.6098775 , 581.53451462, 607.22224773,\n",
      "       549.87462813, 522.37636622, 533.76513254, 439.37788128,\n",
      "       437.09888259, 592.31584485, 669.82746742, 592.01663525,\n",
      "       597.07230042, 597.51240164, 732.10605797, 694.37287539,\n",
      "       600.22696169, 529.54083307, 475.68450536, 549.19229931,\n",
      "       538.22986094, 490.72208051])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([514.5088088 , 508.05592927, 639.58053533, 658.89585885,\n",
      "       607.11396555, 662.96302643, 623.20154476, 551.67016265,\n",
      "       648.06221241, 647.35774585, 555.63289089, 563.85025964,\n",
      "       543.66405584, 506.29295049, 609.75057181, 707.14155238,\n",
      "       571.44576343, 604.4266225 , 577.85684727, 565.12944302])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([405.77162346, 261.78093214, 257.79632772, 359.61398809,\n",
      "       300.97891002, 323.07814374, 360.51164299, 286.45978574,\n",
      "       296.09527034, 152.42525801, 197.44811645, 274.68819015,\n",
      "       247.35305589, 374.20690796, 282.19155374, 275.74591981,\n",
      "       318.92733395, 398.74035381, 286.78178474, 360.52363199,\n",
      "       351.88463431, 285.26129367, 335.01022329, 350.24709861,\n",
      "       309.63551757, 327.15226648, 317.45700632, 335.20605714,\n",
      "       373.26065529, 433.43357323])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([315.82157975, 427.89093968, 289.63949291, 301.77052434,\n",
      "       367.48165357, 372.13463675, 324.83322363, 327.12874953,\n",
      "       320.05395004, 310.90629308, 380.879735  , 387.76152403,\n",
      "       330.557742  , 388.11743607, 390.37958736, 408.2225002 ,\n",
      "       410.23820866, 336.26542706, 319.41677054, 386.73270881])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([391.1802466 , 378.83606312, 413.5852798 , 345.70142055,\n",
      "       315.11381903, 346.66881669, 386.61950519, 426.58713648,\n",
      "       499.00199322, 496.48184556, 438.03423281, 423.59440741,\n",
      "       375.37548583, 404.09737706, 409.62535098, 437.76060348,\n",
      "       394.47503303, 439.97630195, 517.19346552, 461.76251444,\n",
      "       457.355711  , 479.62716775, 433.19914647, 443.23925318,\n",
      "       444.05635782, 446.12106416, 405.39701126, 427.84486792,\n",
      "       466.91572959, 527.49026645])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([526.50191535, 580.10855533, 460.73538646, 484.74676984,\n",
      "       478.32468609, 567.74701886, 461.14155259, 402.6968913 ,\n",
      "       407.19234085, 335.50445663, 383.89448025, 402.829297  ,\n",
      "       448.14808423, 478.51272274, 563.09347579, 552.31094708,\n",
      "       462.87334599, 413.79781564, 472.07663971, 410.40991406])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([532.80160653, 468.19758845, 559.84221298, 535.04153282,\n",
      "       564.85427982, 530.47387032, 609.74709298, 635.44991269,\n",
      "       535.91731397, 555.47851634, 560.26732238, 592.77245291,\n",
      "       492.78699422, 528.47301518, 576.62673148, 449.02573821,\n",
      "       420.73057322, 383.2050939 , 453.90534066, 539.24259306,\n",
      "       605.47121043, 556.67616859, 608.30828456, 489.31267053,\n",
      "       414.13221475, 481.92922581, 537.03024237, 531.27077547,\n",
      "       424.76439816, 480.27019493])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([456.60722947, 534.68799708, 560.27001124, 495.50885821,\n",
      "       490.43221638, 469.54868894, 509.0216617 , 580.5186044 ,\n",
      "       509.56832961, 547.35738325, 523.72622733, 493.15355426,\n",
      "       520.45598519, 488.64675133, 495.78048545, 472.70249657,\n",
      "       618.45257776, 589.3399963 , 522.26342527, 549.53276579])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([424.76439816, 480.27019493, 456.60722947, 534.68799708,\n",
      "       560.27001124, 495.50885821, 490.43221638, 469.54868894,\n",
      "       509.0216617 , 580.5186044 , 509.56832961, 547.35738325,\n",
      "       523.72622733, 493.15355426, 520.45598519, 488.64675133,\n",
      "       495.78048545, 472.70249657, 618.45257776, 589.3399963 ,\n",
      "       522.26342527, 549.53276579, 550.7361558 , 541.6098775 ,\n",
      "       581.53451462, 607.22224773, 549.87462813, 522.37636622,\n",
      "       533.76513254, 439.37788128])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([437.09888259, 592.31584485, 669.82746742, 592.01663525,\n",
      "       597.07230042, 597.51240164, 732.10605797, 694.37287539,\n",
      "       600.22696169, 529.54083307, 475.68450536, 549.19229931,\n",
      "       538.22986094, 490.72208051, 514.5088088 , 508.05592927,\n",
      "       639.58053533, 658.89585885, 607.11396555, 662.96302643])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([293.2503612 , 461.66825185, 371.01366887, 340.76572611,\n",
      "       337.21491073, 319.24262795, 265.1709739 , 298.00957865,\n",
      "       242.49635296, 240.13836305, 236.14417874, 260.83591413,\n",
      "       384.60274617, 226.89460208, 273.27294491, 203.17066281,\n",
      "       217.19312001, 314.43807321, 300.01414871, 227.87519988,\n",
      "       220.79051407, 295.02199764, 255.88545794, 278.47209267,\n",
      "       287.98710364, 252.91959566, 378.32322455, 361.8077385 ,\n",
      "       206.60592959, 258.34179417])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([255.41322766, 318.51060943, 268.61541097, 272.76425551,\n",
      "       315.14139906, 348.59105903, 256.83218886, 258.63997842,\n",
      "       265.18095026, 255.64151055, 373.23511922, 357.72595578,\n",
      "       254.37809822, 325.97651154, 427.7355496 , 405.77162346,\n",
      "       261.78093214, 257.79632772, 359.61398809, 300.97891002])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([466.91572959, 527.49026645, 526.50191535, 580.10855533,\n",
      "       460.73538646, 484.74676984, 478.32468609, 567.74701886,\n",
      "       461.14155259, 402.6968913 , 407.19234085, 335.50445663,\n",
      "       383.89448025, 402.829297  , 448.14808423, 478.51272274,\n",
      "       563.09347579, 552.31094708, 462.87334599, 413.79781564,\n",
      "       472.07663971, 410.40991406, 533.57006609, 564.84733814,\n",
      "       478.79369029, 382.97566996, 505.07405096, 493.43569082,\n",
      "       538.94192392, 425.49096814])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([419.33548104, 463.8273169 , 481.11941758, 460.64102572,\n",
      "       505.29460783, 443.84386519, 458.00712312, 487.44691126,\n",
      "       515.62869948, 536.31984124, 451.87212301, 394.45919165,\n",
      "       521.30830427, 532.80160653, 468.19758845, 559.84221298,\n",
      "       535.04153282, 564.85427982, 530.47387032, 609.74709298])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([478.32468609, 567.74701886, 461.14155259, 402.6968913 ,\n",
      "       407.19234085, 335.50445663, 383.89448025, 402.829297  ,\n",
      "       448.14808423, 478.51272274, 563.09347579, 552.31094708,\n",
      "       462.87334599, 413.79781564, 472.07663971, 410.40991406,\n",
      "       533.57006609, 564.84733814, 478.79369029, 382.97566996,\n",
      "       505.07405096, 493.43569082, 538.94192392, 425.49096814,\n",
      "       419.33548104, 463.8273169 , 481.11941758, 460.64102572,\n",
      "       505.29460783, 443.84386519])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([458.00712312, 487.44691126, 515.62869948, 536.31984124,\n",
      "       451.87212301, 394.45919165, 521.30830427, 532.80160653,\n",
      "       468.19758845, 559.84221298, 535.04153282, 564.85427982,\n",
      "       530.47387032, 609.74709298, 635.44991269, 535.91731397,\n",
      "       555.47851634, 560.26732238, 592.77245291, 492.78699422])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([563.09347579, 552.31094708, 462.87334599, 413.79781564,\n",
      "       472.07663971, 410.40991406, 533.57006609, 564.84733814,\n",
      "       478.79369029, 382.97566996, 505.07405096, 493.43569082,\n",
      "       538.94192392, 425.49096814, 419.33548104, 463.8273169 ,\n",
      "       481.11941758, 460.64102572, 505.29460783, 443.84386519,\n",
      "       458.00712312, 487.44691126, 515.62869948, 536.31984124,\n",
      "       451.87212301, 394.45919165, 521.30830427, 532.80160653,\n",
      "       468.19758845, 559.84221298])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([535.04153282, 564.85427982, 530.47387032, 609.74709298,\n",
      "       635.44991269, 535.91731397, 555.47851634, 560.26732238,\n",
      "       592.77245291, 492.78699422, 528.47301518, 576.62673148,\n",
      "       449.02573821, 420.73057322, 383.2050939 , 453.90534066,\n",
      "       539.24259306, 605.47121043, 556.67616859, 608.30828456])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([592.77245291, 492.78699422, 528.47301518, 576.62673148,\n",
      "       449.02573821, 420.73057322, 383.2050939 , 453.90534066,\n",
      "       539.24259306, 605.47121043, 556.67616859, 608.30828456,\n",
      "       489.31267053, 414.13221475, 481.92922581, 537.03024237,\n",
      "       531.27077547, 424.76439816, 480.27019493, 456.60722947,\n",
      "       534.68799708, 560.27001124, 495.50885821, 490.43221638,\n",
      "       469.54868894, 509.0216617 , 580.5186044 , 509.56832961,\n",
      "       547.35738325, 523.72622733])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([493.15355426, 520.45598519, 488.64675133, 495.78048545,\n",
      "       472.70249657, 618.45257776, 589.3399963 , 522.26342527,\n",
      "       549.53276579, 550.7361558 , 541.6098775 , 581.53451462,\n",
      "       607.22224773, 549.87462813, 522.37636622, 533.76513254,\n",
      "       439.37788128, 437.09888259, 592.31584485, 669.82746742])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([405.39701126, 427.84486792, 466.91572959, 527.49026645,\n",
      "       526.50191535, 580.10855533, 460.73538646, 484.74676984,\n",
      "       478.32468609, 567.74701886, 461.14155259, 402.6968913 ,\n",
      "       407.19234085, 335.50445663, 383.89448025, 402.829297  ,\n",
      "       448.14808423, 478.51272274, 563.09347579, 552.31094708,\n",
      "       462.87334599, 413.79781564, 472.07663971, 410.40991406,\n",
      "       533.57006609, 564.84733814, 478.79369029, 382.97566996,\n",
      "       505.07405096, 493.43569082])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([538.94192392, 425.49096814, 419.33548104, 463.8273169 ,\n",
      "       481.11941758, 460.64102572, 505.29460783, 443.84386519,\n",
      "       458.00712312, 487.44691126, 515.62869948, 536.31984124,\n",
      "       451.87212301, 394.45919165, 521.30830427, 532.80160653,\n",
      "       468.19758845, 559.84221298, 535.04153282, 564.85427982])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([388.11743607, 390.37958736, 408.2225002 , 410.23820866,\n",
      "       336.26542706, 319.41677054, 386.73270881, 408.68045252,\n",
      "       379.54368873, 374.68737687, 434.71295722, 367.58405072,\n",
      "       389.15715379, 372.35580464, 359.08299146, 423.66672844,\n",
      "       438.84670817, 437.61444311, 463.16543088, 410.89882285,\n",
      "       419.07923581, 381.30008063, 394.0134984 , 383.27079237,\n",
      "       387.32550047, 417.0279523 , 358.91573723, 476.45969901,\n",
      "       381.52225147, 314.31964945])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([421.39247611, 452.38691493, 444.22925323, 442.59260105,\n",
      "       411.5736148 , 354.91586408, 383.56673024, 364.72357312,\n",
      "       434.29575526, 413.08282734, 361.27286765, 370.69375928,\n",
      "       416.44682588, 385.58899509, 355.57554051, 401.83538562,\n",
      "       424.40409635, 391.1802466 , 378.83606312, 413.5852798 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([523.72622733, 493.15355426, 520.45598519, 488.64675133,\n",
      "       495.78048545, 472.70249657, 618.45257776, 589.3399963 ,\n",
      "       522.26342527, 549.53276579, 550.7361558 , 541.6098775 ,\n",
      "       581.53451462, 607.22224773, 549.87462813, 522.37636622,\n",
      "       533.76513254, 439.37788128, 437.09888259, 592.31584485,\n",
      "       669.82746742, 592.01663525, 597.07230042, 597.51240164,\n",
      "       732.10605797, 694.37287539, 600.22696169, 529.54083307,\n",
      "       475.68450536, 549.19229931])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([538.22986094, 490.72208051, 514.5088088 , 508.05592927,\n",
      "       639.58053533, 658.89585885, 607.11396555, 662.96302643,\n",
      "       623.20154476, 551.67016265, 648.06221241, 647.35774585,\n",
      "       555.63289089, 563.85025964, 543.66405584, 506.29295049,\n",
      "       609.75057181, 707.14155238, 571.44576343, 604.4266225 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([424.40409635, 391.1802466 , 378.83606312, 413.5852798 ,\n",
      "       345.70142055, 315.11381903, 346.66881669, 386.61950519,\n",
      "       426.58713648, 499.00199322, 496.48184556, 438.03423281,\n",
      "       423.59440741, 375.37548583, 404.09737706, 409.62535098,\n",
      "       437.76060348, 394.47503303, 439.97630195, 517.19346552,\n",
      "       461.76251444, 457.355711  , 479.62716775, 433.19914647,\n",
      "       443.23925318, 444.05635782, 446.12106416, 405.39701126,\n",
      "       427.84486792, 466.91572959])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([527.49026645, 526.50191535, 580.10855533, 460.73538646,\n",
      "       484.74676984, 478.32468609, 567.74701886, 461.14155259,\n",
      "       402.6968913 , 407.19234085, 335.50445663, 383.89448025,\n",
      "       402.829297  , 448.14808423, 478.51272274, 563.09347579,\n",
      "       552.31094708, 462.87334599, 413.79781564, 472.07663971])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([472.70249657, 618.45257776, 589.3399963 , 522.26342527,\n",
      "       549.53276579, 550.7361558 , 541.6098775 , 581.53451462,\n",
      "       607.22224773, 549.87462813, 522.37636622, 533.76513254,\n",
      "       439.37788128, 437.09888259, 592.31584485, 669.82746742,\n",
      "       592.01663525, 597.07230042, 597.51240164, 732.10605797,\n",
      "       694.37287539, 600.22696169, 529.54083307, 475.68450536,\n",
      "       549.19229931, 538.22986094, 490.72208051, 514.5088088 ,\n",
      "       508.05592927, 639.58053533])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([658.89585885, 607.11396555, 662.96302643, 623.20154476,\n",
      "       551.67016265, 648.06221241, 647.35774585, 555.63289089,\n",
      "       563.85025964, 543.66405584, 506.29295049, 609.75057181,\n",
      "       707.14155238, 571.44576343, 604.4266225 , 577.85684727,\n",
      "       565.12944302, 561.70663192, 546.92000432, 587.34387768])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([533.57006609, 564.84733814, 478.79369029, 382.97566996,\n",
      "       505.07405096, 493.43569082, 538.94192392, 425.49096814,\n",
      "       419.33548104, 463.8273169 , 481.11941758, 460.64102572,\n",
      "       505.29460783, 443.84386519, 458.00712312, 487.44691126,\n",
      "       515.62869948, 536.31984124, 451.87212301, 394.45919165,\n",
      "       521.30830427, 532.80160653, 468.19758845, 559.84221298,\n",
      "       535.04153282, 564.85427982, 530.47387032, 609.74709298,\n",
      "       635.44991269, 535.91731397])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([555.47851634, 560.26732238, 592.77245291, 492.78699422,\n",
      "       528.47301518, 576.62673148, 449.02573821, 420.73057322,\n",
      "       383.2050939 , 453.90534066, 539.24259306, 605.47121043,\n",
      "       556.67616859, 608.30828456, 489.31267053, 414.13221475,\n",
      "       481.92922581, 537.03024237, 531.27077547, 424.76439816])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([461.14155259, 402.6968913 , 407.19234085, 335.50445663,\n",
      "       383.89448025, 402.829297  , 448.14808423, 478.51272274,\n",
      "       563.09347579, 552.31094708, 462.87334599, 413.79781564,\n",
      "       472.07663971, 410.40991406, 533.57006609, 564.84733814,\n",
      "       478.79369029, 382.97566996, 505.07405096, 493.43569082,\n",
      "       538.94192392, 425.49096814, 419.33548104, 463.8273169 ,\n",
      "       481.11941758, 460.64102572, 505.29460783, 443.84386519,\n",
      "       458.00712312, 487.44691126])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([515.62869948, 536.31984124, 451.87212301, 394.45919165,\n",
      "       521.30830427, 532.80160653, 468.19758845, 559.84221298,\n",
      "       535.04153282, 564.85427982, 530.47387032, 609.74709298,\n",
      "       635.44991269, 535.91731397, 555.47851634, 560.26732238,\n",
      "       592.77245291, 492.78699422, 528.47301518, 576.62673148])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([694.37287539, 600.22696169, 529.54083307, 475.68450536,\n",
      "       549.19229931, 538.22986094, 490.72208051, 514.5088088 ,\n",
      "       508.05592927, 639.58053533, 658.89585885, 607.11396555,\n",
      "       662.96302643, 623.20154476, 551.67016265, 648.06221241,\n",
      "       647.35774585, 555.63289089, 563.85025964, 543.66405584,\n",
      "       506.29295049, 609.75057181, 707.14155238, 571.44576343,\n",
      "       604.4266225 , 577.85684727, 565.12944302, 561.70663192,\n",
      "       546.92000432, 587.34387768])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([563.28894392, 603.15946382, 609.79642481, 597.68857407,\n",
      "       561.29017164, 563.99484285, 636.27975237, 653.7286504 ,\n",
      "       580.21410777, 603.71020602, 655.75754555, 552.61491167,\n",
      "       614.86465122, 596.29610199, 638.34407776, 596.98361388,\n",
      "       521.09639918, 505.46555349, 589.70267609, 636.73604882])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([385.58899509, 355.57554051, 401.83538562, 424.40409635,\n",
      "       391.1802466 , 378.83606312, 413.5852798 , 345.70142055,\n",
      "       315.11381903, 346.66881669, 386.61950519, 426.58713648,\n",
      "       499.00199322, 496.48184556, 438.03423281, 423.59440741,\n",
      "       375.37548583, 404.09737706, 409.62535098, 437.76060348,\n",
      "       394.47503303, 439.97630195, 517.19346552, 461.76251444,\n",
      "       457.355711  , 479.62716775, 433.19914647, 443.23925318,\n",
      "       444.05635782, 446.12106416])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([405.39701126, 427.84486792, 466.91572959, 527.49026645,\n",
      "       526.50191535, 580.10855533, 460.73538646, 484.74676984,\n",
      "       478.32468609, 567.74701886, 461.14155259, 402.6968913 ,\n",
      "       407.19234085, 335.50445663, 383.89448025, 402.829297  ,\n",
      "       448.14808423, 478.51272274, 563.09347579, 552.31094708])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([580.10855533, 460.73538646, 484.74676984, 478.32468609,\n",
      "       567.74701886, 461.14155259, 402.6968913 , 407.19234085,\n",
      "       335.50445663, 383.89448025, 402.829297  , 448.14808423,\n",
      "       478.51272274, 563.09347579, 552.31094708, 462.87334599,\n",
      "       413.79781564, 472.07663971, 410.40991406, 533.57006609,\n",
      "       564.84733814, 478.79369029, 382.97566996, 505.07405096,\n",
      "       493.43569082, 538.94192392, 425.49096814, 419.33548104,\n",
      "       463.8273169 , 481.11941758])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([460.64102572, 505.29460783, 443.84386519, 458.00712312,\n",
      "       487.44691126, 515.62869948, 536.31984124, 451.87212301,\n",
      "       394.45919165, 521.30830427, 532.80160653, 468.19758845,\n",
      "       559.84221298, 535.04153282, 564.85427982, 530.47387032,\n",
      "       609.74709298, 635.44991269, 535.91731397, 555.47851634])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([408.2225002 , 410.23820866, 336.26542706, 319.41677054,\n",
      "       386.73270881, 408.68045252, 379.54368873, 374.68737687,\n",
      "       434.71295722, 367.58405072, 389.15715379, 372.35580464,\n",
      "       359.08299146, 423.66672844, 438.84670817, 437.61444311,\n",
      "       463.16543088, 410.89882285, 419.07923581, 381.30008063,\n",
      "       394.0134984 , 383.27079237, 387.32550047, 417.0279523 ,\n",
      "       358.91573723, 476.45969901, 381.52225147, 314.31964945,\n",
      "       421.39247611, 452.38691493])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([444.22925323, 442.59260105, 411.5736148 , 354.91586408,\n",
      "       383.56673024, 364.72357312, 434.29575526, 413.08282734,\n",
      "       361.27286765, 370.69375928, 416.44682588, 385.58899509,\n",
      "       355.57554051, 401.83538562, 424.40409635, 391.1802466 ,\n",
      "       378.83606312, 413.5852798 , 345.70142055, 315.11381903])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([456.60722947, 534.68799708, 560.27001124, 495.50885821,\n",
      "       490.43221638, 469.54868894, 509.0216617 , 580.5186044 ,\n",
      "       509.56832961, 547.35738325, 523.72622733, 493.15355426,\n",
      "       520.45598519, 488.64675133, 495.78048545, 472.70249657,\n",
      "       618.45257776, 589.3399963 , 522.26342527, 549.53276579,\n",
      "       550.7361558 , 541.6098775 , 581.53451462, 607.22224773,\n",
      "       549.87462813, 522.37636622, 533.76513254, 439.37788128,\n",
      "       437.09888259, 592.31584485])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([669.82746742, 592.01663525, 597.07230042, 597.51240164,\n",
      "       732.10605797, 694.37287539, 600.22696169, 529.54083307,\n",
      "       475.68450536, 549.19229931, 538.22986094, 490.72208051,\n",
      "       514.5088088 , 508.05592927, 639.58053533, 658.89585885,\n",
      "       607.11396555, 662.96302643, 623.20154476, 551.67016265])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([460.64102572, 505.29460783, 443.84386519, 458.00712312,\n",
      "       487.44691126, 515.62869948, 536.31984124, 451.87212301,\n",
      "       394.45919165, 521.30830427, 532.80160653, 468.19758845,\n",
      "       559.84221298, 535.04153282, 564.85427982, 530.47387032,\n",
      "       609.74709298, 635.44991269, 535.91731397, 555.47851634,\n",
      "       560.26732238, 592.77245291, 492.78699422, 528.47301518,\n",
      "       576.62673148, 449.02573821, 420.73057322, 383.2050939 ,\n",
      "       453.90534066, 539.24259306])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([605.47121043, 556.67616859, 608.30828456, 489.31267053,\n",
      "       414.13221475, 481.92922581, 537.03024237, 531.27077547,\n",
      "       424.76439816, 480.27019493, 456.60722947, 534.68799708,\n",
      "       560.27001124, 495.50885821, 490.43221638, 469.54868894,\n",
      "       509.0216617 , 580.5186044 , 509.56832961, 547.35738325])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([481.92922581, 537.03024237, 531.27077547, 424.76439816,\n",
      "       480.27019493, 456.60722947, 534.68799708, 560.27001124,\n",
      "       495.50885821, 490.43221638, 469.54868894, 509.0216617 ,\n",
      "       580.5186044 , 509.56832961, 547.35738325, 523.72622733,\n",
      "       493.15355426, 520.45598519, 488.64675133, 495.78048545,\n",
      "       472.70249657, 618.45257776, 589.3399963 , 522.26342527,\n",
      "       549.53276579, 550.7361558 , 541.6098775 , 581.53451462,\n",
      "       607.22224773, 549.87462813])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([522.37636622, 533.76513254, 439.37788128, 437.09888259,\n",
      "       592.31584485, 669.82746742, 592.01663525, 597.07230042,\n",
      "       597.51240164, 732.10605797, 694.37287539, 600.22696169,\n",
      "       529.54083307, 475.68450536, 549.19229931, 538.22986094,\n",
      "       490.72208051, 514.5088088 , 508.05592927, 639.58053533])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([522.37636622, 533.76513254, 439.37788128, 437.09888259,\n",
      "       592.31584485, 669.82746742, 592.01663525, 597.07230042,\n",
      "       597.51240164, 732.10605797, 694.37287539, 600.22696169,\n",
      "       529.54083307, 475.68450536, 549.19229931, 538.22986094,\n",
      "       490.72208051, 514.5088088 , 508.05592927, 639.58053533,\n",
      "       658.89585885, 607.11396555, 662.96302643, 623.20154476,\n",
      "       551.67016265, 648.06221241, 647.35774585, 555.63289089,\n",
      "       563.85025964, 543.66405584])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([506.29295049, 609.75057181, 707.14155238, 571.44576343,\n",
      "       604.4266225 , 577.85684727, 565.12944302, 561.70663192,\n",
      "       546.92000432, 587.34387768, 563.28894392, 603.15946382,\n",
      "       609.79642481, 597.68857407, 561.29017164, 563.99484285,\n",
      "       636.27975237, 653.7286504 , 580.21410777, 603.71020602])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([600.22696169, 529.54083307, 475.68450536, 549.19229931,\n",
      "       538.22986094, 490.72208051, 514.5088088 , 508.05592927,\n",
      "       639.58053533, 658.89585885, 607.11396555, 662.96302643,\n",
      "       623.20154476, 551.67016265, 648.06221241, 647.35774585,\n",
      "       555.63289089, 563.85025964, 543.66405584, 506.29295049,\n",
      "       609.75057181, 707.14155238, 571.44576343, 604.4266225 ,\n",
      "       577.85684727, 565.12944302, 561.70663192, 546.92000432,\n",
      "       587.34387768, 563.28894392])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([603.15946382, 609.79642481, 597.68857407, 561.29017164,\n",
      "       563.99484285, 636.27975237, 653.7286504 , 580.21410777,\n",
      "       603.71020602, 655.75754555, 552.61491167, 614.86465122,\n",
      "       596.29610199, 638.34407776, 596.98361388, 521.09639918,\n",
      "       505.46555349, 589.70267609, 636.73604882, 589.67698049])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([522.26342527, 549.53276579, 550.7361558 , 541.6098775 ,\n",
      "       581.53451462, 607.22224773, 549.87462813, 522.37636622,\n",
      "       533.76513254, 439.37788128, 437.09888259, 592.31584485,\n",
      "       669.82746742, 592.01663525, 597.07230042, 597.51240164,\n",
      "       732.10605797, 694.37287539, 600.22696169, 529.54083307,\n",
      "       475.68450536, 549.19229931, 538.22986094, 490.72208051,\n",
      "       514.5088088 , 508.05592927, 639.58053533, 658.89585885,\n",
      "       607.11396555, 662.96302643])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([623.20154476, 551.67016265, 648.06221241, 647.35774585,\n",
      "       555.63289089, 563.85025964, 543.66405584, 506.29295049,\n",
      "       609.75057181, 707.14155238, 571.44576343, 604.4266225 ,\n",
      "       577.85684727, 565.12944302, 561.70663192, 546.92000432,\n",
      "       587.34387768, 563.28894392, 603.15946382, 609.79642481])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([191.31567738, 228.94610511, 238.09168848, 267.12643163,\n",
      "       239.73651743, 296.96554321, 241.19461979, 358.19748099,\n",
      "       314.38420433, 205.25786975, 159.52202019, 227.55789279,\n",
      "       224.56358967, 261.6293727 , 269.64979598, 241.11349397,\n",
      "       191.94529955, 141.51864415, 178.65095344, 264.60280642,\n",
      "       264.23905845, 183.7540556 , 223.49117695, 260.65355218,\n",
      "       206.39405506, 233.54247196, 248.43310869, 190.08114942,\n",
      "       240.53880393, 280.09782758])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([315.35893007, 327.78137488, 209.29589023, 180.7943296 ,\n",
      "       256.05134529, 287.62799663, 293.2503612 , 461.66825185,\n",
      "       371.01366887, 340.76572611, 337.21491073, 319.24262795,\n",
      "       265.1709739 , 298.00957865, 242.49635296, 240.13836305,\n",
      "       236.14417874, 260.83591413, 384.60274617, 226.89460208])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([526.50191535, 580.10855533, 460.73538646, 484.74676984,\n",
      "       478.32468609, 567.74701886, 461.14155259, 402.6968913 ,\n",
      "       407.19234085, 335.50445663, 383.89448025, 402.829297  ,\n",
      "       448.14808423, 478.51272274, 563.09347579, 552.31094708,\n",
      "       462.87334599, 413.79781564, 472.07663971, 410.40991406,\n",
      "       533.57006609, 564.84733814, 478.79369029, 382.97566996,\n",
      "       505.07405096, 493.43569082, 538.94192392, 425.49096814,\n",
      "       419.33548104, 463.8273169 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([481.11941758, 460.64102572, 505.29460783, 443.84386519,\n",
      "       458.00712312, 487.44691126, 515.62869948, 536.31984124,\n",
      "       451.87212301, 394.45919165, 521.30830427, 532.80160653,\n",
      "       468.19758845, 559.84221298, 535.04153282, 564.85427982,\n",
      "       530.47387032, 609.74709298, 635.44991269, 535.91731397])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([206.39405506, 233.54247196, 248.43310869, 190.08114942,\n",
      "       240.53880393, 280.09782758, 315.35893007, 327.78137488,\n",
      "       209.29589023, 180.7943296 , 256.05134529, 287.62799663,\n",
      "       293.2503612 , 461.66825185, 371.01366887, 340.76572611,\n",
      "       337.21491073, 319.24262795, 265.1709739 , 298.00957865,\n",
      "       242.49635296, 240.13836305, 236.14417874, 260.83591413,\n",
      "       384.60274617, 226.89460208, 273.27294491, 203.17066281,\n",
      "       217.19312001, 314.43807321])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([300.01414871, 227.87519988, 220.79051407, 295.02199764,\n",
      "       255.88545794, 278.47209267, 287.98710364, 252.91959566,\n",
      "       378.32322455, 361.8077385 , 206.60592959, 258.34179417,\n",
      "       255.41322766, 318.51060943, 268.61541097, 272.76425551,\n",
      "       315.14139906, 348.59105903, 256.83218886, 258.63997842])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([413.79781564, 472.07663971, 410.40991406, 533.57006609,\n",
      "       564.84733814, 478.79369029, 382.97566996, 505.07405096,\n",
      "       493.43569082, 538.94192392, 425.49096814, 419.33548104,\n",
      "       463.8273169 , 481.11941758, 460.64102572, 505.29460783,\n",
      "       443.84386519, 458.00712312, 487.44691126, 515.62869948,\n",
      "       536.31984124, 451.87212301, 394.45919165, 521.30830427,\n",
      "       532.80160653, 468.19758845, 559.84221298, 535.04153282,\n",
      "       564.85427982, 530.47387032])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([609.74709298, 635.44991269, 535.91731397, 555.47851634,\n",
      "       560.26732238, 592.77245291, 492.78699422, 528.47301518,\n",
      "       576.62673148, 449.02573821, 420.73057322, 383.2050939 ,\n",
      "       453.90534066, 539.24259306, 605.47121043, 556.67616859,\n",
      "       608.30828456, 489.31267053, 414.13221475, 481.92922581])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([367.48165357, 372.13463675, 324.83322363, 327.12874953,\n",
      "       320.05395004, 310.90629308, 380.879735  , 387.76152403,\n",
      "       330.557742  , 388.11743607, 390.37958736, 408.2225002 ,\n",
      "       410.23820866, 336.26542706, 319.41677054, 386.73270881,\n",
      "       408.68045252, 379.54368873, 374.68737687, 434.71295722,\n",
      "       367.58405072, 389.15715379, 372.35580464, 359.08299146,\n",
      "       423.66672844, 438.84670817, 437.61444311, 463.16543088,\n",
      "       410.89882285, 419.07923581])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([381.30008063, 394.0134984 , 383.27079237, 387.32550047,\n",
      "       417.0279523 , 358.91573723, 476.45969901, 381.52225147,\n",
      "       314.31964945, 421.39247611, 452.38691493, 444.22925323,\n",
      "       442.59260105, 411.5736148 , 354.91586408, 383.56673024,\n",
      "       364.72357312, 434.29575526, 413.08282734, 361.27286765])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([481.11941758, 460.64102572, 505.29460783, 443.84386519,\n",
      "       458.00712312, 487.44691126, 515.62869948, 536.31984124,\n",
      "       451.87212301, 394.45919165, 521.30830427, 532.80160653,\n",
      "       468.19758845, 559.84221298, 535.04153282, 564.85427982,\n",
      "       530.47387032, 609.74709298, 635.44991269, 535.91731397,\n",
      "       555.47851634, 560.26732238, 592.77245291, 492.78699422,\n",
      "       528.47301518, 576.62673148, 449.02573821, 420.73057322,\n",
      "       383.2050939 , 453.90534066])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([539.24259306, 605.47121043, 556.67616859, 608.30828456,\n",
      "       489.31267053, 414.13221475, 481.92922581, 537.03024237,\n",
      "       531.27077547, 424.76439816, 480.27019493, 456.60722947,\n",
      "       534.68799708, 560.27001124, 495.50885821, 490.43221638,\n",
      "       469.54868894, 509.0216617 , 580.5186044 , 509.56832961])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([458.00712312, 487.44691126, 515.62869948, 536.31984124,\n",
      "       451.87212301, 394.45919165, 521.30830427, 532.80160653,\n",
      "       468.19758845, 559.84221298, 535.04153282, 564.85427982,\n",
      "       530.47387032, 609.74709298, 635.44991269, 535.91731397,\n",
      "       555.47851634, 560.26732238, 592.77245291, 492.78699422,\n",
      "       528.47301518, 576.62673148, 449.02573821, 420.73057322,\n",
      "       383.2050939 , 453.90534066, 539.24259306, 605.47121043,\n",
      "       556.67616859, 608.30828456])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([489.31267053, 414.13221475, 481.92922581, 537.03024237,\n",
      "       531.27077547, 424.76439816, 480.27019493, 456.60722947,\n",
      "       534.68799708, 560.27001124, 495.50885821, 490.43221638,\n",
      "       469.54868894, 509.0216617 , 580.5186044 , 509.56832961,\n",
      "       547.35738325, 523.72622733, 493.15355426, 520.45598519])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([475.68450536, 549.19229931, 538.22986094, 490.72208051,\n",
      "       514.5088088 , 508.05592927, 639.58053533, 658.89585885,\n",
      "       607.11396555, 662.96302643, 623.20154476, 551.67016265,\n",
      "       648.06221241, 647.35774585, 555.63289089, 563.85025964,\n",
      "       543.66405584, 506.29295049, 609.75057181, 707.14155238,\n",
      "       571.44576343, 604.4266225 , 577.85684727, 565.12944302,\n",
      "       561.70663192, 546.92000432, 587.34387768, 563.28894392,\n",
      "       603.15946382, 609.79642481])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([597.68857407, 561.29017164, 563.99484285, 636.27975237,\n",
      "       653.7286504 , 580.21410777, 603.71020602, 655.75754555,\n",
      "       552.61491167, 614.86465122, 596.29610199, 638.34407776,\n",
      "       596.98361388, 521.09639918, 505.46555349, 589.70267609,\n",
      "       636.73604882, 589.67698049, 644.77612963, 561.24317062])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([669.82746742, 592.01663525, 597.07230042, 597.51240164,\n",
      "       732.10605797, 694.37287539, 600.22696169, 529.54083307,\n",
      "       475.68450536, 549.19229931, 538.22986094, 490.72208051,\n",
      "       514.5088088 , 508.05592927, 639.58053533, 658.89585885,\n",
      "       607.11396555, 662.96302643, 623.20154476, 551.67016265,\n",
      "       648.06221241, 647.35774585, 555.63289089, 563.85025964,\n",
      "       543.66405584, 506.29295049, 609.75057181, 707.14155238,\n",
      "       571.44576343, 604.4266225 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([577.85684727, 565.12944302, 561.70663192, 546.92000432,\n",
      "       587.34387768, 563.28894392, 603.15946382, 609.79642481,\n",
      "       597.68857407, 561.29017164, 563.99484285, 636.27975237,\n",
      "       653.7286504 , 580.21410777, 603.71020602, 655.75754555,\n",
      "       552.61491167, 614.86465122, 596.29610199, 638.34407776])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([639.58053533, 658.89585885, 607.11396555, 662.96302643,\n",
      "       623.20154476, 551.67016265, 648.06221241, 647.35774585,\n",
      "       555.63289089, 563.85025964, 543.66405584, 506.29295049,\n",
      "       609.75057181, 707.14155238, 571.44576343, 604.4266225 ,\n",
      "       577.85684727, 565.12944302, 561.70663192, 546.92000432,\n",
      "       587.34387768, 563.28894392, 603.15946382, 609.79642481,\n",
      "       597.68857407, 561.29017164, 563.99484285, 636.27975237,\n",
      "       653.7286504 , 580.21410777])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([603.71020602, 655.75754555, 552.61491167, 614.86465122,\n",
      "       596.29610199, 638.34407776, 596.98361388, 521.09639918,\n",
      "       505.46555349, 589.70267609, 636.73604882, 589.67698049,\n",
      "       644.77612963, 561.24317062, 596.79057361, 568.82052745,\n",
      "       576.8558281 , 622.05026979, 594.06184224, 602.02666574])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([551.67016265, 648.06221241, 647.35774585, 555.63289089,\n",
      "       563.85025964, 543.66405584, 506.29295049, 609.75057181,\n",
      "       707.14155238, 571.44576343, 604.4266225 , 577.85684727,\n",
      "       565.12944302, 561.70663192, 546.92000432, 587.34387768,\n",
      "       563.28894392, 603.15946382, 609.79642481, 597.68857407,\n",
      "       561.29017164, 563.99484285, 636.27975237, 653.7286504 ,\n",
      "       580.21410777, 603.71020602, 655.75754555, 552.61491167,\n",
      "       614.86465122, 596.29610199])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([638.34407776, 596.98361388, 521.09639918, 505.46555349,\n",
      "       589.70267609, 636.73604882, 589.67698049, 644.77612963,\n",
      "       561.24317062, 596.79057361, 568.82052745, 576.8558281 ,\n",
      "       622.05026979, 594.06184224, 602.02666574, 680.36988082,\n",
      "       633.39966005, 675.71884524, 605.40908925, 650.71947409])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([196.4136086 , 173.2665378 , 161.73243664,  99.10058533,\n",
      "       189.91445947, 217.03019629, 218.01662357, 135.62151463,\n",
      "       215.66343029, 121.17746619, 167.28600356, 283.28864689,\n",
      "       164.69125998, 125.10364505, 159.87748372, 144.34205834,\n",
      "        82.29692285, 140.1662113 , 115.85726673, 173.41590414,\n",
      "       135.43190224, 233.88614205, 168.22234072, 149.37417651,\n",
      "       211.20263936, 134.80757158, 168.40259944, 248.81801436,\n",
      "       131.08170877, 163.32682593])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([198.28054145, 230.55995397, 143.44437565,  99.18040759,\n",
      "       184.29382082, 211.78528286, 211.13288375, 216.64507465,\n",
      "       168.69752245, 193.57349357, 214.00622395, 168.32951   ,\n",
      "       277.04961628, 260.96146028, 156.82804628, 211.79379382,\n",
      "       165.25249826, 223.55048858, 276.7108538 , 191.76163782])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([618.45257776, 589.3399963 , 522.26342527, 549.53276579,\n",
      "       550.7361558 , 541.6098775 , 581.53451462, 607.22224773,\n",
      "       549.87462813, 522.37636622, 533.76513254, 439.37788128,\n",
      "       437.09888259, 592.31584485, 669.82746742, 592.01663525,\n",
      "       597.07230042, 597.51240164, 732.10605797, 694.37287539,\n",
      "       600.22696169, 529.54083307, 475.68450536, 549.19229931,\n",
      "       538.22986094, 490.72208051, 514.5088088 , 508.05592927,\n",
      "       639.58053533, 658.89585885])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([607.11396555, 662.96302643, 623.20154476, 551.67016265,\n",
      "       648.06221241, 647.35774585, 555.63289089, 563.85025964,\n",
      "       543.66405584, 506.29295049, 609.75057181, 707.14155238,\n",
      "       571.44576343, 604.4266225 , 577.85684727, 565.12944302,\n",
      "       561.70663192, 546.92000432, 587.34387768, 563.28894392])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([453.90534066, 539.24259306, 605.47121043, 556.67616859,\n",
      "       608.30828456, 489.31267053, 414.13221475, 481.92922581,\n",
      "       537.03024237, 531.27077547, 424.76439816, 480.27019493,\n",
      "       456.60722947, 534.68799708, 560.27001124, 495.50885821,\n",
      "       490.43221638, 469.54868894, 509.0216617 , 580.5186044 ,\n",
      "       509.56832961, 547.35738325, 523.72622733, 493.15355426,\n",
      "       520.45598519, 488.64675133, 495.78048545, 472.70249657,\n",
      "       618.45257776, 589.3399963 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([522.26342527, 549.53276579, 550.7361558 , 541.6098775 ,\n",
      "       581.53451462, 607.22224773, 549.87462813, 522.37636622,\n",
      "       533.76513254, 439.37788128, 437.09888259, 592.31584485,\n",
      "       669.82746742, 592.01663525, 597.07230042, 597.51240164,\n",
      "       732.10605797, 694.37287539, 600.22696169, 529.54083307])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([157.91140713, 246.51495725, 196.23187143, 170.08294304,\n",
      "       152.27811061,  53.64930572, 108.24262341, 147.88143388,\n",
      "       274.83401078, 196.4136086 , 173.2665378 , 161.73243664,\n",
      "        99.10058533, 189.91445947, 217.03019629, 218.01662357,\n",
      "       135.62151463, 215.66343029, 121.17746619, 167.28600356,\n",
      "       283.28864689, 164.69125998, 125.10364505, 159.87748372,\n",
      "       144.34205834,  82.29692285, 140.1662113 , 115.85726673,\n",
      "       173.41590414, 135.43190224])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([233.88614205, 168.22234072, 149.37417651, 211.20263936,\n",
      "       134.80757158, 168.40259944, 248.81801436, 131.08170877,\n",
      "       163.32682593, 198.28054145, 230.55995397, 143.44437565,\n",
      "        99.18040759, 184.29382082, 211.78528286, 211.13288375,\n",
      "       216.64507465, 168.69752245, 193.57349357, 214.00622395])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([460.73538646, 484.74676984, 478.32468609, 567.74701886,\n",
      "       461.14155259, 402.6968913 , 407.19234085, 335.50445663,\n",
      "       383.89448025, 402.829297  , 448.14808423, 478.51272274,\n",
      "       563.09347579, 552.31094708, 462.87334599, 413.79781564,\n",
      "       472.07663971, 410.40991406, 533.57006609, 564.84733814,\n",
      "       478.79369029, 382.97566996, 505.07405096, 493.43569082,\n",
      "       538.94192392, 425.49096814, 419.33548104, 463.8273169 ,\n",
      "       481.11941758, 460.64102572])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([505.29460783, 443.84386519, 458.00712312, 487.44691126,\n",
      "       515.62869948, 536.31984124, 451.87212301, 394.45919165,\n",
      "       521.30830427, 532.80160653, 468.19758845, 559.84221298,\n",
      "       535.04153282, 564.85427982, 530.47387032, 609.74709298,\n",
      "       635.44991269, 535.91731397, 555.47851634, 560.26732238])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([515.62869948, 536.31984124, 451.87212301, 394.45919165,\n",
      "       521.30830427, 532.80160653, 468.19758845, 559.84221298,\n",
      "       535.04153282, 564.85427982, 530.47387032, 609.74709298,\n",
      "       635.44991269, 535.91731397, 555.47851634, 560.26732238,\n",
      "       592.77245291, 492.78699422, 528.47301518, 576.62673148,\n",
      "       449.02573821, 420.73057322, 383.2050939 , 453.90534066,\n",
      "       539.24259306, 605.47121043, 556.67616859, 608.30828456,\n",
      "       489.31267053, 414.13221475])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([481.92922581, 537.03024237, 531.27077547, 424.76439816,\n",
      "       480.27019493, 456.60722947, 534.68799708, 560.27001124,\n",
      "       495.50885821, 490.43221638, 469.54868894, 509.0216617 ,\n",
      "       580.5186044 , 509.56832961, 547.35738325, 523.72622733,\n",
      "       493.15355426, 520.45598519, 488.64675133, 495.78048545])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([597.07230042, 597.51240164, 732.10605797, 694.37287539,\n",
      "       600.22696169, 529.54083307, 475.68450536, 549.19229931,\n",
      "       538.22986094, 490.72208051, 514.5088088 , 508.05592927,\n",
      "       639.58053533, 658.89585885, 607.11396555, 662.96302643,\n",
      "       623.20154476, 551.67016265, 648.06221241, 647.35774585,\n",
      "       555.63289089, 563.85025964, 543.66405584, 506.29295049,\n",
      "       609.75057181, 707.14155238, 571.44576343, 604.4266225 ,\n",
      "       577.85684727, 565.12944302])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([561.70663192, 546.92000432, 587.34387768, 563.28894392,\n",
      "       603.15946382, 609.79642481, 597.68857407, 561.29017164,\n",
      "       563.99484285, 636.27975237, 653.7286504 , 580.21410777,\n",
      "       603.71020602, 655.75754555, 552.61491167, 614.86465122,\n",
      "       596.29610199, 638.34407776, 596.98361388, 521.09639918])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([438.84670817, 437.61444311, 463.16543088, 410.89882285,\n",
      "       419.07923581, 381.30008063, 394.0134984 , 383.27079237,\n",
      "       387.32550047, 417.0279523 , 358.91573723, 476.45969901,\n",
      "       381.52225147, 314.31964945, 421.39247611, 452.38691493,\n",
      "       444.22925323, 442.59260105, 411.5736148 , 354.91586408,\n",
      "       383.56673024, 364.72357312, 434.29575526, 413.08282734,\n",
      "       361.27286765, 370.69375928, 416.44682588, 385.58899509,\n",
      "       355.57554051, 401.83538562])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([424.40409635, 391.1802466 , 378.83606312, 413.5852798 ,\n",
      "       345.70142055, 315.11381903, 346.66881669, 386.61950519,\n",
      "       426.58713648, 499.00199322, 496.48184556, 438.03423281,\n",
      "       423.59440741, 375.37548583, 404.09737706, 409.62535098,\n",
      "       437.76060348, 394.47503303, 439.97630195, 517.19346552])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([732.10605797, 694.37287539, 600.22696169, 529.54083307,\n",
      "       475.68450536, 549.19229931, 538.22986094, 490.72208051,\n",
      "       514.5088088 , 508.05592927, 639.58053533, 658.89585885,\n",
      "       607.11396555, 662.96302643, 623.20154476, 551.67016265,\n",
      "       648.06221241, 647.35774585, 555.63289089, 563.85025964,\n",
      "       543.66405584, 506.29295049, 609.75057181, 707.14155238,\n",
      "       571.44576343, 604.4266225 , 577.85684727, 565.12944302,\n",
      "       561.70663192, 546.92000432])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([587.34387768, 563.28894392, 603.15946382, 609.79642481,\n",
      "       597.68857407, 561.29017164, 563.99484285, 636.27975237,\n",
      "       653.7286504 , 580.21410777, 603.71020602, 655.75754555,\n",
      "       552.61491167, 614.86465122, 596.29610199, 638.34407776,\n",
      "       596.98361388, 521.09639918, 505.46555349, 589.70267609])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([577.85684727, 565.12944302, 561.70663192, 546.92000432,\n",
      "       587.34387768, 563.28894392, 603.15946382, 609.79642481,\n",
      "       597.68857407, 561.29017164, 563.99484285, 636.27975237,\n",
      "       653.7286504 , 580.21410777, 603.71020602, 655.75754555,\n",
      "       552.61491167, 614.86465122, 596.29610199, 638.34407776,\n",
      "       596.98361388, 521.09639918, 505.46555349, 589.70267609,\n",
      "       636.73604882, 589.67698049, 644.77612963, 561.24317062,\n",
      "       596.79057361, 568.82052745])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([576.8558281 , 622.05026979, 594.06184224, 602.02666574,\n",
      "       680.36988082, 633.39966005, 675.71884524, 605.40908925,\n",
      "       650.71947409, 727.41228097, 557.18612905, 562.01216424,\n",
      "       655.97105619, 650.10028912, 668.16176824, 631.85717443,\n",
      "       648.09903429, 649.27642821, 712.67236658, 696.27975741])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([550.7361558 , 541.6098775 , 581.53451462, 607.22224773,\n",
      "       549.87462813, 522.37636622, 533.76513254, 439.37788128,\n",
      "       437.09888259, 592.31584485, 669.82746742, 592.01663525,\n",
      "       597.07230042, 597.51240164, 732.10605797, 694.37287539,\n",
      "       600.22696169, 529.54083307, 475.68450536, 549.19229931,\n",
      "       538.22986094, 490.72208051, 514.5088088 , 508.05592927,\n",
      "       639.58053533, 658.89585885, 607.11396555, 662.96302643,\n",
      "       623.20154476, 551.67016265])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([648.06221241, 647.35774585, 555.63289089, 563.85025964,\n",
      "       543.66405584, 506.29295049, 609.75057181, 707.14155238,\n",
      "       571.44576343, 604.4266225 , 577.85684727, 565.12944302,\n",
      "       561.70663192, 546.92000432, 587.34387768, 563.28894392,\n",
      "       603.15946382, 609.79642481, 597.68857407, 561.29017164])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([505.29460783, 443.84386519, 458.00712312, 487.44691126,\n",
      "       515.62869948, 536.31984124, 451.87212301, 394.45919165,\n",
      "       521.30830427, 532.80160653, 468.19758845, 559.84221298,\n",
      "       535.04153282, 564.85427982, 530.47387032, 609.74709298,\n",
      "       635.44991269, 535.91731397, 555.47851634, 560.26732238,\n",
      "       592.77245291, 492.78699422, 528.47301518, 576.62673148,\n",
      "       449.02573821, 420.73057322, 383.2050939 , 453.90534066,\n",
      "       539.24259306, 605.47121043])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([556.67616859, 608.30828456, 489.31267053, 414.13221475,\n",
      "       481.92922581, 537.03024237, 531.27077547, 424.76439816,\n",
      "       480.27019493, 456.60722947, 534.68799708, 560.27001124,\n",
      "       495.50885821, 490.43221638, 469.54868894, 509.0216617 ,\n",
      "       580.5186044 , 509.56832961, 547.35738325, 523.72622733])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([256.05134529, 287.62799663, 293.2503612 , 461.66825185,\n",
      "       371.01366887, 340.76572611, 337.21491073, 319.24262795,\n",
      "       265.1709739 , 298.00957865, 242.49635296, 240.13836305,\n",
      "       236.14417874, 260.83591413, 384.60274617, 226.89460208,\n",
      "       273.27294491, 203.17066281, 217.19312001, 314.43807321,\n",
      "       300.01414871, 227.87519988, 220.79051407, 295.02199764,\n",
      "       255.88545794, 278.47209267, 287.98710364, 252.91959566,\n",
      "       378.32322455, 361.8077385 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([206.60592959, 258.34179417, 255.41322766, 318.51060943,\n",
      "       268.61541097, 272.76425551, 315.14139906, 348.59105903,\n",
      "       256.83218886, 258.63997842, 265.18095026, 255.64151055,\n",
      "       373.23511922, 357.72595578, 254.37809822, 325.97651154,\n",
      "       427.7355496 , 405.77162346, 261.78093214, 257.79632772])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([495.78048545, 472.70249657, 618.45257776, 589.3399963 ,\n",
      "       522.26342527, 549.53276579, 550.7361558 , 541.6098775 ,\n",
      "       581.53451462, 607.22224773, 549.87462813, 522.37636622,\n",
      "       533.76513254, 439.37788128, 437.09888259, 592.31584485,\n",
      "       669.82746742, 592.01663525, 597.07230042, 597.51240164,\n",
      "       732.10605797, 694.37287539, 600.22696169, 529.54083307,\n",
      "       475.68450536, 549.19229931, 538.22986094, 490.72208051,\n",
      "       514.5088088 , 508.05592927])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([639.58053533, 658.89585885, 607.11396555, 662.96302643,\n",
      "       623.20154476, 551.67016265, 648.06221241, 647.35774585,\n",
      "       555.63289089, 563.85025964, 543.66405584, 506.29295049,\n",
      "       609.75057181, 707.14155238, 571.44576343, 604.4266225 ,\n",
      "       577.85684727, 565.12944302, 561.70663192, 546.92000432])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([531.27077547, 424.76439816, 480.27019493, 456.60722947,\n",
      "       534.68799708, 560.27001124, 495.50885821, 490.43221638,\n",
      "       469.54868894, 509.0216617 , 580.5186044 , 509.56832961,\n",
      "       547.35738325, 523.72622733, 493.15355426, 520.45598519,\n",
      "       488.64675133, 495.78048545, 472.70249657, 618.45257776,\n",
      "       589.3399963 , 522.26342527, 549.53276579, 550.7361558 ,\n",
      "       541.6098775 , 581.53451462, 607.22224773, 549.87462813,\n",
      "       522.37636622, 533.76513254])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([439.37788128, 437.09888259, 592.31584485, 669.82746742,\n",
      "       592.01663525, 597.07230042, 597.51240164, 732.10605797,\n",
      "       694.37287539, 600.22696169, 529.54083307, 475.68450536,\n",
      "       549.19229931, 538.22986094, 490.72208051, 514.5088088 ,\n",
      "       508.05592927, 639.58053533, 658.89585885, 607.11396555])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([607.11396555, 662.96302643, 623.20154476, 551.67016265,\n",
      "       648.06221241, 647.35774585, 555.63289089, 563.85025964,\n",
      "       543.66405584, 506.29295049, 609.75057181, 707.14155238,\n",
      "       571.44576343, 604.4266225 , 577.85684727, 565.12944302,\n",
      "       561.70663192, 546.92000432, 587.34387768, 563.28894392,\n",
      "       603.15946382, 609.79642481, 597.68857407, 561.29017164,\n",
      "       563.99484285, 636.27975237, 653.7286504 , 580.21410777,\n",
      "       603.71020602, 655.75754555])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([552.61491167, 614.86465122, 596.29610199, 638.34407776,\n",
      "       596.98361388, 521.09639918, 505.46555349, 589.70267609,\n",
      "       636.73604882, 589.67698049, 644.77612963, 561.24317062,\n",
      "       596.79057361, 568.82052745, 576.8558281 , 622.05026979,\n",
      "       594.06184224, 602.02666574, 680.36988082, 633.39966005])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([564.85427982, 530.47387032, 609.74709298, 635.44991269,\n",
      "       535.91731397, 555.47851634, 560.26732238, 592.77245291,\n",
      "       492.78699422, 528.47301518, 576.62673148, 449.02573821,\n",
      "       420.73057322, 383.2050939 , 453.90534066, 539.24259306,\n",
      "       605.47121043, 556.67616859, 608.30828456, 489.31267053,\n",
      "       414.13221475, 481.92922581, 537.03024237, 531.27077547,\n",
      "       424.76439816, 480.27019493, 456.60722947, 534.68799708,\n",
      "       560.27001124, 495.50885821])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([490.43221638, 469.54868894, 509.0216617 , 580.5186044 ,\n",
      "       509.56832961, 547.35738325, 523.72622733, 493.15355426,\n",
      "       520.45598519, 488.64675133, 495.78048545, 472.70249657,\n",
      "       618.45257776, 589.3399963 , 522.26342527, 549.53276579,\n",
      "       550.7361558 , 541.6098775 , 581.53451462, 607.22224773])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([508.05592927, 639.58053533, 658.89585885, 607.11396555,\n",
      "       662.96302643, 623.20154476, 551.67016265, 648.06221241,\n",
      "       647.35774585, 555.63289089, 563.85025964, 543.66405584,\n",
      "       506.29295049, 609.75057181, 707.14155238, 571.44576343,\n",
      "       604.4266225 , 577.85684727, 565.12944302, 561.70663192,\n",
      "       546.92000432, 587.34387768, 563.28894392, 603.15946382,\n",
      "       609.79642481, 597.68857407, 561.29017164, 563.99484285,\n",
      "       636.27975237, 653.7286504 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([580.21410777, 603.71020602, 655.75754555, 552.61491167,\n",
      "       614.86465122, 596.29610199, 638.34407776, 596.98361388,\n",
      "       521.09639918, 505.46555349, 589.70267609, 636.73604882,\n",
      "       589.67698049, 644.77612963, 561.24317062, 596.79057361,\n",
      "       568.82052745, 576.8558281 , 622.05026979, 594.06184224])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([478.51272274, 563.09347579, 552.31094708, 462.87334599,\n",
      "       413.79781564, 472.07663971, 410.40991406, 533.57006609,\n",
      "       564.84733814, 478.79369029, 382.97566996, 505.07405096,\n",
      "       493.43569082, 538.94192392, 425.49096814, 419.33548104,\n",
      "       463.8273169 , 481.11941758, 460.64102572, 505.29460783,\n",
      "       443.84386519, 458.00712312, 487.44691126, 515.62869948,\n",
      "       536.31984124, 451.87212301, 394.45919165, 521.30830427,\n",
      "       532.80160653, 468.19758845])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([559.84221298, 535.04153282, 564.85427982, 530.47387032,\n",
      "       609.74709298, 635.44991269, 535.91731397, 555.47851634,\n",
      "       560.26732238, 592.77245291, 492.78699422, 528.47301518,\n",
      "       576.62673148, 449.02573821, 420.73057322, 383.2050939 ,\n",
      "       453.90534066, 539.24259306, 605.47121043, 556.67616859])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([609.74709298, 635.44991269, 535.91731397, 555.47851634,\n",
      "       560.26732238, 592.77245291, 492.78699422, 528.47301518,\n",
      "       576.62673148, 449.02573821, 420.73057322, 383.2050939 ,\n",
      "       453.90534066, 539.24259306, 605.47121043, 556.67616859,\n",
      "       608.30828456, 489.31267053, 414.13221475, 481.92922581,\n",
      "       537.03024237, 531.27077547, 424.76439816, 480.27019493,\n",
      "       456.60722947, 534.68799708, 560.27001124, 495.50885821,\n",
      "       490.43221638, 469.54868894])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([509.0216617 , 580.5186044 , 509.56832961, 547.35738325,\n",
      "       523.72622733, 493.15355426, 520.45598519, 488.64675133,\n",
      "       495.78048545, 472.70249657, 618.45257776, 589.3399963 ,\n",
      "       522.26342527, 549.53276579, 550.7361558 , 541.6098775 ,\n",
      "       581.53451462, 607.22224773, 549.87462813, 522.37636622])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([528.47301518, 576.62673148, 449.02573821, 420.73057322,\n",
      "       383.2050939 , 453.90534066, 539.24259306, 605.47121043,\n",
      "       556.67616859, 608.30828456, 489.31267053, 414.13221475,\n",
      "       481.92922581, 537.03024237, 531.27077547, 424.76439816,\n",
      "       480.27019493, 456.60722947, 534.68799708, 560.27001124,\n",
      "       495.50885821, 490.43221638, 469.54868894, 509.0216617 ,\n",
      "       580.5186044 , 509.56832961, 547.35738325, 523.72622733,\n",
      "       493.15355426, 520.45598519])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([488.64675133, 495.78048545, 472.70249657, 618.45257776,\n",
      "       589.3399963 , 522.26342527, 549.53276579, 550.7361558 ,\n",
      "       541.6098775 , 581.53451462, 607.22224773, 549.87462813,\n",
      "       522.37636622, 533.76513254, 439.37788128, 437.09888259,\n",
      "       592.31584485, 669.82746742, 592.01663525, 597.07230042])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([592.31584485, 669.82746742, 592.01663525, 597.07230042,\n",
      "       597.51240164, 732.10605797, 694.37287539, 600.22696169,\n",
      "       529.54083307, 475.68450536, 549.19229931, 538.22986094,\n",
      "       490.72208051, 514.5088088 , 508.05592927, 639.58053533,\n",
      "       658.89585885, 607.11396555, 662.96302643, 623.20154476,\n",
      "       551.67016265, 648.06221241, 647.35774585, 555.63289089,\n",
      "       563.85025964, 543.66405584, 506.29295049, 609.75057181,\n",
      "       707.14155238, 571.44576343])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([604.4266225 , 577.85684727, 565.12944302, 561.70663192,\n",
      "       546.92000432, 587.34387768, 563.28894392, 603.15946382,\n",
      "       609.79642481, 597.68857407, 561.29017164, 563.99484285,\n",
      "       636.27975237, 653.7286504 , 580.21410777, 603.71020602,\n",
      "       655.75754555, 552.61491167, 614.86465122, 596.29610199])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([623.20154476, 551.67016265, 648.06221241, 647.35774585,\n",
      "       555.63289089, 563.85025964, 543.66405584, 506.29295049,\n",
      "       609.75057181, 707.14155238, 571.44576343, 604.4266225 ,\n",
      "       577.85684727, 565.12944302, 561.70663192, 546.92000432,\n",
      "       587.34387768, 563.28894392, 603.15946382, 609.79642481,\n",
      "       597.68857407, 561.29017164, 563.99484285, 636.27975237,\n",
      "       653.7286504 , 580.21410777, 603.71020602, 655.75754555,\n",
      "       552.61491167, 614.86465122])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([596.29610199, 638.34407776, 596.98361388, 521.09639918,\n",
      "       505.46555349, 589.70267609, 636.73604882, 589.67698049,\n",
      "       644.77612963, 561.24317062, 596.79057361, 568.82052745,\n",
      "       576.8558281 , 622.05026979, 594.06184224, 602.02666574,\n",
      "       680.36988082, 633.39966005, 675.71884524, 605.40908925])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([597.51240164, 732.10605797, 694.37287539, 600.22696169,\n",
      "       529.54083307, 475.68450536, 549.19229931, 538.22986094,\n",
      "       490.72208051, 514.5088088 , 508.05592927, 639.58053533,\n",
      "       658.89585885, 607.11396555, 662.96302643, 623.20154476,\n",
      "       551.67016265, 648.06221241, 647.35774585, 555.63289089,\n",
      "       563.85025964, 543.66405584, 506.29295049, 609.75057181,\n",
      "       707.14155238, 571.44576343, 604.4266225 , 577.85684727,\n",
      "       565.12944302, 561.70663192])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([546.92000432, 587.34387768, 563.28894392, 603.15946382,\n",
      "       609.79642481, 597.68857407, 561.29017164, 563.99484285,\n",
      "       636.27975237, 653.7286504 , 580.21410777, 603.71020602,\n",
      "       655.75754555, 552.61491167, 614.86465122, 596.29610199,\n",
      "       638.34407776, 596.98361388, 521.09639918, 505.46555349])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([581.53451462, 607.22224773, 549.87462813, 522.37636622,\n",
      "       533.76513254, 439.37788128, 437.09888259, 592.31584485,\n",
      "       669.82746742, 592.01663525, 597.07230042, 597.51240164,\n",
      "       732.10605797, 694.37287539, 600.22696169, 529.54083307,\n",
      "       475.68450536, 549.19229931, 538.22986094, 490.72208051,\n",
      "       514.5088088 , 508.05592927, 639.58053533, 658.89585885,\n",
      "       607.11396555, 662.96302643, 623.20154476, 551.67016265,\n",
      "       648.06221241, 647.35774585])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([555.63289089, 563.85025964, 543.66405584, 506.29295049,\n",
      "       609.75057181, 707.14155238, 571.44576343, 604.4266225 ,\n",
      "       577.85684727, 565.12944302, 561.70663192, 546.92000432,\n",
      "       587.34387768, 563.28894392, 603.15946382, 609.79642481,\n",
      "       597.68857407, 561.29017164, 563.99484285, 636.27975237])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([555.63289089, 563.85025964, 543.66405584, 506.29295049,\n",
      "       609.75057181, 707.14155238, 571.44576343, 604.4266225 ,\n",
      "       577.85684727, 565.12944302, 561.70663192, 546.92000432,\n",
      "       587.34387768, 563.28894392, 603.15946382, 609.79642481,\n",
      "       597.68857407, 561.29017164, 563.99484285, 636.27975237,\n",
      "       653.7286504 , 580.21410777, 603.71020602, 655.75754555,\n",
      "       552.61491167, 614.86465122, 596.29610199, 638.34407776,\n",
      "       596.98361388, 521.09639918])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([505.46555349, 589.70267609, 636.73604882, 589.67698049,\n",
      "       644.77612963, 561.24317062, 596.79057361, 568.82052745,\n",
      "       576.8558281 , 622.05026979, 594.06184224, 602.02666574,\n",
      "       680.36988082, 633.39966005, 675.71884524, 605.40908925,\n",
      "       650.71947409, 727.41228097, 557.18612905, 562.01216424])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([559.84221298, 535.04153282, 564.85427982, 530.47387032,\n",
      "       609.74709298, 635.44991269, 535.91731397, 555.47851634,\n",
      "       560.26732238, 592.77245291, 492.78699422, 528.47301518,\n",
      "       576.62673148, 449.02573821, 420.73057322, 383.2050939 ,\n",
      "       453.90534066, 539.24259306, 605.47121043, 556.67616859,\n",
      "       608.30828456, 489.31267053, 414.13221475, 481.92922581,\n",
      "       537.03024237, 531.27077547, 424.76439816, 480.27019493,\n",
      "       456.60722947, 534.68799708])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([560.27001124, 495.50885821, 490.43221638, 469.54868894,\n",
      "       509.0216617 , 580.5186044 , 509.56832961, 547.35738325,\n",
      "       523.72622733, 493.15355426, 520.45598519, 488.64675133,\n",
      "       495.78048545, 472.70249657, 618.45257776, 589.3399963 ,\n",
      "       522.26342527, 549.53276579, 550.7361558 , 541.6098775 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([547.35738325, 523.72622733, 493.15355426, 520.45598519,\n",
      "       488.64675133, 495.78048545, 472.70249657, 618.45257776,\n",
      "       589.3399963 , 522.26342527, 549.53276579, 550.7361558 ,\n",
      "       541.6098775 , 581.53451462, 607.22224773, 549.87462813,\n",
      "       522.37636622, 533.76513254, 439.37788128, 437.09888259,\n",
      "       592.31584485, 669.82746742, 592.01663525, 597.07230042,\n",
      "       597.51240164, 732.10605797, 694.37287539, 600.22696169,\n",
      "       529.54083307, 475.68450536])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([549.19229931, 538.22986094, 490.72208051, 514.5088088 ,\n",
      "       508.05592927, 639.58053533, 658.89585885, 607.11396555,\n",
      "       662.96302643, 623.20154476, 551.67016265, 648.06221241,\n",
      "       647.35774585, 555.63289089, 563.85025964, 543.66405584,\n",
      "       506.29295049, 609.75057181, 707.14155238, 571.44576343])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([647.35774585, 555.63289089, 563.85025964, 543.66405584,\n",
      "       506.29295049, 609.75057181, 707.14155238, 571.44576343,\n",
      "       604.4266225 , 577.85684727, 565.12944302, 561.70663192,\n",
      "       546.92000432, 587.34387768, 563.28894392, 603.15946382,\n",
      "       609.79642481, 597.68857407, 561.29017164, 563.99484285,\n",
      "       636.27975237, 653.7286504 , 580.21410777, 603.71020602,\n",
      "       655.75754555, 552.61491167, 614.86465122, 596.29610199,\n",
      "       638.34407776, 596.98361388])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([521.09639918, 505.46555349, 589.70267609, 636.73604882,\n",
      "       589.67698049, 644.77612963, 561.24317062, 596.79057361,\n",
      "       568.82052745, 576.8558281 , 622.05026979, 594.06184224,\n",
      "       602.02666574, 680.36988082, 633.39966005, 675.71884524,\n",
      "       605.40908925, 650.71947409, 727.41228097, 557.18612905])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([468.19758845, 559.84221298, 535.04153282, 564.85427982,\n",
      "       530.47387032, 609.74709298, 635.44991269, 535.91731397,\n",
      "       555.47851634, 560.26732238, 592.77245291, 492.78699422,\n",
      "       528.47301518, 576.62673148, 449.02573821, 420.73057322,\n",
      "       383.2050939 , 453.90534066, 539.24259306, 605.47121043,\n",
      "       556.67616859, 608.30828456, 489.31267053, 414.13221475,\n",
      "       481.92922581, 537.03024237, 531.27077547, 424.76439816,\n",
      "       480.27019493, 456.60722947])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([534.68799708, 560.27001124, 495.50885821, 490.43221638,\n",
      "       469.54868894, 509.0216617 , 580.5186044 , 509.56832961,\n",
      "       547.35738325, 523.72622733, 493.15355426, 520.45598519,\n",
      "       488.64675133, 495.78048545, 472.70249657, 618.45257776,\n",
      "       589.3399963 , 522.26342527, 549.53276579, 550.7361558 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([383.2050939 , 453.90534066, 539.24259306, 605.47121043,\n",
      "       556.67616859, 608.30828456, 489.31267053, 414.13221475,\n",
      "       481.92922581, 537.03024237, 531.27077547, 424.76439816,\n",
      "       480.27019493, 456.60722947, 534.68799708, 560.27001124,\n",
      "       495.50885821, 490.43221638, 469.54868894, 509.0216617 ,\n",
      "       580.5186044 , 509.56832961, 547.35738325, 523.72622733,\n",
      "       493.15355426, 520.45598519, 488.64675133, 495.78048545,\n",
      "       472.70249657, 618.45257776])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([589.3399963 , 522.26342527, 549.53276579, 550.7361558 ,\n",
      "       541.6098775 , 581.53451462, 607.22224773, 549.87462813,\n",
      "       522.37636622, 533.76513254, 439.37788128, 437.09888259,\n",
      "       592.31584485, 669.82746742, 592.01663525, 597.07230042,\n",
      "       597.51240164, 732.10605797, 694.37287539, 600.22696169])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([576.8558281 , 622.05026979, 594.06184224, 602.02666574,\n",
      "       680.36988082, 633.39966005, 675.71884524, 605.40908925,\n",
      "       650.71947409, 727.41228097, 557.18612905, 562.01216424,\n",
      "       655.97105619, 650.10028912, 668.16176824, 631.85717443,\n",
      "       648.09903429, 649.27642821, 712.67236658, 696.27975741,\n",
      "       686.1849128 , 649.98371354, 631.18617055, 631.07502274,\n",
      "       673.75730796, 651.98129603, 673.73626744, 777.07324188,\n",
      "       758.2234969 , 680.18442364])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([729.54997166, 678.89672873, 568.29571713, 581.78740582,\n",
      "       555.83498193, 615.91695795, 665.67546333, 761.52671654,\n",
      "       733.19317991, 684.17926491, 723.09299308, 592.89210242,\n",
      "       653.62634842, 724.79417847, 630.92834603, 717.67629451,\n",
      "       728.46205325, 681.62189532, 718.02118063, 820.5333744 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([487.44691126, 515.62869948, 536.31984124, 451.87212301,\n",
      "       394.45919165, 521.30830427, 532.80160653, 468.19758845,\n",
      "       559.84221298, 535.04153282, 564.85427982, 530.47387032,\n",
      "       609.74709298, 635.44991269, 535.91731397, 555.47851634,\n",
      "       560.26732238, 592.77245291, 492.78699422, 528.47301518,\n",
      "       576.62673148, 449.02573821, 420.73057322, 383.2050939 ,\n",
      "       453.90534066, 539.24259306, 605.47121043, 556.67616859,\n",
      "       608.30828456, 489.31267053])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([414.13221475, 481.92922581, 537.03024237, 531.27077547,\n",
      "       424.76439816, 480.27019493, 456.60722947, 534.68799708,\n",
      "       560.27001124, 495.50885821, 490.43221638, 469.54868894,\n",
      "       509.0216617 , 580.5186044 , 509.56832961, 547.35738325,\n",
      "       523.72622733, 493.15355426, 520.45598519, 488.64675133])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([437.61444311, 463.16543088, 410.89882285, 419.07923581,\n",
      "       381.30008063, 394.0134984 , 383.27079237, 387.32550047,\n",
      "       417.0279523 , 358.91573723, 476.45969901, 381.52225147,\n",
      "       314.31964945, 421.39247611, 452.38691493, 444.22925323,\n",
      "       442.59260105, 411.5736148 , 354.91586408, 383.56673024,\n",
      "       364.72357312, 434.29575526, 413.08282734, 361.27286765,\n",
      "       370.69375928, 416.44682588, 385.58899509, 355.57554051,\n",
      "       401.83538562, 424.40409635])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([391.1802466 , 378.83606312, 413.5852798 , 345.70142055,\n",
      "       315.11381903, 346.66881669, 386.61950519, 426.58713648,\n",
      "       499.00199322, 496.48184556, 438.03423281, 423.59440741,\n",
      "       375.37548583, 404.09737706, 409.62535098, 437.76060348,\n",
      "       394.47503303, 439.97630195, 517.19346552, 461.76251444])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([488.64675133, 495.78048545, 472.70249657, 618.45257776,\n",
      "       589.3399963 , 522.26342527, 549.53276579, 550.7361558 ,\n",
      "       541.6098775 , 581.53451462, 607.22224773, 549.87462813,\n",
      "       522.37636622, 533.76513254, 439.37788128, 437.09888259,\n",
      "       592.31584485, 669.82746742, 592.01663525, 597.07230042,\n",
      "       597.51240164, 732.10605797, 694.37287539, 600.22696169,\n",
      "       529.54083307, 475.68450536, 549.19229931, 538.22986094,\n",
      "       490.72208051, 514.5088088 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([508.05592927, 639.58053533, 658.89585885, 607.11396555,\n",
      "       662.96302643, 623.20154476, 551.67016265, 648.06221241,\n",
      "       647.35774585, 555.63289089, 563.85025964, 543.66405584,\n",
      "       506.29295049, 609.75057181, 707.14155238, 571.44576343,\n",
      "       604.4266225 , 577.85684727, 565.12944302, 561.70663192])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([479.62716775, 433.19914647, 443.23925318, 444.05635782,\n",
      "       446.12106416, 405.39701126, 427.84486792, 466.91572959,\n",
      "       527.49026645, 526.50191535, 580.10855533, 460.73538646,\n",
      "       484.74676984, 478.32468609, 567.74701886, 461.14155259,\n",
      "       402.6968913 , 407.19234085, 335.50445663, 383.89448025,\n",
      "       402.829297  , 448.14808423, 478.51272274, 563.09347579,\n",
      "       552.31094708, 462.87334599, 413.79781564, 472.07663971,\n",
      "       410.40991406, 533.57006609])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([564.84733814, 478.79369029, 382.97566996, 505.07405096,\n",
      "       493.43569082, 538.94192392, 425.49096814, 419.33548104,\n",
      "       463.8273169 , 481.11941758, 460.64102572, 505.29460783,\n",
      "       443.84386519, 458.00712312, 487.44691126, 515.62869948,\n",
      "       536.31984124, 451.87212301, 394.45919165, 521.30830427])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([622.05026979, 594.06184224, 602.02666574, 680.36988082,\n",
      "       633.39966005, 675.71884524, 605.40908925, 650.71947409,\n",
      "       727.41228097, 557.18612905, 562.01216424, 655.97105619,\n",
      "       650.10028912, 668.16176824, 631.85717443, 648.09903429,\n",
      "       649.27642821, 712.67236658, 696.27975741, 686.1849128 ,\n",
      "       649.98371354, 631.18617055, 631.07502274, 673.75730796,\n",
      "       651.98129603, 673.73626744, 777.07324188, 758.2234969 ,\n",
      "       680.18442364, 729.54997166])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([678.89672873, 568.29571713, 581.78740582, 555.83498193,\n",
      "       615.91695795, 665.67546333, 761.52671654, 733.19317991,\n",
      "       684.17926491, 723.09299308, 592.89210242, 653.62634842,\n",
      "       724.79417847, 630.92834603, 717.67629451, 728.46205325,\n",
      "       681.62189532, 718.02118063, 820.5333744 , 755.39247769])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([480.27019493, 456.60722947, 534.68799708, 560.27001124,\n",
      "       495.50885821, 490.43221638, 469.54868894, 509.0216617 ,\n",
      "       580.5186044 , 509.56832961, 547.35738325, 523.72622733,\n",
      "       493.15355426, 520.45598519, 488.64675133, 495.78048545,\n",
      "       472.70249657, 618.45257776, 589.3399963 , 522.26342527,\n",
      "       549.53276579, 550.7361558 , 541.6098775 , 581.53451462,\n",
      "       607.22224773, 549.87462813, 522.37636622, 533.76513254,\n",
      "       439.37788128, 437.09888259])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([592.31584485, 669.82746742, 592.01663525, 597.07230042,\n",
      "       597.51240164, 732.10605797, 694.37287539, 600.22696169,\n",
      "       529.54083307, 475.68450536, 549.19229931, 538.22986094,\n",
      "       490.72208051, 514.5088088 , 508.05592927, 639.58053533,\n",
      "       658.89585885, 607.11396555, 662.96302643, 623.20154476])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([425.49096814, 419.33548104, 463.8273169 , 481.11941758,\n",
      "       460.64102572, 505.29460783, 443.84386519, 458.00712312,\n",
      "       487.44691126, 515.62869948, 536.31984124, 451.87212301,\n",
      "       394.45919165, 521.30830427, 532.80160653, 468.19758845,\n",
      "       559.84221298, 535.04153282, 564.85427982, 530.47387032,\n",
      "       609.74709298, 635.44991269, 535.91731397, 555.47851634,\n",
      "       560.26732238, 592.77245291, 492.78699422, 528.47301518,\n",
      "       576.62673148, 449.02573821])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([420.73057322, 383.2050939 , 453.90534066, 539.24259306,\n",
      "       605.47121043, 556.67616859, 608.30828456, 489.31267053,\n",
      "       414.13221475, 481.92922581, 537.03024237, 531.27077547,\n",
      "       424.76439816, 480.27019493, 456.60722947, 534.68799708,\n",
      "       560.27001124, 495.50885821, 490.43221638, 469.54868894])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([449.02573821, 420.73057322, 383.2050939 , 453.90534066,\n",
      "       539.24259306, 605.47121043, 556.67616859, 608.30828456,\n",
      "       489.31267053, 414.13221475, 481.92922581, 537.03024237,\n",
      "       531.27077547, 424.76439816, 480.27019493, 456.60722947,\n",
      "       534.68799708, 560.27001124, 495.50885821, 490.43221638,\n",
      "       469.54868894, 509.0216617 , 580.5186044 , 509.56832961,\n",
      "       547.35738325, 523.72622733, 493.15355426, 520.45598519,\n",
      "       488.64675133, 495.78048545])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([472.70249657, 618.45257776, 589.3399963 , 522.26342527,\n",
      "       549.53276579, 550.7361558 , 541.6098775 , 581.53451462,\n",
      "       607.22224773, 549.87462813, 522.37636622, 533.76513254,\n",
      "       439.37788128, 437.09888259, 592.31584485, 669.82746742,\n",
      "       592.01663525, 597.07230042, 597.51240164, 732.10605797])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([636.73604882, 589.67698049, 644.77612963, 561.24317062,\n",
      "       596.79057361, 568.82052745, 576.8558281 , 622.05026979,\n",
      "       594.06184224, 602.02666574, 680.36988082, 633.39966005,\n",
      "       675.71884524, 605.40908925, 650.71947409, 727.41228097,\n",
      "       557.18612905, 562.01216424, 655.97105619, 650.10028912,\n",
      "       668.16176824, 631.85717443, 648.09903429, 649.27642821,\n",
      "       712.67236658, 696.27975741, 686.1849128 , 649.98371354,\n",
      "       631.18617055, 631.07502274])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([673.75730796, 651.98129603, 673.73626744, 777.07324188,\n",
      "       758.2234969 , 680.18442364, 729.54997166, 678.89672873,\n",
      "       568.29571713, 581.78740582, 555.83498193, 615.91695795,\n",
      "       665.67546333, 761.52671654, 733.19317991, 684.17926491,\n",
      "       723.09299308, 592.89210242, 653.62634842, 724.79417847])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([608.30828456, 489.31267053, 414.13221475, 481.92922581,\n",
      "       537.03024237, 531.27077547, 424.76439816, 480.27019493,\n",
      "       456.60722947, 534.68799708, 560.27001124, 495.50885821,\n",
      "       490.43221638, 469.54868894, 509.0216617 , 580.5186044 ,\n",
      "       509.56832961, 547.35738325, 523.72622733, 493.15355426,\n",
      "       520.45598519, 488.64675133, 495.78048545, 472.70249657,\n",
      "       618.45257776, 589.3399963 , 522.26342527, 549.53276579,\n",
      "       550.7361558 , 541.6098775 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([581.53451462, 607.22224773, 549.87462813, 522.37636622,\n",
      "       533.76513254, 439.37788128, 437.09888259, 592.31584485,\n",
      "       669.82746742, 592.01663525, 597.07230042, 597.51240164,\n",
      "       732.10605797, 694.37287539, 600.22696169, 529.54083307,\n",
      "       475.68450536, 549.19229931, 538.22986094, 490.72208051])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([576.62673148, 449.02573821, 420.73057322, 383.2050939 ,\n",
      "       453.90534066, 539.24259306, 605.47121043, 556.67616859,\n",
      "       608.30828456, 489.31267053, 414.13221475, 481.92922581,\n",
      "       537.03024237, 531.27077547, 424.76439816, 480.27019493,\n",
      "       456.60722947, 534.68799708, 560.27001124, 495.50885821,\n",
      "       490.43221638, 469.54868894, 509.0216617 , 580.5186044 ,\n",
      "       509.56832961, 547.35738325, 523.72622733, 493.15355426,\n",
      "       520.45598519, 488.64675133])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([495.78048545, 472.70249657, 618.45257776, 589.3399963 ,\n",
      "       522.26342527, 549.53276579, 550.7361558 , 541.6098775 ,\n",
      "       581.53451462, 607.22224773, 549.87462813, 522.37636622,\n",
      "       533.76513254, 439.37788128, 437.09888259, 592.31584485,\n",
      "       669.82746742, 592.01663525, 597.07230042, 597.51240164])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([597.68857407, 561.29017164, 563.99484285, 636.27975237,\n",
      "       653.7286504 , 580.21410777, 603.71020602, 655.75754555,\n",
      "       552.61491167, 614.86465122, 596.29610199, 638.34407776,\n",
      "       596.98361388, 521.09639918, 505.46555349, 589.70267609,\n",
      "       636.73604882, 589.67698049, 644.77612963, 561.24317062,\n",
      "       596.79057361, 568.82052745, 576.8558281 , 622.05026979,\n",
      "       594.06184224, 602.02666574, 680.36988082, 633.39966005,\n",
      "       675.71884524, 605.40908925])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([650.71947409, 727.41228097, 557.18612905, 562.01216424,\n",
      "       655.97105619, 650.10028912, 668.16176824, 631.85717443,\n",
      "       648.09903429, 649.27642821, 712.67236658, 696.27975741,\n",
      "       686.1849128 , 649.98371354, 631.18617055, 631.07502274,\n",
      "       673.75730796, 651.98129603, 673.73626744, 777.07324188])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([496.48184556, 438.03423281, 423.59440741, 375.37548583,\n",
      "       404.09737706, 409.62535098, 437.76060348, 394.47503303,\n",
      "       439.97630195, 517.19346552, 461.76251444, 457.355711  ,\n",
      "       479.62716775, 433.19914647, 443.23925318, 444.05635782,\n",
      "       446.12106416, 405.39701126, 427.84486792, 466.91572959,\n",
      "       527.49026645, 526.50191535, 580.10855533, 460.73538646,\n",
      "       484.74676984, 478.32468609, 567.74701886, 461.14155259,\n",
      "       402.6968913 , 407.19234085])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([335.50445663, 383.89448025, 402.829297  , 448.14808423,\n",
      "       478.51272274, 563.09347579, 552.31094708, 462.87334599,\n",
      "       413.79781564, 472.07663971, 410.40991406, 533.57006609,\n",
      "       564.84733814, 478.79369029, 382.97566996, 505.07405096,\n",
      "       493.43569082, 538.94192392, 425.49096814, 419.33548104])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([560.27001124, 495.50885821, 490.43221638, 469.54868894,\n",
      "       509.0216617 , 580.5186044 , 509.56832961, 547.35738325,\n",
      "       523.72622733, 493.15355426, 520.45598519, 488.64675133,\n",
      "       495.78048545, 472.70249657, 618.45257776, 589.3399963 ,\n",
      "       522.26342527, 549.53276579, 550.7361558 , 541.6098775 ,\n",
      "       581.53451462, 607.22224773, 549.87462813, 522.37636622,\n",
      "       533.76513254, 439.37788128, 437.09888259, 592.31584485,\n",
      "       669.82746742, 592.01663525])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([597.07230042, 597.51240164, 732.10605797, 694.37287539,\n",
      "       600.22696169, 529.54083307, 475.68450536, 549.19229931,\n",
      "       538.22986094, 490.72208051, 514.5088088 , 508.05592927,\n",
      "       639.58053533, 658.89585885, 607.11396555, 662.96302643,\n",
      "       623.20154476, 551.67016265, 648.06221241, 647.35774585])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([538.22986094, 490.72208051, 514.5088088 , 508.05592927,\n",
      "       639.58053533, 658.89585885, 607.11396555, 662.96302643,\n",
      "       623.20154476, 551.67016265, 648.06221241, 647.35774585,\n",
      "       555.63289089, 563.85025964, 543.66405584, 506.29295049,\n",
      "       609.75057181, 707.14155238, 571.44576343, 604.4266225 ,\n",
      "       577.85684727, 565.12944302, 561.70663192, 546.92000432,\n",
      "       587.34387768, 563.28894392, 603.15946382, 609.79642481,\n",
      "       597.68857407, 561.29017164])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([563.99484285, 636.27975237, 653.7286504 , 580.21410777,\n",
      "       603.71020602, 655.75754555, 552.61491167, 614.86465122,\n",
      "       596.29610199, 638.34407776, 596.98361388, 521.09639918,\n",
      "       505.46555349, 589.70267609, 636.73604882, 589.67698049,\n",
      "       644.77612963, 561.24317062, 596.79057361, 568.82052745])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([529.54083307, 475.68450536, 549.19229931, 538.22986094,\n",
      "       490.72208051, 514.5088088 , 508.05592927, 639.58053533,\n",
      "       658.89585885, 607.11396555, 662.96302643, 623.20154476,\n",
      "       551.67016265, 648.06221241, 647.35774585, 555.63289089,\n",
      "       563.85025964, 543.66405584, 506.29295049, 609.75057181,\n",
      "       707.14155238, 571.44576343, 604.4266225 , 577.85684727,\n",
      "       565.12944302, 561.70663192, 546.92000432, 587.34387768,\n",
      "       563.28894392, 603.15946382])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([609.79642481, 597.68857407, 561.29017164, 563.99484285,\n",
      "       636.27975237, 653.7286504 , 580.21410777, 603.71020602,\n",
      "       655.75754555, 552.61491167, 614.86465122, 596.29610199,\n",
      "       638.34407776, 596.98361388, 521.09639918, 505.46555349,\n",
      "       589.70267609, 636.73604882, 589.67698049, 644.77612963])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([602.02666574, 680.36988082, 633.39966005, 675.71884524,\n",
      "       605.40908925, 650.71947409, 727.41228097, 557.18612905,\n",
      "       562.01216424, 655.97105619, 650.10028912, 668.16176824,\n",
      "       631.85717443, 648.09903429, 649.27642821, 712.67236658,\n",
      "       696.27975741, 686.1849128 , 649.98371354, 631.18617055,\n",
      "       631.07502274, 673.75730796, 651.98129603, 673.73626744,\n",
      "       777.07324188, 758.2234969 , 680.18442364, 729.54997166,\n",
      "       678.89672873, 568.29571713])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([581.78740582, 555.83498193, 615.91695795, 665.67546333,\n",
      "       761.52671654, 733.19317991, 684.17926491, 723.09299308,\n",
      "       592.89210242, 653.62634842, 724.79417847, 630.92834603,\n",
      "       717.67629451, 728.46205325, 681.62189532, 718.02118063,\n",
      "       820.5333744 , 755.39247769, 721.11223575, 683.36485669])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([324.83322363, 327.12874953, 320.05395004, 310.90629308,\n",
      "       380.879735  , 387.76152403, 330.557742  , 388.11743607,\n",
      "       390.37958736, 408.2225002 , 410.23820866, 336.26542706,\n",
      "       319.41677054, 386.73270881, 408.68045252, 379.54368873,\n",
      "       374.68737687, 434.71295722, 367.58405072, 389.15715379,\n",
      "       372.35580464, 359.08299146, 423.66672844, 438.84670817,\n",
      "       437.61444311, 463.16543088, 410.89882285, 419.07923581,\n",
      "       381.30008063, 394.0134984 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([383.27079237, 387.32550047, 417.0279523 , 358.91573723,\n",
      "       476.45969901, 381.52225147, 314.31964945, 421.39247611,\n",
      "       452.38691493, 444.22925323, 442.59260105, 411.5736148 ,\n",
      "       354.91586408, 383.56673024, 364.72357312, 434.29575526,\n",
      "       413.08282734, 361.27286765, 370.69375928, 416.44682588])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([675.71884524, 605.40908925, 650.71947409, 727.41228097,\n",
      "       557.18612905, 562.01216424, 655.97105619, 650.10028912,\n",
      "       668.16176824, 631.85717443, 648.09903429, 649.27642821,\n",
      "       712.67236658, 696.27975741, 686.1849128 , 649.98371354,\n",
      "       631.18617055, 631.07502274, 673.75730796, 651.98129603,\n",
      "       673.73626744, 777.07324188, 758.2234969 , 680.18442364,\n",
      "       729.54997166, 678.89672873, 568.29571713, 581.78740582,\n",
      "       555.83498193, 615.91695795])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([665.67546333, 761.52671654, 733.19317991, 684.17926491,\n",
      "       723.09299308, 592.89210242, 653.62634842, 724.79417847,\n",
      "       630.92834603, 717.67629451, 728.46205325, 681.62189532,\n",
      "       718.02118063, 820.5333744 , 755.39247769, 721.11223575,\n",
      "       683.36485669, 648.91690775, 722.89139063, 674.15797097])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([509.0216617 , 580.5186044 , 509.56832961, 547.35738325,\n",
      "       523.72622733, 493.15355426, 520.45598519, 488.64675133,\n",
      "       495.78048545, 472.70249657, 618.45257776, 589.3399963 ,\n",
      "       522.26342527, 549.53276579, 550.7361558 , 541.6098775 ,\n",
      "       581.53451462, 607.22224773, 549.87462813, 522.37636622,\n",
      "       533.76513254, 439.37788128, 437.09888259, 592.31584485,\n",
      "       669.82746742, 592.01663525, 597.07230042, 597.51240164,\n",
      "       732.10605797, 694.37287539])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([600.22696169, 529.54083307, 475.68450536, 549.19229931,\n",
      "       538.22986094, 490.72208051, 514.5088088 , 508.05592927,\n",
      "       639.58053533, 658.89585885, 607.11396555, 662.96302643,\n",
      "       623.20154476, 551.67016265, 648.06221241, 647.35774585,\n",
      "       555.63289089, 563.85025964, 543.66405584, 506.29295049])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 25.89407055,  94.10193173,  29.1604104 ,  62.45920462,\n",
      "       -19.259528  , -30.02332054,  54.25897839, 115.20075505,\n",
      "       104.4155928 ,  81.90616437,  66.43128026,   3.62500397,\n",
      "        16.29659574,  42.40312683, 125.84206218, 124.86800076,\n",
      "        10.80211719,  69.67090736,  73.00015151,  49.1362523 ,\n",
      "       106.98454433, 155.1638923 , 163.89514009,  76.87066266,\n",
      "        66.65283723, 105.2879106 , 152.28966719,  96.70344248,\n",
      "        83.77352599,  41.23114256])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 17.99426911, 114.11325318, 184.04522052, 131.63722049,\n",
      "       158.90144505, 147.21045867,  86.83575307, 115.00959466,\n",
      "       192.5315456 , 142.60208615, 193.93866008,  16.93172331,\n",
      "       103.93682066, 129.66886997, 103.55737712, 115.40403437,\n",
      "        19.93344086,  65.42368531, 126.55694824, 199.66504437])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([506.29295049, 609.75057181, 707.14155238, 571.44576343,\n",
      "       604.4266225 , 577.85684727, 565.12944302, 561.70663192,\n",
      "       546.92000432, 587.34387768, 563.28894392, 603.15946382,\n",
      "       609.79642481, 597.68857407, 561.29017164, 563.99484285,\n",
      "       636.27975237, 653.7286504 , 580.21410777, 603.71020602,\n",
      "       655.75754555, 552.61491167, 614.86465122, 596.29610199,\n",
      "       638.34407776, 596.98361388, 521.09639918, 505.46555349,\n",
      "       589.70267609, 636.73604882])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([589.67698049, 644.77612963, 561.24317062, 596.79057361,\n",
      "       568.82052745, 576.8558281 , 622.05026979, 594.06184224,\n",
      "       602.02666574, 680.36988082, 633.39966005, 675.71884524,\n",
      "       605.40908925, 650.71947409, 727.41228097, 557.18612905,\n",
      "       562.01216424, 655.97105619, 650.10028912, 668.16176824])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([561.29017164, 563.99484285, 636.27975237, 653.7286504 ,\n",
      "       580.21410777, 603.71020602, 655.75754555, 552.61491167,\n",
      "       614.86465122, 596.29610199, 638.34407776, 596.98361388,\n",
      "       521.09639918, 505.46555349, 589.70267609, 636.73604882,\n",
      "       589.67698049, 644.77612963, 561.24317062, 596.79057361,\n",
      "       568.82052745, 576.8558281 , 622.05026979, 594.06184224,\n",
      "       602.02666574, 680.36988082, 633.39966005, 675.71884524,\n",
      "       605.40908925, 650.71947409])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([727.41228097, 557.18612905, 562.01216424, 655.97105619,\n",
      "       650.10028912, 668.16176824, 631.85717443, 648.09903429,\n",
      "       649.27642821, 712.67236658, 696.27975741, 686.1849128 ,\n",
      "       649.98371354, 631.18617055, 631.07502274, 673.75730796,\n",
      "       651.98129603, 673.73626744, 777.07324188, 758.2234969 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([603.71020602, 655.75754555, 552.61491167, 614.86465122,\n",
      "       596.29610199, 638.34407776, 596.98361388, 521.09639918,\n",
      "       505.46555349, 589.70267609, 636.73604882, 589.67698049,\n",
      "       644.77612963, 561.24317062, 596.79057361, 568.82052745,\n",
      "       576.8558281 , 622.05026979, 594.06184224, 602.02666574,\n",
      "       680.36988082, 633.39966005, 675.71884524, 605.40908925,\n",
      "       650.71947409, 727.41228097, 557.18612905, 562.01216424,\n",
      "       655.97105619, 650.10028912])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([668.16176824, 631.85717443, 648.09903429, 649.27642821,\n",
      "       712.67236658, 696.27975741, 686.1849128 , 649.98371354,\n",
      "       631.18617055, 631.07502274, 673.75730796, 651.98129603,\n",
      "       673.73626744, 777.07324188, 758.2234969 , 680.18442364,\n",
      "       729.54997166, 678.89672873, 568.29571713, 581.78740582])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([337.21491073, 319.24262795, 265.1709739 , 298.00957865,\n",
      "       242.49635296, 240.13836305, 236.14417874, 260.83591413,\n",
      "       384.60274617, 226.89460208, 273.27294491, 203.17066281,\n",
      "       217.19312001, 314.43807321, 300.01414871, 227.87519988,\n",
      "       220.79051407, 295.02199764, 255.88545794, 278.47209267,\n",
      "       287.98710364, 252.91959566, 378.32322455, 361.8077385 ,\n",
      "       206.60592959, 258.34179417, 255.41322766, 318.51060943,\n",
      "       268.61541097, 272.76425551])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([315.14139906, 348.59105903, 256.83218886, 258.63997842,\n",
      "       265.18095026, 255.64151055, 373.23511922, 357.72595578,\n",
      "       254.37809822, 325.97651154, 427.7355496 , 405.77162346,\n",
      "       261.78093214, 257.79632772, 359.61398809, 300.97891002,\n",
      "       323.07814374, 360.51164299, 286.45978574, 296.09527034])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([552.31094708, 462.87334599, 413.79781564, 472.07663971,\n",
      "       410.40991406, 533.57006609, 564.84733814, 478.79369029,\n",
      "       382.97566996, 505.07405096, 493.43569082, 538.94192392,\n",
      "       425.49096814, 419.33548104, 463.8273169 , 481.11941758,\n",
      "       460.64102572, 505.29460783, 443.84386519, 458.00712312,\n",
      "       487.44691126, 515.62869948, 536.31984124, 451.87212301,\n",
      "       394.45919165, 521.30830427, 532.80160653, 468.19758845,\n",
      "       559.84221298, 535.04153282])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([564.85427982, 530.47387032, 609.74709298, 635.44991269,\n",
      "       535.91731397, 555.47851634, 560.26732238, 592.77245291,\n",
      "       492.78699422, 528.47301518, 576.62673148, 449.02573821,\n",
      "       420.73057322, 383.2050939 , 453.90534066, 539.24259306,\n",
      "       605.47121043, 556.67616859, 608.30828456, 489.31267053])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([631.85717443, 648.09903429, 649.27642821, 712.67236658,\n",
      "       696.27975741, 686.1849128 , 649.98371354, 631.18617055,\n",
      "       631.07502274, 673.75730796, 651.98129603, 673.73626744,\n",
      "       777.07324188, 758.2234969 , 680.18442364, 729.54997166,\n",
      "       678.89672873, 568.29571713, 581.78740582, 555.83498193,\n",
      "       615.91695795, 665.67546333, 761.52671654, 733.19317991,\n",
      "       684.17926491, 723.09299308, 592.89210242, 653.62634842,\n",
      "       724.79417847, 630.92834603])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([717.67629451, 728.46205325, 681.62189532, 718.02118063,\n",
      "       820.5333744 , 755.39247769, 721.11223575, 683.36485669,\n",
      "       648.91690775, 722.89139063, 674.15797097, 690.34147284,\n",
      "       679.89555162, 718.86986728, 732.00333529, 767.83519502,\n",
      "       707.24881753, 689.61478723, 656.32769369, 668.64987017])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([537.03024237, 531.27077547, 424.76439816, 480.27019493,\n",
      "       456.60722947, 534.68799708, 560.27001124, 495.50885821,\n",
      "       490.43221638, 469.54868894, 509.0216617 , 580.5186044 ,\n",
      "       509.56832961, 547.35738325, 523.72622733, 493.15355426,\n",
      "       520.45598519, 488.64675133, 495.78048545, 472.70249657,\n",
      "       618.45257776, 589.3399963 , 522.26342527, 549.53276579,\n",
      "       550.7361558 , 541.6098775 , 581.53451462, 607.22224773,\n",
      "       549.87462813, 522.37636622])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([533.76513254, 439.37788128, 437.09888259, 592.31584485,\n",
      "       669.82746742, 592.01663525, 597.07230042, 597.51240164,\n",
      "       732.10605797, 694.37287539, 600.22696169, 529.54083307,\n",
      "       475.68450536, 549.19229931, 538.22986094, 490.72208051,\n",
      "       514.5088088 , 508.05592927, 639.58053533, 658.89585885])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([549.19229931, 538.22986094, 490.72208051, 514.5088088 ,\n",
      "       508.05592927, 639.58053533, 658.89585885, 607.11396555,\n",
      "       662.96302643, 623.20154476, 551.67016265, 648.06221241,\n",
      "       647.35774585, 555.63289089, 563.85025964, 543.66405584,\n",
      "       506.29295049, 609.75057181, 707.14155238, 571.44576343,\n",
      "       604.4266225 , 577.85684727, 565.12944302, 561.70663192,\n",
      "       546.92000432, 587.34387768, 563.28894392, 603.15946382,\n",
      "       609.79642481, 597.68857407])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([561.29017164, 563.99484285, 636.27975237, 653.7286504 ,\n",
      "       580.21410777, 603.71020602, 655.75754555, 552.61491167,\n",
      "       614.86465122, 596.29610199, 638.34407776, 596.98361388,\n",
      "       521.09639918, 505.46555349, 589.70267609, 636.73604882,\n",
      "       589.67698049, 644.77612963, 561.24317062, 596.79057361])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([648.09903429, 649.27642821, 712.67236658, 696.27975741,\n",
      "       686.1849128 , 649.98371354, 631.18617055, 631.07502274,\n",
      "       673.75730796, 651.98129603, 673.73626744, 777.07324188,\n",
      "       758.2234969 , 680.18442364, 729.54997166, 678.89672873,\n",
      "       568.29571713, 581.78740582, 555.83498193, 615.91695795,\n",
      "       665.67546333, 761.52671654, 733.19317991, 684.17926491,\n",
      "       723.09299308, 592.89210242, 653.62634842, 724.79417847,\n",
      "       630.92834603, 717.67629451])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([728.46205325, 681.62189532, 718.02118063, 820.5333744 ,\n",
      "       755.39247769, 721.11223575, 683.36485669, 648.91690775,\n",
      "       722.89139063, 674.15797097, 690.34147284, 679.89555162,\n",
      "       718.86986728, 732.00333529, 767.83519502, 707.24881753,\n",
      "       689.61478723, 656.32769369, 668.64987017, 719.83064084])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([549.87462813, 522.37636622, 533.76513254, 439.37788128,\n",
      "       437.09888259, 592.31584485, 669.82746742, 592.01663525,\n",
      "       597.07230042, 597.51240164, 732.10605797, 694.37287539,\n",
      "       600.22696169, 529.54083307, 475.68450536, 549.19229931,\n",
      "       538.22986094, 490.72208051, 514.5088088 , 508.05592927,\n",
      "       639.58053533, 658.89585885, 607.11396555, 662.96302643,\n",
      "       623.20154476, 551.67016265, 648.06221241, 647.35774585,\n",
      "       555.63289089, 563.85025964])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([543.66405584, 506.29295049, 609.75057181, 707.14155238,\n",
      "       571.44576343, 604.4266225 , 577.85684727, 565.12944302,\n",
      "       561.70663192, 546.92000432, 587.34387768, 563.28894392,\n",
      "       603.15946382, 609.79642481, 597.68857407, 561.29017164,\n",
      "       563.99484285, 636.27975237, 653.7286504 , 580.21410777])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([568.82052745, 576.8558281 , 622.05026979, 594.06184224,\n",
      "       602.02666574, 680.36988082, 633.39966005, 675.71884524,\n",
      "       605.40908925, 650.71947409, 727.41228097, 557.18612905,\n",
      "       562.01216424, 655.97105619, 650.10028912, 668.16176824,\n",
      "       631.85717443, 648.09903429, 649.27642821, 712.67236658,\n",
      "       696.27975741, 686.1849128 , 649.98371354, 631.18617055,\n",
      "       631.07502274, 673.75730796, 651.98129603, 673.73626744,\n",
      "       777.07324188, 758.2234969 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([680.18442364, 729.54997166, 678.89672873, 568.29571713,\n",
      "       581.78740582, 555.83498193, 615.91695795, 665.67546333,\n",
      "       761.52671654, 733.19317991, 684.17926491, 723.09299308,\n",
      "       592.89210242, 653.62634842, 724.79417847, 630.92834603,\n",
      "       717.67629451, 728.46205325, 681.62189532, 718.02118063])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([335.50445663, 383.89448025, 402.829297  , 448.14808423,\n",
      "       478.51272274, 563.09347579, 552.31094708, 462.87334599,\n",
      "       413.79781564, 472.07663971, 410.40991406, 533.57006609,\n",
      "       564.84733814, 478.79369029, 382.97566996, 505.07405096,\n",
      "       493.43569082, 538.94192392, 425.49096814, 419.33548104,\n",
      "       463.8273169 , 481.11941758, 460.64102572, 505.29460783,\n",
      "       443.84386519, 458.00712312, 487.44691126, 515.62869948,\n",
      "       536.31984124, 451.87212301])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([394.45919165, 521.30830427, 532.80160653, 468.19758845,\n",
      "       559.84221298, 535.04153282, 564.85427982, 530.47387032,\n",
      "       609.74709298, 635.44991269, 535.91731397, 555.47851634,\n",
      "       560.26732238, 592.77245291, 492.78699422, 528.47301518,\n",
      "       576.62673148, 449.02573821, 420.73057322, 383.2050939 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([546.92000432, 587.34387768, 563.28894392, 603.15946382,\n",
      "       609.79642481, 597.68857407, 561.29017164, 563.99484285,\n",
      "       636.27975237, 653.7286504 , 580.21410777, 603.71020602,\n",
      "       655.75754555, 552.61491167, 614.86465122, 596.29610199,\n",
      "       638.34407776, 596.98361388, 521.09639918, 505.46555349,\n",
      "       589.70267609, 636.73604882, 589.67698049, 644.77612963,\n",
      "       561.24317062, 596.79057361, 568.82052745, 576.8558281 ,\n",
      "       622.05026979, 594.06184224])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([602.02666574, 680.36988082, 633.39966005, 675.71884524,\n",
      "       605.40908925, 650.71947409, 727.41228097, 557.18612905,\n",
      "       562.01216424, 655.97105619, 650.10028912, 668.16176824,\n",
      "       631.85717443, 648.09903429, 649.27642821, 712.67236658,\n",
      "       696.27975741, 686.1849128 , 649.98371354, 631.18617055])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([727.41228097, 557.18612905, 562.01216424, 655.97105619,\n",
      "       650.10028912, 668.16176824, 631.85717443, 648.09903429,\n",
      "       649.27642821, 712.67236658, 696.27975741, 686.1849128 ,\n",
      "       649.98371354, 631.18617055, 631.07502274, 673.75730796,\n",
      "       651.98129603, 673.73626744, 777.07324188, 758.2234969 ,\n",
      "       680.18442364, 729.54997166, 678.89672873, 568.29571713,\n",
      "       581.78740582, 555.83498193, 615.91695795, 665.67546333,\n",
      "       761.52671654, 733.19317991])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([684.17926491, 723.09299308, 592.89210242, 653.62634842,\n",
      "       724.79417847, 630.92834603, 717.67629451, 728.46205325,\n",
      "       681.62189532, 718.02118063, 820.5333744 , 755.39247769,\n",
      "       721.11223575, 683.36485669, 648.91690775, 722.89139063,\n",
      "       674.15797097, 690.34147284, 679.89555162, 718.86986728])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([402.829297  , 448.14808423, 478.51272274, 563.09347579,\n",
      "       552.31094708, 462.87334599, 413.79781564, 472.07663971,\n",
      "       410.40991406, 533.57006609, 564.84733814, 478.79369029,\n",
      "       382.97566996, 505.07405096, 493.43569082, 538.94192392,\n",
      "       425.49096814, 419.33548104, 463.8273169 , 481.11941758,\n",
      "       460.64102572, 505.29460783, 443.84386519, 458.00712312,\n",
      "       487.44691126, 515.62869948, 536.31984124, 451.87212301,\n",
      "       394.45919165, 521.30830427])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([532.80160653, 468.19758845, 559.84221298, 535.04153282,\n",
      "       564.85427982, 530.47387032, 609.74709298, 635.44991269,\n",
      "       535.91731397, 555.47851634, 560.26732238, 592.77245291,\n",
      "       492.78699422, 528.47301518, 576.62673148, 449.02573821,\n",
      "       420.73057322, 383.2050939 , 453.90534066, 539.24259306])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([580.21410777, 603.71020602, 655.75754555, 552.61491167,\n",
      "       614.86465122, 596.29610199, 638.34407776, 596.98361388,\n",
      "       521.09639918, 505.46555349, 589.70267609, 636.73604882,\n",
      "       589.67698049, 644.77612963, 561.24317062, 596.79057361,\n",
      "       568.82052745, 576.8558281 , 622.05026979, 594.06184224,\n",
      "       602.02666574, 680.36988082, 633.39966005, 675.71884524,\n",
      "       605.40908925, 650.71947409, 727.41228097, 557.18612905,\n",
      "       562.01216424, 655.97105619])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([650.10028912, 668.16176824, 631.85717443, 648.09903429,\n",
      "       649.27642821, 712.67236658, 696.27975741, 686.1849128 ,\n",
      "       649.98371354, 631.18617055, 631.07502274, 673.75730796,\n",
      "       651.98129603, 673.73626744, 777.07324188, 758.2234969 ,\n",
      "       680.18442364, 729.54997166, 678.89672873, 568.29571713])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([678.89672873, 568.29571713, 581.78740582, 555.83498193,\n",
      "       615.91695795, 665.67546333, 761.52671654, 733.19317991,\n",
      "       684.17926491, 723.09299308, 592.89210242, 653.62634842,\n",
      "       724.79417847, 630.92834603, 717.67629451, 728.46205325,\n",
      "       681.62189532, 718.02118063, 820.5333744 , 755.39247769,\n",
      "       721.11223575, 683.36485669, 648.91690775, 722.89139063,\n",
      "       674.15797097, 690.34147284, 679.89555162, 718.86986728,\n",
      "       732.00333529, 767.83519502])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([707.24881753, 689.61478723, 656.32769369, 668.64987017,\n",
      "       719.83064084, 758.57681546, 687.1679321 , 743.60502328,\n",
      "       802.44119057, 771.56266707, 828.12820822, 726.32636326,\n",
      "       651.49920215, 608.68186599, 758.13233494, 784.85025629,\n",
      "       742.43778409, 744.95265199, 681.04995163, 830.47041928])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([596.29610199, 638.34407776, 596.98361388, 521.09639918,\n",
      "       505.46555349, 589.70267609, 636.73604882, 589.67698049,\n",
      "       644.77612963, 561.24317062, 596.79057361, 568.82052745,\n",
      "       576.8558281 , 622.05026979, 594.06184224, 602.02666574,\n",
      "       680.36988082, 633.39966005, 675.71884524, 605.40908925,\n",
      "       650.71947409, 727.41228097, 557.18612905, 562.01216424,\n",
      "       655.97105619, 650.10028912, 668.16176824, 631.85717443,\n",
      "       648.09903429, 649.27642821])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([712.67236658, 696.27975741, 686.1849128 , 649.98371354,\n",
      "       631.18617055, 631.07502274, 673.75730796, 651.98129603,\n",
      "       673.73626744, 777.07324188, 758.2234969 , 680.18442364,\n",
      "       729.54997166, 678.89672873, 568.29571713, 581.78740582,\n",
      "       555.83498193, 615.91695795, 665.67546333, 761.52671654])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([505.07405096, 493.43569082, 538.94192392, 425.49096814,\n",
      "       419.33548104, 463.8273169 , 481.11941758, 460.64102572,\n",
      "       505.29460783, 443.84386519, 458.00712312, 487.44691126,\n",
      "       515.62869948, 536.31984124, 451.87212301, 394.45919165,\n",
      "       521.30830427, 532.80160653, 468.19758845, 559.84221298,\n",
      "       535.04153282, 564.85427982, 530.47387032, 609.74709298,\n",
      "       635.44991269, 535.91731397, 555.47851634, 560.26732238,\n",
      "       592.77245291, 492.78699422])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([528.47301518, 576.62673148, 449.02573821, 420.73057322,\n",
      "       383.2050939 , 453.90534066, 539.24259306, 605.47121043,\n",
      "       556.67616859, 608.30828456, 489.31267053, 414.13221475,\n",
      "       481.92922581, 537.03024237, 531.27077547, 424.76439816,\n",
      "       480.27019493, 456.60722947, 534.68799708, 560.27001124])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([680.18442364, 729.54997166, 678.89672873, 568.29571713,\n",
      "       581.78740582, 555.83498193, 615.91695795, 665.67546333,\n",
      "       761.52671654, 733.19317991, 684.17926491, 723.09299308,\n",
      "       592.89210242, 653.62634842, 724.79417847, 630.92834603,\n",
      "       717.67629451, 728.46205325, 681.62189532, 718.02118063,\n",
      "       820.5333744 , 755.39247769, 721.11223575, 683.36485669,\n",
      "       648.91690775, 722.89139063, 674.15797097, 690.34147284,\n",
      "       679.89555162, 718.86986728])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([732.00333529, 767.83519502, 707.24881753, 689.61478723,\n",
      "       656.32769369, 668.64987017, 719.83064084, 758.57681546,\n",
      "       687.1679321 , 743.60502328, 802.44119057, 771.56266707,\n",
      "       828.12820822, 726.32636326, 651.49920215, 608.68186599,\n",
      "       758.13233494, 784.85025629, 742.43778409, 744.95265199])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([665.67546333, 761.52671654, 733.19317991, 684.17926491,\n",
      "       723.09299308, 592.89210242, 653.62634842, 724.79417847,\n",
      "       630.92834603, 717.67629451, 728.46205325, 681.62189532,\n",
      "       718.02118063, 820.5333744 , 755.39247769, 721.11223575,\n",
      "       683.36485669, 648.91690775, 722.89139063, 674.15797097,\n",
      "       690.34147284, 679.89555162, 718.86986728, 732.00333529,\n",
      "       767.83519502, 707.24881753, 689.61478723, 656.32769369,\n",
      "       668.64987017, 719.83064084])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([758.57681546, 687.1679321 , 743.60502328, 802.44119057,\n",
      "       771.56266707, 828.12820822, 726.32636326, 651.49920215,\n",
      "       608.68186599, 758.13233494, 784.85025629, 742.43778409,\n",
      "       744.95265199, 681.04995163, 830.47041928, 788.31397118,\n",
      "       749.23453274, 775.92944027, 776.84226997, 761.63590883])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([589.70267609, 636.73604882, 589.67698049, 644.77612963,\n",
      "       561.24317062, 596.79057361, 568.82052745, 576.8558281 ,\n",
      "       622.05026979, 594.06184224, 602.02666574, 680.36988082,\n",
      "       633.39966005, 675.71884524, 605.40908925, 650.71947409,\n",
      "       727.41228097, 557.18612905, 562.01216424, 655.97105619,\n",
      "       650.10028912, 668.16176824, 631.85717443, 648.09903429,\n",
      "       649.27642821, 712.67236658, 696.27975741, 686.1849128 ,\n",
      "       649.98371354, 631.18617055])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([631.07502274, 673.75730796, 651.98129603, 673.73626744,\n",
      "       777.07324188, 758.2234969 , 680.18442364, 729.54997166,\n",
      "       678.89672873, 568.29571713, 581.78740582, 555.83498193,\n",
      "       615.91695795, 665.67546333, 761.52671654, 733.19317991,\n",
      "       684.17926491, 723.09299308, 592.89210242, 653.62634842])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([521.09639918, 505.46555349, 589.70267609, 636.73604882,\n",
      "       589.67698049, 644.77612963, 561.24317062, 596.79057361,\n",
      "       568.82052745, 576.8558281 , 622.05026979, 594.06184224,\n",
      "       602.02666574, 680.36988082, 633.39966005, 675.71884524,\n",
      "       605.40908925, 650.71947409, 727.41228097, 557.18612905,\n",
      "       562.01216424, 655.97105619, 650.10028912, 668.16176824,\n",
      "       631.85717443, 648.09903429, 649.27642821, 712.67236658,\n",
      "       696.27975741, 686.1849128 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([649.98371354, 631.18617055, 631.07502274, 673.75730796,\n",
      "       651.98129603, 673.73626744, 777.07324188, 758.2234969 ,\n",
      "       680.18442364, 729.54997166, 678.89672873, 568.29571713,\n",
      "       581.78740582, 555.83498193, 615.91695795, 665.67546333,\n",
      "       761.52671654, 733.19317991, 684.17926491, 723.09299308])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([650.71947409, 727.41228097, 557.18612905, 562.01216424,\n",
      "       655.97105619, 650.10028912, 668.16176824, 631.85717443,\n",
      "       648.09903429, 649.27642821, 712.67236658, 696.27975741,\n",
      "       686.1849128 , 649.98371354, 631.18617055, 631.07502274,\n",
      "       673.75730796, 651.98129603, 673.73626744, 777.07324188,\n",
      "       758.2234969 , 680.18442364, 729.54997166, 678.89672873,\n",
      "       568.29571713, 581.78740582, 555.83498193, 615.91695795,\n",
      "       665.67546333, 761.52671654])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([733.19317991, 684.17926491, 723.09299308, 592.89210242,\n",
      "       653.62634842, 724.79417847, 630.92834603, 717.67629451,\n",
      "       728.46205325, 681.62189532, 718.02118063, 820.5333744 ,\n",
      "       755.39247769, 721.11223575, 683.36485669, 648.91690775,\n",
      "       722.89139063, 674.15797097, 690.34147284, 679.89555162])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([724.79417847, 630.92834603, 717.67629451, 728.46205325,\n",
      "       681.62189532, 718.02118063, 820.5333744 , 755.39247769,\n",
      "       721.11223575, 683.36485669, 648.91690775, 722.89139063,\n",
      "       674.15797097, 690.34147284, 679.89555162, 718.86986728,\n",
      "       732.00333529, 767.83519502, 707.24881753, 689.61478723,\n",
      "       656.32769369, 668.64987017, 719.83064084, 758.57681546,\n",
      "       687.1679321 , 743.60502328, 802.44119057, 771.56266707,\n",
      "       828.12820822, 726.32636326])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([651.49920215, 608.68186599, 758.13233494, 784.85025629,\n",
      "       742.43778409, 744.95265199, 681.04995163, 830.47041928,\n",
      "       788.31397118, 749.23453274, 775.92944027, 776.84226997,\n",
      "       761.63590883, 706.47664247, 747.91110009, 842.84008478,\n",
      "       850.5771681 , 857.88656147, 759.24070059, 692.35079504])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([408.68045252, 379.54368873, 374.68737687, 434.71295722,\n",
      "       367.58405072, 389.15715379, 372.35580464, 359.08299146,\n",
      "       423.66672844, 438.84670817, 437.61444311, 463.16543088,\n",
      "       410.89882285, 419.07923581, 381.30008063, 394.0134984 ,\n",
      "       383.27079237, 387.32550047, 417.0279523 , 358.91573723,\n",
      "       476.45969901, 381.52225147, 314.31964945, 421.39247611,\n",
      "       452.38691493, 444.22925323, 442.59260105, 411.5736148 ,\n",
      "       354.91586408, 383.56673024])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([364.72357312, 434.29575526, 413.08282734, 361.27286765,\n",
      "       370.69375928, 416.44682588, 385.58899509, 355.57554051,\n",
      "       401.83538562, 424.40409635, 391.1802466 , 378.83606312,\n",
      "       413.5852798 , 345.70142055, 315.11381903, 346.66881669,\n",
      "       386.61950519, 426.58713648, 499.00199322, 496.48184556])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([777.07324188, 758.2234969 , 680.18442364, 729.54997166,\n",
      "       678.89672873, 568.29571713, 581.78740582, 555.83498193,\n",
      "       615.91695795, 665.67546333, 761.52671654, 733.19317991,\n",
      "       684.17926491, 723.09299308, 592.89210242, 653.62634842,\n",
      "       724.79417847, 630.92834603, 717.67629451, 728.46205325,\n",
      "       681.62189532, 718.02118063, 820.5333744 , 755.39247769,\n",
      "       721.11223575, 683.36485669, 648.91690775, 722.89139063,\n",
      "       674.15797097, 690.34147284])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([679.89555162, 718.86986728, 732.00333529, 767.83519502,\n",
      "       707.24881753, 689.61478723, 656.32769369, 668.64987017,\n",
      "       719.83064084, 758.57681546, 687.1679321 , 743.60502328,\n",
      "       802.44119057, 771.56266707, 828.12820822, 726.32636326,\n",
      "       651.49920215, 608.68186599, 758.13233494, 784.85025629])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([680.36988082, 633.39966005, 675.71884524, 605.40908925,\n",
      "       650.71947409, 727.41228097, 557.18612905, 562.01216424,\n",
      "       655.97105619, 650.10028912, 668.16176824, 631.85717443,\n",
      "       648.09903429, 649.27642821, 712.67236658, 696.27975741,\n",
      "       686.1849128 , 649.98371354, 631.18617055, 631.07502274,\n",
      "       673.75730796, 651.98129603, 673.73626744, 777.07324188,\n",
      "       758.2234969 , 680.18442364, 729.54997166, 678.89672873,\n",
      "       568.29571713, 581.78740582])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([555.83498193, 615.91695795, 665.67546333, 761.52671654,\n",
      "       733.19317991, 684.17926491, 723.09299308, 592.89210242,\n",
      "       653.62634842, 724.79417847, 630.92834603, 717.67629451,\n",
      "       728.46205325, 681.62189532, 718.02118063, 820.5333744 ,\n",
      "       755.39247769, 721.11223575, 683.36485669, 648.91690775])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([414.13221475, 481.92922581, 537.03024237, 531.27077547,\n",
      "       424.76439816, 480.27019493, 456.60722947, 534.68799708,\n",
      "       560.27001124, 495.50885821, 490.43221638, 469.54868894,\n",
      "       509.0216617 , 580.5186044 , 509.56832961, 547.35738325,\n",
      "       523.72622733, 493.15355426, 520.45598519, 488.64675133,\n",
      "       495.78048545, 472.70249657, 618.45257776, 589.3399963 ,\n",
      "       522.26342527, 549.53276579, 550.7361558 , 541.6098775 ,\n",
      "       581.53451462, 607.22224773])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([549.87462813, 522.37636622, 533.76513254, 439.37788128,\n",
      "       437.09888259, 592.31584485, 669.82746742, 592.01663525,\n",
      "       597.07230042, 597.51240164, 732.10605797, 694.37287539,\n",
      "       600.22696169, 529.54083307, 475.68450536, 549.19229931,\n",
      "       538.22986094, 490.72208051, 514.5088088 , 508.05592927])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([309.63551757, 327.15226648, 317.45700632, 335.20605714,\n",
      "       373.26065529, 433.43357323, 315.82157975, 427.89093968,\n",
      "       289.63949291, 301.77052434, 367.48165357, 372.13463675,\n",
      "       324.83322363, 327.12874953, 320.05395004, 310.90629308,\n",
      "       380.879735  , 387.76152403, 330.557742  , 388.11743607,\n",
      "       390.37958736, 408.2225002 , 410.23820866, 336.26542706,\n",
      "       319.41677054, 386.73270881, 408.68045252, 379.54368873,\n",
      "       374.68737687, 434.71295722])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([367.58405072, 389.15715379, 372.35580464, 359.08299146,\n",
      "       423.66672844, 438.84670817, 437.61444311, 463.16543088,\n",
      "       410.89882285, 419.07923581, 381.30008063, 394.0134984 ,\n",
      "       383.27079237, 387.32550047, 417.0279523 , 358.91573723,\n",
      "       476.45969901, 381.52225147, 314.31964945, 421.39247611])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([534.68799708, 560.27001124, 495.50885821, 490.43221638,\n",
      "       469.54868894, 509.0216617 , 580.5186044 , 509.56832961,\n",
      "       547.35738325, 523.72622733, 493.15355426, 520.45598519,\n",
      "       488.64675133, 495.78048545, 472.70249657, 618.45257776,\n",
      "       589.3399963 , 522.26342527, 549.53276579, 550.7361558 ,\n",
      "       541.6098775 , 581.53451462, 607.22224773, 549.87462813,\n",
      "       522.37636622, 533.76513254, 439.37788128, 437.09888259,\n",
      "       592.31584485, 669.82746742])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([592.01663525, 597.07230042, 597.51240164, 732.10605797,\n",
      "       694.37287539, 600.22696169, 529.54083307, 475.68450536,\n",
      "       549.19229931, 538.22986094, 490.72208051, 514.5088088 ,\n",
      "       508.05592927, 639.58053533, 658.89585885, 607.11396555,\n",
      "       662.96302643, 623.20154476, 551.67016265, 648.06221241])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([707.14155238, 571.44576343, 604.4266225 , 577.85684727,\n",
      "       565.12944302, 561.70663192, 546.92000432, 587.34387768,\n",
      "       563.28894392, 603.15946382, 609.79642481, 597.68857407,\n",
      "       561.29017164, 563.99484285, 636.27975237, 653.7286504 ,\n",
      "       580.21410777, 603.71020602, 655.75754555, 552.61491167,\n",
      "       614.86465122, 596.29610199, 638.34407776, 596.98361388,\n",
      "       521.09639918, 505.46555349, 589.70267609, 636.73604882,\n",
      "       589.67698049, 644.77612963])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([561.24317062, 596.79057361, 568.82052745, 576.8558281 ,\n",
      "       622.05026979, 594.06184224, 602.02666574, 680.36988082,\n",
      "       633.39966005, 675.71884524, 605.40908925, 650.71947409,\n",
      "       727.41228097, 557.18612905, 562.01216424, 655.97105619,\n",
      "       650.10028912, 668.16176824, 631.85717443, 648.09903429])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([686.1849128 , 649.98371354, 631.18617055, 631.07502274,\n",
      "       673.75730796, 651.98129603, 673.73626744, 777.07324188,\n",
      "       758.2234969 , 680.18442364, 729.54997166, 678.89672873,\n",
      "       568.29571713, 581.78740582, 555.83498193, 615.91695795,\n",
      "       665.67546333, 761.52671654, 733.19317991, 684.17926491,\n",
      "       723.09299308, 592.89210242, 653.62634842, 724.79417847,\n",
      "       630.92834603, 717.67629451, 728.46205325, 681.62189532,\n",
      "       718.02118063, 820.5333744 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([755.39247769, 721.11223575, 683.36485669, 648.91690775,\n",
      "       722.89139063, 674.15797097, 690.34147284, 679.89555162,\n",
      "       718.86986728, 732.00333529, 767.83519502, 707.24881753,\n",
      "       689.61478723, 656.32769369, 668.64987017, 719.83064084,\n",
      "       758.57681546, 687.1679321 , 743.60502328, 802.44119057])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([655.97105619, 650.10028912, 668.16176824, 631.85717443,\n",
      "       648.09903429, 649.27642821, 712.67236658, 696.27975741,\n",
      "       686.1849128 , 649.98371354, 631.18617055, 631.07502274,\n",
      "       673.75730796, 651.98129603, 673.73626744, 777.07324188,\n",
      "       758.2234969 , 680.18442364, 729.54997166, 678.89672873,\n",
      "       568.29571713, 581.78740582, 555.83498193, 615.91695795,\n",
      "       665.67546333, 761.52671654, 733.19317991, 684.17926491,\n",
      "       723.09299308, 592.89210242])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([653.62634842, 724.79417847, 630.92834603, 717.67629451,\n",
      "       728.46205325, 681.62189532, 718.02118063, 820.5333744 ,\n",
      "       755.39247769, 721.11223575, 683.36485669, 648.91690775,\n",
      "       722.89139063, 674.15797097, 690.34147284, 679.89555162,\n",
      "       718.86986728, 732.00333529, 767.83519502, 707.24881753])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([493.15355426, 520.45598519, 488.64675133, 495.78048545,\n",
      "       472.70249657, 618.45257776, 589.3399963 , 522.26342527,\n",
      "       549.53276579, 550.7361558 , 541.6098775 , 581.53451462,\n",
      "       607.22224773, 549.87462813, 522.37636622, 533.76513254,\n",
      "       439.37788128, 437.09888259, 592.31584485, 669.82746742,\n",
      "       592.01663525, 597.07230042, 597.51240164, 732.10605797,\n",
      "       694.37287539, 600.22696169, 529.54083307, 475.68450536,\n",
      "       549.19229931, 538.22986094])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([490.72208051, 514.5088088 , 508.05592927, 639.58053533,\n",
      "       658.89585885, 607.11396555, 662.96302643, 623.20154476,\n",
      "       551.67016265, 648.06221241, 647.35774585, 555.63289089,\n",
      "       563.85025964, 543.66405584, 506.29295049, 609.75057181,\n",
      "       707.14155238, 571.44576343, 604.4266225 , 577.85684727])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([562.01216424, 655.97105619, 650.10028912, 668.16176824,\n",
      "       631.85717443, 648.09903429, 649.27642821, 712.67236658,\n",
      "       696.27975741, 686.1849128 , 649.98371354, 631.18617055,\n",
      "       631.07502274, 673.75730796, 651.98129603, 673.73626744,\n",
      "       777.07324188, 758.2234969 , 680.18442364, 729.54997166,\n",
      "       678.89672873, 568.29571713, 581.78740582, 555.83498193,\n",
      "       615.91695795, 665.67546333, 761.52671654, 733.19317991,\n",
      "       684.17926491, 723.09299308])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([592.89210242, 653.62634842, 724.79417847, 630.92834603,\n",
      "       717.67629451, 728.46205325, 681.62189532, 718.02118063,\n",
      "       820.5333744 , 755.39247769, 721.11223575, 683.36485669,\n",
      "       648.91690775, 722.89139063, 674.15797097, 690.34147284,\n",
      "       679.89555162, 718.86986728, 732.00333529, 767.83519502])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([421.39247611, 452.38691493, 444.22925323, 442.59260105,\n",
      "       411.5736148 , 354.91586408, 383.56673024, 364.72357312,\n",
      "       434.29575526, 413.08282734, 361.27286765, 370.69375928,\n",
      "       416.44682588, 385.58899509, 355.57554051, 401.83538562,\n",
      "       424.40409635, 391.1802466 , 378.83606312, 413.5852798 ,\n",
      "       345.70142055, 315.11381903, 346.66881669, 386.61950519,\n",
      "       426.58713648, 499.00199322, 496.48184556, 438.03423281,\n",
      "       423.59440741, 375.37548583])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([404.09737706, 409.62535098, 437.76060348, 394.47503303,\n",
      "       439.97630195, 517.19346552, 461.76251444, 457.355711  ,\n",
      "       479.62716775, 433.19914647, 443.23925318, 444.05635782,\n",
      "       446.12106416, 405.39701126, 427.84486792, 466.91572959,\n",
      "       527.49026645, 526.50191535, 580.10855533, 460.73538646])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([631.07502274, 673.75730796, 651.98129603, 673.73626744,\n",
      "       777.07324188, 758.2234969 , 680.18442364, 729.54997166,\n",
      "       678.89672873, 568.29571713, 581.78740582, 555.83498193,\n",
      "       615.91695795, 665.67546333, 761.52671654, 733.19317991,\n",
      "       684.17926491, 723.09299308, 592.89210242, 653.62634842,\n",
      "       724.79417847, 630.92834603, 717.67629451, 728.46205325,\n",
      "       681.62189532, 718.02118063, 820.5333744 , 755.39247769,\n",
      "       721.11223575, 683.36485669])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([648.91690775, 722.89139063, 674.15797097, 690.34147284,\n",
      "       679.89555162, 718.86986728, 732.00333529, 767.83519502,\n",
      "       707.24881753, 689.61478723, 656.32769369, 668.64987017,\n",
      "       719.83064084, 758.57681546, 687.1679321 , 743.60502328,\n",
      "       802.44119057, 771.56266707, 828.12820822, 726.32636326])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([285.26129367, 335.01022329, 350.24709861, 309.63551757,\n",
      "       327.15226648, 317.45700632, 335.20605714, 373.26065529,\n",
      "       433.43357323, 315.82157975, 427.89093968, 289.63949291,\n",
      "       301.77052434, 367.48165357, 372.13463675, 324.83322363,\n",
      "       327.12874953, 320.05395004, 310.90629308, 380.879735  ,\n",
      "       387.76152403, 330.557742  , 388.11743607, 390.37958736,\n",
      "       408.2225002 , 410.23820866, 336.26542706, 319.41677054,\n",
      "       386.73270881, 408.68045252])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([379.54368873, 374.68737687, 434.71295722, 367.58405072,\n",
      "       389.15715379, 372.35580464, 359.08299146, 423.66672844,\n",
      "       438.84670817, 437.61444311, 463.16543088, 410.89882285,\n",
      "       419.07923581, 381.30008063, 394.0134984 , 383.27079237,\n",
      "       387.32550047, 417.0279523 , 358.91573723, 476.45969901])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([543.66405584, 506.29295049, 609.75057181, 707.14155238,\n",
      "       571.44576343, 604.4266225 , 577.85684727, 565.12944302,\n",
      "       561.70663192, 546.92000432, 587.34387768, 563.28894392,\n",
      "       603.15946382, 609.79642481, 597.68857407, 561.29017164,\n",
      "       563.99484285, 636.27975237, 653.7286504 , 580.21410777,\n",
      "       603.71020602, 655.75754555, 552.61491167, 614.86465122,\n",
      "       596.29610199, 638.34407776, 596.98361388, 521.09639918,\n",
      "       505.46555349, 589.70267609])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([636.73604882, 589.67698049, 644.77612963, 561.24317062,\n",
      "       596.79057361, 568.82052745, 576.8558281 , 622.05026979,\n",
      "       594.06184224, 602.02666574, 680.36988082, 633.39966005,\n",
      "       675.71884524, 605.40908925, 650.71947409, 727.41228097,\n",
      "       557.18612905, 562.01216424, 655.97105619, 650.10028912])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([490.72208051, 514.5088088 , 508.05592927, 639.58053533,\n",
      "       658.89585885, 607.11396555, 662.96302643, 623.20154476,\n",
      "       551.67016265, 648.06221241, 647.35774585, 555.63289089,\n",
      "       563.85025964, 543.66405584, 506.29295049, 609.75057181,\n",
      "       707.14155238, 571.44576343, 604.4266225 , 577.85684727,\n",
      "       565.12944302, 561.70663192, 546.92000432, 587.34387768,\n",
      "       563.28894392, 603.15946382, 609.79642481, 597.68857407,\n",
      "       561.29017164, 563.99484285])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([636.27975237, 653.7286504 , 580.21410777, 603.71020602,\n",
      "       655.75754555, 552.61491167, 614.86465122, 596.29610199,\n",
      "       638.34407776, 596.98361388, 521.09639918, 505.46555349,\n",
      "       589.70267609, 636.73604882, 589.67698049, 644.77612963,\n",
      "       561.24317062, 596.79057361, 568.82052745, 576.8558281 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([648.91690775, 722.89139063, 674.15797097, 690.34147284,\n",
      "       679.89555162, 718.86986728, 732.00333529, 767.83519502,\n",
      "       707.24881753, 689.61478723, 656.32769369, 668.64987017,\n",
      "       719.83064084, 758.57681546, 687.1679321 , 743.60502328,\n",
      "       802.44119057, 771.56266707, 828.12820822, 726.32636326,\n",
      "       651.49920215, 608.68186599, 758.13233494, 784.85025629,\n",
      "       742.43778409, 744.95265199, 681.04995163, 830.47041928,\n",
      "       788.31397118, 749.23453274])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([775.92944027, 776.84226997, 761.63590883, 706.47664247,\n",
      "       747.91110009, 842.84008478, 850.5771681 , 857.88656147,\n",
      "       759.24070059, 692.35079504, 719.83979052, 745.37251598,\n",
      "       804.07326495, 689.79394523, 795.39479652, 773.4935943 ,\n",
      "       733.44333496, 696.48647192, 651.50446061, 760.04488904])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([561.70663192, 546.92000432, 587.34387768, 563.28894392,\n",
      "       603.15946382, 609.79642481, 597.68857407, 561.29017164,\n",
      "       563.99484285, 636.27975237, 653.7286504 , 580.21410777,\n",
      "       603.71020602, 655.75754555, 552.61491167, 614.86465122,\n",
      "       596.29610199, 638.34407776, 596.98361388, 521.09639918,\n",
      "       505.46555349, 589.70267609, 636.73604882, 589.67698049,\n",
      "       644.77612963, 561.24317062, 596.79057361, 568.82052745,\n",
      "       576.8558281 , 622.05026979])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([594.06184224, 602.02666574, 680.36988082, 633.39966005,\n",
      "       675.71884524, 605.40908925, 650.71947409, 727.41228097,\n",
      "       557.18612905, 562.01216424, 655.97105619, 650.10028912,\n",
      "       668.16176824, 631.85717443, 648.09903429, 649.27642821,\n",
      "       712.67236658, 696.27975741, 686.1849128 , 649.98371354])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([668.64987017, 719.83064084, 758.57681546, 687.1679321 ,\n",
      "       743.60502328, 802.44119057, 771.56266707, 828.12820822,\n",
      "       726.32636326, 651.49920215, 608.68186599, 758.13233494,\n",
      "       784.85025629, 742.43778409, 744.95265199, 681.04995163,\n",
      "       830.47041928, 788.31397118, 749.23453274, 775.92944027,\n",
      "       776.84226997, 761.63590883, 706.47664247, 747.91110009,\n",
      "       842.84008478, 850.5771681 , 857.88656147, 759.24070059,\n",
      "       692.35079504, 719.83979052])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([745.37251598, 804.07326495, 689.79394523, 795.39479652,\n",
      "       773.4935943 , 733.44333496, 696.48647192, 651.50446061,\n",
      "       760.04488904, 774.24036403, 698.7297109 , 669.98055191,\n",
      "       714.36151018, 834.70411715, 785.46547122, 690.58578351,\n",
      "       723.47617913, 745.1619075 , 627.8142917 , 709.36185236])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([733.19317991, 684.17926491, 723.09299308, 592.89210242,\n",
      "       653.62634842, 724.79417847, 630.92834603, 717.67629451,\n",
      "       728.46205325, 681.62189532, 718.02118063, 820.5333744 ,\n",
      "       755.39247769, 721.11223575, 683.36485669, 648.91690775,\n",
      "       722.89139063, 674.15797097, 690.34147284, 679.89555162,\n",
      "       718.86986728, 732.00333529, 767.83519502, 707.24881753,\n",
      "       689.61478723, 656.32769369, 668.64987017, 719.83064084,\n",
      "       758.57681546, 687.1679321 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([743.60502328, 802.44119057, 771.56266707, 828.12820822,\n",
      "       726.32636326, 651.49920215, 608.68186599, 758.13233494,\n",
      "       784.85025629, 742.43778409, 744.95265199, 681.04995163,\n",
      "       830.47041928, 788.31397118, 749.23453274, 775.92944027,\n",
      "       776.84226997, 761.63590883, 706.47664247, 747.91110009])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([648.06221241, 647.35774585, 555.63289089, 563.85025964,\n",
      "       543.66405584, 506.29295049, 609.75057181, 707.14155238,\n",
      "       571.44576343, 604.4266225 , 577.85684727, 565.12944302,\n",
      "       561.70663192, 546.92000432, 587.34387768, 563.28894392,\n",
      "       603.15946382, 609.79642481, 597.68857407, 561.29017164,\n",
      "       563.99484285, 636.27975237, 653.7286504 , 580.21410777,\n",
      "       603.71020602, 655.75754555, 552.61491167, 614.86465122,\n",
      "       596.29610199, 638.34407776])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([596.98361388, 521.09639918, 505.46555349, 589.70267609,\n",
      "       636.73604882, 589.67698049, 644.77612963, 561.24317062,\n",
      "       596.79057361, 568.82052745, 576.8558281 , 622.05026979,\n",
      "       594.06184224, 602.02666574, 680.36988082, 633.39966005,\n",
      "       675.71884524, 605.40908925, 650.71947409, 727.41228097])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([563.85025964, 543.66405584, 506.29295049, 609.75057181,\n",
      "       707.14155238, 571.44576343, 604.4266225 , 577.85684727,\n",
      "       565.12944302, 561.70663192, 546.92000432, 587.34387768,\n",
      "       563.28894392, 603.15946382, 609.79642481, 597.68857407,\n",
      "       561.29017164, 563.99484285, 636.27975237, 653.7286504 ,\n",
      "       580.21410777, 603.71020602, 655.75754555, 552.61491167,\n",
      "       614.86465122, 596.29610199, 638.34407776, 596.98361388,\n",
      "       521.09639918, 505.46555349])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([589.70267609, 636.73604882, 589.67698049, 644.77612963,\n",
      "       561.24317062, 596.79057361, 568.82052745, 576.8558281 ,\n",
      "       622.05026979, 594.06184224, 602.02666574, 680.36988082,\n",
      "       633.39966005, 675.71884524, 605.40908925, 650.71947409,\n",
      "       727.41228097, 557.18612905, 562.01216424, 655.97105619])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([555.83498193, 615.91695795, 665.67546333, 761.52671654,\n",
      "       733.19317991, 684.17926491, 723.09299308, 592.89210242,\n",
      "       653.62634842, 724.79417847, 630.92834603, 717.67629451,\n",
      "       728.46205325, 681.62189532, 718.02118063, 820.5333744 ,\n",
      "       755.39247769, 721.11223575, 683.36485669, 648.91690775,\n",
      "       722.89139063, 674.15797097, 690.34147284, 679.89555162,\n",
      "       718.86986728, 732.00333529, 767.83519502, 707.24881753,\n",
      "       689.61478723, 656.32769369])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([668.64987017, 719.83064084, 758.57681546, 687.1679321 ,\n",
      "       743.60502328, 802.44119057, 771.56266707, 828.12820822,\n",
      "       726.32636326, 651.49920215, 608.68186599, 758.13233494,\n",
      "       784.85025629, 742.43778409, 744.95265199, 681.04995163,\n",
      "       830.47041928, 788.31397118, 749.23453274, 775.92944027])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([539.24259306, 605.47121043, 556.67616859, 608.30828456,\n",
      "       489.31267053, 414.13221475, 481.92922581, 537.03024237,\n",
      "       531.27077547, 424.76439816, 480.27019493, 456.60722947,\n",
      "       534.68799708, 560.27001124, 495.50885821, 490.43221638,\n",
      "       469.54868894, 509.0216617 , 580.5186044 , 509.56832961,\n",
      "       547.35738325, 523.72622733, 493.15355426, 520.45598519,\n",
      "       488.64675133, 495.78048545, 472.70249657, 618.45257776,\n",
      "       589.3399963 , 522.26342527])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([549.53276579, 550.7361558 , 541.6098775 , 581.53451462,\n",
      "       607.22224773, 549.87462813, 522.37636622, 533.76513254,\n",
      "       439.37788128, 437.09888259, 592.31584485, 669.82746742,\n",
      "       592.01663525, 597.07230042, 597.51240164, 732.10605797,\n",
      "       694.37287539, 600.22696169, 529.54083307, 475.68450536])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([743.60502328, 802.44119057, 771.56266707, 828.12820822,\n",
      "       726.32636326, 651.49920215, 608.68186599, 758.13233494,\n",
      "       784.85025629, 742.43778409, 744.95265199, 681.04995163,\n",
      "       830.47041928, 788.31397118, 749.23453274, 775.92944027,\n",
      "       776.84226997, 761.63590883, 706.47664247, 747.91110009,\n",
      "       842.84008478, 850.5771681 , 857.88656147, 759.24070059,\n",
      "       692.35079504, 719.83979052, 745.37251598, 804.07326495,\n",
      "       689.79394523, 795.39479652])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([773.4935943 , 733.44333496, 696.48647192, 651.50446061,\n",
      "       760.04488904, 774.24036403, 698.7297109 , 669.98055191,\n",
      "       714.36151018, 834.70411715, 785.46547122, 690.58578351,\n",
      "       723.47617913, 745.1619075 , 627.8142917 , 709.36185236,\n",
      "       750.69673545, 800.72938966, 880.17979644, 873.23872791])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([568.29571713, 581.78740582, 555.83498193, 615.91695795,\n",
      "       665.67546333, 761.52671654, 733.19317991, 684.17926491,\n",
      "       723.09299308, 592.89210242, 653.62634842, 724.79417847,\n",
      "       630.92834603, 717.67629451, 728.46205325, 681.62189532,\n",
      "       718.02118063, 820.5333744 , 755.39247769, 721.11223575,\n",
      "       683.36485669, 648.91690775, 722.89139063, 674.15797097,\n",
      "       690.34147284, 679.89555162, 718.86986728, 732.00333529,\n",
      "       767.83519502, 707.24881753])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([689.61478723, 656.32769369, 668.64987017, 719.83064084,\n",
      "       758.57681546, 687.1679321 , 743.60502328, 802.44119057,\n",
      "       771.56266707, 828.12820822, 726.32636326, 651.49920215,\n",
      "       608.68186599, 758.13233494, 784.85025629, 742.43778409,\n",
      "       744.95265199, 681.04995163, 830.47041928, 788.31397118])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([828.12820822, 726.32636326, 651.49920215, 608.68186599,\n",
      "       758.13233494, 784.85025629, 742.43778409, 744.95265199,\n",
      "       681.04995163, 830.47041928, 788.31397118, 749.23453274,\n",
      "       775.92944027, 776.84226997, 761.63590883, 706.47664247,\n",
      "       747.91110009, 842.84008478, 850.5771681 , 857.88656147,\n",
      "       759.24070059, 692.35079504, 719.83979052, 745.37251598,\n",
      "       804.07326495, 689.79394523, 795.39479652, 773.4935943 ,\n",
      "       733.44333496, 696.48647192])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([651.50446061, 760.04488904, 774.24036403, 698.7297109 ,\n",
      "       669.98055191, 714.36151018, 834.70411715, 785.46547122,\n",
      "       690.58578351, 723.47617913, 745.1619075 , 627.8142917 ,\n",
      "       709.36185236, 750.69673545, 800.72938966, 880.17979644,\n",
      "       873.23872791, 793.98585857, 722.50694506, 884.27854574])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([761.52671654, 733.19317991, 684.17926491, 723.09299308,\n",
      "       592.89210242, 653.62634842, 724.79417847, 630.92834603,\n",
      "       717.67629451, 728.46205325, 681.62189532, 718.02118063,\n",
      "       820.5333744 , 755.39247769, 721.11223575, 683.36485669,\n",
      "       648.91690775, 722.89139063, 674.15797097, 690.34147284,\n",
      "       679.89555162, 718.86986728, 732.00333529, 767.83519502,\n",
      "       707.24881753, 689.61478723, 656.32769369, 668.64987017,\n",
      "       719.83064084, 758.57681546])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([687.1679321 , 743.60502328, 802.44119057, 771.56266707,\n",
      "       828.12820822, 726.32636326, 651.49920215, 608.68186599,\n",
      "       758.13233494, 784.85025629, 742.43778409, 744.95265199,\n",
      "       681.04995163, 830.47041928, 788.31397118, 749.23453274,\n",
      "       775.92944027, 776.84226997, 761.63590883, 706.47664247])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([439.37788128, 437.09888259, 592.31584485, 669.82746742,\n",
      "       592.01663525, 597.07230042, 597.51240164, 732.10605797,\n",
      "       694.37287539, 600.22696169, 529.54083307, 475.68450536,\n",
      "       549.19229931, 538.22986094, 490.72208051, 514.5088088 ,\n",
      "       508.05592927, 639.58053533, 658.89585885, 607.11396555,\n",
      "       662.96302643, 623.20154476, 551.67016265, 648.06221241,\n",
      "       647.35774585, 555.63289089, 563.85025964, 543.66405584,\n",
      "       506.29295049, 609.75057181])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([707.14155238, 571.44576343, 604.4266225 , 577.85684727,\n",
      "       565.12944302, 561.70663192, 546.92000432, 587.34387768,\n",
      "       563.28894392, 603.15946382, 609.79642481, 597.68857407,\n",
      "       561.29017164, 563.99484285, 636.27975237, 653.7286504 ,\n",
      "       580.21410777, 603.71020602, 655.75754555, 552.61491167])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([758.13233494, 784.85025629, 742.43778409, 744.95265199,\n",
      "       681.04995163, 830.47041928, 788.31397118, 749.23453274,\n",
      "       775.92944027, 776.84226997, 761.63590883, 706.47664247,\n",
      "       747.91110009, 842.84008478, 850.5771681 , 857.88656147,\n",
      "       759.24070059, 692.35079504, 719.83979052, 745.37251598,\n",
      "       804.07326495, 689.79394523, 795.39479652, 773.4935943 ,\n",
      "       733.44333496, 696.48647192, 651.50446061, 760.04488904,\n",
      "       774.24036403, 698.7297109 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([669.98055191, 714.36151018, 834.70411715, 785.46547122,\n",
      "       690.58578351, 723.47617913, 745.1619075 , 627.8142917 ,\n",
      "       709.36185236, 750.69673545, 800.72938966, 880.17979644,\n",
      "       873.23872791, 793.98585857, 722.50694506, 884.27854574,\n",
      "       835.09507914, 790.9940369 , 784.06169243, 794.03170193])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([557.18612905, 562.01216424, 655.97105619, 650.10028912,\n",
      "       668.16176824, 631.85717443, 648.09903429, 649.27642821,\n",
      "       712.67236658, 696.27975741, 686.1849128 , 649.98371354,\n",
      "       631.18617055, 631.07502274, 673.75730796, 651.98129603,\n",
      "       673.73626744, 777.07324188, 758.2234969 , 680.18442364,\n",
      "       729.54997166, 678.89672873, 568.29571713, 581.78740582,\n",
      "       555.83498193, 615.91695795, 665.67546333, 761.52671654,\n",
      "       733.19317991, 684.17926491])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([723.09299308, 592.89210242, 653.62634842, 724.79417847,\n",
      "       630.92834603, 717.67629451, 728.46205325, 681.62189532,\n",
      "       718.02118063, 820.5333744 , 755.39247769, 721.11223575,\n",
      "       683.36485669, 648.91690775, 722.89139063, 674.15797097,\n",
      "       690.34147284, 679.89555162, 718.86986728, 732.00333529])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([555.47851634, 560.26732238, 592.77245291, 492.78699422,\n",
      "       528.47301518, 576.62673148, 449.02573821, 420.73057322,\n",
      "       383.2050939 , 453.90534066, 539.24259306, 605.47121043,\n",
      "       556.67616859, 608.30828456, 489.31267053, 414.13221475,\n",
      "       481.92922581, 537.03024237, 531.27077547, 424.76439816,\n",
      "       480.27019493, 456.60722947, 534.68799708, 560.27001124,\n",
      "       495.50885821, 490.43221638, 469.54868894, 509.0216617 ,\n",
      "       580.5186044 , 509.56832961])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([547.35738325, 523.72622733, 493.15355426, 520.45598519,\n",
      "       488.64675133, 495.78048545, 472.70249657, 618.45257776,\n",
      "       589.3399963 , 522.26342527, 549.53276579, 550.7361558 ,\n",
      "       541.6098775 , 581.53451462, 607.22224773, 549.87462813,\n",
      "       522.37636622, 533.76513254, 439.37788128, 437.09888259])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([631.18617055, 631.07502274, 673.75730796, 651.98129603,\n",
      "       673.73626744, 777.07324188, 758.2234969 , 680.18442364,\n",
      "       729.54997166, 678.89672873, 568.29571713, 581.78740582,\n",
      "       555.83498193, 615.91695795, 665.67546333, 761.52671654,\n",
      "       733.19317991, 684.17926491, 723.09299308, 592.89210242,\n",
      "       653.62634842, 724.79417847, 630.92834603, 717.67629451,\n",
      "       728.46205325, 681.62189532, 718.02118063, 820.5333744 ,\n",
      "       755.39247769, 721.11223575])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([683.36485669, 648.91690775, 722.89139063, 674.15797097,\n",
      "       690.34147284, 679.89555162, 718.86986728, 732.00333529,\n",
      "       767.83519502, 707.24881753, 689.61478723, 656.32769369,\n",
      "       668.64987017, 719.83064084, 758.57681546, 687.1679321 ,\n",
      "       743.60502328, 802.44119057, 771.56266707, 828.12820822])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([607.22224773, 549.87462813, 522.37636622, 533.76513254,\n",
      "       439.37788128, 437.09888259, 592.31584485, 669.82746742,\n",
      "       592.01663525, 597.07230042, 597.51240164, 732.10605797,\n",
      "       694.37287539, 600.22696169, 529.54083307, 475.68450536,\n",
      "       549.19229931, 538.22986094, 490.72208051, 514.5088088 ,\n",
      "       508.05592927, 639.58053533, 658.89585885, 607.11396555,\n",
      "       662.96302643, 623.20154476, 551.67016265, 648.06221241,\n",
      "       647.35774585, 555.63289089])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([563.85025964, 543.66405584, 506.29295049, 609.75057181,\n",
      "       707.14155238, 571.44576343, 604.4266225 , 577.85684727,\n",
      "       565.12944302, 561.70663192, 546.92000432, 587.34387768,\n",
      "       563.28894392, 603.15946382, 609.79642481, 597.68857407,\n",
      "       561.29017164, 563.99484285, 636.27975237, 653.7286504 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([561.24317062, 596.79057361, 568.82052745, 576.8558281 ,\n",
      "       622.05026979, 594.06184224, 602.02666574, 680.36988082,\n",
      "       633.39966005, 675.71884524, 605.40908925, 650.71947409,\n",
      "       727.41228097, 557.18612905, 562.01216424, 655.97105619,\n",
      "       650.10028912, 668.16176824, 631.85717443, 648.09903429,\n",
      "       649.27642821, 712.67236658, 696.27975741, 686.1849128 ,\n",
      "       649.98371354, 631.18617055, 631.07502274, 673.75730796,\n",
      "       651.98129603, 673.73626744])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([777.07324188, 758.2234969 , 680.18442364, 729.54997166,\n",
      "       678.89672873, 568.29571713, 581.78740582, 555.83498193,\n",
      "       615.91695795, 665.67546333, 761.52671654, 733.19317991,\n",
      "       684.17926491, 723.09299308, 592.89210242, 653.62634842,\n",
      "       724.79417847, 630.92834603, 717.67629451, 728.46205325])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([729.54997166, 678.89672873, 568.29571713, 581.78740582,\n",
      "       555.83498193, 615.91695795, 665.67546333, 761.52671654,\n",
      "       733.19317991, 684.17926491, 723.09299308, 592.89210242,\n",
      "       653.62634842, 724.79417847, 630.92834603, 717.67629451,\n",
      "       728.46205325, 681.62189532, 718.02118063, 820.5333744 ,\n",
      "       755.39247769, 721.11223575, 683.36485669, 648.91690775,\n",
      "       722.89139063, 674.15797097, 690.34147284, 679.89555162,\n",
      "       718.86986728, 732.00333529])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([767.83519502, 707.24881753, 689.61478723, 656.32769369,\n",
      "       668.64987017, 719.83064084, 758.57681546, 687.1679321 ,\n",
      "       743.60502328, 802.44119057, 771.56266707, 828.12820822,\n",
      "       726.32636326, 651.49920215, 608.68186599, 758.13233494,\n",
      "       784.85025629, 742.43778409, 744.95265199, 681.04995163])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([636.27975237, 653.7286504 , 580.21410777, 603.71020602,\n",
      "       655.75754555, 552.61491167, 614.86465122, 596.29610199,\n",
      "       638.34407776, 596.98361388, 521.09639918, 505.46555349,\n",
      "       589.70267609, 636.73604882, 589.67698049, 644.77612963,\n",
      "       561.24317062, 596.79057361, 568.82052745, 576.8558281 ,\n",
      "       622.05026979, 594.06184224, 602.02666574, 680.36988082,\n",
      "       633.39966005, 675.71884524, 605.40908925, 650.71947409,\n",
      "       727.41228097, 557.18612905])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([562.01216424, 655.97105619, 650.10028912, 668.16176824,\n",
      "       631.85717443, 648.09903429, 649.27642821, 712.67236658,\n",
      "       696.27975741, 686.1849128 , 649.98371354, 631.18617055,\n",
      "       631.07502274, 673.75730796, 651.98129603, 673.73626744,\n",
      "       777.07324188, 758.2234969 , 680.18442364, 729.54997166])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([674.15797097, 690.34147284, 679.89555162, 718.86986728,\n",
      "       732.00333529, 767.83519502, 707.24881753, 689.61478723,\n",
      "       656.32769369, 668.64987017, 719.83064084, 758.57681546,\n",
      "       687.1679321 , 743.60502328, 802.44119057, 771.56266707,\n",
      "       828.12820822, 726.32636326, 651.49920215, 608.68186599,\n",
      "       758.13233494, 784.85025629, 742.43778409, 744.95265199,\n",
      "       681.04995163, 830.47041928, 788.31397118, 749.23453274,\n",
      "       775.92944027, 776.84226997])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([761.63590883, 706.47664247, 747.91110009, 842.84008478,\n",
      "       850.5771681 , 857.88656147, 759.24070059, 692.35079504,\n",
      "       719.83979052, 745.37251598, 804.07326495, 689.79394523,\n",
      "       795.39479652, 773.4935943 , 733.44333496, 696.48647192,\n",
      "       651.50446061, 760.04488904, 774.24036403, 698.7297109 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([719.83064084, 758.57681546, 687.1679321 , 743.60502328,\n",
      "       802.44119057, 771.56266707, 828.12820822, 726.32636326,\n",
      "       651.49920215, 608.68186599, 758.13233494, 784.85025629,\n",
      "       742.43778409, 744.95265199, 681.04995163, 830.47041928,\n",
      "       788.31397118, 749.23453274, 775.92944027, 776.84226997,\n",
      "       761.63590883, 706.47664247, 747.91110009, 842.84008478,\n",
      "       850.5771681 , 857.88656147, 759.24070059, 692.35079504,\n",
      "       719.83979052, 745.37251598])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([804.07326495, 689.79394523, 795.39479652, 773.4935943 ,\n",
      "       733.44333496, 696.48647192, 651.50446061, 760.04488904,\n",
      "       774.24036403, 698.7297109 , 669.98055191, 714.36151018,\n",
      "       834.70411715, 785.46547122, 690.58578351, 723.47617913,\n",
      "       745.1619075 , 627.8142917 , 709.36185236, 750.69673545])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([651.98129603, 673.73626744, 777.07324188, 758.2234969 ,\n",
      "       680.18442364, 729.54997166, 678.89672873, 568.29571713,\n",
      "       581.78740582, 555.83498193, 615.91695795, 665.67546333,\n",
      "       761.52671654, 733.19317991, 684.17926491, 723.09299308,\n",
      "       592.89210242, 653.62634842, 724.79417847, 630.92834603,\n",
      "       717.67629451, 728.46205325, 681.62189532, 718.02118063,\n",
      "       820.5333744 , 755.39247769, 721.11223575, 683.36485669,\n",
      "       648.91690775, 722.89139063])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([674.15797097, 690.34147284, 679.89555162, 718.86986728,\n",
      "       732.00333529, 767.83519502, 707.24881753, 689.61478723,\n",
      "       656.32769369, 668.64987017, 719.83064084, 758.57681546,\n",
      "       687.1679321 , 743.60502328, 802.44119057, 771.56266707,\n",
      "       828.12820822, 726.32636326, 651.49920215, 608.68186599])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([644.77612963, 561.24317062, 596.79057361, 568.82052745,\n",
      "       576.8558281 , 622.05026979, 594.06184224, 602.02666574,\n",
      "       680.36988082, 633.39966005, 675.71884524, 605.40908925,\n",
      "       650.71947409, 727.41228097, 557.18612905, 562.01216424,\n",
      "       655.97105619, 650.10028912, 668.16176824, 631.85717443,\n",
      "       648.09903429, 649.27642821, 712.67236658, 696.27975741,\n",
      "       686.1849128 , 649.98371354, 631.18617055, 631.07502274,\n",
      "       673.75730796, 651.98129603])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([673.73626744, 777.07324188, 758.2234969 , 680.18442364,\n",
      "       729.54997166, 678.89672873, 568.29571713, 581.78740582,\n",
      "       555.83498193, 615.91695795, 665.67546333, 761.52671654,\n",
      "       733.19317991, 684.17926491, 723.09299308, 592.89210242,\n",
      "       653.62634842, 724.79417847, 630.92834603, 717.67629451])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([514.5088088 , 508.05592927, 639.58053533, 658.89585885,\n",
      "       607.11396555, 662.96302643, 623.20154476, 551.67016265,\n",
      "       648.06221241, 647.35774585, 555.63289089, 563.85025964,\n",
      "       543.66405584, 506.29295049, 609.75057181, 707.14155238,\n",
      "       571.44576343, 604.4266225 , 577.85684727, 565.12944302,\n",
      "       561.70663192, 546.92000432, 587.34387768, 563.28894392,\n",
      "       603.15946382, 609.79642481, 597.68857407, 561.29017164,\n",
      "       563.99484285, 636.27975237])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([653.7286504 , 580.21410777, 603.71020602, 655.75754555,\n",
      "       552.61491167, 614.86465122, 596.29610199, 638.34407776,\n",
      "       596.98361388, 521.09639918, 505.46555349, 589.70267609,\n",
      "       636.73604882, 589.67698049, 644.77612963, 561.24317062,\n",
      "       596.79057361, 568.82052745, 576.8558281 , 622.05026979])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([673.73626744, 777.07324188, 758.2234969 , 680.18442364,\n",
      "       729.54997166, 678.89672873, 568.29571713, 581.78740582,\n",
      "       555.83498193, 615.91695795, 665.67546333, 761.52671654,\n",
      "       733.19317991, 684.17926491, 723.09299308, 592.89210242,\n",
      "       653.62634842, 724.79417847, 630.92834603, 717.67629451,\n",
      "       728.46205325, 681.62189532, 718.02118063, 820.5333744 ,\n",
      "       755.39247769, 721.11223575, 683.36485669, 648.91690775,\n",
      "       722.89139063, 674.15797097])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([690.34147284, 679.89555162, 718.86986728, 732.00333529,\n",
      "       767.83519502, 707.24881753, 689.61478723, 656.32769369,\n",
      "       668.64987017, 719.83064084, 758.57681546, 687.1679321 ,\n",
      "       743.60502328, 802.44119057, 771.56266707, 828.12820822,\n",
      "       726.32636326, 651.49920215, 608.68186599, 758.13233494])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([608.68186599, 758.13233494, 784.85025629, 742.43778409,\n",
      "       744.95265199, 681.04995163, 830.47041928, 788.31397118,\n",
      "       749.23453274, 775.92944027, 776.84226997, 761.63590883,\n",
      "       706.47664247, 747.91110009, 842.84008478, 850.5771681 ,\n",
      "       857.88656147, 759.24070059, 692.35079504, 719.83979052,\n",
      "       745.37251598, 804.07326495, 689.79394523, 795.39479652,\n",
      "       773.4935943 , 733.44333496, 696.48647192, 651.50446061,\n",
      "       760.04488904, 774.24036403])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([698.7297109 , 669.98055191, 714.36151018, 834.70411715,\n",
      "       785.46547122, 690.58578351, 723.47617913, 745.1619075 ,\n",
      "       627.8142917 , 709.36185236, 750.69673545, 800.72938966,\n",
      "       880.17979644, 873.23872791, 793.98585857, 722.50694506,\n",
      "       884.27854574, 835.09507914, 790.9940369 , 784.06169243])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([759.24070059, 692.35079504, 719.83979052, 745.37251598,\n",
      "       804.07326495, 689.79394523, 795.39479652, 773.4935943 ,\n",
      "       733.44333496, 696.48647192, 651.50446061, 760.04488904,\n",
      "       774.24036403, 698.7297109 , 669.98055191, 714.36151018,\n",
      "       834.70411715, 785.46547122, 690.58578351, 723.47617913,\n",
      "       745.1619075 , 627.8142917 , 709.36185236, 750.69673545,\n",
      "       800.72938966, 880.17979644, 873.23872791, 793.98585857,\n",
      "       722.50694506, 884.27854574])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([835.09507914, 790.9940369 , 784.06169243, 794.03170193,\n",
      "       782.43241217, 755.51672945, 747.93387285, 773.69529753,\n",
      "       760.13137483, 743.52554413, 780.65476182, 780.51270032,\n",
      "       863.92123596, 695.31521813, 799.46781486, 876.53041534,\n",
      "       724.65218252, 743.8072481 , 770.72811344, 722.73099485])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([755.39247769, 721.11223575, 683.36485669, 648.91690775,\n",
      "       722.89139063, 674.15797097, 690.34147284, 679.89555162,\n",
      "       718.86986728, 732.00333529, 767.83519502, 707.24881753,\n",
      "       689.61478723, 656.32769369, 668.64987017, 719.83064084,\n",
      "       758.57681546, 687.1679321 , 743.60502328, 802.44119057,\n",
      "       771.56266707, 828.12820822, 726.32636326, 651.49920215,\n",
      "       608.68186599, 758.13233494, 784.85025629, 742.43778409,\n",
      "       744.95265199, 681.04995163])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([830.47041928, 788.31397118, 749.23453274, 775.92944027,\n",
      "       776.84226997, 761.63590883, 706.47664247, 747.91110009,\n",
      "       842.84008478, 850.5771681 , 857.88656147, 759.24070059,\n",
      "       692.35079504, 719.83979052, 745.37251598, 804.07326495,\n",
      "       689.79394523, 795.39479652, 773.4935943 , 733.44333496])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([638.34407776, 596.98361388, 521.09639918, 505.46555349,\n",
      "       589.70267609, 636.73604882, 589.67698049, 644.77612963,\n",
      "       561.24317062, 596.79057361, 568.82052745, 576.8558281 ,\n",
      "       622.05026979, 594.06184224, 602.02666574, 680.36988082,\n",
      "       633.39966005, 675.71884524, 605.40908925, 650.71947409,\n",
      "       727.41228097, 557.18612905, 562.01216424, 655.97105619,\n",
      "       650.10028912, 668.16176824, 631.85717443, 648.09903429,\n",
      "       649.27642821, 712.67236658])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([696.27975741, 686.1849128 , 649.98371354, 631.18617055,\n",
      "       631.07502274, 673.75730796, 651.98129603, 673.73626744,\n",
      "       777.07324188, 758.2234969 , 680.18442364, 729.54997166,\n",
      "       678.89672873, 568.29571713, 581.78740582, 555.83498193,\n",
      "       615.91695795, 665.67546333, 761.52671654, 733.19317991])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([437.09888259, 592.31584485, 669.82746742, 592.01663525,\n",
      "       597.07230042, 597.51240164, 732.10605797, 694.37287539,\n",
      "       600.22696169, 529.54083307, 475.68450536, 549.19229931,\n",
      "       538.22986094, 490.72208051, 514.5088088 , 508.05592927,\n",
      "       639.58053533, 658.89585885, 607.11396555, 662.96302643,\n",
      "       623.20154476, 551.67016265, 648.06221241, 647.35774585,\n",
      "       555.63289089, 563.85025964, 543.66405584, 506.29295049,\n",
      "       609.75057181, 707.14155238])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([571.44576343, 604.4266225 , 577.85684727, 565.12944302,\n",
      "       561.70663192, 546.92000432, 587.34387768, 563.28894392,\n",
      "       603.15946382, 609.79642481, 597.68857407, 561.29017164,\n",
      "       563.99484285, 636.27975237, 653.7286504 , 580.21410777,\n",
      "       603.71020602, 655.75754555, 552.61491167, 614.86465122])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([649.98371354, 631.18617055, 631.07502274, 673.75730796,\n",
      "       651.98129603, 673.73626744, 777.07324188, 758.2234969 ,\n",
      "       680.18442364, 729.54997166, 678.89672873, 568.29571713,\n",
      "       581.78740582, 555.83498193, 615.91695795, 665.67546333,\n",
      "       761.52671654, 733.19317991, 684.17926491, 723.09299308,\n",
      "       592.89210242, 653.62634842, 724.79417847, 630.92834603,\n",
      "       717.67629451, 728.46205325, 681.62189532, 718.02118063,\n",
      "       820.5333744 , 755.39247769])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([721.11223575, 683.36485669, 648.91690775, 722.89139063,\n",
      "       674.15797097, 690.34147284, 679.89555162, 718.86986728,\n",
      "       732.00333529, 767.83519502, 707.24881753, 689.61478723,\n",
      "       656.32769369, 668.64987017, 719.83064084, 758.57681546,\n",
      "       687.1679321 , 743.60502328, 802.44119057, 771.56266707])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([749.23453274, 775.92944027, 776.84226997, 761.63590883,\n",
      "       706.47664247, 747.91110009, 842.84008478, 850.5771681 ,\n",
      "       857.88656147, 759.24070059, 692.35079504, 719.83979052,\n",
      "       745.37251598, 804.07326495, 689.79394523, 795.39479652,\n",
      "       773.4935943 , 733.44333496, 696.48647192, 651.50446061,\n",
      "       760.04488904, 774.24036403, 698.7297109 , 669.98055191,\n",
      "       714.36151018, 834.70411715, 785.46547122, 690.58578351,\n",
      "       723.47617913, 745.1619075 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([627.8142917 , 709.36185236, 750.69673545, 800.72938966,\n",
      "       880.17979644, 873.23872791, 793.98585857, 722.50694506,\n",
      "       884.27854574, 835.09507914, 790.9940369 , 784.06169243,\n",
      "       794.03170193, 782.43241217, 755.51672945, 747.93387285,\n",
      "       773.69529753, 760.13137483, 743.52554413, 780.65476182])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([773.4935943 , 733.44333496, 696.48647192, 651.50446061,\n",
      "       760.04488904, 774.24036403, 698.7297109 , 669.98055191,\n",
      "       714.36151018, 834.70411715, 785.46547122, 690.58578351,\n",
      "       723.47617913, 745.1619075 , 627.8142917 , 709.36185236,\n",
      "       750.69673545, 800.72938966, 880.17979644, 873.23872791,\n",
      "       793.98585857, 722.50694506, 884.27854574, 835.09507914,\n",
      "       790.9940369 , 784.06169243, 794.03170193, 782.43241217,\n",
      "       755.51672945, 747.93387285])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([773.69529753, 760.13137483, 743.52554413, 780.65476182,\n",
      "       780.51270032, 863.92123596, 695.31521813, 799.46781486,\n",
      "       876.53041534, 724.65218252, 743.8072481 , 770.72811344,\n",
      "       722.73099485, 733.79990542, 728.76976372, 866.57003017,\n",
      "       887.52255784, 896.85451539, 877.50053638, 775.79215333])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([605.40908925, 650.71947409, 727.41228097, 557.18612905,\n",
      "       562.01216424, 655.97105619, 650.10028912, 668.16176824,\n",
      "       631.85717443, 648.09903429, 649.27642821, 712.67236658,\n",
      "       696.27975741, 686.1849128 , 649.98371354, 631.18617055,\n",
      "       631.07502274, 673.75730796, 651.98129603, 673.73626744,\n",
      "       777.07324188, 758.2234969 , 680.18442364, 729.54997166,\n",
      "       678.89672873, 568.29571713, 581.78740582, 555.83498193,\n",
      "       615.91695795, 665.67546333])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([761.52671654, 733.19317991, 684.17926491, 723.09299308,\n",
      "       592.89210242, 653.62634842, 724.79417847, 630.92834603,\n",
      "       717.67629451, 728.46205325, 681.62189532, 718.02118063,\n",
      "       820.5333744 , 755.39247769, 721.11223575, 683.36485669,\n",
      "       648.91690775, 722.89139063, 674.15797097, 690.34147284])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([587.34387768, 563.28894392, 603.15946382, 609.79642481,\n",
      "       597.68857407, 561.29017164, 563.99484285, 636.27975237,\n",
      "       653.7286504 , 580.21410777, 603.71020602, 655.75754555,\n",
      "       552.61491167, 614.86465122, 596.29610199, 638.34407776,\n",
      "       596.98361388, 521.09639918, 505.46555349, 589.70267609,\n",
      "       636.73604882, 589.67698049, 644.77612963, 561.24317062,\n",
      "       596.79057361, 568.82052745, 576.8558281 , 622.05026979,\n",
      "       594.06184224, 602.02666574])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([680.36988082, 633.39966005, 675.71884524, 605.40908925,\n",
      "       650.71947409, 727.41228097, 557.18612905, 562.01216424,\n",
      "       655.97105619, 650.10028912, 668.16176824, 631.85717443,\n",
      "       648.09903429, 649.27642821, 712.67236658, 696.27975741,\n",
      "       686.1849128 , 649.98371354, 631.18617055, 631.07502274])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([842.84008478, 850.5771681 , 857.88656147, 759.24070059,\n",
      "       692.35079504, 719.83979052, 745.37251598, 804.07326495,\n",
      "       689.79394523, 795.39479652, 773.4935943 , 733.44333496,\n",
      "       696.48647192, 651.50446061, 760.04488904, 774.24036403,\n",
      "       698.7297109 , 669.98055191, 714.36151018, 834.70411715,\n",
      "       785.46547122, 690.58578351, 723.47617913, 745.1619075 ,\n",
      "       627.8142917 , 709.36185236, 750.69673545, 800.72938966,\n",
      "       880.17979644, 873.23872791])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([793.98585857, 722.50694506, 884.27854574, 835.09507914,\n",
      "       790.9940369 , 784.06169243, 794.03170193, 782.43241217,\n",
      "       755.51672945, 747.93387285, 773.69529753, 760.13137483,\n",
      "       743.52554413, 780.65476182, 780.51270032, 863.92123596,\n",
      "       695.31521813, 799.46781486, 876.53041534, 724.65218252])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([696.27975741, 686.1849128 , 649.98371354, 631.18617055,\n",
      "       631.07502274, 673.75730796, 651.98129603, 673.73626744,\n",
      "       777.07324188, 758.2234969 , 680.18442364, 729.54997166,\n",
      "       678.89672873, 568.29571713, 581.78740582, 555.83498193,\n",
      "       615.91695795, 665.67546333, 761.52671654, 733.19317991,\n",
      "       684.17926491, 723.09299308, 592.89210242, 653.62634842,\n",
      "       724.79417847, 630.92834603, 717.67629451, 728.46205325,\n",
      "       681.62189532, 718.02118063])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([820.5333744 , 755.39247769, 721.11223575, 683.36485669,\n",
      "       648.91690775, 722.89139063, 674.15797097, 690.34147284,\n",
      "       679.89555162, 718.86986728, 732.00333529, 767.83519502,\n",
      "       707.24881753, 689.61478723, 656.32769369, 668.64987017,\n",
      "       719.83064084, 758.57681546, 687.1679321 , 743.60502328])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([692.35079504, 719.83979052, 745.37251598, 804.07326495,\n",
      "       689.79394523, 795.39479652, 773.4935943 , 733.44333496,\n",
      "       696.48647192, 651.50446061, 760.04488904, 774.24036403,\n",
      "       698.7297109 , 669.98055191, 714.36151018, 834.70411715,\n",
      "       785.46547122, 690.58578351, 723.47617913, 745.1619075 ,\n",
      "       627.8142917 , 709.36185236, 750.69673545, 800.72938966,\n",
      "       880.17979644, 873.23872791, 793.98585857, 722.50694506,\n",
      "       884.27854574, 835.09507914])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([790.9940369 , 784.06169243, 794.03170193, 782.43241217,\n",
      "       755.51672945, 747.93387285, 773.69529753, 760.13137483,\n",
      "       743.52554413, 780.65476182, 780.51270032, 863.92123596,\n",
      "       695.31521813, 799.46781486, 876.53041534, 724.65218252,\n",
      "       743.8072481 , 770.72811344, 722.73099485, 733.79990542])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([733.44333496, 696.48647192, 651.50446061, 760.04488904,\n",
      "       774.24036403, 698.7297109 , 669.98055191, 714.36151018,\n",
      "       834.70411715, 785.46547122, 690.58578351, 723.47617913,\n",
      "       745.1619075 , 627.8142917 , 709.36185236, 750.69673545,\n",
      "       800.72938966, 880.17979644, 873.23872791, 793.98585857,\n",
      "       722.50694506, 884.27854574, 835.09507914, 790.9940369 ,\n",
      "       784.06169243, 794.03170193, 782.43241217, 755.51672945,\n",
      "       747.93387285, 773.69529753])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([760.13137483, 743.52554413, 780.65476182, 780.51270032,\n",
      "       863.92123596, 695.31521813, 799.46781486, 876.53041534,\n",
      "       724.65218252, 743.8072481 , 770.72811344, 722.73099485,\n",
      "       733.79990542, 728.76976372, 866.57003017, 887.52255784,\n",
      "       896.85451539, 877.50053638, 775.79215333, 766.04648599])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([732.00333529, 767.83519502, 707.24881753, 689.61478723,\n",
      "       656.32769369, 668.64987017, 719.83064084, 758.57681546,\n",
      "       687.1679321 , 743.60502328, 802.44119057, 771.56266707,\n",
      "       828.12820822, 726.32636326, 651.49920215, 608.68186599,\n",
      "       758.13233494, 784.85025629, 742.43778409, 744.95265199,\n",
      "       681.04995163, 830.47041928, 788.31397118, 749.23453274,\n",
      "       775.92944027, 776.84226997, 761.63590883, 706.47664247,\n",
      "       747.91110009, 842.84008478])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([850.5771681 , 857.88656147, 759.24070059, 692.35079504,\n",
      "       719.83979052, 745.37251598, 804.07326495, 689.79394523,\n",
      "       795.39479652, 773.4935943 , 733.44333496, 696.48647192,\n",
      "       651.50446061, 760.04488904, 774.24036403, 698.7297109 ,\n",
      "       669.98055191, 714.36151018, 834.70411715, 785.46547122])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([696.48647192, 651.50446061, 760.04488904, 774.24036403,\n",
      "       698.7297109 , 669.98055191, 714.36151018, 834.70411715,\n",
      "       785.46547122, 690.58578351, 723.47617913, 745.1619075 ,\n",
      "       627.8142917 , 709.36185236, 750.69673545, 800.72938966,\n",
      "       880.17979644, 873.23872791, 793.98585857, 722.50694506,\n",
      "       884.27854574, 835.09507914, 790.9940369 , 784.06169243,\n",
      "       794.03170193, 782.43241217, 755.51672945, 747.93387285,\n",
      "       773.69529753, 760.13137483])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([743.52554413, 780.65476182, 780.51270032, 863.92123596,\n",
      "       695.31521813, 799.46781486, 876.53041534, 724.65218252,\n",
      "       743.8072481 , 770.72811344, 722.73099485, 733.79990542,\n",
      "       728.76976372, 866.57003017, 887.52255784, 896.85451539,\n",
      "       877.50053638, 775.79215333, 766.04648599, 824.39341901])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([273.27294491, 203.17066281, 217.19312001, 314.43807321,\n",
      "       300.01414871, 227.87519988, 220.79051407, 295.02199764,\n",
      "       255.88545794, 278.47209267, 287.98710364, 252.91959566,\n",
      "       378.32322455, 361.8077385 , 206.60592959, 258.34179417,\n",
      "       255.41322766, 318.51060943, 268.61541097, 272.76425551,\n",
      "       315.14139906, 348.59105903, 256.83218886, 258.63997842,\n",
      "       265.18095026, 255.64151055, 373.23511922, 357.72595578,\n",
      "       254.37809822, 325.97651154])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([427.7355496 , 405.77162346, 261.78093214, 257.79632772,\n",
      "       359.61398809, 300.97891002, 323.07814374, 360.51164299,\n",
      "       286.45978574, 296.09527034, 152.42525801, 197.44811645,\n",
      "       274.68819015, 247.35305589, 374.20690796, 282.19155374,\n",
      "       275.74591981, 318.92733395, 398.74035381, 286.78178474])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([383.56673024, 364.72357312, 434.29575526, 413.08282734,\n",
      "       361.27286765, 370.69375928, 416.44682588, 385.58899509,\n",
      "       355.57554051, 401.83538562, 424.40409635, 391.1802466 ,\n",
      "       378.83606312, 413.5852798 , 345.70142055, 315.11381903,\n",
      "       346.66881669, 386.61950519, 426.58713648, 499.00199322,\n",
      "       496.48184556, 438.03423281, 423.59440741, 375.37548583,\n",
      "       404.09737706, 409.62535098, 437.76060348, 394.47503303,\n",
      "       439.97630195, 517.19346552])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([461.76251444, 457.355711  , 479.62716775, 433.19914647,\n",
      "       443.23925318, 444.05635782, 446.12106416, 405.39701126,\n",
      "       427.84486792, 466.91572959, 527.49026645, 526.50191535,\n",
      "       580.10855533, 460.73538646, 484.74676984, 478.32468609,\n",
      "       567.74701886, 461.14155259, 402.6968913 , 407.19234085])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([315.82157975, 427.89093968, 289.63949291, 301.77052434,\n",
      "       367.48165357, 372.13463675, 324.83322363, 327.12874953,\n",
      "       320.05395004, 310.90629308, 380.879735  , 387.76152403,\n",
      "       330.557742  , 388.11743607, 390.37958736, 408.2225002 ,\n",
      "       410.23820866, 336.26542706, 319.41677054, 386.73270881,\n",
      "       408.68045252, 379.54368873, 374.68737687, 434.71295722,\n",
      "       367.58405072, 389.15715379, 372.35580464, 359.08299146,\n",
      "       423.66672844, 438.84670817])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([437.61444311, 463.16543088, 410.89882285, 419.07923581,\n",
      "       381.30008063, 394.0134984 , 383.27079237, 387.32550047,\n",
      "       417.0279523 , 358.91573723, 476.45969901, 381.52225147,\n",
      "       314.31964945, 421.39247611, 452.38691493, 444.22925323,\n",
      "       442.59260105, 411.5736148 , 354.91586408, 383.56673024])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([571.44576343, 604.4266225 , 577.85684727, 565.12944302,\n",
      "       561.70663192, 546.92000432, 587.34387768, 563.28894392,\n",
      "       603.15946382, 609.79642481, 597.68857407, 561.29017164,\n",
      "       563.99484285, 636.27975237, 653.7286504 , 580.21410777,\n",
      "       603.71020602, 655.75754555, 552.61491167, 614.86465122,\n",
      "       596.29610199, 638.34407776, 596.98361388, 521.09639918,\n",
      "       505.46555349, 589.70267609, 636.73604882, 589.67698049,\n",
      "       644.77612963, 561.24317062])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([596.79057361, 568.82052745, 576.8558281 , 622.05026979,\n",
      "       594.06184224, 602.02666574, 680.36988082, 633.39966005,\n",
      "       675.71884524, 605.40908925, 650.71947409, 727.41228097,\n",
      "       557.18612905, 562.01216424, 655.97105619, 650.10028912,\n",
      "       668.16176824, 631.85717443, 648.09903429, 649.27642821])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([742.43778409, 744.95265199, 681.04995163, 830.47041928,\n",
      "       788.31397118, 749.23453274, 775.92944027, 776.84226997,\n",
      "       761.63590883, 706.47664247, 747.91110009, 842.84008478,\n",
      "       850.5771681 , 857.88656147, 759.24070059, 692.35079504,\n",
      "       719.83979052, 745.37251598, 804.07326495, 689.79394523,\n",
      "       795.39479652, 773.4935943 , 733.44333496, 696.48647192,\n",
      "       651.50446061, 760.04488904, 774.24036403, 698.7297109 ,\n",
      "       669.98055191, 714.36151018])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([834.70411715, 785.46547122, 690.58578351, 723.47617913,\n",
      "       745.1619075 , 627.8142917 , 709.36185236, 750.69673545,\n",
      "       800.72938966, 880.17979644, 873.23872791, 793.98585857,\n",
      "       722.50694506, 884.27854574, 835.09507914, 790.9940369 ,\n",
      "       784.06169243, 794.03170193, 782.43241217, 755.51672945])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([747.91110009, 842.84008478, 850.5771681 , 857.88656147,\n",
      "       759.24070059, 692.35079504, 719.83979052, 745.37251598,\n",
      "       804.07326495, 689.79394523, 795.39479652, 773.4935943 ,\n",
      "       733.44333496, 696.48647192, 651.50446061, 760.04488904,\n",
      "       774.24036403, 698.7297109 , 669.98055191, 714.36151018,\n",
      "       834.70411715, 785.46547122, 690.58578351, 723.47617913,\n",
      "       745.1619075 , 627.8142917 , 709.36185236, 750.69673545,\n",
      "       800.72938966, 880.17979644])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([873.23872791, 793.98585857, 722.50694506, 884.27854574,\n",
      "       835.09507914, 790.9940369 , 784.06169243, 794.03170193,\n",
      "       782.43241217, 755.51672945, 747.93387285, 773.69529753,\n",
      "       760.13137483, 743.52554413, 780.65476182, 780.51270032,\n",
      "       863.92123596, 695.31521813, 799.46781486, 876.53041534])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([509.56832961, 547.35738325, 523.72622733, 493.15355426,\n",
      "       520.45598519, 488.64675133, 495.78048545, 472.70249657,\n",
      "       618.45257776, 589.3399963 , 522.26342527, 549.53276579,\n",
      "       550.7361558 , 541.6098775 , 581.53451462, 607.22224773,\n",
      "       549.87462813, 522.37636622, 533.76513254, 439.37788128,\n",
      "       437.09888259, 592.31584485, 669.82746742, 592.01663525,\n",
      "       597.07230042, 597.51240164, 732.10605797, 694.37287539,\n",
      "       600.22696169, 529.54083307])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([475.68450536, 549.19229931, 538.22986094, 490.72208051,\n",
      "       514.5088088 , 508.05592927, 639.58053533, 658.89585885,\n",
      "       607.11396555, 662.96302643, 623.20154476, 551.67016265,\n",
      "       648.06221241, 647.35774585, 555.63289089, 563.85025964,\n",
      "       543.66405584, 506.29295049, 609.75057181, 707.14155238])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([536.31984124, 451.87212301, 394.45919165, 521.30830427,\n",
      "       532.80160653, 468.19758845, 559.84221298, 535.04153282,\n",
      "       564.85427982, 530.47387032, 609.74709298, 635.44991269,\n",
      "       535.91731397, 555.47851634, 560.26732238, 592.77245291,\n",
      "       492.78699422, 528.47301518, 576.62673148, 449.02573821,\n",
      "       420.73057322, 383.2050939 , 453.90534066, 539.24259306,\n",
      "       605.47121043, 556.67616859, 608.30828456, 489.31267053,\n",
      "       414.13221475, 481.92922581])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([537.03024237, 531.27077547, 424.76439816, 480.27019493,\n",
      "       456.60722947, 534.68799708, 560.27001124, 495.50885821,\n",
      "       490.43221638, 469.54868894, 509.0216617 , 580.5186044 ,\n",
      "       509.56832961, 547.35738325, 523.72622733, 493.15355426,\n",
      "       520.45598519, 488.64675133, 495.78048545, 472.70249657])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([615.91695795, 665.67546333, 761.52671654, 733.19317991,\n",
      "       684.17926491, 723.09299308, 592.89210242, 653.62634842,\n",
      "       724.79417847, 630.92834603, 717.67629451, 728.46205325,\n",
      "       681.62189532, 718.02118063, 820.5333744 , 755.39247769,\n",
      "       721.11223575, 683.36485669, 648.91690775, 722.89139063,\n",
      "       674.15797097, 690.34147284, 679.89555162, 718.86986728,\n",
      "       732.00333529, 767.83519502, 707.24881753, 689.61478723,\n",
      "       656.32769369, 668.64987017])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([719.83064084, 758.57681546, 687.1679321 , 743.60502328,\n",
      "       802.44119057, 771.56266707, 828.12820822, 726.32636326,\n",
      "       651.49920215, 608.68186599, 758.13233494, 784.85025629,\n",
      "       742.43778409, 744.95265199, 681.04995163, 830.47041928,\n",
      "       788.31397118, 749.23453274, 775.92944027, 776.84226997])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([433.19914647, 443.23925318, 444.05635782, 446.12106416,\n",
      "       405.39701126, 427.84486792, 466.91572959, 527.49026645,\n",
      "       526.50191535, 580.10855533, 460.73538646, 484.74676984,\n",
      "       478.32468609, 567.74701886, 461.14155259, 402.6968913 ,\n",
      "       407.19234085, 335.50445663, 383.89448025, 402.829297  ,\n",
      "       448.14808423, 478.51272274, 563.09347579, 552.31094708,\n",
      "       462.87334599, 413.79781564, 472.07663971, 410.40991406,\n",
      "       533.57006609, 564.84733814])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([478.79369029, 382.97566996, 505.07405096, 493.43569082,\n",
      "       538.94192392, 425.49096814, 419.33548104, 463.8273169 ,\n",
      "       481.11941758, 460.64102572, 505.29460783, 443.84386519,\n",
      "       458.00712312, 487.44691126, 515.62869948, 536.31984124,\n",
      "       451.87212301, 394.45919165, 521.30830427, 532.80160653])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([589.3399963 , 522.26342527, 549.53276579, 550.7361558 ,\n",
      "       541.6098775 , 581.53451462, 607.22224773, 549.87462813,\n",
      "       522.37636622, 533.76513254, 439.37788128, 437.09888259,\n",
      "       592.31584485, 669.82746742, 592.01663525, 597.07230042,\n",
      "       597.51240164, 732.10605797, 694.37287539, 600.22696169,\n",
      "       529.54083307, 475.68450536, 549.19229931, 538.22986094,\n",
      "       490.72208051, 514.5088088 , 508.05592927, 639.58053533,\n",
      "       658.89585885, 607.11396555])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([662.96302643, 623.20154476, 551.67016265, 648.06221241,\n",
      "       647.35774585, 555.63289089, 563.85025964, 543.66405584,\n",
      "       506.29295049, 609.75057181, 707.14155238, 571.44576343,\n",
      "       604.4266225 , 577.85684727, 565.12944302, 561.70663192,\n",
      "       546.92000432, 587.34387768, 563.28894392, 603.15946382])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([673.75730796, 651.98129603, 673.73626744, 777.07324188,\n",
      "       758.2234969 , 680.18442364, 729.54997166, 678.89672873,\n",
      "       568.29571713, 581.78740582, 555.83498193, 615.91695795,\n",
      "       665.67546333, 761.52671654, 733.19317991, 684.17926491,\n",
      "       723.09299308, 592.89210242, 653.62634842, 724.79417847,\n",
      "       630.92834603, 717.67629451, 728.46205325, 681.62189532,\n",
      "       718.02118063, 820.5333744 , 755.39247769, 721.11223575,\n",
      "       683.36485669, 648.91690775])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([722.89139063, 674.15797097, 690.34147284, 679.89555162,\n",
      "       718.86986728, 732.00333529, 767.83519502, 707.24881753,\n",
      "       689.61478723, 656.32769369, 668.64987017, 719.83064084,\n",
      "       758.57681546, 687.1679321 , 743.60502328, 802.44119057,\n",
      "       771.56266707, 828.12820822, 726.32636326, 651.49920215])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([830.47041928, 788.31397118, 749.23453274, 775.92944027,\n",
      "       776.84226997, 761.63590883, 706.47664247, 747.91110009,\n",
      "       842.84008478, 850.5771681 , 857.88656147, 759.24070059,\n",
      "       692.35079504, 719.83979052, 745.37251598, 804.07326495,\n",
      "       689.79394523, 795.39479652, 773.4935943 , 733.44333496,\n",
      "       696.48647192, 651.50446061, 760.04488904, 774.24036403,\n",
      "       698.7297109 , 669.98055191, 714.36151018, 834.70411715,\n",
      "       785.46547122, 690.58578351])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([723.47617913, 745.1619075 , 627.8142917 , 709.36185236,\n",
      "       750.69673545, 800.72938966, 880.17979644, 873.23872791,\n",
      "       793.98585857, 722.50694506, 884.27854574, 835.09507914,\n",
      "       790.9940369 , 784.06169243, 794.03170193, 782.43241217,\n",
      "       755.51672945, 747.93387285, 773.69529753, 760.13137483])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([651.50446061, 760.04488904, 774.24036403, 698.7297109 ,\n",
      "       669.98055191, 714.36151018, 834.70411715, 785.46547122,\n",
      "       690.58578351, 723.47617913, 745.1619075 , 627.8142917 ,\n",
      "       709.36185236, 750.69673545, 800.72938966, 880.17979644,\n",
      "       873.23872791, 793.98585857, 722.50694506, 884.27854574,\n",
      "       835.09507914, 790.9940369 , 784.06169243, 794.03170193,\n",
      "       782.43241217, 755.51672945, 747.93387285, 773.69529753,\n",
      "       760.13137483, 743.52554413])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([780.65476182, 780.51270032, 863.92123596, 695.31521813,\n",
      "       799.46781486, 876.53041534, 724.65218252, 743.8072481 ,\n",
      "       770.72811344, 722.73099485, 733.79990542, 728.76976372,\n",
      "       866.57003017, 887.52255784, 896.85451539, 877.50053638,\n",
      "       775.79215333, 766.04648599, 824.39341901, 762.38914573])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([771.56266707, 828.12820822, 726.32636326, 651.49920215,\n",
      "       608.68186599, 758.13233494, 784.85025629, 742.43778409,\n",
      "       744.95265199, 681.04995163, 830.47041928, 788.31397118,\n",
      "       749.23453274, 775.92944027, 776.84226997, 761.63590883,\n",
      "       706.47664247, 747.91110009, 842.84008478, 850.5771681 ,\n",
      "       857.88656147, 759.24070059, 692.35079504, 719.83979052,\n",
      "       745.37251598, 804.07326495, 689.79394523, 795.39479652,\n",
      "       773.4935943 , 733.44333496])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([696.48647192, 651.50446061, 760.04488904, 774.24036403,\n",
      "       698.7297109 , 669.98055191, 714.36151018, 834.70411715,\n",
      "       785.46547122, 690.58578351, 723.47617913, 745.1619075 ,\n",
      "       627.8142917 , 709.36185236, 750.69673545, 800.72938966,\n",
      "       880.17979644, 873.23872791, 793.98585857, 722.50694506])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([681.62189532, 718.02118063, 820.5333744 , 755.39247769,\n",
      "       721.11223575, 683.36485669, 648.91690775, 722.89139063,\n",
      "       674.15797097, 690.34147284, 679.89555162, 718.86986728,\n",
      "       732.00333529, 767.83519502, 707.24881753, 689.61478723,\n",
      "       656.32769369, 668.64987017, 719.83064084, 758.57681546,\n",
      "       687.1679321 , 743.60502328, 802.44119057, 771.56266707,\n",
      "       828.12820822, 726.32636326, 651.49920215, 608.68186599,\n",
      "       758.13233494, 784.85025629])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([742.43778409, 744.95265199, 681.04995163, 830.47041928,\n",
      "       788.31397118, 749.23453274, 775.92944027, 776.84226997,\n",
      "       761.63590883, 706.47664247, 747.91110009, 842.84008478,\n",
      "       850.5771681 , 857.88656147, 759.24070059, 692.35079504,\n",
      "       719.83979052, 745.37251598, 804.07326495, 689.79394523])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([760.04488904, 774.24036403, 698.7297109 , 669.98055191,\n",
      "       714.36151018, 834.70411715, 785.46547122, 690.58578351,\n",
      "       723.47617913, 745.1619075 , 627.8142917 , 709.36185236,\n",
      "       750.69673545, 800.72938966, 880.17979644, 873.23872791,\n",
      "       793.98585857, 722.50694506, 884.27854574, 835.09507914,\n",
      "       790.9940369 , 784.06169243, 794.03170193, 782.43241217,\n",
      "       755.51672945, 747.93387285, 773.69529753, 760.13137483,\n",
      "       743.52554413, 780.65476182])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([780.51270032, 863.92123596, 695.31521813, 799.46781486,\n",
      "       876.53041534, 724.65218252, 743.8072481 , 770.72811344,\n",
      "       722.73099485, 733.79990542, 728.76976372, 866.57003017,\n",
      "       887.52255784, 896.85451539, 877.50053638, 775.79215333,\n",
      "       766.04648599, 824.39341901, 762.38914573, 826.6395924 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([565.12944302, 561.70663192, 546.92000432, 587.34387768,\n",
      "       563.28894392, 603.15946382, 609.79642481, 597.68857407,\n",
      "       561.29017164, 563.99484285, 636.27975237, 653.7286504 ,\n",
      "       580.21410777, 603.71020602, 655.75754555, 552.61491167,\n",
      "       614.86465122, 596.29610199, 638.34407776, 596.98361388,\n",
      "       521.09639918, 505.46555349, 589.70267609, 636.73604882,\n",
      "       589.67698049, 644.77612963, 561.24317062, 596.79057361,\n",
      "       568.82052745, 576.8558281 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([622.05026979, 594.06184224, 602.02666574, 680.36988082,\n",
      "       633.39966005, 675.71884524, 605.40908925, 650.71947409,\n",
      "       727.41228097, 557.18612905, 562.01216424, 655.97105619,\n",
      "       650.10028912, 668.16176824, 631.85717443, 648.09903429,\n",
      "       649.27642821, 712.67236658, 696.27975741, 686.1849128 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([744.95265199, 681.04995163, 830.47041928, 788.31397118,\n",
      "       749.23453274, 775.92944027, 776.84226997, 761.63590883,\n",
      "       706.47664247, 747.91110009, 842.84008478, 850.5771681 ,\n",
      "       857.88656147, 759.24070059, 692.35079504, 719.83979052,\n",
      "       745.37251598, 804.07326495, 689.79394523, 795.39479652,\n",
      "       773.4935943 , 733.44333496, 696.48647192, 651.50446061,\n",
      "       760.04488904, 774.24036403, 698.7297109 , 669.98055191,\n",
      "       714.36151018, 834.70411715])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([785.46547122, 690.58578351, 723.47617913, 745.1619075 ,\n",
      "       627.8142917 , 709.36185236, 750.69673545, 800.72938966,\n",
      "       880.17979644, 873.23872791, 793.98585857, 722.50694506,\n",
      "       884.27854574, 835.09507914, 790.9940369 , 784.06169243,\n",
      "       794.03170193, 782.43241217, 755.51672945, 747.93387285])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([592.01663525, 597.07230042, 597.51240164, 732.10605797,\n",
      "       694.37287539, 600.22696169, 529.54083307, 475.68450536,\n",
      "       549.19229931, 538.22986094, 490.72208051, 514.5088088 ,\n",
      "       508.05592927, 639.58053533, 658.89585885, 607.11396555,\n",
      "       662.96302643, 623.20154476, 551.67016265, 648.06221241,\n",
      "       647.35774585, 555.63289089, 563.85025964, 543.66405584,\n",
      "       506.29295049, 609.75057181, 707.14155238, 571.44576343,\n",
      "       604.4266225 , 577.85684727])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([565.12944302, 561.70663192, 546.92000432, 587.34387768,\n",
      "       563.28894392, 603.15946382, 609.79642481, 597.68857407,\n",
      "       561.29017164, 563.99484285, 636.27975237, 653.7286504 ,\n",
      "       580.21410777, 603.71020602, 655.75754555, 552.61491167,\n",
      "       614.86465122, 596.29610199, 638.34407776, 596.98361388])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([722.89139063, 674.15797097, 690.34147284, 679.89555162,\n",
      "       718.86986728, 732.00333529, 767.83519502, 707.24881753,\n",
      "       689.61478723, 656.32769369, 668.64987017, 719.83064084,\n",
      "       758.57681546, 687.1679321 , 743.60502328, 802.44119057,\n",
      "       771.56266707, 828.12820822, 726.32636326, 651.49920215,\n",
      "       608.68186599, 758.13233494, 784.85025629, 742.43778409,\n",
      "       744.95265199, 681.04995163, 830.47041928, 788.31397118,\n",
      "       749.23453274, 775.92944027])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([776.84226997, 761.63590883, 706.47664247, 747.91110009,\n",
      "       842.84008478, 850.5771681 , 857.88656147, 759.24070059,\n",
      "       692.35079504, 719.83979052, 745.37251598, 804.07326495,\n",
      "       689.79394523, 795.39479652, 773.4935943 , 733.44333496,\n",
      "       696.48647192, 651.50446061, 760.04488904, 774.24036403])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([794.03170193, 782.43241217, 755.51672945, 747.93387285,\n",
      "       773.69529753, 760.13137483, 743.52554413, 780.65476182,\n",
      "       780.51270032, 863.92123596, 695.31521813, 799.46781486,\n",
      "       876.53041534, 724.65218252, 743.8072481 , 770.72811344,\n",
      "       722.73099485, 733.79990542, 728.76976372, 866.57003017,\n",
      "       887.52255784, 896.85451539, 877.50053638, 775.79215333,\n",
      "       766.04648599, 824.39341901, 762.38914573, 826.6395924 ,\n",
      "       817.13219304, 796.22790491])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([846.83172639, 857.07268611, 815.69791161, 875.33317673,\n",
      "       794.92274497, 836.89239776, 867.16387476, 826.78026331,\n",
      "       841.22937628, 774.8583325 , 851.97983988, 839.78847353,\n",
      "       806.18137957, 876.3270062 , 822.22514043, 755.48010844,\n",
      "       729.86087559, 832.20711867, 784.38456467, 902.87694654])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([604.4266225 , 577.85684727, 565.12944302, 561.70663192,\n",
      "       546.92000432, 587.34387768, 563.28894392, 603.15946382,\n",
      "       609.79642481, 597.68857407, 561.29017164, 563.99484285,\n",
      "       636.27975237, 653.7286504 , 580.21410777, 603.71020602,\n",
      "       655.75754555, 552.61491167, 614.86465122, 596.29610199,\n",
      "       638.34407776, 596.98361388, 521.09639918, 505.46555349,\n",
      "       589.70267609, 636.73604882, 589.67698049, 644.77612963,\n",
      "       561.24317062, 596.79057361])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([568.82052745, 576.8558281 , 622.05026979, 594.06184224,\n",
      "       602.02666574, 680.36988082, 633.39966005, 675.71884524,\n",
      "       605.40908925, 650.71947409, 727.41228097, 557.18612905,\n",
      "       562.01216424, 655.97105619, 650.10028912, 668.16176824,\n",
      "       631.85717443, 648.09903429, 649.27642821, 712.67236658])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([784.85025629, 742.43778409, 744.95265199, 681.04995163,\n",
      "       830.47041928, 788.31397118, 749.23453274, 775.92944027,\n",
      "       776.84226997, 761.63590883, 706.47664247, 747.91110009,\n",
      "       842.84008478, 850.5771681 , 857.88656147, 759.24070059,\n",
      "       692.35079504, 719.83979052, 745.37251598, 804.07326495,\n",
      "       689.79394523, 795.39479652, 773.4935943 , 733.44333496,\n",
      "       696.48647192, 651.50446061, 760.04488904, 774.24036403,\n",
      "       698.7297109 , 669.98055191])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([714.36151018, 834.70411715, 785.46547122, 690.58578351,\n",
      "       723.47617913, 745.1619075 , 627.8142917 , 709.36185236,\n",
      "       750.69673545, 800.72938966, 880.17979644, 873.23872791,\n",
      "       793.98585857, 722.50694506, 884.27854574, 835.09507914,\n",
      "       790.9940369 , 784.06169243, 794.03170193, 782.43241217])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([556.67616859, 608.30828456, 489.31267053, 414.13221475,\n",
      "       481.92922581, 537.03024237, 531.27077547, 424.76439816,\n",
      "       480.27019493, 456.60722947, 534.68799708, 560.27001124,\n",
      "       495.50885821, 490.43221638, 469.54868894, 509.0216617 ,\n",
      "       580.5186044 , 509.56832961, 547.35738325, 523.72622733,\n",
      "       493.15355426, 520.45598519, 488.64675133, 495.78048545,\n",
      "       472.70249657, 618.45257776, 589.3399963 , 522.26342527,\n",
      "       549.53276579, 550.7361558 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([541.6098775 , 581.53451462, 607.22224773, 549.87462813,\n",
      "       522.37636622, 533.76513254, 439.37788128, 437.09888259,\n",
      "       592.31584485, 669.82746742, 592.01663525, 597.07230042,\n",
      "       597.51240164, 732.10605797, 694.37287539, 600.22696169,\n",
      "       529.54083307, 475.68450536, 549.19229931, 538.22986094])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([679.89555162, 718.86986728, 732.00333529, 767.83519502,\n",
      "       707.24881753, 689.61478723, 656.32769369, 668.64987017,\n",
      "       719.83064084, 758.57681546, 687.1679321 , 743.60502328,\n",
      "       802.44119057, 771.56266707, 828.12820822, 726.32636326,\n",
      "       651.49920215, 608.68186599, 758.13233494, 784.85025629,\n",
      "       742.43778409, 744.95265199, 681.04995163, 830.47041928,\n",
      "       788.31397118, 749.23453274, 775.92944027, 776.84226997,\n",
      "       761.63590883, 706.47664247])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([747.91110009, 842.84008478, 850.5771681 , 857.88656147,\n",
      "       759.24070059, 692.35079504, 719.83979052, 745.37251598,\n",
      "       804.07326495, 689.79394523, 795.39479652, 773.4935943 ,\n",
      "       733.44333496, 696.48647192, 651.50446061, 760.04488904,\n",
      "       774.24036403, 698.7297109 , 669.98055191, 714.36151018])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([880.17979644, 873.23872791, 793.98585857, 722.50694506,\n",
      "       884.27854574, 835.09507914, 790.9940369 , 784.06169243,\n",
      "       794.03170193, 782.43241217, 755.51672945, 747.93387285,\n",
      "       773.69529753, 760.13137483, 743.52554413, 780.65476182,\n",
      "       780.51270032, 863.92123596, 695.31521813, 799.46781486,\n",
      "       876.53041534, 724.65218252, 743.8072481 , 770.72811344,\n",
      "       722.73099485, 733.79990542, 728.76976372, 866.57003017,\n",
      "       887.52255784, 896.85451539])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([877.50053638, 775.79215333, 766.04648599, 824.39341901,\n",
      "       762.38914573, 826.6395924 , 817.13219304, 796.22790491,\n",
      "       846.83172639, 857.07268611, 815.69791161, 875.33317673,\n",
      "       794.92274497, 836.89239776, 867.16387476, 826.78026331,\n",
      "       841.22937628, 774.8583325 , 851.97983988, 839.78847353])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([581.78740582, 555.83498193, 615.91695795, 665.67546333,\n",
      "       761.52671654, 733.19317991, 684.17926491, 723.09299308,\n",
      "       592.89210242, 653.62634842, 724.79417847, 630.92834603,\n",
      "       717.67629451, 728.46205325, 681.62189532, 718.02118063,\n",
      "       820.5333744 , 755.39247769, 721.11223575, 683.36485669,\n",
      "       648.91690775, 722.89139063, 674.15797097, 690.34147284,\n",
      "       679.89555162, 718.86986728, 732.00333529, 767.83519502,\n",
      "       707.24881753, 689.61478723])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([656.32769369, 668.64987017, 719.83064084, 758.57681546,\n",
      "       687.1679321 , 743.60502328, 802.44119057, 771.56266707,\n",
      "       828.12820822, 726.32636326, 651.49920215, 608.68186599,\n",
      "       758.13233494, 784.85025629, 742.43778409, 744.95265199,\n",
      "       681.04995163, 830.47041928, 788.31397118, 749.23453274])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([505.46555349, 589.70267609, 636.73604882, 589.67698049,\n",
      "       644.77612963, 561.24317062, 596.79057361, 568.82052745,\n",
      "       576.8558281 , 622.05026979, 594.06184224, 602.02666574,\n",
      "       680.36988082, 633.39966005, 675.71884524, 605.40908925,\n",
      "       650.71947409, 727.41228097, 557.18612905, 562.01216424,\n",
      "       655.97105619, 650.10028912, 668.16176824, 631.85717443,\n",
      "       648.09903429, 649.27642821, 712.67236658, 696.27975741,\n",
      "       686.1849128 , 649.98371354])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([631.18617055, 631.07502274, 673.75730796, 651.98129603,\n",
      "       673.73626744, 777.07324188, 758.2234969 , 680.18442364,\n",
      "       729.54997166, 678.89672873, 568.29571713, 581.78740582,\n",
      "       555.83498193, 615.91695795, 665.67546333, 761.52671654,\n",
      "       733.19317991, 684.17926491, 723.09299308, 592.89210242])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([728.46205325, 681.62189532, 718.02118063, 820.5333744 ,\n",
      "       755.39247769, 721.11223575, 683.36485669, 648.91690775,\n",
      "       722.89139063, 674.15797097, 690.34147284, 679.89555162,\n",
      "       718.86986728, 732.00333529, 767.83519502, 707.24881753,\n",
      "       689.61478723, 656.32769369, 668.64987017, 719.83064084,\n",
      "       758.57681546, 687.1679321 , 743.60502328, 802.44119057,\n",
      "       771.56266707, 828.12820822, 726.32636326, 651.49920215,\n",
      "       608.68186599, 758.13233494])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([784.85025629, 742.43778409, 744.95265199, 681.04995163,\n",
      "       830.47041928, 788.31397118, 749.23453274, 775.92944027,\n",
      "       776.84226997, 761.63590883, 706.47664247, 747.91110009,\n",
      "       842.84008478, 850.5771681 , 857.88656147, 759.24070059,\n",
      "       692.35079504, 719.83979052, 745.37251598, 804.07326495])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([795.39479652, 773.4935943 , 733.44333496, 696.48647192,\n",
      "       651.50446061, 760.04488904, 774.24036403, 698.7297109 ,\n",
      "       669.98055191, 714.36151018, 834.70411715, 785.46547122,\n",
      "       690.58578351, 723.47617913, 745.1619075 , 627.8142917 ,\n",
      "       709.36185236, 750.69673545, 800.72938966, 880.17979644,\n",
      "       873.23872791, 793.98585857, 722.50694506, 884.27854574,\n",
      "       835.09507914, 790.9940369 , 784.06169243, 794.03170193,\n",
      "       782.43241217, 755.51672945])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([747.93387285, 773.69529753, 760.13137483, 743.52554413,\n",
      "       780.65476182, 780.51270032, 863.92123596, 695.31521813,\n",
      "       799.46781486, 876.53041534, 724.65218252, 743.8072481 ,\n",
      "       770.72811344, 722.73099485, 733.79990542, 728.76976372,\n",
      "       866.57003017, 887.52255784, 896.85451539, 877.50053638])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([630.92834603, 717.67629451, 728.46205325, 681.62189532,\n",
      "       718.02118063, 820.5333744 , 755.39247769, 721.11223575,\n",
      "       683.36485669, 648.91690775, 722.89139063, 674.15797097,\n",
      "       690.34147284, 679.89555162, 718.86986728, 732.00333529,\n",
      "       767.83519502, 707.24881753, 689.61478723, 656.32769369,\n",
      "       668.64987017, 719.83064084, 758.57681546, 687.1679321 ,\n",
      "       743.60502328, 802.44119057, 771.56266707, 828.12820822,\n",
      "       726.32636326, 651.49920215])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([608.68186599, 758.13233494, 784.85025629, 742.43778409,\n",
      "       744.95265199, 681.04995163, 830.47041928, 788.31397118,\n",
      "       749.23453274, 775.92944027, 776.84226997, 761.63590883,\n",
      "       706.47664247, 747.91110009, 842.84008478, 850.5771681 ,\n",
      "       857.88656147, 759.24070059, 692.35079504, 719.83979052])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([758.57681546, 687.1679321 , 743.60502328, 802.44119057,\n",
      "       771.56266707, 828.12820822, 726.32636326, 651.49920215,\n",
      "       608.68186599, 758.13233494, 784.85025629, 742.43778409,\n",
      "       744.95265199, 681.04995163, 830.47041928, 788.31397118,\n",
      "       749.23453274, 775.92944027, 776.84226997, 761.63590883,\n",
      "       706.47664247, 747.91110009, 842.84008478, 850.5771681 ,\n",
      "       857.88656147, 759.24070059, 692.35079504, 719.83979052,\n",
      "       745.37251598, 804.07326495])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([689.79394523, 795.39479652, 773.4935943 , 733.44333496,\n",
      "       696.48647192, 651.50446061, 760.04488904, 774.24036403,\n",
      "       698.7297109 , 669.98055191, 714.36151018, 834.70411715,\n",
      "       785.46547122, 690.58578351, 723.47617913, 745.1619075 ,\n",
      "       627.8142917 , 709.36185236, 750.69673545, 800.72938966])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([714.36151018, 834.70411715, 785.46547122, 690.58578351,\n",
      "       723.47617913, 745.1619075 , 627.8142917 , 709.36185236,\n",
      "       750.69673545, 800.72938966, 880.17979644, 873.23872791,\n",
      "       793.98585857, 722.50694506, 884.27854574, 835.09507914,\n",
      "       790.9940369 , 784.06169243, 794.03170193, 782.43241217,\n",
      "       755.51672945, 747.93387285, 773.69529753, 760.13137483,\n",
      "       743.52554413, 780.65476182, 780.51270032, 863.92123596,\n",
      "       695.31521813, 799.46781486])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([876.53041534, 724.65218252, 743.8072481 , 770.72811344,\n",
      "       722.73099485, 733.79990542, 728.76976372, 866.57003017,\n",
      "       887.52255784, 896.85451539, 877.50053638, 775.79215333,\n",
      "       766.04648599, 824.39341901, 762.38914573, 826.6395924 ,\n",
      "       817.13219304, 796.22790491, 846.83172639, 857.07268611])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([863.92123596, 695.31521813, 799.46781486, 876.53041534,\n",
      "       724.65218252, 743.8072481 , 770.72811344, 722.73099485,\n",
      "       733.79990542, 728.76976372, 866.57003017, 887.52255784,\n",
      "       896.85451539, 877.50053638, 775.79215333, 766.04648599,\n",
      "       824.39341901, 762.38914573, 826.6395924 , 817.13219304,\n",
      "       796.22790491, 846.83172639, 857.07268611, 815.69791161,\n",
      "       875.33317673, 794.92274497, 836.89239776, 867.16387476,\n",
      "       826.78026331, 841.22937628])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([774.8583325 , 851.97983988, 839.78847353, 806.18137957,\n",
      "       876.3270062 , 822.22514043, 755.48010844, 729.86087559,\n",
      "       832.20711867, 784.38456467, 902.87694654, 773.59564939,\n",
      "       889.41788188, 887.39322146, 850.96512158, 820.95104652,\n",
      "       858.00387261, 855.81095587, 906.99625232, 882.58263097])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([609.79642481, 597.68857407, 561.29017164, 563.99484285,\n",
      "       636.27975237, 653.7286504 , 580.21410777, 603.71020602,\n",
      "       655.75754555, 552.61491167, 614.86465122, 596.29610199,\n",
      "       638.34407776, 596.98361388, 521.09639918, 505.46555349,\n",
      "       589.70267609, 636.73604882, 589.67698049, 644.77612963,\n",
      "       561.24317062, 596.79057361, 568.82052745, 576.8558281 ,\n",
      "       622.05026979, 594.06184224, 602.02666574, 680.36988082,\n",
      "       633.39966005, 675.71884524])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([605.40908925, 650.71947409, 727.41228097, 557.18612905,\n",
      "       562.01216424, 655.97105619, 650.10028912, 668.16176824,\n",
      "       631.85717443, 648.09903429, 649.27642821, 712.67236658,\n",
      "       696.27975741, 686.1849128 , 649.98371354, 631.18617055,\n",
      "       631.07502274, 673.75730796, 651.98129603, 673.73626744])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([770.72811344, 722.73099485, 733.79990542, 728.76976372,\n",
      "       866.57003017, 887.52255784, 896.85451539, 877.50053638,\n",
      "       775.79215333, 766.04648599, 824.39341901, 762.38914573,\n",
      "       826.6395924 , 817.13219304, 796.22790491, 846.83172639,\n",
      "       857.07268611, 815.69791161, 875.33317673, 794.92274497,\n",
      "       836.89239776, 867.16387476, 826.78026331, 841.22937628,\n",
      "       774.8583325 , 851.97983988, 839.78847353, 806.18137957,\n",
      "       876.3270062 , 822.22514043])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([755.48010844, 729.86087559, 832.20711867, 784.38456467,\n",
      "       902.87694654, 773.59564939, 889.41788188, 887.39322146,\n",
      "       850.96512158, 820.95104652, 858.00387261, 855.81095587,\n",
      "       906.99625232, 882.58263097, 867.8616934 , 841.9761503 ,\n",
      "       846.99786451, 871.15717441, 780.21953936, 757.60855562])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([594.06184224, 602.02666574, 680.36988082, 633.39966005,\n",
      "       675.71884524, 605.40908925, 650.71947409, 727.41228097,\n",
      "       557.18612905, 562.01216424, 655.97105619, 650.10028912,\n",
      "       668.16176824, 631.85717443, 648.09903429, 649.27642821,\n",
      "       712.67236658, 696.27975741, 686.1849128 , 649.98371354,\n",
      "       631.18617055, 631.07502274, 673.75730796, 651.98129603,\n",
      "       673.73626744, 777.07324188, 758.2234969 , 680.18442364,\n",
      "       729.54997166, 678.89672873])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([568.29571713, 581.78740582, 555.83498193, 615.91695795,\n",
      "       665.67546333, 761.52671654, 733.19317991, 684.17926491,\n",
      "       723.09299308, 592.89210242, 653.62634842, 724.79417847,\n",
      "       630.92834603, 717.67629451, 728.46205325, 681.62189532,\n",
      "       718.02118063, 820.5333744 , 755.39247769, 721.11223575])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([719.83979052, 745.37251598, 804.07326495, 689.79394523,\n",
      "       795.39479652, 773.4935943 , 733.44333496, 696.48647192,\n",
      "       651.50446061, 760.04488904, 774.24036403, 698.7297109 ,\n",
      "       669.98055191, 714.36151018, 834.70411715, 785.46547122,\n",
      "       690.58578351, 723.47617913, 745.1619075 , 627.8142917 ,\n",
      "       709.36185236, 750.69673545, 800.72938966, 880.17979644,\n",
      "       873.23872791, 793.98585857, 722.50694506, 884.27854574,\n",
      "       835.09507914, 790.9940369 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([784.06169243, 794.03170193, 782.43241217, 755.51672945,\n",
      "       747.93387285, 773.69529753, 760.13137483, 743.52554413,\n",
      "       780.65476182, 780.51270032, 863.92123596, 695.31521813,\n",
      "       799.46781486, 876.53041534, 724.65218252, 743.8072481 ,\n",
      "       770.72811344, 722.73099485, 733.79990542, 728.76976372])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([684.17926491, 723.09299308, 592.89210242, 653.62634842,\n",
      "       724.79417847, 630.92834603, 717.67629451, 728.46205325,\n",
      "       681.62189532, 718.02118063, 820.5333744 , 755.39247769,\n",
      "       721.11223575, 683.36485669, 648.91690775, 722.89139063,\n",
      "       674.15797097, 690.34147284, 679.89555162, 718.86986728,\n",
      "       732.00333529, 767.83519502, 707.24881753, 689.61478723,\n",
      "       656.32769369, 668.64987017, 719.83064084, 758.57681546,\n",
      "       687.1679321 , 743.60502328])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([802.44119057, 771.56266707, 828.12820822, 726.32636326,\n",
      "       651.49920215, 608.68186599, 758.13233494, 784.85025629,\n",
      "       742.43778409, 744.95265199, 681.04995163, 830.47041928,\n",
      "       788.31397118, 749.23453274, 775.92944027, 776.84226997,\n",
      "       761.63590883, 706.47664247, 747.91110009, 842.84008478])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([745.1619075 , 627.8142917 , 709.36185236, 750.69673545,\n",
      "       800.72938966, 880.17979644, 873.23872791, 793.98585857,\n",
      "       722.50694506, 884.27854574, 835.09507914, 790.9940369 ,\n",
      "       784.06169243, 794.03170193, 782.43241217, 755.51672945,\n",
      "       747.93387285, 773.69529753, 760.13137483, 743.52554413,\n",
      "       780.65476182, 780.51270032, 863.92123596, 695.31521813,\n",
      "       799.46781486, 876.53041534, 724.65218252, 743.8072481 ,\n",
      "       770.72811344, 722.73099485])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([733.79990542, 728.76976372, 866.57003017, 887.52255784,\n",
      "       896.85451539, 877.50053638, 775.79215333, 766.04648599,\n",
      "       824.39341901, 762.38914573, 826.6395924 , 817.13219304,\n",
      "       796.22790491, 846.83172639, 857.07268611, 815.69791161,\n",
      "       875.33317673, 794.92274497, 836.89239776, 867.16387476])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([650.10028912, 668.16176824, 631.85717443, 648.09903429,\n",
      "       649.27642821, 712.67236658, 696.27975741, 686.1849128 ,\n",
      "       649.98371354, 631.18617055, 631.07502274, 673.75730796,\n",
      "       651.98129603, 673.73626744, 777.07324188, 758.2234969 ,\n",
      "       680.18442364, 729.54997166, 678.89672873, 568.29571713,\n",
      "       581.78740582, 555.83498193, 615.91695795, 665.67546333,\n",
      "       761.52671654, 733.19317991, 684.17926491, 723.09299308,\n",
      "       592.89210242, 653.62634842])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([724.79417847, 630.92834603, 717.67629451, 728.46205325,\n",
      "       681.62189532, 718.02118063, 820.5333744 , 755.39247769,\n",
      "       721.11223575, 683.36485669, 648.91690775, 722.89139063,\n",
      "       674.15797097, 690.34147284, 679.89555162, 718.86986728,\n",
      "       732.00333529, 767.83519502, 707.24881753, 689.61478723])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([541.6098775 , 581.53451462, 607.22224773, 549.87462813,\n",
      "       522.37636622, 533.76513254, 439.37788128, 437.09888259,\n",
      "       592.31584485, 669.82746742, 592.01663525, 597.07230042,\n",
      "       597.51240164, 732.10605797, 694.37287539, 600.22696169,\n",
      "       529.54083307, 475.68450536, 549.19229931, 538.22986094,\n",
      "       490.72208051, 514.5088088 , 508.05592927, 639.58053533,\n",
      "       658.89585885, 607.11396555, 662.96302643, 623.20154476,\n",
      "       551.67016265, 648.06221241])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([647.35774585, 555.63289089, 563.85025964, 543.66405584,\n",
      "       506.29295049, 609.75057181, 707.14155238, 571.44576343,\n",
      "       604.4266225 , 577.85684727, 565.12944302, 561.70663192,\n",
      "       546.92000432, 587.34387768, 563.28894392, 603.15946382,\n",
      "       609.79642481, 597.68857407, 561.29017164, 563.99484285])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([774.24036403, 698.7297109 , 669.98055191, 714.36151018,\n",
      "       834.70411715, 785.46547122, 690.58578351, 723.47617913,\n",
      "       745.1619075 , 627.8142917 , 709.36185236, 750.69673545,\n",
      "       800.72938966, 880.17979644, 873.23872791, 793.98585857,\n",
      "       722.50694506, 884.27854574, 835.09507914, 790.9940369 ,\n",
      "       784.06169243, 794.03170193, 782.43241217, 755.51672945,\n",
      "       747.93387285, 773.69529753, 760.13137483, 743.52554413,\n",
      "       780.65476182, 780.51270032])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([863.92123596, 695.31521813, 799.46781486, 876.53041534,\n",
      "       724.65218252, 743.8072481 , 770.72811344, 722.73099485,\n",
      "       733.79990542, 728.76976372, 866.57003017, 887.52255784,\n",
      "       896.85451539, 877.50053638, 775.79215333, 766.04648599,\n",
      "       824.39341901, 762.38914573, 826.6395924 , 817.13219304])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([535.04153282, 564.85427982, 530.47387032, 609.74709298,\n",
      "       635.44991269, 535.91731397, 555.47851634, 560.26732238,\n",
      "       592.77245291, 492.78699422, 528.47301518, 576.62673148,\n",
      "       449.02573821, 420.73057322, 383.2050939 , 453.90534066,\n",
      "       539.24259306, 605.47121043, 556.67616859, 608.30828456,\n",
      "       489.31267053, 414.13221475, 481.92922581, 537.03024237,\n",
      "       531.27077547, 424.76439816, 480.27019493, 456.60722947,\n",
      "       534.68799708, 560.27001124])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([495.50885821, 490.43221638, 469.54868894, 509.0216617 ,\n",
      "       580.5186044 , 509.56832961, 547.35738325, 523.72622733,\n",
      "       493.15355426, 520.45598519, 488.64675133, 495.78048545,\n",
      "       472.70249657, 618.45257776, 589.3399963 , 522.26342527,\n",
      "       549.53276579, 550.7361558 , 541.6098775 , 581.53451462])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([689.79394523, 795.39479652, 773.4935943 , 733.44333496,\n",
      "       696.48647192, 651.50446061, 760.04488904, 774.24036403,\n",
      "       698.7297109 , 669.98055191, 714.36151018, 834.70411715,\n",
      "       785.46547122, 690.58578351, 723.47617913, 745.1619075 ,\n",
      "       627.8142917 , 709.36185236, 750.69673545, 800.72938966,\n",
      "       880.17979644, 873.23872791, 793.98585857, 722.50694506,\n",
      "       884.27854574, 835.09507914, 790.9940369 , 784.06169243,\n",
      "       794.03170193, 782.43241217])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([755.51672945, 747.93387285, 773.69529753, 760.13137483,\n",
      "       743.52554413, 780.65476182, 780.51270032, 863.92123596,\n",
      "       695.31521813, 799.46781486, 876.53041534, 724.65218252,\n",
      "       743.8072481 , 770.72811344, 722.73099485, 733.79990542,\n",
      "       728.76976372, 866.57003017, 887.52255784, 896.85451539])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([687.1679321 , 743.60502328, 802.44119057, 771.56266707,\n",
      "       828.12820822, 726.32636326, 651.49920215, 608.68186599,\n",
      "       758.13233494, 784.85025629, 742.43778409, 744.95265199,\n",
      "       681.04995163, 830.47041928, 788.31397118, 749.23453274,\n",
      "       775.92944027, 776.84226997, 761.63590883, 706.47664247,\n",
      "       747.91110009, 842.84008478, 850.5771681 , 857.88656147,\n",
      "       759.24070059, 692.35079504, 719.83979052, 745.37251598,\n",
      "       804.07326495, 689.79394523])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([795.39479652, 773.4935943 , 733.44333496, 696.48647192,\n",
      "       651.50446061, 760.04488904, 774.24036403, 698.7297109 ,\n",
      "       669.98055191, 714.36151018, 834.70411715, 785.46547122,\n",
      "       690.58578351, 723.47617913, 745.1619075 , 627.8142917 ,\n",
      "       709.36185236, 750.69673545, 800.72938966, 880.17979644])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([649.27642821, 712.67236658, 696.27975741, 686.1849128 ,\n",
      "       649.98371354, 631.18617055, 631.07502274, 673.75730796,\n",
      "       651.98129603, 673.73626744, 777.07324188, 758.2234969 ,\n",
      "       680.18442364, 729.54997166, 678.89672873, 568.29571713,\n",
      "       581.78740582, 555.83498193, 615.91695795, 665.67546333,\n",
      "       761.52671654, 733.19317991, 684.17926491, 723.09299308,\n",
      "       592.89210242, 653.62634842, 724.79417847, 630.92834603,\n",
      "       717.67629451, 728.46205325])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([681.62189532, 718.02118063, 820.5333744 , 755.39247769,\n",
      "       721.11223575, 683.36485669, 648.91690775, 722.89139063,\n",
      "       674.15797097, 690.34147284, 679.89555162, 718.86986728,\n",
      "       732.00333529, 767.83519502, 707.24881753, 689.61478723,\n",
      "       656.32769369, 668.64987017, 719.83064084, 758.57681546])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([820.5333744 , 755.39247769, 721.11223575, 683.36485669,\n",
      "       648.91690775, 722.89139063, 674.15797097, 690.34147284,\n",
      "       679.89555162, 718.86986728, 732.00333529, 767.83519502,\n",
      "       707.24881753, 689.61478723, 656.32769369, 668.64987017,\n",
      "       719.83064084, 758.57681546, 687.1679321 , 743.60502328,\n",
      "       802.44119057, 771.56266707, 828.12820822, 726.32636326,\n",
      "       651.49920215, 608.68186599, 758.13233494, 784.85025629,\n",
      "       742.43778409, 744.95265199])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([681.04995163, 830.47041928, 788.31397118, 749.23453274,\n",
      "       775.92944027, 776.84226997, 761.63590883, 706.47664247,\n",
      "       747.91110009, 842.84008478, 850.5771681 , 857.88656147,\n",
      "       759.24070059, 692.35079504, 719.83979052, 745.37251598,\n",
      "       804.07326495, 689.79394523, 795.39479652, 773.4935943 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([655.75754555, 552.61491167, 614.86465122, 596.29610199,\n",
      "       638.34407776, 596.98361388, 521.09639918, 505.46555349,\n",
      "       589.70267609, 636.73604882, 589.67698049, 644.77612963,\n",
      "       561.24317062, 596.79057361, 568.82052745, 576.8558281 ,\n",
      "       622.05026979, 594.06184224, 602.02666574, 680.36988082,\n",
      "       633.39966005, 675.71884524, 605.40908925, 650.71947409,\n",
      "       727.41228097, 557.18612905, 562.01216424, 655.97105619,\n",
      "       650.10028912, 668.16176824])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([631.85717443, 648.09903429, 649.27642821, 712.67236658,\n",
      "       696.27975741, 686.1849128 , 649.98371354, 631.18617055,\n",
      "       631.07502274, 673.75730796, 651.98129603, 673.73626744,\n",
      "       777.07324188, 758.2234969 , 680.18442364, 729.54997166,\n",
      "       678.89672873, 568.29571713, 581.78740582, 555.83498193])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([653.62634842, 724.79417847, 630.92834603, 717.67629451,\n",
      "       728.46205325, 681.62189532, 718.02118063, 820.5333744 ,\n",
      "       755.39247769, 721.11223575, 683.36485669, 648.91690775,\n",
      "       722.89139063, 674.15797097, 690.34147284, 679.89555162,\n",
      "       718.86986728, 732.00333529, 767.83519502, 707.24881753,\n",
      "       689.61478723, 656.32769369, 668.64987017, 719.83064084,\n",
      "       758.57681546, 687.1679321 , 743.60502328, 802.44119057,\n",
      "       771.56266707, 828.12820822])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([726.32636326, 651.49920215, 608.68186599, 758.13233494,\n",
      "       784.85025629, 742.43778409, 744.95265199, 681.04995163,\n",
      "       830.47041928, 788.31397118, 749.23453274, 775.92944027,\n",
      "       776.84226997, 761.63590883, 706.47664247, 747.91110009,\n",
      "       842.84008478, 850.5771681 , 857.88656147, 759.24070059])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([712.67236658, 696.27975741, 686.1849128 , 649.98371354,\n",
      "       631.18617055, 631.07502274, 673.75730796, 651.98129603,\n",
      "       673.73626744, 777.07324188, 758.2234969 , 680.18442364,\n",
      "       729.54997166, 678.89672873, 568.29571713, 581.78740582,\n",
      "       555.83498193, 615.91695795, 665.67546333, 761.52671654,\n",
      "       733.19317991, 684.17926491, 723.09299308, 592.89210242,\n",
      "       653.62634842, 724.79417847, 630.92834603, 717.67629451,\n",
      "       728.46205325, 681.62189532])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([718.02118063, 820.5333744 , 755.39247769, 721.11223575,\n",
      "       683.36485669, 648.91690775, 722.89139063, 674.15797097,\n",
      "       690.34147284, 679.89555162, 718.86986728, 732.00333529,\n",
      "       767.83519502, 707.24881753, 689.61478723, 656.32769369,\n",
      "       668.64987017, 719.83064084, 758.57681546, 687.1679321 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([800.72938966, 880.17979644, 873.23872791, 793.98585857,\n",
      "       722.50694506, 884.27854574, 835.09507914, 790.9940369 ,\n",
      "       784.06169243, 794.03170193, 782.43241217, 755.51672945,\n",
      "       747.93387285, 773.69529753, 760.13137483, 743.52554413,\n",
      "       780.65476182, 780.51270032, 863.92123596, 695.31521813,\n",
      "       799.46781486, 876.53041534, 724.65218252, 743.8072481 ,\n",
      "       770.72811344, 722.73099485, 733.79990542, 728.76976372,\n",
      "       866.57003017, 887.52255784])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([896.85451539, 877.50053638, 775.79215333, 766.04648599,\n",
      "       824.39341901, 762.38914573, 826.6395924 , 817.13219304,\n",
      "       796.22790491, 846.83172639, 857.07268611, 815.69791161,\n",
      "       875.33317673, 794.92274497, 836.89239776, 867.16387476,\n",
      "       826.78026331, 841.22937628, 774.8583325 , 851.97983988])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([826.6395924 , 817.13219304, 796.22790491, 846.83172639,\n",
      "       857.07268611, 815.69791161, 875.33317673, 794.92274497,\n",
      "       836.89239776, 867.16387476, 826.78026331, 841.22937628,\n",
      "       774.8583325 , 851.97983988, 839.78847353, 806.18137957,\n",
      "       876.3270062 , 822.22514043, 755.48010844, 729.86087559,\n",
      "       832.20711867, 784.38456467, 902.87694654, 773.59564939,\n",
      "       889.41788188, 887.39322146, 850.96512158, 820.95104652,\n",
      "       858.00387261, 855.81095587])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([906.99625232, 882.58263097, 867.8616934 , 841.9761503 ,\n",
      "       846.99786451, 871.15717441, 780.21953936, 757.60855562,\n",
      "       864.58609949, 881.58160417, 859.3996499 , 862.91979102,\n",
      "       883.65609382, 848.05515331, 819.54479791, 861.61104418,\n",
      "       823.77193688, 871.634727  , 794.34800497, 890.0528615 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([493.43569082, 538.94192392, 425.49096814, 419.33548104,\n",
      "       463.8273169 , 481.11941758, 460.64102572, 505.29460783,\n",
      "       443.84386519, 458.00712312, 487.44691126, 515.62869948,\n",
      "       536.31984124, 451.87212301, 394.45919165, 521.30830427,\n",
      "       532.80160653, 468.19758845, 559.84221298, 535.04153282,\n",
      "       564.85427982, 530.47387032, 609.74709298, 635.44991269,\n",
      "       535.91731397, 555.47851634, 560.26732238, 592.77245291,\n",
      "       492.78699422, 528.47301518])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([576.62673148, 449.02573821, 420.73057322, 383.2050939 ,\n",
      "       453.90534066, 539.24259306, 605.47121043, 556.67616859,\n",
      "       608.30828456, 489.31267053, 414.13221475, 481.92922581,\n",
      "       537.03024237, 531.27077547, 424.76439816, 480.27019493,\n",
      "       456.60722947, 534.68799708, 560.27001124, 495.50885821])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([707.24881753, 689.61478723, 656.32769369, 668.64987017,\n",
      "       719.83064084, 758.57681546, 687.1679321 , 743.60502328,\n",
      "       802.44119057, 771.56266707, 828.12820822, 726.32636326,\n",
      "       651.49920215, 608.68186599, 758.13233494, 784.85025629,\n",
      "       742.43778409, 744.95265199, 681.04995163, 830.47041928,\n",
      "       788.31397118, 749.23453274, 775.92944027, 776.84226997,\n",
      "       761.63590883, 706.47664247, 747.91110009, 842.84008478,\n",
      "       850.5771681 , 857.88656147])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([759.24070059, 692.35079504, 719.83979052, 745.37251598,\n",
      "       804.07326495, 689.79394523, 795.39479652, 773.4935943 ,\n",
      "       733.44333496, 696.48647192, 651.50446061, 760.04488904,\n",
      "       774.24036403, 698.7297109 , 669.98055191, 714.36151018,\n",
      "       834.70411715, 785.46547122, 690.58578351, 723.47617913])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([788.31397118, 749.23453274, 775.92944027, 776.84226997,\n",
      "       761.63590883, 706.47664247, 747.91110009, 842.84008478,\n",
      "       850.5771681 , 857.88656147, 759.24070059, 692.35079504,\n",
      "       719.83979052, 745.37251598, 804.07326495, 689.79394523,\n",
      "       795.39479652, 773.4935943 , 733.44333496, 696.48647192,\n",
      "       651.50446061, 760.04488904, 774.24036403, 698.7297109 ,\n",
      "       669.98055191, 714.36151018, 834.70411715, 785.46547122,\n",
      "       690.58578351, 723.47617913])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([745.1619075 , 627.8142917 , 709.36185236, 750.69673545,\n",
      "       800.72938966, 880.17979644, 873.23872791, 793.98585857,\n",
      "       722.50694506, 884.27854574, 835.09507914, 790.9940369 ,\n",
      "       784.06169243, 794.03170193, 782.43241217, 755.51672945,\n",
      "       747.93387285, 773.69529753, 760.13137483, 743.52554413])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([887.52255784, 896.85451539, 877.50053638, 775.79215333,\n",
      "       766.04648599, 824.39341901, 762.38914573, 826.6395924 ,\n",
      "       817.13219304, 796.22790491, 846.83172639, 857.07268611,\n",
      "       815.69791161, 875.33317673, 794.92274497, 836.89239776,\n",
      "       867.16387476, 826.78026331, 841.22937628, 774.8583325 ,\n",
      "       851.97983988, 839.78847353, 806.18137957, 876.3270062 ,\n",
      "       822.22514043, 755.48010844, 729.86087559, 832.20711867,\n",
      "       784.38456467, 902.87694654])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([773.59564939, 889.41788188, 887.39322146, 850.96512158,\n",
      "       820.95104652, 858.00387261, 855.81095587, 906.99625232,\n",
      "       882.58263097, 867.8616934 , 841.9761503 , 846.99786451,\n",
      "       871.15717441, 780.21953936, 757.60855562, 864.58609949,\n",
      "       881.58160417, 859.3996499 , 862.91979102, 883.65609382])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([758.2234969 , 680.18442364, 729.54997166, 678.89672873,\n",
      "       568.29571713, 581.78740582, 555.83498193, 615.91695795,\n",
      "       665.67546333, 761.52671654, 733.19317991, 684.17926491,\n",
      "       723.09299308, 592.89210242, 653.62634842, 724.79417847,\n",
      "       630.92834603, 717.67629451, 728.46205325, 681.62189532,\n",
      "       718.02118063, 820.5333744 , 755.39247769, 721.11223575,\n",
      "       683.36485669, 648.91690775, 722.89139063, 674.15797097,\n",
      "       690.34147284, 679.89555162])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([718.86986728, 732.00333529, 767.83519502, 707.24881753,\n",
      "       689.61478723, 656.32769369, 668.64987017, 719.83064084,\n",
      "       758.57681546, 687.1679321 , 743.60502328, 802.44119057,\n",
      "       771.56266707, 828.12820822, 726.32636326, 651.49920215,\n",
      "       608.68186599, 758.13233494, 784.85025629, 742.43778409])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([775.79215333, 766.04648599, 824.39341901, 762.38914573,\n",
      "       826.6395924 , 817.13219304, 796.22790491, 846.83172639,\n",
      "       857.07268611, 815.69791161, 875.33317673, 794.92274497,\n",
      "       836.89239776, 867.16387476, 826.78026331, 841.22937628,\n",
      "       774.8583325 , 851.97983988, 839.78847353, 806.18137957,\n",
      "       876.3270062 , 822.22514043, 755.48010844, 729.86087559,\n",
      "       832.20711867, 784.38456467, 902.87694654, 773.59564939,\n",
      "       889.41788188, 887.39322146])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([850.96512158, 820.95104652, 858.00387261, 855.81095587,\n",
      "       906.99625232, 882.58263097, 867.8616934 , 841.9761503 ,\n",
      "       846.99786451, 871.15717441, 780.21953936, 757.60855562,\n",
      "       864.58609949, 881.58160417, 859.3996499 , 862.91979102,\n",
      "       883.65609382, 848.05515331, 819.54479791, 861.61104418])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([709.36185236, 750.69673545, 800.72938966, 880.17979644,\n",
      "       873.23872791, 793.98585857, 722.50694506, 884.27854574,\n",
      "       835.09507914, 790.9940369 , 784.06169243, 794.03170193,\n",
      "       782.43241217, 755.51672945, 747.93387285, 773.69529753,\n",
      "       760.13137483, 743.52554413, 780.65476182, 780.51270032,\n",
      "       863.92123596, 695.31521813, 799.46781486, 876.53041534,\n",
      "       724.65218252, 743.8072481 , 770.72811344, 722.73099485,\n",
      "       733.79990542, 728.76976372])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([866.57003017, 887.52255784, 896.85451539, 877.50053638,\n",
      "       775.79215333, 766.04648599, 824.39341901, 762.38914573,\n",
      "       826.6395924 , 817.13219304, 796.22790491, 846.83172639,\n",
      "       857.07268611, 815.69791161, 875.33317673, 794.92274497,\n",
      "       836.89239776, 867.16387476, 826.78026331, 841.22937628])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([776.84226997, 761.63590883, 706.47664247, 747.91110009,\n",
      "       842.84008478, 850.5771681 , 857.88656147, 759.24070059,\n",
      "       692.35079504, 719.83979052, 745.37251598, 804.07326495,\n",
      "       689.79394523, 795.39479652, 773.4935943 , 733.44333496,\n",
      "       696.48647192, 651.50446061, 760.04488904, 774.24036403,\n",
      "       698.7297109 , 669.98055191, 714.36151018, 834.70411715,\n",
      "       785.46547122, 690.58578351, 723.47617913, 745.1619075 ,\n",
      "       627.8142917 , 709.36185236])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([750.69673545, 800.72938966, 880.17979644, 873.23872791,\n",
      "       793.98585857, 722.50694506, 884.27854574, 835.09507914,\n",
      "       790.9940369 , 784.06169243, 794.03170193, 782.43241217,\n",
      "       755.51672945, 747.93387285, 773.69529753, 760.13137483,\n",
      "       743.52554413, 780.65476182, 780.51270032, 863.92123596])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([592.89210242, 653.62634842, 724.79417847, 630.92834603,\n",
      "       717.67629451, 728.46205325, 681.62189532, 718.02118063,\n",
      "       820.5333744 , 755.39247769, 721.11223575, 683.36485669,\n",
      "       648.91690775, 722.89139063, 674.15797097, 690.34147284,\n",
      "       679.89555162, 718.86986728, 732.00333529, 767.83519502,\n",
      "       707.24881753, 689.61478723, 656.32769369, 668.64987017,\n",
      "       719.83064084, 758.57681546, 687.1679321 , 743.60502328,\n",
      "       802.44119057, 771.56266707])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([828.12820822, 726.32636326, 651.49920215, 608.68186599,\n",
      "       758.13233494, 784.85025629, 742.43778409, 744.95265199,\n",
      "       681.04995163, 830.47041928, 788.31397118, 749.23453274,\n",
      "       775.92944027, 776.84226997, 761.63590883, 706.47664247,\n",
      "       747.91110009, 842.84008478, 850.5771681 , 857.88656147])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([662.96302643, 623.20154476, 551.67016265, 648.06221241,\n",
      "       647.35774585, 555.63289089, 563.85025964, 543.66405584,\n",
      "       506.29295049, 609.75057181, 707.14155238, 571.44576343,\n",
      "       604.4266225 , 577.85684727, 565.12944302, 561.70663192,\n",
      "       546.92000432, 587.34387768, 563.28894392, 603.15946382,\n",
      "       609.79642481, 597.68857407, 561.29017164, 563.99484285,\n",
      "       636.27975237, 653.7286504 , 580.21410777, 603.71020602,\n",
      "       655.75754555, 552.61491167])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([614.86465122, 596.29610199, 638.34407776, 596.98361388,\n",
      "       521.09639918, 505.46555349, 589.70267609, 636.73604882,\n",
      "       589.67698049, 644.77612963, 561.24317062, 596.79057361,\n",
      "       568.82052745, 576.8558281 , 622.05026979, 594.06184224,\n",
      "       602.02666574, 680.36988082, 633.39966005, 675.71884524])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([817.13219304, 796.22790491, 846.83172639, 857.07268611,\n",
      "       815.69791161, 875.33317673, 794.92274497, 836.89239776,\n",
      "       867.16387476, 826.78026331, 841.22937628, 774.8583325 ,\n",
      "       851.97983988, 839.78847353, 806.18137957, 876.3270062 ,\n",
      "       822.22514043, 755.48010844, 729.86087559, 832.20711867,\n",
      "       784.38456467, 902.87694654, 773.59564939, 889.41788188,\n",
      "       887.39322146, 850.96512158, 820.95104652, 858.00387261,\n",
      "       855.81095587, 906.99625232])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([882.58263097, 867.8616934 , 841.9761503 , 846.99786451,\n",
      "       871.15717441, 780.21953936, 757.60855562, 864.58609949,\n",
      "       881.58160417, 859.3996499 , 862.91979102, 883.65609382,\n",
      "       848.05515331, 819.54479791, 861.61104418, 823.77193688,\n",
      "       871.634727  , 794.34800497, 890.0528615 , 916.1347314 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([877.50053638, 775.79215333, 766.04648599, 824.39341901,\n",
      "       762.38914573, 826.6395924 , 817.13219304, 796.22790491,\n",
      "       846.83172639, 857.07268611, 815.69791161, 875.33317673,\n",
      "       794.92274497, 836.89239776, 867.16387476, 826.78026331,\n",
      "       841.22937628, 774.8583325 , 851.97983988, 839.78847353,\n",
      "       806.18137957, 876.3270062 , 822.22514043, 755.48010844,\n",
      "       729.86087559, 832.20711867, 784.38456467, 902.87694654,\n",
      "       773.59564939, 889.41788188])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([887.39322146, 850.96512158, 820.95104652, 858.00387261,\n",
      "       855.81095587, 906.99625232, 882.58263097, 867.8616934 ,\n",
      "       841.9761503 , 846.99786451, 871.15717441, 780.21953936,\n",
      "       757.60855562, 864.58609949, 881.58160417, 859.3996499 ,\n",
      "       862.91979102, 883.65609382, 848.05515331, 819.54479791])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([884.27854574, 835.09507914, 790.9940369 , 784.06169243,\n",
      "       794.03170193, 782.43241217, 755.51672945, 747.93387285,\n",
      "       773.69529753, 760.13137483, 743.52554413, 780.65476182,\n",
      "       780.51270032, 863.92123596, 695.31521813, 799.46781486,\n",
      "       876.53041534, 724.65218252, 743.8072481 , 770.72811344,\n",
      "       722.73099485, 733.79990542, 728.76976372, 866.57003017,\n",
      "       887.52255784, 896.85451539, 877.50053638, 775.79215333,\n",
      "       766.04648599, 824.39341901])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([762.38914573, 826.6395924 , 817.13219304, 796.22790491,\n",
      "       846.83172639, 857.07268611, 815.69791161, 875.33317673,\n",
      "       794.92274497, 836.89239776, 867.16387476, 826.78026331,\n",
      "       841.22937628, 774.8583325 , 851.97983988, 839.78847353,\n",
      "       806.18137957, 876.3270062 , 822.22514043, 755.48010844])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([698.7297109 , 669.98055191, 714.36151018, 834.70411715,\n",
      "       785.46547122, 690.58578351, 723.47617913, 745.1619075 ,\n",
      "       627.8142917 , 709.36185236, 750.69673545, 800.72938966,\n",
      "       880.17979644, 873.23872791, 793.98585857, 722.50694506,\n",
      "       884.27854574, 835.09507914, 790.9940369 , 784.06169243,\n",
      "       794.03170193, 782.43241217, 755.51672945, 747.93387285,\n",
      "       773.69529753, 760.13137483, 743.52554413, 780.65476182,\n",
      "       780.51270032, 863.92123596])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([695.31521813, 799.46781486, 876.53041534, 724.65218252,\n",
      "       743.8072481 , 770.72811344, 722.73099485, 733.79990542,\n",
      "       728.76976372, 866.57003017, 887.52255784, 896.85451539,\n",
      "       877.50053638, 775.79215333, 766.04648599, 824.39341901,\n",
      "       762.38914573, 826.6395924 , 817.13219304, 796.22790491])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([824.39341901, 762.38914573, 826.6395924 , 817.13219304,\n",
      "       796.22790491, 846.83172639, 857.07268611, 815.69791161,\n",
      "       875.33317673, 794.92274497, 836.89239776, 867.16387476,\n",
      "       826.78026331, 841.22937628, 774.8583325 , 851.97983988,\n",
      "       839.78847353, 806.18137957, 876.3270062 , 822.22514043,\n",
      "       755.48010844, 729.86087559, 832.20711867, 784.38456467,\n",
      "       902.87694654, 773.59564939, 889.41788188, 887.39322146,\n",
      "       850.96512158, 820.95104652])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([858.00387261, 855.81095587, 906.99625232, 882.58263097,\n",
      "       867.8616934 , 841.9761503 , 846.99786451, 871.15717441,\n",
      "       780.21953936, 757.60855562, 864.58609949, 881.58160417,\n",
      "       859.3996499 , 862.91979102, 883.65609382, 848.05515331,\n",
      "       819.54479791, 861.61104418, 823.77193688, 871.634727  ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([683.36485669, 648.91690775, 722.89139063, 674.15797097,\n",
      "       690.34147284, 679.89555162, 718.86986728, 732.00333529,\n",
      "       767.83519502, 707.24881753, 689.61478723, 656.32769369,\n",
      "       668.64987017, 719.83064084, 758.57681546, 687.1679321 ,\n",
      "       743.60502328, 802.44119057, 771.56266707, 828.12820822,\n",
      "       726.32636326, 651.49920215, 608.68186599, 758.13233494,\n",
      "       784.85025629, 742.43778409, 744.95265199, 681.04995163,\n",
      "       830.47041928, 788.31397118])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([749.23453274, 775.92944027, 776.84226997, 761.63590883,\n",
      "       706.47664247, 747.91110009, 842.84008478, 850.5771681 ,\n",
      "       857.88656147, 759.24070059, 692.35079504, 719.83979052,\n",
      "       745.37251598, 804.07326495, 689.79394523, 795.39479652,\n",
      "       773.4935943 , 733.44333496, 696.48647192, 651.50446061])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([729.86087559, 832.20711867, 784.38456467, 902.87694654,\n",
      "       773.59564939, 889.41788188, 887.39322146, 850.96512158,\n",
      "       820.95104652, 858.00387261, 855.81095587, 906.99625232,\n",
      "       882.58263097, 867.8616934 , 841.9761503 , 846.99786451,\n",
      "       871.15717441, 780.21953936, 757.60855562, 864.58609949,\n",
      "       881.58160417, 859.3996499 , 862.91979102, 883.65609382,\n",
      "       848.05515331, 819.54479791, 861.61104418, 823.77193688,\n",
      "       871.634727  , 794.34800497])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([890.0528615 , 916.1347314 , 901.84355457, 934.63290199,\n",
      "       984.66471466, 970.16030028, 818.00754688, 784.76478819,\n",
      "       821.16300342, 868.15554952, 909.64502365, 861.98607758,\n",
      "       884.23519565, 854.64268169, 844.90353406, 807.12054261,\n",
      "       815.2239179 , 802.85865266, 814.48982293, 923.60552682])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([766.04648599, 824.39341901, 762.38914573, 826.6395924 ,\n",
      "       817.13219304, 796.22790491, 846.83172639, 857.07268611,\n",
      "       815.69791161, 875.33317673, 794.92274497, 836.89239776,\n",
      "       867.16387476, 826.78026331, 841.22937628, 774.8583325 ,\n",
      "       851.97983988, 839.78847353, 806.18137957, 876.3270062 ,\n",
      "       822.22514043, 755.48010844, 729.86087559, 832.20711867,\n",
      "       784.38456467, 902.87694654, 773.59564939, 889.41788188,\n",
      "       887.39322146, 850.96512158])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([820.95104652, 858.00387261, 855.81095587, 906.99625232,\n",
      "       882.58263097, 867.8616934 , 841.9761503 , 846.99786451,\n",
      "       871.15717441, 780.21953936, 757.60855562, 864.58609949,\n",
      "       881.58160417, 859.3996499 , 862.91979102, 883.65609382,\n",
      "       848.05515331, 819.54479791, 861.61104418, 823.77193688])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([743.52554413, 780.65476182, 780.51270032, 863.92123596,\n",
      "       695.31521813, 799.46781486, 876.53041534, 724.65218252,\n",
      "       743.8072481 , 770.72811344, 722.73099485, 733.79990542,\n",
      "       728.76976372, 866.57003017, 887.52255784, 896.85451539,\n",
      "       877.50053638, 775.79215333, 766.04648599, 824.39341901,\n",
      "       762.38914573, 826.6395924 , 817.13219304, 796.22790491,\n",
      "       846.83172639, 857.07268611, 815.69791161, 875.33317673,\n",
      "       794.92274497, 836.89239776])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([867.16387476, 826.78026331, 841.22937628, 774.8583325 ,\n",
      "       851.97983988, 839.78847353, 806.18137957, 876.3270062 ,\n",
      "       822.22514043, 755.48010844, 729.86087559, 832.20711867,\n",
      "       784.38456467, 902.87694654, 773.59564939, 889.41788188,\n",
      "       887.39322146, 850.96512158, 820.95104652, 858.00387261])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([580.5186044 , 509.56832961, 547.35738325, 523.72622733,\n",
      "       493.15355426, 520.45598519, 488.64675133, 495.78048545,\n",
      "       472.70249657, 618.45257776, 589.3399963 , 522.26342527,\n",
      "       549.53276579, 550.7361558 , 541.6098775 , 581.53451462,\n",
      "       607.22224773, 549.87462813, 522.37636622, 533.76513254,\n",
      "       439.37788128, 437.09888259, 592.31584485, 669.82746742,\n",
      "       592.01663525, 597.07230042, 597.51240164, 732.10605797,\n",
      "       694.37287539, 600.22696169])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([529.54083307, 475.68450536, 549.19229931, 538.22986094,\n",
      "       490.72208051, 514.5088088 , 508.05592927, 639.58053533,\n",
      "       658.89585885, 607.11396555, 662.96302643, 623.20154476,\n",
      "       551.67016265, 648.06221241, 647.35774585, 555.63289089,\n",
      "       563.85025964, 543.66405584, 506.29295049, 609.75057181])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([835.09507914, 790.9940369 , 784.06169243, 794.03170193,\n",
      "       782.43241217, 755.51672945, 747.93387285, 773.69529753,\n",
      "       760.13137483, 743.52554413, 780.65476182, 780.51270032,\n",
      "       863.92123596, 695.31521813, 799.46781486, 876.53041534,\n",
      "       724.65218252, 743.8072481 , 770.72811344, 722.73099485,\n",
      "       733.79990542, 728.76976372, 866.57003017, 887.52255784,\n",
      "       896.85451539, 877.50053638, 775.79215333, 766.04648599,\n",
      "       824.39341901, 762.38914573])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([826.6395924 , 817.13219304, 796.22790491, 846.83172639,\n",
      "       857.07268611, 815.69791161, 875.33317673, 794.92274497,\n",
      "       836.89239776, 867.16387476, 826.78026331, 841.22937628,\n",
      "       774.8583325 , 851.97983988, 839.78847353, 806.18137957,\n",
      "       876.3270062 , 822.22514043, 755.48010844, 729.86087559])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([806.18137957, 876.3270062 , 822.22514043, 755.48010844,\n",
      "       729.86087559, 832.20711867, 784.38456467, 902.87694654,\n",
      "       773.59564939, 889.41788188, 887.39322146, 850.96512158,\n",
      "       820.95104652, 858.00387261, 855.81095587, 906.99625232,\n",
      "       882.58263097, 867.8616934 , 841.9761503 , 846.99786451,\n",
      "       871.15717441, 780.21953936, 757.60855562, 864.58609949,\n",
      "       881.58160417, 859.3996499 , 862.91979102, 883.65609382,\n",
      "       848.05515331, 819.54479791])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([861.61104418, 823.77193688, 871.634727  , 794.34800497,\n",
      "       890.0528615 , 916.1347314 , 901.84355457, 934.63290199,\n",
      "       984.66471466, 970.16030028, 818.00754688, 784.76478819,\n",
      "       821.16300342, 868.15554952, 909.64502365, 861.98607758,\n",
      "       884.23519565, 854.64268169, 844.90353406, 807.12054261])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([867.8616934 , 841.9761503 , 846.99786451, 871.15717441,\n",
      "       780.21953936, 757.60855562, 864.58609949, 881.58160417,\n",
      "       859.3996499 , 862.91979102, 883.65609382, 848.05515331,\n",
      "       819.54479791, 861.61104418, 823.77193688, 871.634727  ,\n",
      "       794.34800497, 890.0528615 , 916.1347314 , 901.84355457,\n",
      "       934.63290199, 984.66471466, 970.16030028, 818.00754688,\n",
      "       784.76478819, 821.16300342, 868.15554952, 909.64502365,\n",
      "       861.98607758, 884.23519565])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 854.64268169,  844.90353406,  807.12054261,  815.2239179 ,\n",
      "        802.85865266,  814.48982293,  923.60552682,  869.34745536,\n",
      "       1012.69305322,  979.26171637,  929.55501488,  868.02206953,\n",
      "        922.48195382,  889.64371938,  902.27492431, 1036.70076886,\n",
      "        959.19804806,  973.71324439,  902.93070215,  900.14532383])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([416.44682588, 385.58899509, 355.57554051, 401.83538562,\n",
      "       424.40409635, 391.1802466 , 378.83606312, 413.5852798 ,\n",
      "       345.70142055, 315.11381903, 346.66881669, 386.61950519,\n",
      "       426.58713648, 499.00199322, 496.48184556, 438.03423281,\n",
      "       423.59440741, 375.37548583, 404.09737706, 409.62535098,\n",
      "       437.76060348, 394.47503303, 439.97630195, 517.19346552,\n",
      "       461.76251444, 457.355711  , 479.62716775, 433.19914647,\n",
      "       443.23925318, 444.05635782])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([446.12106416, 405.39701126, 427.84486792, 466.91572959,\n",
      "       527.49026645, 526.50191535, 580.10855533, 460.73538646,\n",
      "       484.74676984, 478.32468609, 567.74701886, 461.14155259,\n",
      "       402.6968913 , 407.19234085, 335.50445663, 383.89448025,\n",
      "       402.829297  , 448.14808423, 478.51272274, 563.09347579])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([773.69529753, 760.13137483, 743.52554413, 780.65476182,\n",
      "       780.51270032, 863.92123596, 695.31521813, 799.46781486,\n",
      "       876.53041534, 724.65218252, 743.8072481 , 770.72811344,\n",
      "       722.73099485, 733.79990542, 728.76976372, 866.57003017,\n",
      "       887.52255784, 896.85451539, 877.50053638, 775.79215333,\n",
      "       766.04648599, 824.39341901, 762.38914573, 826.6395924 ,\n",
      "       817.13219304, 796.22790491, 846.83172639, 857.07268611,\n",
      "       815.69791161, 875.33317673])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([794.92274497, 836.89239776, 867.16387476, 826.78026331,\n",
      "       841.22937628, 774.8583325 , 851.97983988, 839.78847353,\n",
      "       806.18137957, 876.3270062 , 822.22514043, 755.48010844,\n",
      "       729.86087559, 832.20711867, 784.38456467, 902.87694654,\n",
      "       773.59564939, 889.41788188, 887.39322146, 850.96512158])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([434.29575526, 413.08282734, 361.27286765, 370.69375928,\n",
      "       416.44682588, 385.58899509, 355.57554051, 401.83538562,\n",
      "       424.40409635, 391.1802466 , 378.83606312, 413.5852798 ,\n",
      "       345.70142055, 315.11381903, 346.66881669, 386.61950519,\n",
      "       426.58713648, 499.00199322, 496.48184556, 438.03423281,\n",
      "       423.59440741, 375.37548583, 404.09737706, 409.62535098,\n",
      "       437.76060348, 394.47503303, 439.97630195, 517.19346552,\n",
      "       461.76251444, 457.355711  ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([479.62716775, 433.19914647, 443.23925318, 444.05635782,\n",
      "       446.12106416, 405.39701126, 427.84486792, 466.91572959,\n",
      "       527.49026645, 526.50191535, 580.10855533, 460.73538646,\n",
      "       484.74676984, 478.32468609, 567.74701886, 461.14155259,\n",
      "       402.6968913 , 407.19234085, 335.50445663, 383.89448025])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([381.30008063, 394.0134984 , 383.27079237, 387.32550047,\n",
      "       417.0279523 , 358.91573723, 476.45969901, 381.52225147,\n",
      "       314.31964945, 421.39247611, 452.38691493, 444.22925323,\n",
      "       442.59260105, 411.5736148 , 354.91586408, 383.56673024,\n",
      "       364.72357312, 434.29575526, 413.08282734, 361.27286765,\n",
      "       370.69375928, 416.44682588, 385.58899509, 355.57554051,\n",
      "       401.83538562, 424.40409635, 391.1802466 , 378.83606312,\n",
      "       413.5852798 , 345.70142055])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([315.11381903, 346.66881669, 386.61950519, 426.58713648,\n",
      "       499.00199322, 496.48184556, 438.03423281, 423.59440741,\n",
      "       375.37548583, 404.09737706, 409.62535098, 437.76060348,\n",
      "       394.47503303, 439.97630195, 517.19346552, 461.76251444,\n",
      "       457.355711  , 479.62716775, 433.19914647, 443.23925318])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([757.60855562, 864.58609949, 881.58160417, 859.3996499 ,\n",
      "       862.91979102, 883.65609382, 848.05515331, 819.54479791,\n",
      "       861.61104418, 823.77193688, 871.634727  , 794.34800497,\n",
      "       890.0528615 , 916.1347314 , 901.84355457, 934.63290199,\n",
      "       984.66471466, 970.16030028, 818.00754688, 784.76478819,\n",
      "       821.16300342, 868.15554952, 909.64502365, 861.98607758,\n",
      "       884.23519565, 854.64268169, 844.90353406, 807.12054261,\n",
      "       815.2239179 , 802.85865266])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 814.48982293,  923.60552682,  869.34745536, 1012.69305322,\n",
      "        979.26171637,  929.55501488,  868.02206953,  922.48195382,\n",
      "        889.64371938,  902.27492431, 1036.70076886,  959.19804806,\n",
      "        973.71324439,  902.93070215,  900.14532383, 1000.23202533,\n",
      "        921.42712426,  998.73708587,  990.95733222,  911.4989099 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([822.22514043, 755.48010844, 729.86087559, 832.20711867,\n",
      "       784.38456467, 902.87694654, 773.59564939, 889.41788188,\n",
      "       887.39322146, 850.96512158, 820.95104652, 858.00387261,\n",
      "       855.81095587, 906.99625232, 882.58263097, 867.8616934 ,\n",
      "       841.9761503 , 846.99786451, 871.15717441, 780.21953936,\n",
      "       757.60855562, 864.58609949, 881.58160417, 859.3996499 ,\n",
      "       862.91979102, 883.65609382, 848.05515331, 819.54479791,\n",
      "       861.61104418, 823.77193688])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([871.634727  , 794.34800497, 890.0528615 , 916.1347314 ,\n",
      "       901.84355457, 934.63290199, 984.66471466, 970.16030028,\n",
      "       818.00754688, 784.76478819, 821.16300342, 868.15554952,\n",
      "       909.64502365, 861.98607758, 884.23519565, 854.64268169,\n",
      "       844.90353406, 807.12054261, 815.2239179 , 802.85865266])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([784.06169243, 794.03170193, 782.43241217, 755.51672945,\n",
      "       747.93387285, 773.69529753, 760.13137483, 743.52554413,\n",
      "       780.65476182, 780.51270032, 863.92123596, 695.31521813,\n",
      "       799.46781486, 876.53041534, 724.65218252, 743.8072481 ,\n",
      "       770.72811344, 722.73099485, 733.79990542, 728.76976372,\n",
      "       866.57003017, 887.52255784, 896.85451539, 877.50053638,\n",
      "       775.79215333, 766.04648599, 824.39341901, 762.38914573,\n",
      "       826.6395924 , 817.13219304])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([796.22790491, 846.83172639, 857.07268611, 815.69791161,\n",
      "       875.33317673, 794.92274497, 836.89239776, 867.16387476,\n",
      "       826.78026331, 841.22937628, 774.8583325 , 851.97983988,\n",
      "       839.78847353, 806.18137957, 876.3270062 , 822.22514043,\n",
      "       755.48010844, 729.86087559, 832.20711867, 784.38456467])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([850.96512158, 820.95104652, 858.00387261, 855.81095587,\n",
      "       906.99625232, 882.58263097, 867.8616934 , 841.9761503 ,\n",
      "       846.99786451, 871.15717441, 780.21953936, 757.60855562,\n",
      "       864.58609949, 881.58160417, 859.3996499 , 862.91979102,\n",
      "       883.65609382, 848.05515331, 819.54479791, 861.61104418,\n",
      "       823.77193688, 871.634727  , 794.34800497, 890.0528615 ,\n",
      "       916.1347314 , 901.84355457, 934.63290199, 984.66471466,\n",
      "       970.16030028, 818.00754688])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 784.76478819,  821.16300342,  868.15554952,  909.64502365,\n",
      "        861.98607758,  884.23519565,  854.64268169,  844.90353406,\n",
      "        807.12054261,  815.2239179 ,  802.85865266,  814.48982293,\n",
      "        923.60552682,  869.34745536, 1012.69305322,  979.26171637,\n",
      "        929.55501488,  868.02206953,  922.48195382,  889.64371938])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([722.50694506, 884.27854574, 835.09507914, 790.9940369 ,\n",
      "       784.06169243, 794.03170193, 782.43241217, 755.51672945,\n",
      "       747.93387285, 773.69529753, 760.13137483, 743.52554413,\n",
      "       780.65476182, 780.51270032, 863.92123596, 695.31521813,\n",
      "       799.46781486, 876.53041534, 724.65218252, 743.8072481 ,\n",
      "       770.72811344, 722.73099485, 733.79990542, 728.76976372,\n",
      "       866.57003017, 887.52255784, 896.85451539, 877.50053638,\n",
      "       775.79215333, 766.04648599])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([824.39341901, 762.38914573, 826.6395924 , 817.13219304,\n",
      "       796.22790491, 846.83172639, 857.07268611, 815.69791161,\n",
      "       875.33317673, 794.92274497, 836.89239776, 867.16387476,\n",
      "       826.78026331, 841.22937628, 774.8583325 , 851.97983988,\n",
      "       839.78847353, 806.18137957, 876.3270062 , 822.22514043])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([530.47387032, 609.74709298, 635.44991269, 535.91731397,\n",
      "       555.47851634, 560.26732238, 592.77245291, 492.78699422,\n",
      "       528.47301518, 576.62673148, 449.02573821, 420.73057322,\n",
      "       383.2050939 , 453.90534066, 539.24259306, 605.47121043,\n",
      "       556.67616859, 608.30828456, 489.31267053, 414.13221475,\n",
      "       481.92922581, 537.03024237, 531.27077547, 424.76439816,\n",
      "       480.27019493, 456.60722947, 534.68799708, 560.27001124,\n",
      "       495.50885821, 490.43221638])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([469.54868894, 509.0216617 , 580.5186044 , 509.56832961,\n",
      "       547.35738325, 523.72622733, 493.15355426, 520.45598519,\n",
      "       488.64675133, 495.78048545, 472.70249657, 618.45257776,\n",
      "       589.3399963 , 522.26342527, 549.53276579, 550.7361558 ,\n",
      "       541.6098775 , 581.53451462, 607.22224773, 549.87462813])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([876.3270062 , 822.22514043, 755.48010844, 729.86087559,\n",
      "       832.20711867, 784.38456467, 902.87694654, 773.59564939,\n",
      "       889.41788188, 887.39322146, 850.96512158, 820.95104652,\n",
      "       858.00387261, 855.81095587, 906.99625232, 882.58263097,\n",
      "       867.8616934 , 841.9761503 , 846.99786451, 871.15717441,\n",
      "       780.21953936, 757.60855562, 864.58609949, 881.58160417,\n",
      "       859.3996499 , 862.91979102, 883.65609382, 848.05515331,\n",
      "       819.54479791, 861.61104418])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([823.77193688, 871.634727  , 794.34800497, 890.0528615 ,\n",
      "       916.1347314 , 901.84355457, 934.63290199, 984.66471466,\n",
      "       970.16030028, 818.00754688, 784.76478819, 821.16300342,\n",
      "       868.15554952, 909.64502365, 861.98607758, 884.23519565,\n",
      "       854.64268169, 844.90353406, 807.12054261, 815.2239179 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([690.34147284, 679.89555162, 718.86986728, 732.00333529,\n",
      "       767.83519502, 707.24881753, 689.61478723, 656.32769369,\n",
      "       668.64987017, 719.83064084, 758.57681546, 687.1679321 ,\n",
      "       743.60502328, 802.44119057, 771.56266707, 828.12820822,\n",
      "       726.32636326, 651.49920215, 608.68186599, 758.13233494,\n",
      "       784.85025629, 742.43778409, 744.95265199, 681.04995163,\n",
      "       830.47041928, 788.31397118, 749.23453274, 775.92944027,\n",
      "       776.84226997, 761.63590883])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([706.47664247, 747.91110009, 842.84008478, 850.5771681 ,\n",
      "       857.88656147, 759.24070059, 692.35079504, 719.83979052,\n",
      "       745.37251598, 804.07326495, 689.79394523, 795.39479652,\n",
      "       773.4935943 , 733.44333496, 696.48647192, 651.50446061,\n",
      "       760.04488904, 774.24036403, 698.7297109 , 669.98055191])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([750.69673545, 800.72938966, 880.17979644, 873.23872791,\n",
      "       793.98585857, 722.50694506, 884.27854574, 835.09507914,\n",
      "       790.9940369 , 784.06169243, 794.03170193, 782.43241217,\n",
      "       755.51672945, 747.93387285, 773.69529753, 760.13137483,\n",
      "       743.52554413, 780.65476182, 780.51270032, 863.92123596,\n",
      "       695.31521813, 799.46781486, 876.53041534, 724.65218252,\n",
      "       743.8072481 , 770.72811344, 722.73099485, 733.79990542,\n",
      "       728.76976372, 866.57003017])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([887.52255784, 896.85451539, 877.50053638, 775.79215333,\n",
      "       766.04648599, 824.39341901, 762.38914573, 826.6395924 ,\n",
      "       817.13219304, 796.22790491, 846.83172639, 857.07268611,\n",
      "       815.69791161, 875.33317673, 794.92274497, 836.89239776,\n",
      "       867.16387476, 826.78026331, 841.22937628, 774.8583325 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([603.15946382, 609.79642481, 597.68857407, 561.29017164,\n",
      "       563.99484285, 636.27975237, 653.7286504 , 580.21410777,\n",
      "       603.71020602, 655.75754555, 552.61491167, 614.86465122,\n",
      "       596.29610199, 638.34407776, 596.98361388, 521.09639918,\n",
      "       505.46555349, 589.70267609, 636.73604882, 589.67698049,\n",
      "       644.77612963, 561.24317062, 596.79057361, 568.82052745,\n",
      "       576.8558281 , 622.05026979, 594.06184224, 602.02666574,\n",
      "       680.36988082, 633.39966005])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([675.71884524, 605.40908925, 650.71947409, 727.41228097,\n",
      "       557.18612905, 562.01216424, 655.97105619, 650.10028912,\n",
      "       668.16176824, 631.85717443, 648.09903429, 649.27642821,\n",
      "       712.67236658, 696.27975741, 686.1849128 , 649.98371354,\n",
      "       631.18617055, 631.07502274, 673.75730796, 651.98129603])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([855.81095587, 906.99625232, 882.58263097, 867.8616934 ,\n",
      "       841.9761503 , 846.99786451, 871.15717441, 780.21953936,\n",
      "       757.60855562, 864.58609949, 881.58160417, 859.3996499 ,\n",
      "       862.91979102, 883.65609382, 848.05515331, 819.54479791,\n",
      "       861.61104418, 823.77193688, 871.634727  , 794.34800497,\n",
      "       890.0528615 , 916.1347314 , 901.84355457, 934.63290199,\n",
      "       984.66471466, 970.16030028, 818.00754688, 784.76478819,\n",
      "       821.16300342, 868.15554952])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 909.64502365,  861.98607758,  884.23519565,  854.64268169,\n",
      "        844.90353406,  807.12054261,  815.2239179 ,  802.85865266,\n",
      "        814.48982293,  923.60552682,  869.34745536, 1012.69305322,\n",
      "        979.26171637,  929.55501488,  868.02206953,  922.48195382,\n",
      "        889.64371938,  902.27492431, 1036.70076886,  959.19804806])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([775.92944027, 776.84226997, 761.63590883, 706.47664247,\n",
      "       747.91110009, 842.84008478, 850.5771681 , 857.88656147,\n",
      "       759.24070059, 692.35079504, 719.83979052, 745.37251598,\n",
      "       804.07326495, 689.79394523, 795.39479652, 773.4935943 ,\n",
      "       733.44333496, 696.48647192, 651.50446061, 760.04488904,\n",
      "       774.24036403, 698.7297109 , 669.98055191, 714.36151018,\n",
      "       834.70411715, 785.46547122, 690.58578351, 723.47617913,\n",
      "       745.1619075 , 627.8142917 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([709.36185236, 750.69673545, 800.72938966, 880.17979644,\n",
      "       873.23872791, 793.98585857, 722.50694506, 884.27854574,\n",
      "       835.09507914, 790.9940369 , 784.06169243, 794.03170193,\n",
      "       782.43241217, 755.51672945, 747.93387285, 773.69529753,\n",
      "       760.13137483, 743.52554413, 780.65476182, 780.51270032])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([802.44119057, 771.56266707, 828.12820822, 726.32636326,\n",
      "       651.49920215, 608.68186599, 758.13233494, 784.85025629,\n",
      "       742.43778409, 744.95265199, 681.04995163, 830.47041928,\n",
      "       788.31397118, 749.23453274, 775.92944027, 776.84226997,\n",
      "       761.63590883, 706.47664247, 747.91110009, 842.84008478,\n",
      "       850.5771681 , 857.88656147, 759.24070059, 692.35079504,\n",
      "       719.83979052, 745.37251598, 804.07326495, 689.79394523,\n",
      "       795.39479652, 773.4935943 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([733.44333496, 696.48647192, 651.50446061, 760.04488904,\n",
      "       774.24036403, 698.7297109 , 669.98055191, 714.36151018,\n",
      "       834.70411715, 785.46547122, 690.58578351, 723.47617913,\n",
      "       745.1619075 , 627.8142917 , 709.36185236, 750.69673545,\n",
      "       800.72938966, 880.17979644, 873.23872791, 793.98585857])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([846.83172639, 857.07268611, 815.69791161, 875.33317673,\n",
      "       794.92274497, 836.89239776, 867.16387476, 826.78026331,\n",
      "       841.22937628, 774.8583325 , 851.97983988, 839.78847353,\n",
      "       806.18137957, 876.3270062 , 822.22514043, 755.48010844,\n",
      "       729.86087559, 832.20711867, 784.38456467, 902.87694654,\n",
      "       773.59564939, 889.41788188, 887.39322146, 850.96512158,\n",
      "       820.95104652, 858.00387261, 855.81095587, 906.99625232,\n",
      "       882.58263097, 867.8616934 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([841.9761503 , 846.99786451, 871.15717441, 780.21953936,\n",
      "       757.60855562, 864.58609949, 881.58160417, 859.3996499 ,\n",
      "       862.91979102, 883.65609382, 848.05515331, 819.54479791,\n",
      "       861.61104418, 823.77193688, 871.634727  , 794.34800497,\n",
      "       890.0528615 , 916.1347314 , 901.84355457, 934.63290199])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 848.05515331,  819.54479791,  861.61104418,  823.77193688,\n",
      "        871.634727  ,  794.34800497,  890.0528615 ,  916.1347314 ,\n",
      "        901.84355457,  934.63290199,  984.66471466,  970.16030028,\n",
      "        818.00754688,  784.76478819,  821.16300342,  868.15554952,\n",
      "        909.64502365,  861.98607758,  884.23519565,  854.64268169,\n",
      "        844.90353406,  807.12054261,  815.2239179 ,  802.85865266,\n",
      "        814.48982293,  923.60552682,  869.34745536, 1012.69305322,\n",
      "        979.26171637,  929.55501488])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 868.02206953,  922.48195382,  889.64371938,  902.27492431,\n",
      "       1036.70076886,  959.19804806,  973.71324439,  902.93070215,\n",
      "        900.14532383, 1000.23202533,  921.42712426,  998.73708587,\n",
      "        990.95733222,  911.4989099 ,  943.0991758 ,  982.24411303,\n",
      "        975.88649061,  862.21052577,  857.87276556,  893.84910538])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([858.00387261, 855.81095587, 906.99625232, 882.58263097,\n",
      "       867.8616934 , 841.9761503 , 846.99786451, 871.15717441,\n",
      "       780.21953936, 757.60855562, 864.58609949, 881.58160417,\n",
      "       859.3996499 , 862.91979102, 883.65609382, 848.05515331,\n",
      "       819.54479791, 861.61104418, 823.77193688, 871.634727  ,\n",
      "       794.34800497, 890.0528615 , 916.1347314 , 901.84355457,\n",
      "       934.63290199, 984.66471466, 970.16030028, 818.00754688,\n",
      "       784.76478819, 821.16300342])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 868.15554952,  909.64502365,  861.98607758,  884.23519565,\n",
      "        854.64268169,  844.90353406,  807.12054261,  815.2239179 ,\n",
      "        802.85865266,  814.48982293,  923.60552682,  869.34745536,\n",
      "       1012.69305322,  979.26171637,  929.55501488,  868.02206953,\n",
      "        922.48195382,  889.64371938,  902.27492431, 1036.70076886])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([767.83519502, 707.24881753, 689.61478723, 656.32769369,\n",
      "       668.64987017, 719.83064084, 758.57681546, 687.1679321 ,\n",
      "       743.60502328, 802.44119057, 771.56266707, 828.12820822,\n",
      "       726.32636326, 651.49920215, 608.68186599, 758.13233494,\n",
      "       784.85025629, 742.43778409, 744.95265199, 681.04995163,\n",
      "       830.47041928, 788.31397118, 749.23453274, 775.92944027,\n",
      "       776.84226997, 761.63590883, 706.47664247, 747.91110009,\n",
      "       842.84008478, 850.5771681 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([857.88656147, 759.24070059, 692.35079504, 719.83979052,\n",
      "       745.37251598, 804.07326495, 689.79394523, 795.39479652,\n",
      "       773.4935943 , 733.44333496, 696.48647192, 651.50446061,\n",
      "       760.04488904, 774.24036403, 698.7297109 , 669.98055191,\n",
      "       714.36151018, 834.70411715, 785.46547122, 690.58578351])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([563.28894392, 603.15946382, 609.79642481, 597.68857407,\n",
      "       561.29017164, 563.99484285, 636.27975237, 653.7286504 ,\n",
      "       580.21410777, 603.71020602, 655.75754555, 552.61491167,\n",
      "       614.86465122, 596.29610199, 638.34407776, 596.98361388,\n",
      "       521.09639918, 505.46555349, 589.70267609, 636.73604882,\n",
      "       589.67698049, 644.77612963, 561.24317062, 596.79057361,\n",
      "       568.82052745, 576.8558281 , 622.05026979, 594.06184224,\n",
      "       602.02666574, 680.36988082])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([633.39966005, 675.71884524, 605.40908925, 650.71947409,\n",
      "       727.41228097, 557.18612905, 562.01216424, 655.97105619,\n",
      "       650.10028912, 668.16176824, 631.85717443, 648.09903429,\n",
      "       649.27642821, 712.67236658, 696.27975741, 686.1849128 ,\n",
      "       649.98371354, 631.18617055, 631.07502274, 673.75730796])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([733.79990542, 728.76976372, 866.57003017, 887.52255784,\n",
      "       896.85451539, 877.50053638, 775.79215333, 766.04648599,\n",
      "       824.39341901, 762.38914573, 826.6395924 , 817.13219304,\n",
      "       796.22790491, 846.83172639, 857.07268611, 815.69791161,\n",
      "       875.33317673, 794.92274497, 836.89239776, 867.16387476,\n",
      "       826.78026331, 841.22937628, 774.8583325 , 851.97983988,\n",
      "       839.78847353, 806.18137957, 876.3270062 , 822.22514043,\n",
      "       755.48010844, 729.86087559])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([832.20711867, 784.38456467, 902.87694654, 773.59564939,\n",
      "       889.41788188, 887.39322146, 850.96512158, 820.95104652,\n",
      "       858.00387261, 855.81095587, 906.99625232, 882.58263097,\n",
      "       867.8616934 , 841.9761503 , 846.99786451, 871.15717441,\n",
      "       780.21953936, 757.60855562, 864.58609949, 881.58160417])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([773.59564939, 889.41788188, 887.39322146, 850.96512158,\n",
      "       820.95104652, 858.00387261, 855.81095587, 906.99625232,\n",
      "       882.58263097, 867.8616934 , 841.9761503 , 846.99786451,\n",
      "       871.15717441, 780.21953936, 757.60855562, 864.58609949,\n",
      "       881.58160417, 859.3996499 , 862.91979102, 883.65609382,\n",
      "       848.05515331, 819.54479791, 861.61104418, 823.77193688,\n",
      "       871.634727  , 794.34800497, 890.0528615 , 916.1347314 ,\n",
      "       901.84355457, 934.63290199])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 984.66471466,  970.16030028,  818.00754688,  784.76478819,\n",
      "        821.16300342,  868.15554952,  909.64502365,  861.98607758,\n",
      "        884.23519565,  854.64268169,  844.90353406,  807.12054261,\n",
      "        815.2239179 ,  802.85865266,  814.48982293,  923.60552682,\n",
      "        869.34745536, 1012.69305322,  979.26171637,  929.55501488])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([832.20711867, 784.38456467, 902.87694654, 773.59564939,\n",
      "       889.41788188, 887.39322146, 850.96512158, 820.95104652,\n",
      "       858.00387261, 855.81095587, 906.99625232, 882.58263097,\n",
      "       867.8616934 , 841.9761503 , 846.99786451, 871.15717441,\n",
      "       780.21953936, 757.60855562, 864.58609949, 881.58160417,\n",
      "       859.3996499 , 862.91979102, 883.65609382, 848.05515331,\n",
      "       819.54479791, 861.61104418, 823.77193688, 871.634727  ,\n",
      "       794.34800497, 890.0528615 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([916.1347314 , 901.84355457, 934.63290199, 984.66471466,\n",
      "       970.16030028, 818.00754688, 784.76478819, 821.16300342,\n",
      "       868.15554952, 909.64502365, 861.98607758, 884.23519565,\n",
      "       854.64268169, 844.90353406, 807.12054261, 815.2239179 ,\n",
      "       802.85865266, 814.48982293, 923.60552682, 869.34745536])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([782.43241217, 755.51672945, 747.93387285, 773.69529753,\n",
      "       760.13137483, 743.52554413, 780.65476182, 780.51270032,\n",
      "       863.92123596, 695.31521813, 799.46781486, 876.53041534,\n",
      "       724.65218252, 743.8072481 , 770.72811344, 722.73099485,\n",
      "       733.79990542, 728.76976372, 866.57003017, 887.52255784,\n",
      "       896.85451539, 877.50053638, 775.79215333, 766.04648599,\n",
      "       824.39341901, 762.38914573, 826.6395924 , 817.13219304,\n",
      "       796.22790491, 846.83172639])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([857.07268611, 815.69791161, 875.33317673, 794.92274497,\n",
      "       836.89239776, 867.16387476, 826.78026331, 841.22937628,\n",
      "       774.8583325 , 851.97983988, 839.78847353, 806.18137957,\n",
      "       876.3270062 , 822.22514043, 755.48010844, 729.86087559,\n",
      "       832.20711867, 784.38456467, 902.87694654, 773.59564939])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([549.53276579, 550.7361558 , 541.6098775 , 581.53451462,\n",
      "       607.22224773, 549.87462813, 522.37636622, 533.76513254,\n",
      "       439.37788128, 437.09888259, 592.31584485, 669.82746742,\n",
      "       592.01663525, 597.07230042, 597.51240164, 732.10605797,\n",
      "       694.37287539, 600.22696169, 529.54083307, 475.68450536,\n",
      "       549.19229931, 538.22986094, 490.72208051, 514.5088088 ,\n",
      "       508.05592927, 639.58053533, 658.89585885, 607.11396555,\n",
      "       662.96302643, 623.20154476])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([551.67016265, 648.06221241, 647.35774585, 555.63289089,\n",
      "       563.85025964, 543.66405584, 506.29295049, 609.75057181,\n",
      "       707.14155238, 571.44576343, 604.4266225 , 577.85684727,\n",
      "       565.12944302, 561.70663192, 546.92000432, 587.34387768,\n",
      "       563.28894392, 603.15946382, 609.79642481, 597.68857407])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([380.879735  , 387.76152403, 330.557742  , 388.11743607,\n",
      "       390.37958736, 408.2225002 , 410.23820866, 336.26542706,\n",
      "       319.41677054, 386.73270881, 408.68045252, 379.54368873,\n",
      "       374.68737687, 434.71295722, 367.58405072, 389.15715379,\n",
      "       372.35580464, 359.08299146, 423.66672844, 438.84670817,\n",
      "       437.61444311, 463.16543088, 410.89882285, 419.07923581,\n",
      "       381.30008063, 394.0134984 , 383.27079237, 387.32550047,\n",
      "       417.0279523 , 358.91573723])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([476.45969901, 381.52225147, 314.31964945, 421.39247611,\n",
      "       452.38691493, 444.22925323, 442.59260105, 411.5736148 ,\n",
      "       354.91586408, 383.56673024, 364.72357312, 434.29575526,\n",
      "       413.08282734, 361.27286765, 370.69375928, 416.44682588,\n",
      "       385.58899509, 355.57554051, 401.83538562, 424.40409635])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([706.47664247, 747.91110009, 842.84008478, 850.5771681 ,\n",
      "       857.88656147, 759.24070059, 692.35079504, 719.83979052,\n",
      "       745.37251598, 804.07326495, 689.79394523, 795.39479652,\n",
      "       773.4935943 , 733.44333496, 696.48647192, 651.50446061,\n",
      "       760.04488904, 774.24036403, 698.7297109 , 669.98055191,\n",
      "       714.36151018, 834.70411715, 785.46547122, 690.58578351,\n",
      "       723.47617913, 745.1619075 , 627.8142917 , 709.36185236,\n",
      "       750.69673545, 800.72938966])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([880.17979644, 873.23872791, 793.98585857, 722.50694506,\n",
      "       884.27854574, 835.09507914, 790.9940369 , 784.06169243,\n",
      "       794.03170193, 782.43241217, 755.51672945, 747.93387285,\n",
      "       773.69529753, 760.13137483, 743.52554413, 780.65476182,\n",
      "       780.51270032, 863.92123596, 695.31521813, 799.46781486])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([826.78026331, 841.22937628, 774.8583325 , 851.97983988,\n",
      "       839.78847353, 806.18137957, 876.3270062 , 822.22514043,\n",
      "       755.48010844, 729.86087559, 832.20711867, 784.38456467,\n",
      "       902.87694654, 773.59564939, 889.41788188, 887.39322146,\n",
      "       850.96512158, 820.95104652, 858.00387261, 855.81095587,\n",
      "       906.99625232, 882.58263097, 867.8616934 , 841.9761503 ,\n",
      "       846.99786451, 871.15717441, 780.21953936, 757.60855562,\n",
      "       864.58609949, 881.58160417])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([859.3996499 , 862.91979102, 883.65609382, 848.05515331,\n",
      "       819.54479791, 861.61104418, 823.77193688, 871.634727  ,\n",
      "       794.34800497, 890.0528615 , 916.1347314 , 901.84355457,\n",
      "       934.63290199, 984.66471466, 970.16030028, 818.00754688,\n",
      "       784.76478819, 821.16300342, 868.15554952, 909.64502365])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 821.16300342,  868.15554952,  909.64502365,  861.98607758,\n",
      "        884.23519565,  854.64268169,  844.90353406,  807.12054261,\n",
      "        815.2239179 ,  802.85865266,  814.48982293,  923.60552682,\n",
      "        869.34745536, 1012.69305322,  979.26171637,  929.55501488,\n",
      "        868.02206953,  922.48195382,  889.64371938,  902.27492431,\n",
      "       1036.70076886,  959.19804806,  973.71324439,  902.93070215,\n",
      "        900.14532383, 1000.23202533,  921.42712426,  998.73708587,\n",
      "        990.95733222,  911.4989099 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 943.0991758 ,  982.24411303,  975.88649061,  862.21052577,\n",
      "        857.87276556,  893.84910538,  914.7797677 ,  956.00910157,\n",
      "        950.98553127,  869.84354752,  923.32683661,  967.6534664 ,\n",
      "        975.40865419, 1003.22990125, 1003.55157614,  982.53631582,\n",
      "        949.02568453,  858.44878802,  927.72946126,  957.00092337])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([760.13137483, 743.52554413, 780.65476182, 780.51270032,\n",
      "       863.92123596, 695.31521813, 799.46781486, 876.53041534,\n",
      "       724.65218252, 743.8072481 , 770.72811344, 722.73099485,\n",
      "       733.79990542, 728.76976372, 866.57003017, 887.52255784,\n",
      "       896.85451539, 877.50053638, 775.79215333, 766.04648599,\n",
      "       824.39341901, 762.38914573, 826.6395924 , 817.13219304,\n",
      "       796.22790491, 846.83172639, 857.07268611, 815.69791161,\n",
      "       875.33317673, 794.92274497])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([836.89239776, 867.16387476, 826.78026331, 841.22937628,\n",
      "       774.8583325 , 851.97983988, 839.78847353, 806.18137957,\n",
      "       876.3270062 , 822.22514043, 755.48010844, 729.86087559,\n",
      "       832.20711867, 784.38456467, 902.87694654, 773.59564939,\n",
      "       889.41788188, 887.39322146, 850.96512158, 820.95104652])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([780.51270032, 863.92123596, 695.31521813, 799.46781486,\n",
      "       876.53041534, 724.65218252, 743.8072481 , 770.72811344,\n",
      "       722.73099485, 733.79990542, 728.76976372, 866.57003017,\n",
      "       887.52255784, 896.85451539, 877.50053638, 775.79215333,\n",
      "       766.04648599, 824.39341901, 762.38914573, 826.6395924 ,\n",
      "       817.13219304, 796.22790491, 846.83172639, 857.07268611,\n",
      "       815.69791161, 875.33317673, 794.92274497, 836.89239776,\n",
      "       867.16387476, 826.78026331])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([841.22937628, 774.8583325 , 851.97983988, 839.78847353,\n",
      "       806.18137957, 876.3270062 , 822.22514043, 755.48010844,\n",
      "       729.86087559, 832.20711867, 784.38456467, 902.87694654,\n",
      "       773.59564939, 889.41788188, 887.39322146, 850.96512158,\n",
      "       820.95104652, 858.00387261, 855.81095587, 906.99625232])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([839.78847353, 806.18137957, 876.3270062 , 822.22514043,\n",
      "       755.48010844, 729.86087559, 832.20711867, 784.38456467,\n",
      "       902.87694654, 773.59564939, 889.41788188, 887.39322146,\n",
      "       850.96512158, 820.95104652, 858.00387261, 855.81095587,\n",
      "       906.99625232, 882.58263097, 867.8616934 , 841.9761503 ,\n",
      "       846.99786451, 871.15717441, 780.21953936, 757.60855562,\n",
      "       864.58609949, 881.58160417, 859.3996499 , 862.91979102,\n",
      "       883.65609382, 848.05515331])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([819.54479791, 861.61104418, 823.77193688, 871.634727  ,\n",
      "       794.34800497, 890.0528615 , 916.1347314 , 901.84355457,\n",
      "       934.63290199, 984.66471466, 970.16030028, 818.00754688,\n",
      "       784.76478819, 821.16300342, 868.15554952, 909.64502365,\n",
      "       861.98607758, 884.23519565, 854.64268169, 844.90353406])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([882.58263097, 867.8616934 , 841.9761503 , 846.99786451,\n",
      "       871.15717441, 780.21953936, 757.60855562, 864.58609949,\n",
      "       881.58160417, 859.3996499 , 862.91979102, 883.65609382,\n",
      "       848.05515331, 819.54479791, 861.61104418, 823.77193688,\n",
      "       871.634727  , 794.34800497, 890.0528615 , 916.1347314 ,\n",
      "       901.84355457, 934.63290199, 984.66471466, 970.16030028,\n",
      "       818.00754688, 784.76478819, 821.16300342, 868.15554952,\n",
      "       909.64502365, 861.98607758])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 884.23519565,  854.64268169,  844.90353406,  807.12054261,\n",
      "        815.2239179 ,  802.85865266,  814.48982293,  923.60552682,\n",
      "        869.34745536, 1012.69305322,  979.26171637,  929.55501488,\n",
      "        868.02206953,  922.48195382,  889.64371938,  902.27492431,\n",
      "       1036.70076886,  959.19804806,  973.71324439,  902.93070215])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 784.76478819,  821.16300342,  868.15554952,  909.64502365,\n",
      "        861.98607758,  884.23519565,  854.64268169,  844.90353406,\n",
      "        807.12054261,  815.2239179 ,  802.85865266,  814.48982293,\n",
      "        923.60552682,  869.34745536, 1012.69305322,  979.26171637,\n",
      "        929.55501488,  868.02206953,  922.48195382,  889.64371938,\n",
      "        902.27492431, 1036.70076886,  959.19804806,  973.71324439,\n",
      "        902.93070215,  900.14532383, 1000.23202533,  921.42712426,\n",
      "        998.73708587,  990.95733222])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 911.4989099 ,  943.0991758 ,  982.24411303,  975.88649061,\n",
      "        862.21052577,  857.87276556,  893.84910538,  914.7797677 ,\n",
      "        956.00910157,  950.98553127,  869.84354752,  923.32683661,\n",
      "        967.6534664 ,  975.40865419, 1003.22990125, 1003.55157614,\n",
      "        982.53631582,  949.02568453,  858.44878802,  927.72946126])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([596.79057361, 568.82052745, 576.8558281 , 622.05026979,\n",
      "       594.06184224, 602.02666574, 680.36988082, 633.39966005,\n",
      "       675.71884524, 605.40908925, 650.71947409, 727.41228097,\n",
      "       557.18612905, 562.01216424, 655.97105619, 650.10028912,\n",
      "       668.16176824, 631.85717443, 648.09903429, 649.27642821,\n",
      "       712.67236658, 696.27975741, 686.1849128 , 649.98371354,\n",
      "       631.18617055, 631.07502274, 673.75730796, 651.98129603,\n",
      "       673.73626744, 777.07324188])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([758.2234969 , 680.18442364, 729.54997166, 678.89672873,\n",
      "       568.29571713, 581.78740582, 555.83498193, 615.91695795,\n",
      "       665.67546333, 761.52671654, 733.19317991, 684.17926491,\n",
      "       723.09299308, 592.89210242, 653.62634842, 724.79417847,\n",
      "       630.92834603, 717.67629451, 728.46205325, 681.62189532])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([596.98361388, 521.09639918, 505.46555349, 589.70267609,\n",
      "       636.73604882, 589.67698049, 644.77612963, 561.24317062,\n",
      "       596.79057361, 568.82052745, 576.8558281 , 622.05026979,\n",
      "       594.06184224, 602.02666574, 680.36988082, 633.39966005,\n",
      "       675.71884524, 605.40908925, 650.71947409, 727.41228097,\n",
      "       557.18612905, 562.01216424, 655.97105619, 650.10028912,\n",
      "       668.16176824, 631.85717443, 648.09903429, 649.27642821,\n",
      "       712.67236658, 696.27975741])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([686.1849128 , 649.98371354, 631.18617055, 631.07502274,\n",
      "       673.75730796, 651.98129603, 673.73626744, 777.07324188,\n",
      "       758.2234969 , 680.18442364, 729.54997166, 678.89672873,\n",
      "       568.29571713, 581.78740582, 555.83498193, 615.91695795,\n",
      "       665.67546333, 761.52671654, 733.19317991, 684.17926491])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([784.38456467, 902.87694654, 773.59564939, 889.41788188,\n",
      "       887.39322146, 850.96512158, 820.95104652, 858.00387261,\n",
      "       855.81095587, 906.99625232, 882.58263097, 867.8616934 ,\n",
      "       841.9761503 , 846.99786451, 871.15717441, 780.21953936,\n",
      "       757.60855562, 864.58609949, 881.58160417, 859.3996499 ,\n",
      "       862.91979102, 883.65609382, 848.05515331, 819.54479791,\n",
      "       861.61104418, 823.77193688, 871.634727  , 794.34800497,\n",
      "       890.0528615 , 916.1347314 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 901.84355457,  934.63290199,  984.66471466,  970.16030028,\n",
      "        818.00754688,  784.76478819,  821.16300342,  868.15554952,\n",
      "        909.64502365,  861.98607758,  884.23519565,  854.64268169,\n",
      "        844.90353406,  807.12054261,  815.2239179 ,  802.85865266,\n",
      "        814.48982293,  923.60552682,  869.34745536, 1012.69305322])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([867.16387476, 826.78026331, 841.22937628, 774.8583325 ,\n",
      "       851.97983988, 839.78847353, 806.18137957, 876.3270062 ,\n",
      "       822.22514043, 755.48010844, 729.86087559, 832.20711867,\n",
      "       784.38456467, 902.87694654, 773.59564939, 889.41788188,\n",
      "       887.39322146, 850.96512158, 820.95104652, 858.00387261,\n",
      "       855.81095587, 906.99625232, 882.58263097, 867.8616934 ,\n",
      "       841.9761503 , 846.99786451, 871.15717441, 780.21953936,\n",
      "       757.60855562, 864.58609949])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([881.58160417, 859.3996499 , 862.91979102, 883.65609382,\n",
      "       848.05515331, 819.54479791, 861.61104418, 823.77193688,\n",
      "       871.634727  , 794.34800497, 890.0528615 , 916.1347314 ,\n",
      "       901.84355457, 934.63290199, 984.66471466, 970.16030028,\n",
      "       818.00754688, 784.76478819, 821.16300342, 868.15554952])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([836.89239776, 867.16387476, 826.78026331, 841.22937628,\n",
      "       774.8583325 , 851.97983988, 839.78847353, 806.18137957,\n",
      "       876.3270062 , 822.22514043, 755.48010844, 729.86087559,\n",
      "       832.20711867, 784.38456467, 902.87694654, 773.59564939,\n",
      "       889.41788188, 887.39322146, 850.96512158, 820.95104652,\n",
      "       858.00387261, 855.81095587, 906.99625232, 882.58263097,\n",
      "       867.8616934 , 841.9761503 , 846.99786451, 871.15717441,\n",
      "       780.21953936, 757.60855562])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([864.58609949, 881.58160417, 859.3996499 , 862.91979102,\n",
      "       883.65609382, 848.05515331, 819.54479791, 861.61104418,\n",
      "       823.77193688, 871.634727  , 794.34800497, 890.0528615 ,\n",
      "       916.1347314 , 901.84355457, 934.63290199, 984.66471466,\n",
      "       970.16030028, 818.00754688, 784.76478819, 821.16300342])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 99.10058533, 189.91445947, 217.03019629, 218.01662357,\n",
      "       135.62151463, 215.66343029, 121.17746619, 167.28600356,\n",
      "       283.28864689, 164.69125998, 125.10364505, 159.87748372,\n",
      "       144.34205834,  82.29692285, 140.1662113 , 115.85726673,\n",
      "       173.41590414, 135.43190224, 233.88614205, 168.22234072,\n",
      "       149.37417651, 211.20263936, 134.80757158, 168.40259944,\n",
      "       248.81801436, 131.08170877, 163.32682593, 198.28054145,\n",
      "       230.55995397, 143.44437565])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 99.18040759, 184.29382082, 211.78528286, 211.13288375,\n",
      "       216.64507465, 168.69752245, 193.57349357, 214.00622395,\n",
      "       168.32951   , 277.04961628, 260.96146028, 156.82804628,\n",
      "       211.79379382, 165.25249826, 223.55048858, 276.7108538 ,\n",
      "       191.76163782, 241.70231579, 247.8262009 , 261.74448658])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([724.65218252, 743.8072481 , 770.72811344, 722.73099485,\n",
      "       733.79990542, 728.76976372, 866.57003017, 887.52255784,\n",
      "       896.85451539, 877.50053638, 775.79215333, 766.04648599,\n",
      "       824.39341901, 762.38914573, 826.6395924 , 817.13219304,\n",
      "       796.22790491, 846.83172639, 857.07268611, 815.69791161,\n",
      "       875.33317673, 794.92274497, 836.89239776, 867.16387476,\n",
      "       826.78026331, 841.22937628, 774.8583325 , 851.97983988,\n",
      "       839.78847353, 806.18137957])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([876.3270062 , 822.22514043, 755.48010844, 729.86087559,\n",
      "       832.20711867, 784.38456467, 902.87694654, 773.59564939,\n",
      "       889.41788188, 887.39322146, 850.96512158, 820.95104652,\n",
      "       858.00387261, 855.81095587, 906.99625232, 882.58263097,\n",
      "       867.8616934 , 841.9761503 , 846.99786451, 871.15717441])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([721.11223575, 683.36485669, 648.91690775, 722.89139063,\n",
      "       674.15797097, 690.34147284, 679.89555162, 718.86986728,\n",
      "       732.00333529, 767.83519502, 707.24881753, 689.61478723,\n",
      "       656.32769369, 668.64987017, 719.83064084, 758.57681546,\n",
      "       687.1679321 , 743.60502328, 802.44119057, 771.56266707,\n",
      "       828.12820822, 726.32636326, 651.49920215, 608.68186599,\n",
      "       758.13233494, 784.85025629, 742.43778409, 744.95265199,\n",
      "       681.04995163, 830.47041928])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([788.31397118, 749.23453274, 775.92944027, 776.84226997,\n",
      "       761.63590883, 706.47664247, 747.91110009, 842.84008478,\n",
      "       850.5771681 , 857.88656147, 759.24070059, 692.35079504,\n",
      "       719.83979052, 745.37251598, 804.07326495, 689.79394523,\n",
      "       795.39479652, 773.4935943 , 733.44333496, 696.48647192])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 984.66471466,  970.16030028,  818.00754688,  784.76478819,\n",
      "        821.16300342,  868.15554952,  909.64502365,  861.98607758,\n",
      "        884.23519565,  854.64268169,  844.90353406,  807.12054261,\n",
      "        815.2239179 ,  802.85865266,  814.48982293,  923.60552682,\n",
      "        869.34745536, 1012.69305322,  979.26171637,  929.55501488,\n",
      "        868.02206953,  922.48195382,  889.64371938,  902.27492431,\n",
      "       1036.70076886,  959.19804806,  973.71324439,  902.93070215,\n",
      "        900.14532383, 1000.23202533])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 921.42712426,  998.73708587,  990.95733222,  911.4989099 ,\n",
      "        943.0991758 ,  982.24411303,  975.88649061,  862.21052577,\n",
      "        857.87276556,  893.84910538,  914.7797677 ,  956.00910157,\n",
      "        950.98553127,  869.84354752,  923.32683661,  967.6534664 ,\n",
      "        975.40865419, 1003.22990125, 1003.55157614,  982.53631582])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([755.51672945, 747.93387285, 773.69529753, 760.13137483,\n",
      "       743.52554413, 780.65476182, 780.51270032, 863.92123596,\n",
      "       695.31521813, 799.46781486, 876.53041534, 724.65218252,\n",
      "       743.8072481 , 770.72811344, 722.73099485, 733.79990542,\n",
      "       728.76976372, 866.57003017, 887.52255784, 896.85451539,\n",
      "       877.50053638, 775.79215333, 766.04648599, 824.39341901,\n",
      "       762.38914573, 826.6395924 , 817.13219304, 796.22790491,\n",
      "       846.83172639, 857.07268611])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([815.69791161, 875.33317673, 794.92274497, 836.89239776,\n",
      "       867.16387476, 826.78026331, 841.22937628, 774.8583325 ,\n",
      "       851.97983988, 839.78847353, 806.18137957, 876.3270062 ,\n",
      "       822.22514043, 755.48010844, 729.86087559, 832.20711867,\n",
      "       784.38456467, 902.87694654, 773.59564939, 889.41788188])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([785.46547122, 690.58578351, 723.47617913, 745.1619075 ,\n",
      "       627.8142917 , 709.36185236, 750.69673545, 800.72938966,\n",
      "       880.17979644, 873.23872791, 793.98585857, 722.50694506,\n",
      "       884.27854574, 835.09507914, 790.9940369 , 784.06169243,\n",
      "       794.03170193, 782.43241217, 755.51672945, 747.93387285,\n",
      "       773.69529753, 760.13137483, 743.52554413, 780.65476182,\n",
      "       780.51270032, 863.92123596, 695.31521813, 799.46781486,\n",
      "       876.53041534, 724.65218252])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([743.8072481 , 770.72811344, 722.73099485, 733.79990542,\n",
      "       728.76976372, 866.57003017, 887.52255784, 896.85451539,\n",
      "       877.50053638, 775.79215333, 766.04648599, 824.39341901,\n",
      "       762.38914573, 826.6395924 , 817.13219304, 796.22790491,\n",
      "       846.83172639, 857.07268611, 815.69791161, 875.33317673])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 902.93070215,  900.14532383, 1000.23202533,  921.42712426,\n",
      "        998.73708587,  990.95733222,  911.4989099 ,  943.0991758 ,\n",
      "        982.24411303,  975.88649061,  862.21052577,  857.87276556,\n",
      "        893.84910538,  914.7797677 ,  956.00910157,  950.98553127,\n",
      "        869.84354752,  923.32683661,  967.6534664 ,  975.40865419,\n",
      "       1003.22990125, 1003.55157614,  982.53631582,  949.02568453,\n",
      "        858.44878802,  927.72946126,  957.00092337,  961.84957712,\n",
      "        887.94565068,  866.15942091])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 972.96430377,  964.76263466,  985.10560512,  967.39478882,\n",
      "        956.53369828, 1001.37762455,  949.18365111,  952.55314568,\n",
      "        935.12600406,  926.72522669,  933.22378088,  963.24465376,\n",
      "        941.21957118, 1015.54262546,  944.97319267,  940.49716249,\n",
      "        938.69094632, 1029.24446491, 1007.91049914, 1029.7894881 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1036.70076886,  959.19804806,  973.71324439,  902.93070215,\n",
      "        900.14532383, 1000.23202533,  921.42712426,  998.73708587,\n",
      "        990.95733222,  911.4989099 ,  943.0991758 ,  982.24411303,\n",
      "        975.88649061,  862.21052577,  857.87276556,  893.84910538,\n",
      "        914.7797677 ,  956.00910157,  950.98553127,  869.84354752,\n",
      "        923.32683661,  967.6534664 ,  975.40865419, 1003.22990125,\n",
      "       1003.55157614,  982.53631582,  949.02568453,  858.44878802,\n",
      "        927.72946126,  957.00092337])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 961.84957712,  887.94565068,  866.15942091,  972.96430377,\n",
      "        964.76263466,  985.10560512,  967.39478882,  956.53369828,\n",
      "       1001.37762455,  949.18365111,  952.55314568,  935.12600406,\n",
      "        926.72522669,  933.22378088,  963.24465376,  941.21957118,\n",
      "       1015.54262546,  944.97319267,  940.49716249,  938.69094632])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([851.97983988, 839.78847353, 806.18137957, 876.3270062 ,\n",
      "       822.22514043, 755.48010844, 729.86087559, 832.20711867,\n",
      "       784.38456467, 902.87694654, 773.59564939, 889.41788188,\n",
      "       887.39322146, 850.96512158, 820.95104652, 858.00387261,\n",
      "       855.81095587, 906.99625232, 882.58263097, 867.8616934 ,\n",
      "       841.9761503 , 846.99786451, 871.15717441, 780.21953936,\n",
      "       757.60855562, 864.58609949, 881.58160417, 859.3996499 ,\n",
      "       862.91979102, 883.65609382])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([848.05515331, 819.54479791, 861.61104418, 823.77193688,\n",
      "       871.634727  , 794.34800497, 890.0528615 , 916.1347314 ,\n",
      "       901.84355457, 934.63290199, 984.66471466, 970.16030028,\n",
      "       818.00754688, 784.76478819, 821.16300342, 868.15554952,\n",
      "       909.64502365, 861.98607758, 884.23519565, 854.64268169])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([881.58160417, 859.3996499 , 862.91979102, 883.65609382,\n",
      "       848.05515331, 819.54479791, 861.61104418, 823.77193688,\n",
      "       871.634727  , 794.34800497, 890.0528615 , 916.1347314 ,\n",
      "       901.84355457, 934.63290199, 984.66471466, 970.16030028,\n",
      "       818.00754688, 784.76478819, 821.16300342, 868.15554952,\n",
      "       909.64502365, 861.98607758, 884.23519565, 854.64268169,\n",
      "       844.90353406, 807.12054261, 815.2239179 , 802.85865266,\n",
      "       814.48982293, 923.60552682])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 869.34745536, 1012.69305322,  979.26171637,  929.55501488,\n",
      "        868.02206953,  922.48195382,  889.64371938,  902.27492431,\n",
      "       1036.70076886,  959.19804806,  973.71324439,  902.93070215,\n",
      "        900.14532383, 1000.23202533,  921.42712426,  998.73708587,\n",
      "        990.95733222,  911.4989099 ,  943.0991758 ,  982.24411303])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([747.93387285, 773.69529753, 760.13137483, 743.52554413,\n",
      "       780.65476182, 780.51270032, 863.92123596, 695.31521813,\n",
      "       799.46781486, 876.53041534, 724.65218252, 743.8072481 ,\n",
      "       770.72811344, 722.73099485, 733.79990542, 728.76976372,\n",
      "       866.57003017, 887.52255784, 896.85451539, 877.50053638,\n",
      "       775.79215333, 766.04648599, 824.39341901, 762.38914573,\n",
      "       826.6395924 , 817.13219304, 796.22790491, 846.83172639,\n",
      "       857.07268611, 815.69791161])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([875.33317673, 794.92274497, 836.89239776, 867.16387476,\n",
      "       826.78026331, 841.22937628, 774.8583325 , 851.97983988,\n",
      "       839.78847353, 806.18137957, 876.3270062 , 822.22514043,\n",
      "       755.48010844, 729.86087559, 832.20711867, 784.38456467,\n",
      "       902.87694654, 773.59564939, 889.41788188, 887.39322146])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 970.16030028,  818.00754688,  784.76478819,  821.16300342,\n",
      "        868.15554952,  909.64502365,  861.98607758,  884.23519565,\n",
      "        854.64268169,  844.90353406,  807.12054261,  815.2239179 ,\n",
      "        802.85865266,  814.48982293,  923.60552682,  869.34745536,\n",
      "       1012.69305322,  979.26171637,  929.55501488,  868.02206953,\n",
      "        922.48195382,  889.64371938,  902.27492431, 1036.70076886,\n",
      "        959.19804806,  973.71324439,  902.93070215,  900.14532383,\n",
      "       1000.23202533,  921.42712426])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 998.73708587,  990.95733222,  911.4989099 ,  943.0991758 ,\n",
      "        982.24411303,  975.88649061,  862.21052577,  857.87276556,\n",
      "        893.84910538,  914.7797677 ,  956.00910157,  950.98553127,\n",
      "        869.84354752,  923.32683661,  967.6534664 ,  975.40865419,\n",
      "       1003.22990125, 1003.55157614,  982.53631582,  949.02568453])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([552.61491167, 614.86465122, 596.29610199, 638.34407776,\n",
      "       596.98361388, 521.09639918, 505.46555349, 589.70267609,\n",
      "       636.73604882, 589.67698049, 644.77612963, 561.24317062,\n",
      "       596.79057361, 568.82052745, 576.8558281 , 622.05026979,\n",
      "       594.06184224, 602.02666574, 680.36988082, 633.39966005,\n",
      "       675.71884524, 605.40908925, 650.71947409, 727.41228097,\n",
      "       557.18612905, 562.01216424, 655.97105619, 650.10028912,\n",
      "       668.16176824, 631.85717443])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([648.09903429, 649.27642821, 712.67236658, 696.27975741,\n",
      "       686.1849128 , 649.98371354, 631.18617055, 631.07502274,\n",
      "       673.75730796, 651.98129603, 673.73626744, 777.07324188,\n",
      "       758.2234969 , 680.18442364, 729.54997166, 678.89672873,\n",
      "       568.29571713, 581.78740582, 555.83498193, 615.91695795])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([871.15717441, 780.21953936, 757.60855562, 864.58609949,\n",
      "       881.58160417, 859.3996499 , 862.91979102, 883.65609382,\n",
      "       848.05515331, 819.54479791, 861.61104418, 823.77193688,\n",
      "       871.634727  , 794.34800497, 890.0528615 , 916.1347314 ,\n",
      "       901.84355457, 934.63290199, 984.66471466, 970.16030028,\n",
      "       818.00754688, 784.76478819, 821.16300342, 868.15554952,\n",
      "       909.64502365, 861.98607758, 884.23519565, 854.64268169,\n",
      "       844.90353406, 807.12054261])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 815.2239179 ,  802.85865266,  814.48982293,  923.60552682,\n",
      "        869.34745536, 1012.69305322,  979.26171637,  929.55501488,\n",
      "        868.02206953,  922.48195382,  889.64371938,  902.27492431,\n",
      "       1036.70076886,  959.19804806,  973.71324439,  902.93070215,\n",
      "        900.14532383, 1000.23202533,  921.42712426,  998.73708587])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([841.9761503 , 846.99786451, 871.15717441, 780.21953936,\n",
      "       757.60855562, 864.58609949, 881.58160417, 859.3996499 ,\n",
      "       862.91979102, 883.65609382, 848.05515331, 819.54479791,\n",
      "       861.61104418, 823.77193688, 871.634727  , 794.34800497,\n",
      "       890.0528615 , 916.1347314 , 901.84355457, 934.63290199,\n",
      "       984.66471466, 970.16030028, 818.00754688, 784.76478819,\n",
      "       821.16300342, 868.15554952, 909.64502365, 861.98607758,\n",
      "       884.23519565, 854.64268169])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 844.90353406,  807.12054261,  815.2239179 ,  802.85865266,\n",
      "        814.48982293,  923.60552682,  869.34745536, 1012.69305322,\n",
      "        979.26171637,  929.55501488,  868.02206953,  922.48195382,\n",
      "        889.64371938,  902.27492431, 1036.70076886,  959.19804806,\n",
      "        973.71324439,  902.93070215,  900.14532383, 1000.23202533])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([755.48010844, 729.86087559, 832.20711867, 784.38456467,\n",
      "       902.87694654, 773.59564939, 889.41788188, 887.39322146,\n",
      "       850.96512158, 820.95104652, 858.00387261, 855.81095587,\n",
      "       906.99625232, 882.58263097, 867.8616934 , 841.9761503 ,\n",
      "       846.99786451, 871.15717441, 780.21953936, 757.60855562,\n",
      "       864.58609949, 881.58160417, 859.3996499 , 862.91979102,\n",
      "       883.65609382, 848.05515331, 819.54479791, 861.61104418,\n",
      "       823.77193688, 871.634727  ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([794.34800497, 890.0528615 , 916.1347314 , 901.84355457,\n",
      "       934.63290199, 984.66471466, 970.16030028, 818.00754688,\n",
      "       784.76478819, 821.16300342, 868.15554952, 909.64502365,\n",
      "       861.98607758, 884.23519565, 854.64268169, 844.90353406,\n",
      "       807.12054261, 815.2239179 , 802.85865266, 814.48982293])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([793.98585857, 722.50694506, 884.27854574, 835.09507914,\n",
      "       790.9940369 , 784.06169243, 794.03170193, 782.43241217,\n",
      "       755.51672945, 747.93387285, 773.69529753, 760.13137483,\n",
      "       743.52554413, 780.65476182, 780.51270032, 863.92123596,\n",
      "       695.31521813, 799.46781486, 876.53041534, 724.65218252,\n",
      "       743.8072481 , 770.72811344, 722.73099485, 733.79990542,\n",
      "       728.76976372, 866.57003017, 887.52255784, 896.85451539,\n",
      "       877.50053638, 775.79215333])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([766.04648599, 824.39341901, 762.38914573, 826.6395924 ,\n",
      "       817.13219304, 796.22790491, 846.83172639, 857.07268611,\n",
      "       815.69791161, 875.33317673, 794.92274497, 836.89239776,\n",
      "       867.16387476, 826.78026331, 841.22937628, 774.8583325 ,\n",
      "       851.97983988, 839.78847353, 806.18137957, 876.3270062 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([799.46781486, 876.53041534, 724.65218252, 743.8072481 ,\n",
      "       770.72811344, 722.73099485, 733.79990542, 728.76976372,\n",
      "       866.57003017, 887.52255784, 896.85451539, 877.50053638,\n",
      "       775.79215333, 766.04648599, 824.39341901, 762.38914573,\n",
      "       826.6395924 , 817.13219304, 796.22790491, 846.83172639,\n",
      "       857.07268611, 815.69791161, 875.33317673, 794.92274497,\n",
      "       836.89239776, 867.16387476, 826.78026331, 841.22937628,\n",
      "       774.8583325 , 851.97983988])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([839.78847353, 806.18137957, 876.3270062 , 822.22514043,\n",
      "       755.48010844, 729.86087559, 832.20711867, 784.38456467,\n",
      "       902.87694654, 773.59564939, 889.41788188, 887.39322146,\n",
      "       850.96512158, 820.95104652, 858.00387261, 855.81095587,\n",
      "       906.99625232, 882.58263097, 867.8616934 , 841.9761503 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 975.88649061,  862.21052577,  857.87276556,  893.84910538,\n",
      "        914.7797677 ,  956.00910157,  950.98553127,  869.84354752,\n",
      "        923.32683661,  967.6534664 ,  975.40865419, 1003.22990125,\n",
      "       1003.55157614,  982.53631582,  949.02568453,  858.44878802,\n",
      "        927.72946126,  957.00092337,  961.84957712,  887.94565068,\n",
      "        866.15942091,  972.96430377,  964.76263466,  985.10560512,\n",
      "        967.39478882,  956.53369828, 1001.37762455,  949.18365111,\n",
      "        952.55314568,  935.12600406])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 926.72522669,  933.22378088,  963.24465376,  941.21957118,\n",
      "       1015.54262546,  944.97319267,  940.49716249,  938.69094632,\n",
      "       1029.24446491, 1007.91049914, 1029.7894881 ,  921.02414826,\n",
      "        957.69581689, 1019.86652719,  997.83420854, 1033.61470903,\n",
      "        975.19803339, 1041.13401513, 1123.42793916, 1016.12489876])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([902.87694654, 773.59564939, 889.41788188, 887.39322146,\n",
      "       850.96512158, 820.95104652, 858.00387261, 855.81095587,\n",
      "       906.99625232, 882.58263097, 867.8616934 , 841.9761503 ,\n",
      "       846.99786451, 871.15717441, 780.21953936, 757.60855562,\n",
      "       864.58609949, 881.58160417, 859.3996499 , 862.91979102,\n",
      "       883.65609382, 848.05515331, 819.54479791, 861.61104418,\n",
      "       823.77193688, 871.634727  , 794.34800497, 890.0528615 ,\n",
      "       916.1347314 , 901.84355457])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 934.63290199,  984.66471466,  970.16030028,  818.00754688,\n",
      "        784.76478819,  821.16300342,  868.15554952,  909.64502365,\n",
      "        861.98607758,  884.23519565,  854.64268169,  844.90353406,\n",
      "        807.12054261,  815.2239179 ,  802.85865266,  814.48982293,\n",
      "        923.60552682,  869.34745536, 1012.69305322,  979.26171637])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 973.71324439,  902.93070215,  900.14532383, 1000.23202533,\n",
      "        921.42712426,  998.73708587,  990.95733222,  911.4989099 ,\n",
      "        943.0991758 ,  982.24411303,  975.88649061,  862.21052577,\n",
      "        857.87276556,  893.84910538,  914.7797677 ,  956.00910157,\n",
      "        950.98553127,  869.84354752,  923.32683661,  967.6534664 ,\n",
      "        975.40865419, 1003.22990125, 1003.55157614,  982.53631582,\n",
      "        949.02568453,  858.44878802,  927.72946126,  957.00092337,\n",
      "        961.84957712,  887.94565068])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 866.15942091,  972.96430377,  964.76263466,  985.10560512,\n",
      "        967.39478882,  956.53369828, 1001.37762455,  949.18365111,\n",
      "        952.55314568,  935.12600406,  926.72522669,  933.22378088,\n",
      "        963.24465376,  941.21957118, 1015.54262546,  944.97319267,\n",
      "        940.49716249,  938.69094632, 1029.24446491, 1007.91049914])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([695.31521813, 799.46781486, 876.53041534, 724.65218252,\n",
      "       743.8072481 , 770.72811344, 722.73099485, 733.79990542,\n",
      "       728.76976372, 866.57003017, 887.52255784, 896.85451539,\n",
      "       877.50053638, 775.79215333, 766.04648599, 824.39341901,\n",
      "       762.38914573, 826.6395924 , 817.13219304, 796.22790491,\n",
      "       846.83172639, 857.07268611, 815.69791161, 875.33317673,\n",
      "       794.92274497, 836.89239776, 867.16387476, 826.78026331,\n",
      "       841.22937628, 774.8583325 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([851.97983988, 839.78847353, 806.18137957, 876.3270062 ,\n",
      "       822.22514043, 755.48010844, 729.86087559, 832.20711867,\n",
      "       784.38456467, 902.87694654, 773.59564939, 889.41788188,\n",
      "       887.39322146, 850.96512158, 820.95104652, 858.00387261,\n",
      "       855.81095587, 906.99625232, 882.58263097, 867.8616934 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 868.02206953,  922.48195382,  889.64371938,  902.27492431,\n",
      "       1036.70076886,  959.19804806,  973.71324439,  902.93070215,\n",
      "        900.14532383, 1000.23202533,  921.42712426,  998.73708587,\n",
      "        990.95733222,  911.4989099 ,  943.0991758 ,  982.24411303,\n",
      "        975.88649061,  862.21052577,  857.87276556,  893.84910538,\n",
      "        914.7797677 ,  956.00910157,  950.98553127,  869.84354752,\n",
      "        923.32683661,  967.6534664 ,  975.40865419, 1003.22990125,\n",
      "       1003.55157614,  982.53631582])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 949.02568453,  858.44878802,  927.72946126,  957.00092337,\n",
      "        961.84957712,  887.94565068,  866.15942091,  972.96430377,\n",
      "        964.76263466,  985.10560512,  967.39478882,  956.53369828,\n",
      "       1001.37762455,  949.18365111,  952.55314568,  935.12600406,\n",
      "        926.72522669,  933.22378088,  963.24465376,  941.21957118])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([658.89585885, 607.11396555, 662.96302643, 623.20154476,\n",
      "       551.67016265, 648.06221241, 647.35774585, 555.63289089,\n",
      "       563.85025964, 543.66405584, 506.29295049, 609.75057181,\n",
      "       707.14155238, 571.44576343, 604.4266225 , 577.85684727,\n",
      "       565.12944302, 561.70663192, 546.92000432, 587.34387768,\n",
      "       563.28894392, 603.15946382, 609.79642481, 597.68857407,\n",
      "       561.29017164, 563.99484285, 636.27975237, 653.7286504 ,\n",
      "       580.21410777, 603.71020602])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([655.75754555, 552.61491167, 614.86465122, 596.29610199,\n",
      "       638.34407776, 596.98361388, 521.09639918, 505.46555349,\n",
      "       589.70267609, 636.73604882, 589.67698049, 644.77612963,\n",
      "       561.24317062, 596.79057361, 568.82052745, 576.8558281 ,\n",
      "       622.05026979, 594.06184224, 602.02666574, 680.36988082])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 998.73708587,  990.95733222,  911.4989099 ,  943.0991758 ,\n",
      "        982.24411303,  975.88649061,  862.21052577,  857.87276556,\n",
      "        893.84910538,  914.7797677 ,  956.00910157,  950.98553127,\n",
      "        869.84354752,  923.32683661,  967.6534664 ,  975.40865419,\n",
      "       1003.22990125, 1003.55157614,  982.53631582,  949.02568453,\n",
      "        858.44878802,  927.72946126,  957.00092337,  961.84957712,\n",
      "        887.94565068,  866.15942091,  972.96430377,  964.76263466,\n",
      "        985.10560512,  967.39478882])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 956.53369828, 1001.37762455,  949.18365111,  952.55314568,\n",
      "        935.12600406,  926.72522669,  933.22378088,  963.24465376,\n",
      "        941.21957118, 1015.54262546,  944.97319267,  940.49716249,\n",
      "        938.69094632, 1029.24446491, 1007.91049914, 1029.7894881 ,\n",
      "        921.02414826,  957.69581689, 1019.86652719,  997.83420854])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([762.38914573, 826.6395924 , 817.13219304, 796.22790491,\n",
      "       846.83172639, 857.07268611, 815.69791161, 875.33317673,\n",
      "       794.92274497, 836.89239776, 867.16387476, 826.78026331,\n",
      "       841.22937628, 774.8583325 , 851.97983988, 839.78847353,\n",
      "       806.18137957, 876.3270062 , 822.22514043, 755.48010844,\n",
      "       729.86087559, 832.20711867, 784.38456467, 902.87694654,\n",
      "       773.59564939, 889.41788188, 887.39322146, 850.96512158,\n",
      "       820.95104652, 858.00387261])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([855.81095587, 906.99625232, 882.58263097, 867.8616934 ,\n",
      "       841.9761503 , 846.99786451, 871.15717441, 780.21953936,\n",
      "       757.60855562, 864.58609949, 881.58160417, 859.3996499 ,\n",
      "       862.91979102, 883.65609382, 848.05515331, 819.54479791,\n",
      "       861.61104418, 823.77193688, 871.634727  , 794.34800497])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([726.32636326, 651.49920215, 608.68186599, 758.13233494,\n",
      "       784.85025629, 742.43778409, 744.95265199, 681.04995163,\n",
      "       830.47041928, 788.31397118, 749.23453274, 775.92944027,\n",
      "       776.84226997, 761.63590883, 706.47664247, 747.91110009,\n",
      "       842.84008478, 850.5771681 , 857.88656147, 759.24070059,\n",
      "       692.35079504, 719.83979052, 745.37251598, 804.07326495,\n",
      "       689.79394523, 795.39479652, 773.4935943 , 733.44333496,\n",
      "       696.48647192, 651.50446061])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([760.04488904, 774.24036403, 698.7297109 , 669.98055191,\n",
      "       714.36151018, 834.70411715, 785.46547122, 690.58578351,\n",
      "       723.47617913, 745.1619075 , 627.8142917 , 709.36185236,\n",
      "       750.69673545, 800.72938966, 880.17979644, 873.23872791,\n",
      "       793.98585857, 722.50694506, 884.27854574, 835.09507914])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([723.09299308, 592.89210242, 653.62634842, 724.79417847,\n",
      "       630.92834603, 717.67629451, 728.46205325, 681.62189532,\n",
      "       718.02118063, 820.5333744 , 755.39247769, 721.11223575,\n",
      "       683.36485669, 648.91690775, 722.89139063, 674.15797097,\n",
      "       690.34147284, 679.89555162, 718.86986728, 732.00333529,\n",
      "       767.83519502, 707.24881753, 689.61478723, 656.32769369,\n",
      "       668.64987017, 719.83064084, 758.57681546, 687.1679321 ,\n",
      "       743.60502328, 802.44119057])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([771.56266707, 828.12820822, 726.32636326, 651.49920215,\n",
      "       608.68186599, 758.13233494, 784.85025629, 742.43778409,\n",
      "       744.95265199, 681.04995163, 830.47041928, 788.31397118,\n",
      "       749.23453274, 775.92944027, 776.84226997, 761.63590883,\n",
      "       706.47664247, 747.91110009, 842.84008478, 850.5771681 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([876.53041534, 724.65218252, 743.8072481 , 770.72811344,\n",
      "       722.73099485, 733.79990542, 728.76976372, 866.57003017,\n",
      "       887.52255784, 896.85451539, 877.50053638, 775.79215333,\n",
      "       766.04648599, 824.39341901, 762.38914573, 826.6395924 ,\n",
      "       817.13219304, 796.22790491, 846.83172639, 857.07268611,\n",
      "       815.69791161, 875.33317673, 794.92274497, 836.89239776,\n",
      "       867.16387476, 826.78026331, 841.22937628, 774.8583325 ,\n",
      "       851.97983988, 839.78847353])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([806.18137957, 876.3270062 , 822.22514043, 755.48010844,\n",
      "       729.86087559, 832.20711867, 784.38456467, 902.87694654,\n",
      "       773.59564939, 889.41788188, 887.39322146, 850.96512158,\n",
      "       820.95104652, 858.00387261, 855.81095587, 906.99625232,\n",
      "       882.58263097, 867.8616934 , 841.9761503 , 846.99786451])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 956.00910157,  950.98553127,  869.84354752,  923.32683661,\n",
      "        967.6534664 ,  975.40865419, 1003.22990125, 1003.55157614,\n",
      "        982.53631582,  949.02568453,  858.44878802,  927.72946126,\n",
      "        957.00092337,  961.84957712,  887.94565068,  866.15942091,\n",
      "        972.96430377,  964.76263466,  985.10560512,  967.39478882,\n",
      "        956.53369828, 1001.37762455,  949.18365111,  952.55314568,\n",
      "        935.12600406,  926.72522669,  933.22378088,  963.24465376,\n",
      "        941.21957118, 1015.54262546])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 944.97319267,  940.49716249,  938.69094632, 1029.24446491,\n",
      "       1007.91049914, 1029.7894881 ,  921.02414826,  957.69581689,\n",
      "       1019.86652719,  997.83420854, 1033.61470903,  975.19803339,\n",
      "       1041.13401513, 1123.42793916, 1016.12489876,  960.41402604,\n",
      "       1046.97405111, 1092.88908797,  997.59030213,  960.7578993 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([850.5771681 , 857.88656147, 759.24070059, 692.35079504,\n",
      "       719.83979052, 745.37251598, 804.07326495, 689.79394523,\n",
      "       795.39479652, 773.4935943 , 733.44333496, 696.48647192,\n",
      "       651.50446061, 760.04488904, 774.24036403, 698.7297109 ,\n",
      "       669.98055191, 714.36151018, 834.70411715, 785.46547122,\n",
      "       690.58578351, 723.47617913, 745.1619075 , 627.8142917 ,\n",
      "       709.36185236, 750.69673545, 800.72938966, 880.17979644,\n",
      "       873.23872791, 793.98585857])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([722.50694506, 884.27854574, 835.09507914, 790.9940369 ,\n",
      "       784.06169243, 794.03170193, 782.43241217, 755.51672945,\n",
      "       747.93387285, 773.69529753, 760.13137483, 743.52554413,\n",
      "       780.65476182, 780.51270032, 863.92123596, 695.31521813,\n",
      "       799.46781486, 876.53041534, 724.65218252, 743.8072481 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 949.02568453,  858.44878802,  927.72946126,  957.00092337,\n",
      "        961.84957712,  887.94565068,  866.15942091,  972.96430377,\n",
      "        964.76263466,  985.10560512,  967.39478882,  956.53369828,\n",
      "       1001.37762455,  949.18365111,  952.55314568,  935.12600406,\n",
      "        926.72522669,  933.22378088,  963.24465376,  941.21957118,\n",
      "       1015.54262546,  944.97319267,  940.49716249,  938.69094632,\n",
      "       1029.24446491, 1007.91049914, 1029.7894881 ,  921.02414826,\n",
      "        957.69581689, 1019.86652719])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 997.83420854, 1033.61470903,  975.19803339, 1041.13401513,\n",
      "       1123.42793916, 1016.12489876,  960.41402604, 1046.97405111,\n",
      "       1092.88908797,  997.59030213,  960.7578993 ,  964.83221813,\n",
      "        914.84459202,  914.65709275,  917.36114995,  928.99988218,\n",
      "        971.59260492, 1002.54363929, 1077.50206276,  981.41467242])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([896.85451539, 877.50053638, 775.79215333, 766.04648599,\n",
      "       824.39341901, 762.38914573, 826.6395924 , 817.13219304,\n",
      "       796.22790491, 846.83172639, 857.07268611, 815.69791161,\n",
      "       875.33317673, 794.92274497, 836.89239776, 867.16387476,\n",
      "       826.78026331, 841.22937628, 774.8583325 , 851.97983988,\n",
      "       839.78847353, 806.18137957, 876.3270062 , 822.22514043,\n",
      "       755.48010844, 729.86087559, 832.20711867, 784.38456467,\n",
      "       902.87694654, 773.59564939])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([889.41788188, 887.39322146, 850.96512158, 820.95104652,\n",
      "       858.00387261, 855.81095587, 906.99625232, 882.58263097,\n",
      "       867.8616934 , 841.9761503 , 846.99786451, 871.15717441,\n",
      "       780.21953936, 757.60855562, 864.58609949, 881.58160417,\n",
      "       859.3996499 , 862.91979102, 883.65609382, 848.05515331])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([651.49920215, 608.68186599, 758.13233494, 784.85025629,\n",
      "       742.43778409, 744.95265199, 681.04995163, 830.47041928,\n",
      "       788.31397118, 749.23453274, 775.92944027, 776.84226997,\n",
      "       761.63590883, 706.47664247, 747.91110009, 842.84008478,\n",
      "       850.5771681 , 857.88656147, 759.24070059, 692.35079504,\n",
      "       719.83979052, 745.37251598, 804.07326495, 689.79394523,\n",
      "       795.39479652, 773.4935943 , 733.44333496, 696.48647192,\n",
      "       651.50446061, 760.04488904])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([774.24036403, 698.7297109 , 669.98055191, 714.36151018,\n",
      "       834.70411715, 785.46547122, 690.58578351, 723.47617913,\n",
      "       745.1619075 , 627.8142917 , 709.36185236, 750.69673545,\n",
      "       800.72938966, 880.17979644, 873.23872791, 793.98585857,\n",
      "       722.50694506, 884.27854574, 835.09507914, 790.9940369 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([609.75057181, 707.14155238, 571.44576343, 604.4266225 ,\n",
      "       577.85684727, 565.12944302, 561.70663192, 546.92000432,\n",
      "       587.34387768, 563.28894392, 603.15946382, 609.79642481,\n",
      "       597.68857407, 561.29017164, 563.99484285, 636.27975237,\n",
      "       653.7286504 , 580.21410777, 603.71020602, 655.75754555,\n",
      "       552.61491167, 614.86465122, 596.29610199, 638.34407776,\n",
      "       596.98361388, 521.09639918, 505.46555349, 589.70267609,\n",
      "       636.73604882, 589.67698049])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([644.77612963, 561.24317062, 596.79057361, 568.82052745,\n",
      "       576.8558281 , 622.05026979, 594.06184224, 602.02666574,\n",
      "       680.36988082, 633.39966005, 675.71884524, 605.40908925,\n",
      "       650.71947409, 727.41228097, 557.18612905, 562.01216424,\n",
      "       655.97105619, 650.10028912, 668.16176824, 631.85717443])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 943.0991758 ,  982.24411303,  975.88649061,  862.21052577,\n",
      "        857.87276556,  893.84910538,  914.7797677 ,  956.00910157,\n",
      "        950.98553127,  869.84354752,  923.32683661,  967.6534664 ,\n",
      "        975.40865419, 1003.22990125, 1003.55157614,  982.53631582,\n",
      "        949.02568453,  858.44878802,  927.72946126,  957.00092337,\n",
      "        961.84957712,  887.94565068,  866.15942091,  972.96430377,\n",
      "        964.76263466,  985.10560512,  967.39478882,  956.53369828,\n",
      "       1001.37762455,  949.18365111])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 952.55314568,  935.12600406,  926.72522669,  933.22378088,\n",
      "        963.24465376,  941.21957118, 1015.54262546,  944.97319267,\n",
      "        940.49716249,  938.69094632, 1029.24446491, 1007.91049914,\n",
      "       1029.7894881 ,  921.02414826,  957.69581689, 1019.86652719,\n",
      "        997.83420854, 1033.61470903,  975.19803339, 1041.13401513])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([906.99625232, 882.58263097, 867.8616934 , 841.9761503 ,\n",
      "       846.99786451, 871.15717441, 780.21953936, 757.60855562,\n",
      "       864.58609949, 881.58160417, 859.3996499 , 862.91979102,\n",
      "       883.65609382, 848.05515331, 819.54479791, 861.61104418,\n",
      "       823.77193688, 871.634727  , 794.34800497, 890.0528615 ,\n",
      "       916.1347314 , 901.84355457, 934.63290199, 984.66471466,\n",
      "       970.16030028, 818.00754688, 784.76478819, 821.16300342,\n",
      "       868.15554952, 909.64502365])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 861.98607758,  884.23519565,  854.64268169,  844.90353406,\n",
      "        807.12054261,  815.2239179 ,  802.85865266,  814.48982293,\n",
      "        923.60552682,  869.34745536, 1012.69305322,  979.26171637,\n",
      "        929.55501488,  868.02206953,  922.48195382,  889.64371938,\n",
      "        902.27492431, 1036.70076886,  959.19804806,  973.71324439])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([681.04995163, 830.47041928, 788.31397118, 749.23453274,\n",
      "       775.92944027, 776.84226997, 761.63590883, 706.47664247,\n",
      "       747.91110009, 842.84008478, 850.5771681 , 857.88656147,\n",
      "       759.24070059, 692.35079504, 719.83979052, 745.37251598,\n",
      "       804.07326495, 689.79394523, 795.39479652, 773.4935943 ,\n",
      "       733.44333496, 696.48647192, 651.50446061, 760.04488904,\n",
      "       774.24036403, 698.7297109 , 669.98055191, 714.36151018,\n",
      "       834.70411715, 785.46547122])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([690.58578351, 723.47617913, 745.1619075 , 627.8142917 ,\n",
      "       709.36185236, 750.69673545, 800.72938966, 880.17979644,\n",
      "       873.23872791, 793.98585857, 722.50694506, 884.27854574,\n",
      "       835.09507914, 790.9940369 , 784.06169243, 794.03170193,\n",
      "       782.43241217, 755.51672945, 747.93387285, 773.69529753])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([864.58609949, 881.58160417, 859.3996499 , 862.91979102,\n",
      "       883.65609382, 848.05515331, 819.54479791, 861.61104418,\n",
      "       823.77193688, 871.634727  , 794.34800497, 890.0528615 ,\n",
      "       916.1347314 , 901.84355457, 934.63290199, 984.66471466,\n",
      "       970.16030028, 818.00754688, 784.76478819, 821.16300342,\n",
      "       868.15554952, 909.64502365, 861.98607758, 884.23519565,\n",
      "       854.64268169, 844.90353406, 807.12054261, 815.2239179 ,\n",
      "       802.85865266, 814.48982293])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 923.60552682,  869.34745536, 1012.69305322,  979.26171637,\n",
      "        929.55501488,  868.02206953,  922.48195382,  889.64371938,\n",
      "        902.27492431, 1036.70076886,  959.19804806,  973.71324439,\n",
      "        902.93070215,  900.14532383, 1000.23202533,  921.42712426,\n",
      "        998.73708587,  990.95733222,  911.4989099 ,  943.0991758 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([866.57003017, 887.52255784, 896.85451539, 877.50053638,\n",
      "       775.79215333, 766.04648599, 824.39341901, 762.38914573,\n",
      "       826.6395924 , 817.13219304, 796.22790491, 846.83172639,\n",
      "       857.07268611, 815.69791161, 875.33317673, 794.92274497,\n",
      "       836.89239776, 867.16387476, 826.78026331, 841.22937628,\n",
      "       774.8583325 , 851.97983988, 839.78847353, 806.18137957,\n",
      "       876.3270062 , 822.22514043, 755.48010844, 729.86087559,\n",
      "       832.20711867, 784.38456467])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([902.87694654, 773.59564939, 889.41788188, 887.39322146,\n",
      "       850.96512158, 820.95104652, 858.00387261, 855.81095587,\n",
      "       906.99625232, 882.58263097, 867.8616934 , 841.9761503 ,\n",
      "       846.99786451, 871.15717441, 780.21953936, 757.60855562,\n",
      "       864.58609949, 881.58160417, 859.3996499 , 862.91979102])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([627.8142917 , 709.36185236, 750.69673545, 800.72938966,\n",
      "       880.17979644, 873.23872791, 793.98585857, 722.50694506,\n",
      "       884.27854574, 835.09507914, 790.9940369 , 784.06169243,\n",
      "       794.03170193, 782.43241217, 755.51672945, 747.93387285,\n",
      "       773.69529753, 760.13137483, 743.52554413, 780.65476182,\n",
      "       780.51270032, 863.92123596, 695.31521813, 799.46781486,\n",
      "       876.53041534, 724.65218252, 743.8072481 , 770.72811344,\n",
      "       722.73099485, 733.79990542])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([728.76976372, 866.57003017, 887.52255784, 896.85451539,\n",
      "       877.50053638, 775.79215333, 766.04648599, 824.39341901,\n",
      "       762.38914573, 826.6395924 , 817.13219304, 796.22790491,\n",
      "       846.83172639, 857.07268611, 815.69791161, 875.33317673,\n",
      "       794.92274497, 836.89239776, 867.16387476, 826.78026331])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([846.99786451, 871.15717441, 780.21953936, 757.60855562,\n",
      "       864.58609949, 881.58160417, 859.3996499 , 862.91979102,\n",
      "       883.65609382, 848.05515331, 819.54479791, 861.61104418,\n",
      "       823.77193688, 871.634727  , 794.34800497, 890.0528615 ,\n",
      "       916.1347314 , 901.84355457, 934.63290199, 984.66471466,\n",
      "       970.16030028, 818.00754688, 784.76478819, 821.16300342,\n",
      "       868.15554952, 909.64502365, 861.98607758, 884.23519565,\n",
      "       854.64268169, 844.90353406])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 807.12054261,  815.2239179 ,  802.85865266,  814.48982293,\n",
      "        923.60552682,  869.34745536, 1012.69305322,  979.26171637,\n",
      "        929.55501488,  868.02206953,  922.48195382,  889.64371938,\n",
      "        902.27492431, 1036.70076886,  959.19804806,  973.71324439,\n",
      "        902.93070215,  900.14532383, 1000.23202533,  921.42712426])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 901.84355457,  934.63290199,  984.66471466,  970.16030028,\n",
      "        818.00754688,  784.76478819,  821.16300342,  868.15554952,\n",
      "        909.64502365,  861.98607758,  884.23519565,  854.64268169,\n",
      "        844.90353406,  807.12054261,  815.2239179 ,  802.85865266,\n",
      "        814.48982293,  923.60552682,  869.34745536, 1012.69305322,\n",
      "        979.26171637,  929.55501488,  868.02206953,  922.48195382,\n",
      "        889.64371938,  902.27492431, 1036.70076886,  959.19804806,\n",
      "        973.71324439,  902.93070215])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 900.14532383, 1000.23202533,  921.42712426,  998.73708587,\n",
      "        990.95733222,  911.4989099 ,  943.0991758 ,  982.24411303,\n",
      "        975.88649061,  862.21052577,  857.87276556,  893.84910538,\n",
      "        914.7797677 ,  956.00910157,  950.98553127,  869.84354752,\n",
      "        923.32683661,  967.6534664 ,  975.40865419, 1003.22990125])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 814.48982293,  923.60552682,  869.34745536, 1012.69305322,\n",
      "        979.26171637,  929.55501488,  868.02206953,  922.48195382,\n",
      "        889.64371938,  902.27492431, 1036.70076886,  959.19804806,\n",
      "        973.71324439,  902.93070215,  900.14532383, 1000.23202533,\n",
      "        921.42712426,  998.73708587,  990.95733222,  911.4989099 ,\n",
      "        943.0991758 ,  982.24411303,  975.88649061,  862.21052577,\n",
      "        857.87276556,  893.84910538,  914.7797677 ,  956.00910157,\n",
      "        950.98553127,  869.84354752])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 923.32683661,  967.6534664 ,  975.40865419, 1003.22990125,\n",
      "       1003.55157614,  982.53631582,  949.02568453,  858.44878802,\n",
      "        927.72946126,  957.00092337,  961.84957712,  887.94565068,\n",
      "        866.15942091,  972.96430377,  964.76263466,  985.10560512,\n",
      "        967.39478882,  956.53369828, 1001.37762455,  949.18365111])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1003.22990125, 1003.55157614,  982.53631582,  949.02568453,\n",
      "        858.44878802,  927.72946126,  957.00092337,  961.84957712,\n",
      "        887.94565068,  866.15942091,  972.96430377,  964.76263466,\n",
      "        985.10560512,  967.39478882,  956.53369828, 1001.37762455,\n",
      "        949.18365111,  952.55314568,  935.12600406,  926.72522669,\n",
      "        933.22378088,  963.24465376,  941.21957118, 1015.54262546,\n",
      "        944.97319267,  940.49716249,  938.69094632, 1029.24446491,\n",
      "       1007.91049914, 1029.7894881 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 921.02414826,  957.69581689, 1019.86652719,  997.83420854,\n",
      "       1033.61470903,  975.19803339, 1041.13401513, 1123.42793916,\n",
      "       1016.12489876,  960.41402604, 1046.97405111, 1092.88908797,\n",
      "        997.59030213,  960.7578993 ,  964.83221813,  914.84459202,\n",
      "        914.65709275,  917.36114995,  928.99988218,  971.59260492])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 794.34800497,  890.0528615 ,  916.1347314 ,  901.84355457,\n",
      "        934.63290199,  984.66471466,  970.16030028,  818.00754688,\n",
      "        784.76478819,  821.16300342,  868.15554952,  909.64502365,\n",
      "        861.98607758,  884.23519565,  854.64268169,  844.90353406,\n",
      "        807.12054261,  815.2239179 ,  802.85865266,  814.48982293,\n",
      "        923.60552682,  869.34745536, 1012.69305322,  979.26171637,\n",
      "        929.55501488,  868.02206953,  922.48195382,  889.64371938,\n",
      "        902.27492431, 1036.70076886])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 959.19804806,  973.71324439,  902.93070215,  900.14532383,\n",
      "       1000.23202533,  921.42712426,  998.73708587,  990.95733222,\n",
      "        911.4989099 ,  943.0991758 ,  982.24411303,  975.88649061,\n",
      "        862.21052577,  857.87276556,  893.84910538,  914.7797677 ,\n",
      "        956.00910157,  950.98553127,  869.84354752,  923.32683661])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([723.47617913, 745.1619075 , 627.8142917 , 709.36185236,\n",
      "       750.69673545, 800.72938966, 880.17979644, 873.23872791,\n",
      "       793.98585857, 722.50694506, 884.27854574, 835.09507914,\n",
      "       790.9940369 , 784.06169243, 794.03170193, 782.43241217,\n",
      "       755.51672945, 747.93387285, 773.69529753, 760.13137483,\n",
      "       743.52554413, 780.65476182, 780.51270032, 863.92123596,\n",
      "       695.31521813, 799.46781486, 876.53041534, 724.65218252,\n",
      "       743.8072481 , 770.72811344])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([722.73099485, 733.79990542, 728.76976372, 866.57003017,\n",
      "       887.52255784, 896.85451539, 877.50053638, 775.79215333,\n",
      "       766.04648599, 824.39341901, 762.38914573, 826.6395924 ,\n",
      "       817.13219304, 796.22790491, 846.83172639, 857.07268611,\n",
      "       815.69791161, 875.33317673, 794.92274497, 836.89239776])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 802.85865266,  814.48982293,  923.60552682,  869.34745536,\n",
      "       1012.69305322,  979.26171637,  929.55501488,  868.02206953,\n",
      "        922.48195382,  889.64371938,  902.27492431, 1036.70076886,\n",
      "        959.19804806,  973.71324439,  902.93070215,  900.14532383,\n",
      "       1000.23202533,  921.42712426,  998.73708587,  990.95733222,\n",
      "        911.4989099 ,  943.0991758 ,  982.24411303,  975.88649061,\n",
      "        862.21052577,  857.87276556,  893.84910538,  914.7797677 ,\n",
      "        956.00910157,  950.98553127])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 869.84354752,  923.32683661,  967.6534664 ,  975.40865419,\n",
      "       1003.22990125, 1003.55157614,  982.53631582,  949.02568453,\n",
      "        858.44878802,  927.72946126,  957.00092337,  961.84957712,\n",
      "        887.94565068,  866.15942091,  972.96430377,  964.76263466,\n",
      "        985.10560512,  967.39478882,  956.53369828, 1001.37762455])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([722.73099485, 733.79990542, 728.76976372, 866.57003017,\n",
      "       887.52255784, 896.85451539, 877.50053638, 775.79215333,\n",
      "       766.04648599, 824.39341901, 762.38914573, 826.6395924 ,\n",
      "       817.13219304, 796.22790491, 846.83172639, 857.07268611,\n",
      "       815.69791161, 875.33317673, 794.92274497, 836.89239776,\n",
      "       867.16387476, 826.78026331, 841.22937628, 774.8583325 ,\n",
      "       851.97983988, 839.78847353, 806.18137957, 876.3270062 ,\n",
      "       822.22514043, 755.48010844])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([729.86087559, 832.20711867, 784.38456467, 902.87694654,\n",
      "       773.59564939, 889.41788188, 887.39322146, 850.96512158,\n",
      "       820.95104652, 858.00387261, 855.81095587, 906.99625232,\n",
      "       882.58263097, 867.8616934 , 841.9761503 , 846.99786451,\n",
      "       871.15717441, 780.21953936, 757.60855562, 864.58609949])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 927.72946126,  957.00092337,  961.84957712,  887.94565068,\n",
      "        866.15942091,  972.96430377,  964.76263466,  985.10560512,\n",
      "        967.39478882,  956.53369828, 1001.37762455,  949.18365111,\n",
      "        952.55314568,  935.12600406,  926.72522669,  933.22378088,\n",
      "        963.24465376,  941.21957118, 1015.54262546,  944.97319267,\n",
      "        940.49716249,  938.69094632, 1029.24446491, 1007.91049914,\n",
      "       1029.7894881 ,  921.02414826,  957.69581689, 1019.86652719,\n",
      "        997.83420854, 1033.61470903])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 975.19803339, 1041.13401513, 1123.42793916, 1016.12489876,\n",
      "        960.41402604, 1046.97405111, 1092.88908797,  997.59030213,\n",
      "        960.7578993 ,  964.83221813,  914.84459202,  914.65709275,\n",
      "        917.36114995,  928.99988218,  971.59260492, 1002.54363929,\n",
      "       1077.50206276,  981.41467242, 1031.38843481, 1008.36789473])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 929.55501488,  868.02206953,  922.48195382,  889.64371938,\n",
      "        902.27492431, 1036.70076886,  959.19804806,  973.71324439,\n",
      "        902.93070215,  900.14532383, 1000.23202533,  921.42712426,\n",
      "        998.73708587,  990.95733222,  911.4989099 ,  943.0991758 ,\n",
      "        982.24411303,  975.88649061,  862.21052577,  857.87276556,\n",
      "        893.84910538,  914.7797677 ,  956.00910157,  950.98553127,\n",
      "        869.84354752,  923.32683661,  967.6534664 ,  975.40865419,\n",
      "       1003.22990125, 1003.55157614])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 982.53631582,  949.02568453,  858.44878802,  927.72946126,\n",
      "        957.00092337,  961.84957712,  887.94565068,  866.15942091,\n",
      "        972.96430377,  964.76263466,  985.10560512,  967.39478882,\n",
      "        956.53369828, 1001.37762455,  949.18365111,  952.55314568,\n",
      "        935.12600406,  926.72522669,  933.22378088,  963.24465376])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1001.37762455,  949.18365111,  952.55314568,  935.12600406,\n",
      "        926.72522669,  933.22378088,  963.24465376,  941.21957118,\n",
      "       1015.54262546,  944.97319267,  940.49716249,  938.69094632,\n",
      "       1029.24446491, 1007.91049914, 1029.7894881 ,  921.02414826,\n",
      "        957.69581689, 1019.86652719,  997.83420854, 1033.61470903,\n",
      "        975.19803339, 1041.13401513, 1123.42793916, 1016.12489876,\n",
      "        960.41402604, 1046.97405111, 1092.88908797,  997.59030213,\n",
      "        960.7578993 ,  964.83221813])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 914.84459202,  914.65709275,  917.36114995,  928.99988218,\n",
      "        971.59260492, 1002.54363929, 1077.50206276,  981.41467242,\n",
      "       1031.38843481, 1008.36789473,  996.67797377, 1034.79060041,\n",
      "        961.82867089, 1001.79836244, 1019.2776188 , 1035.55514077,\n",
      "       1033.78454565, 1139.54181843, 1032.6695729 ,  979.85710706])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 900.14532383, 1000.23202533,  921.42712426,  998.73708587,\n",
      "        990.95733222,  911.4989099 ,  943.0991758 ,  982.24411303,\n",
      "        975.88649061,  862.21052577,  857.87276556,  893.84910538,\n",
      "        914.7797677 ,  956.00910157,  950.98553127,  869.84354752,\n",
      "        923.32683661,  967.6534664 ,  975.40865419, 1003.22990125,\n",
      "       1003.55157614,  982.53631582,  949.02568453,  858.44878802,\n",
      "        927.72946126,  957.00092337,  961.84957712,  887.94565068,\n",
      "        866.15942091,  972.96430377])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 964.76263466,  985.10560512,  967.39478882,  956.53369828,\n",
      "       1001.37762455,  949.18365111,  952.55314568,  935.12600406,\n",
      "        926.72522669,  933.22378088,  963.24465376,  941.21957118,\n",
      "       1015.54262546,  944.97319267,  940.49716249,  938.69094632,\n",
      "       1029.24446491, 1007.91049914, 1029.7894881 ,  921.02414826])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([656.32769369, 668.64987017, 719.83064084, 758.57681546,\n",
      "       687.1679321 , 743.60502328, 802.44119057, 771.56266707,\n",
      "       828.12820822, 726.32636326, 651.49920215, 608.68186599,\n",
      "       758.13233494, 784.85025629, 742.43778409, 744.95265199,\n",
      "       681.04995163, 830.47041928, 788.31397118, 749.23453274,\n",
      "       775.92944027, 776.84226997, 761.63590883, 706.47664247,\n",
      "       747.91110009, 842.84008478, 850.5771681 , 857.88656147,\n",
      "       759.24070059, 692.35079504])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([719.83979052, 745.37251598, 804.07326495, 689.79394523,\n",
      "       795.39479652, 773.4935943 , 733.44333496, 696.48647192,\n",
      "       651.50446061, 760.04488904, 774.24036403, 698.7297109 ,\n",
      "       669.98055191, 714.36151018, 834.70411715, 785.46547122,\n",
      "       690.58578351, 723.47617913, 745.1619075 , 627.8142917 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 952.55314568,  935.12600406,  926.72522669,  933.22378088,\n",
      "        963.24465376,  941.21957118, 1015.54262546,  944.97319267,\n",
      "        940.49716249,  938.69094632, 1029.24446491, 1007.91049914,\n",
      "       1029.7894881 ,  921.02414826,  957.69581689, 1019.86652719,\n",
      "        997.83420854, 1033.61470903,  975.19803339, 1041.13401513,\n",
      "       1123.42793916, 1016.12489876,  960.41402604, 1046.97405111,\n",
      "       1092.88908797,  997.59030213,  960.7578993 ,  964.83221813,\n",
      "        914.84459202,  914.65709275])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 917.36114995,  928.99988218,  971.59260492, 1002.54363929,\n",
      "       1077.50206276,  981.41467242, 1031.38843481, 1008.36789473,\n",
      "        996.67797377, 1034.79060041,  961.82867089, 1001.79836244,\n",
      "       1019.2776188 , 1035.55514077, 1033.78454565, 1139.54181843,\n",
      "       1032.6695729 ,  979.85710706,  970.66424996,  971.55787089])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([804.07326495, 689.79394523, 795.39479652, 773.4935943 ,\n",
      "       733.44333496, 696.48647192, 651.50446061, 760.04488904,\n",
      "       774.24036403, 698.7297109 , 669.98055191, 714.36151018,\n",
      "       834.70411715, 785.46547122, 690.58578351, 723.47617913,\n",
      "       745.1619075 , 627.8142917 , 709.36185236, 750.69673545,\n",
      "       800.72938966, 880.17979644, 873.23872791, 793.98585857,\n",
      "       722.50694506, 884.27854574, 835.09507914, 790.9940369 ,\n",
      "       784.06169243, 794.03170193])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([782.43241217, 755.51672945, 747.93387285, 773.69529753,\n",
      "       760.13137483, 743.52554413, 780.65476182, 780.51270032,\n",
      "       863.92123596, 695.31521813, 799.46781486, 876.53041534,\n",
      "       724.65218252, 743.8072481 , 770.72811344, 722.73099485,\n",
      "       733.79990542, 728.76976372, 866.57003017, 887.52255784])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 982.24411303,  975.88649061,  862.21052577,  857.87276556,\n",
      "        893.84910538,  914.7797677 ,  956.00910157,  950.98553127,\n",
      "        869.84354752,  923.32683661,  967.6534664 ,  975.40865419,\n",
      "       1003.22990125, 1003.55157614,  982.53631582,  949.02568453,\n",
      "        858.44878802,  927.72946126,  957.00092337,  961.84957712,\n",
      "        887.94565068,  866.15942091,  972.96430377,  964.76263466,\n",
      "        985.10560512,  967.39478882,  956.53369828, 1001.37762455,\n",
      "        949.18365111,  952.55314568])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 935.12600406,  926.72522669,  933.22378088,  963.24465376,\n",
      "        941.21957118, 1015.54262546,  944.97319267,  940.49716249,\n",
      "        938.69094632, 1029.24446491, 1007.91049914, 1029.7894881 ,\n",
      "        921.02414826,  957.69581689, 1019.86652719,  997.83420854,\n",
      "       1033.61470903,  975.19803339, 1041.13401513, 1123.42793916])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 869.34745536, 1012.69305322,  979.26171637,  929.55501488,\n",
      "        868.02206953,  922.48195382,  889.64371938,  902.27492431,\n",
      "       1036.70076886,  959.19804806,  973.71324439,  902.93070215,\n",
      "        900.14532383, 1000.23202533,  921.42712426,  998.73708587,\n",
      "        990.95733222,  911.4989099 ,  943.0991758 ,  982.24411303,\n",
      "        975.88649061,  862.21052577,  857.87276556,  893.84910538,\n",
      "        914.7797677 ,  956.00910157,  950.98553127,  869.84354752,\n",
      "        923.32683661,  967.6534664 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 975.40865419, 1003.22990125, 1003.55157614,  982.53631582,\n",
      "        949.02568453,  858.44878802,  927.72946126,  957.00092337,\n",
      "        961.84957712,  887.94565068,  866.15942091,  972.96430377,\n",
      "        964.76263466,  985.10560512,  967.39478882,  956.53369828,\n",
      "       1001.37762455,  949.18365111,  952.55314568,  935.12600406])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 893.84910538,  914.7797677 ,  956.00910157,  950.98553127,\n",
      "        869.84354752,  923.32683661,  967.6534664 ,  975.40865419,\n",
      "       1003.22990125, 1003.55157614,  982.53631582,  949.02568453,\n",
      "        858.44878802,  927.72946126,  957.00092337,  961.84957712,\n",
      "        887.94565068,  866.15942091,  972.96430377,  964.76263466,\n",
      "        985.10560512,  967.39478882,  956.53369828, 1001.37762455,\n",
      "        949.18365111,  952.55314568,  935.12600406,  926.72522669,\n",
      "        933.22378088,  963.24465376])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 941.21957118, 1015.54262546,  944.97319267,  940.49716249,\n",
      "        938.69094632, 1029.24446491, 1007.91049914, 1029.7894881 ,\n",
      "        921.02414826,  957.69581689, 1019.86652719,  997.83420854,\n",
      "       1033.61470903,  975.19803339, 1041.13401513, 1123.42793916,\n",
      "       1016.12489876,  960.41402604, 1046.97405111, 1092.88908797])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([841.22937628, 774.8583325 , 851.97983988, 839.78847353,\n",
      "       806.18137957, 876.3270062 , 822.22514043, 755.48010844,\n",
      "       729.86087559, 832.20711867, 784.38456467, 902.87694654,\n",
      "       773.59564939, 889.41788188, 887.39322146, 850.96512158,\n",
      "       820.95104652, 858.00387261, 855.81095587, 906.99625232,\n",
      "       882.58263097, 867.8616934 , 841.9761503 , 846.99786451,\n",
      "       871.15717441, 780.21953936, 757.60855562, 864.58609949,\n",
      "       881.58160417, 859.3996499 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([862.91979102, 883.65609382, 848.05515331, 819.54479791,\n",
      "       861.61104418, 823.77193688, 871.634727  , 794.34800497,\n",
      "       890.0528615 , 916.1347314 , 901.84355457, 934.63290199,\n",
      "       984.66471466, 970.16030028, 818.00754688, 784.76478819,\n",
      "       821.16300342, 868.15554952, 909.64502365, 861.98607758])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 887.94565068,  866.15942091,  972.96430377,  964.76263466,\n",
      "        985.10560512,  967.39478882,  956.53369828, 1001.37762455,\n",
      "        949.18365111,  952.55314568,  935.12600406,  926.72522669,\n",
      "        933.22378088,  963.24465376,  941.21957118, 1015.54262546,\n",
      "        944.97319267,  940.49716249,  938.69094632, 1029.24446491,\n",
      "       1007.91049914, 1029.7894881 ,  921.02414826,  957.69581689,\n",
      "       1019.86652719,  997.83420854, 1033.61470903,  975.19803339,\n",
      "       1041.13401513, 1123.42793916])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1016.12489876,  960.41402604, 1046.97405111, 1092.88908797,\n",
      "        997.59030213,  960.7578993 ,  964.83221813,  914.84459202,\n",
      "        914.65709275,  917.36114995,  928.99988218,  971.59260492,\n",
      "       1002.54363929, 1077.50206276,  981.41467242, 1031.38843481,\n",
      "       1008.36789473,  996.67797377, 1034.79060041,  961.82867089])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 844.90353406,  807.12054261,  815.2239179 ,  802.85865266,\n",
      "        814.48982293,  923.60552682,  869.34745536, 1012.69305322,\n",
      "        979.26171637,  929.55501488,  868.02206953,  922.48195382,\n",
      "        889.64371938,  902.27492431, 1036.70076886,  959.19804806,\n",
      "        973.71324439,  902.93070215,  900.14532383, 1000.23202533,\n",
      "        921.42712426,  998.73708587,  990.95733222,  911.4989099 ,\n",
      "        943.0991758 ,  982.24411303,  975.88649061,  862.21052577,\n",
      "        857.87276556,  893.84910538])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 914.7797677 ,  956.00910157,  950.98553127,  869.84354752,\n",
      "        923.32683661,  967.6534664 ,  975.40865419, 1003.22990125,\n",
      "       1003.55157614,  982.53631582,  949.02568453,  858.44878802,\n",
      "        927.72946126,  957.00092337,  961.84957712,  887.94565068,\n",
      "        866.15942091,  972.96430377,  964.76263466,  985.10560512])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([875.33317673, 794.92274497, 836.89239776, 867.16387476,\n",
      "       826.78026331, 841.22937628, 774.8583325 , 851.97983988,\n",
      "       839.78847353, 806.18137957, 876.3270062 , 822.22514043,\n",
      "       755.48010844, 729.86087559, 832.20711867, 784.38456467,\n",
      "       902.87694654, 773.59564939, 889.41788188, 887.39322146,\n",
      "       850.96512158, 820.95104652, 858.00387261, 855.81095587,\n",
      "       906.99625232, 882.58263097, 867.8616934 , 841.9761503 ,\n",
      "       846.99786451, 871.15717441])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([780.21953936, 757.60855562, 864.58609949, 881.58160417,\n",
      "       859.3996499 , 862.91979102, 883.65609382, 848.05515331,\n",
      "       819.54479791, 861.61104418, 823.77193688, 871.634727  ,\n",
      "       794.34800497, 890.0528615 , 916.1347314 , 901.84355457,\n",
      "       934.63290199, 984.66471466, 970.16030028, 818.00754688])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([774.8583325 , 851.97983988, 839.78847353, 806.18137957,\n",
      "       876.3270062 , 822.22514043, 755.48010844, 729.86087559,\n",
      "       832.20711867, 784.38456467, 902.87694654, 773.59564939,\n",
      "       889.41788188, 887.39322146, 850.96512158, 820.95104652,\n",
      "       858.00387261, 855.81095587, 906.99625232, 882.58263097,\n",
      "       867.8616934 , 841.9761503 , 846.99786451, 871.15717441,\n",
      "       780.21953936, 757.60855562, 864.58609949, 881.58160417,\n",
      "       859.3996499 , 862.91979102])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([883.65609382, 848.05515331, 819.54479791, 861.61104418,\n",
      "       823.77193688, 871.634727  , 794.34800497, 890.0528615 ,\n",
      "       916.1347314 , 901.84355457, 934.63290199, 984.66471466,\n",
      "       970.16030028, 818.00754688, 784.76478819, 821.16300342,\n",
      "       868.15554952, 909.64502365, 861.98607758, 884.23519565])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 868.15554952,  909.64502365,  861.98607758,  884.23519565,\n",
      "        854.64268169,  844.90353406,  807.12054261,  815.2239179 ,\n",
      "        802.85865266,  814.48982293,  923.60552682,  869.34745536,\n",
      "       1012.69305322,  979.26171637,  929.55501488,  868.02206953,\n",
      "        922.48195382,  889.64371938,  902.27492431, 1036.70076886,\n",
      "        959.19804806,  973.71324439,  902.93070215,  900.14532383,\n",
      "       1000.23202533,  921.42712426,  998.73708587,  990.95733222,\n",
      "        911.4989099 ,  943.0991758 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 982.24411303,  975.88649061,  862.21052577,  857.87276556,\n",
      "        893.84910538,  914.7797677 ,  956.00910157,  950.98553127,\n",
      "        869.84354752,  923.32683661,  967.6534664 ,  975.40865419,\n",
      "       1003.22990125, 1003.55157614,  982.53631582,  949.02568453,\n",
      "        858.44878802,  927.72946126,  957.00092337,  961.84957712])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 990.95733222,  911.4989099 ,  943.0991758 ,  982.24411303,\n",
      "        975.88649061,  862.21052577,  857.87276556,  893.84910538,\n",
      "        914.7797677 ,  956.00910157,  950.98553127,  869.84354752,\n",
      "        923.32683661,  967.6534664 ,  975.40865419, 1003.22990125,\n",
      "       1003.55157614,  982.53631582,  949.02568453,  858.44878802,\n",
      "        927.72946126,  957.00092337,  961.84957712,  887.94565068,\n",
      "        866.15942091,  972.96430377,  964.76263466,  985.10560512,\n",
      "        967.39478882,  956.53369828])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1001.37762455,  949.18365111,  952.55314568,  935.12600406,\n",
      "        926.72522669,  933.22378088,  963.24465376,  941.21957118,\n",
      "       1015.54262546,  944.97319267,  940.49716249,  938.69094632,\n",
      "       1029.24446491, 1007.91049914, 1029.7894881 ,  921.02414826,\n",
      "        957.69581689, 1019.86652719,  997.83420854, 1033.61470903])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1033.61470903,  975.19803339, 1041.13401513, 1123.42793916,\n",
      "       1016.12489876,  960.41402604, 1046.97405111, 1092.88908797,\n",
      "        997.59030213,  960.7578993 ,  964.83221813,  914.84459202,\n",
      "        914.65709275,  917.36114995,  928.99988218,  971.59260492,\n",
      "       1002.54363929, 1077.50206276,  981.41467242, 1031.38843481,\n",
      "       1008.36789473,  996.67797377, 1034.79060041,  961.82867089,\n",
      "       1001.79836244, 1019.2776188 , 1035.55514077, 1033.78454565,\n",
      "       1139.54181843, 1032.6695729 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 979.85710706,  970.66424996,  971.55787089,  969.39897322,\n",
      "       1059.79104685, 1111.27315424, 1024.00153111,  971.80503587,\n",
      "       1026.55935047, 1002.37166937, 1044.2419652 , 1047.00466984,\n",
      "        956.61527463, 1073.01406201, 1146.43226916, 1041.31184508,\n",
      "       1005.12645319, 1037.76355434, 1054.73861892, 1074.97384986])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 975.40865419, 1003.22990125, 1003.55157614,  982.53631582,\n",
      "        949.02568453,  858.44878802,  927.72946126,  957.00092337,\n",
      "        961.84957712,  887.94565068,  866.15942091,  972.96430377,\n",
      "        964.76263466,  985.10560512,  967.39478882,  956.53369828,\n",
      "       1001.37762455,  949.18365111,  952.55314568,  935.12600406,\n",
      "        926.72522669,  933.22378088,  963.24465376,  941.21957118,\n",
      "       1015.54262546,  944.97319267,  940.49716249,  938.69094632,\n",
      "       1029.24446491, 1007.91049914])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1029.7894881 ,  921.02414826,  957.69581689, 1019.86652719,\n",
      "        997.83420854, 1033.61470903,  975.19803339, 1041.13401513,\n",
      "       1123.42793916, 1016.12489876,  960.41402604, 1046.97405111,\n",
      "       1092.88908797,  997.59030213,  960.7578993 ,  964.83221813,\n",
      "        914.84459202,  914.65709275,  917.36114995,  928.99988218])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([859.3996499 , 862.91979102, 883.65609382, 848.05515331,\n",
      "       819.54479791, 861.61104418, 823.77193688, 871.634727  ,\n",
      "       794.34800497, 890.0528615 , 916.1347314 , 901.84355457,\n",
      "       934.63290199, 984.66471466, 970.16030028, 818.00754688,\n",
      "       784.76478819, 821.16300342, 868.15554952, 909.64502365,\n",
      "       861.98607758, 884.23519565, 854.64268169, 844.90353406,\n",
      "       807.12054261, 815.2239179 , 802.85865266, 814.48982293,\n",
      "       923.60552682, 869.34745536])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1012.69305322,  979.26171637,  929.55501488,  868.02206953,\n",
      "        922.48195382,  889.64371938,  902.27492431, 1036.70076886,\n",
      "        959.19804806,  973.71324439,  902.93070215,  900.14532383,\n",
      "       1000.23202533,  921.42712426,  998.73708587,  990.95733222,\n",
      "        911.4989099 ,  943.0991758 ,  982.24411303,  975.88649061])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 883.65609382,  848.05515331,  819.54479791,  861.61104418,\n",
      "        823.77193688,  871.634727  ,  794.34800497,  890.0528615 ,\n",
      "        916.1347314 ,  901.84355457,  934.63290199,  984.66471466,\n",
      "        970.16030028,  818.00754688,  784.76478819,  821.16300342,\n",
      "        868.15554952,  909.64502365,  861.98607758,  884.23519565,\n",
      "        854.64268169,  844.90353406,  807.12054261,  815.2239179 ,\n",
      "        802.85865266,  814.48982293,  923.60552682,  869.34745536,\n",
      "       1012.69305322,  979.26171637])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 929.55501488,  868.02206953,  922.48195382,  889.64371938,\n",
      "        902.27492431, 1036.70076886,  959.19804806,  973.71324439,\n",
      "        902.93070215,  900.14532383, 1000.23202533,  921.42712426,\n",
      "        998.73708587,  990.95733222,  911.4989099 ,  943.0991758 ,\n",
      "        982.24411303,  975.88649061,  862.21052577,  857.87276556])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1123.42793916, 1016.12489876,  960.41402604, 1046.97405111,\n",
      "       1092.88908797,  997.59030213,  960.7578993 ,  964.83221813,\n",
      "        914.84459202,  914.65709275,  917.36114995,  928.99988218,\n",
      "        971.59260492, 1002.54363929, 1077.50206276,  981.41467242,\n",
      "       1031.38843481, 1008.36789473,  996.67797377, 1034.79060041,\n",
      "        961.82867089, 1001.79836244, 1019.2776188 , 1035.55514077,\n",
      "       1033.78454565, 1139.54181843, 1032.6695729 ,  979.85710706,\n",
      "        970.66424996,  971.55787089])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 969.39897322, 1059.79104685, 1111.27315424, 1024.00153111,\n",
      "        971.80503587, 1026.55935047, 1002.37166937, 1044.2419652 ,\n",
      "       1047.00466984,  956.61527463, 1073.01406201, 1146.43226916,\n",
      "       1041.31184508, 1005.12645319, 1037.76355434, 1054.73861892,\n",
      "       1074.97384986, 1150.99978669, 1151.27744666, 1079.38867994])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 807.12054261,  815.2239179 ,  802.85865266,  814.48982293,\n",
      "        923.60552682,  869.34745536, 1012.69305322,  979.26171637,\n",
      "        929.55501488,  868.02206953,  922.48195382,  889.64371938,\n",
      "        902.27492431, 1036.70076886,  959.19804806,  973.71324439,\n",
      "        902.93070215,  900.14532383, 1000.23202533,  921.42712426,\n",
      "        998.73708587,  990.95733222,  911.4989099 ,  943.0991758 ,\n",
      "        982.24411303,  975.88649061,  862.21052577,  857.87276556,\n",
      "        893.84910538,  914.7797677 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 956.00910157,  950.98553127,  869.84354752,  923.32683661,\n",
      "        967.6534664 ,  975.40865419, 1003.22990125, 1003.55157614,\n",
      "        982.53631582,  949.02568453,  858.44878802,  927.72946126,\n",
      "        957.00092337,  961.84957712,  887.94565068,  866.15942091,\n",
      "        972.96430377,  964.76263466,  985.10560512,  967.39478882])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 934.63290199,  984.66471466,  970.16030028,  818.00754688,\n",
      "        784.76478819,  821.16300342,  868.15554952,  909.64502365,\n",
      "        861.98607758,  884.23519565,  854.64268169,  844.90353406,\n",
      "        807.12054261,  815.2239179 ,  802.85865266,  814.48982293,\n",
      "        923.60552682,  869.34745536, 1012.69305322,  979.26171637,\n",
      "        929.55501488,  868.02206953,  922.48195382,  889.64371938,\n",
      "        902.27492431, 1036.70076886,  959.19804806,  973.71324439,\n",
      "        902.93070215,  900.14532383])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1000.23202533,  921.42712426,  998.73708587,  990.95733222,\n",
      "        911.4989099 ,  943.0991758 ,  982.24411303,  975.88649061,\n",
      "        862.21052577,  857.87276556,  893.84910538,  914.7797677 ,\n",
      "        956.00910157,  950.98553127,  869.84354752,  923.32683661,\n",
      "        967.6534664 ,  975.40865419, 1003.22990125, 1003.55157614])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 914.65709275,  917.36114995,  928.99988218,  971.59260492,\n",
      "       1002.54363929, 1077.50206276,  981.41467242, 1031.38843481,\n",
      "       1008.36789473,  996.67797377, 1034.79060041,  961.82867089,\n",
      "       1001.79836244, 1019.2776188 , 1035.55514077, 1033.78454565,\n",
      "       1139.54181843, 1032.6695729 ,  979.85710706,  970.66424996,\n",
      "        971.55787089,  969.39897322, 1059.79104685, 1111.27315424,\n",
      "       1024.00153111,  971.80503587, 1026.55935047, 1002.37166937,\n",
      "       1044.2419652 , 1047.00466984])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 956.61527463, 1073.01406201, 1146.43226916, 1041.31184508,\n",
      "       1005.12645319, 1037.76355434, 1054.73861892, 1074.97384986,\n",
      "       1150.99978669, 1151.27744666, 1079.38867994, 1032.6191415 ,\n",
      "       1050.25584071, 1097.33429888, 1205.78161584, 1133.46971939,\n",
      "       1128.59352223, 1127.96863513, 1184.90842814, 1277.93386638])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 997.59030213,  960.7578993 ,  964.83221813,  914.84459202,\n",
      "        914.65709275,  917.36114995,  928.99988218,  971.59260492,\n",
      "       1002.54363929, 1077.50206276,  981.41467242, 1031.38843481,\n",
      "       1008.36789473,  996.67797377, 1034.79060041,  961.82867089,\n",
      "       1001.79836244, 1019.2776188 , 1035.55514077, 1033.78454565,\n",
      "       1139.54181843, 1032.6695729 ,  979.85710706,  970.66424996,\n",
      "        971.55787089,  969.39897322, 1059.79104685, 1111.27315424,\n",
      "       1024.00153111,  971.80503587])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1026.55935047, 1002.37166937, 1044.2419652 , 1047.00466984,\n",
      "        956.61527463, 1073.01406201, 1146.43226916, 1041.31184508,\n",
      "       1005.12645319, 1037.76355434, 1054.73861892, 1074.97384986,\n",
      "       1150.99978669, 1151.27744666, 1079.38867994, 1032.6191415 ,\n",
      "       1050.25584071, 1097.33429888, 1205.78161584, 1133.46971939])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 869.84354752,  923.32683661,  967.6534664 ,  975.40865419,\n",
      "       1003.22990125, 1003.55157614,  982.53631582,  949.02568453,\n",
      "        858.44878802,  927.72946126,  957.00092337,  961.84957712,\n",
      "        887.94565068,  866.15942091,  972.96430377,  964.76263466,\n",
      "        985.10560512,  967.39478882,  956.53369828, 1001.37762455,\n",
      "        949.18365111,  952.55314568,  935.12600406,  926.72522669,\n",
      "        933.22378088,  963.24465376,  941.21957118, 1015.54262546,\n",
      "        944.97319267,  940.49716249])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 938.69094632, 1029.24446491, 1007.91049914, 1029.7894881 ,\n",
      "        921.02414826,  957.69581689, 1019.86652719,  997.83420854,\n",
      "       1033.61470903,  975.19803339, 1041.13401513, 1123.42793916,\n",
      "       1016.12489876,  960.41402604, 1046.97405111, 1092.88908797,\n",
      "        997.59030213,  960.7578993 ,  964.83221813,  914.84459202])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 909.64502365,  861.98607758,  884.23519565,  854.64268169,\n",
      "        844.90353406,  807.12054261,  815.2239179 ,  802.85865266,\n",
      "        814.48982293,  923.60552682,  869.34745536, 1012.69305322,\n",
      "        979.26171637,  929.55501488,  868.02206953,  922.48195382,\n",
      "        889.64371938,  902.27492431, 1036.70076886,  959.19804806,\n",
      "        973.71324439,  902.93070215,  900.14532383, 1000.23202533,\n",
      "        921.42712426,  998.73708587,  990.95733222,  911.4989099 ,\n",
      "        943.0991758 ,  982.24411303])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 975.88649061,  862.21052577,  857.87276556,  893.84910538,\n",
      "        914.7797677 ,  956.00910157,  950.98553127,  869.84354752,\n",
      "        923.32683661,  967.6534664 ,  975.40865419, 1003.22990125,\n",
      "       1003.55157614,  982.53631582,  949.02568453,  858.44878802,\n",
      "        927.72946126,  957.00092337,  961.84957712,  887.94565068])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 960.41402604, 1046.97405111, 1092.88908797,  997.59030213,\n",
      "        960.7578993 ,  964.83221813,  914.84459202,  914.65709275,\n",
      "        917.36114995,  928.99988218,  971.59260492, 1002.54363929,\n",
      "       1077.50206276,  981.41467242, 1031.38843481, 1008.36789473,\n",
      "        996.67797377, 1034.79060041,  961.82867089, 1001.79836244,\n",
      "       1019.2776188 , 1035.55514077, 1033.78454565, 1139.54181843,\n",
      "       1032.6695729 ,  979.85710706,  970.66424996,  971.55787089,\n",
      "        969.39897322, 1059.79104685])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1111.27315424, 1024.00153111,  971.80503587, 1026.55935047,\n",
      "       1002.37166937, 1044.2419652 , 1047.00466984,  956.61527463,\n",
      "       1073.01406201, 1146.43226916, 1041.31184508, 1005.12645319,\n",
      "       1037.76355434, 1054.73861892, 1074.97384986, 1150.99978669,\n",
      "       1151.27744666, 1079.38867994, 1032.6191415 , 1050.25584071])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([873.23872791, 793.98585857, 722.50694506, 884.27854574,\n",
      "       835.09507914, 790.9940369 , 784.06169243, 794.03170193,\n",
      "       782.43241217, 755.51672945, 747.93387285, 773.69529753,\n",
      "       760.13137483, 743.52554413, 780.65476182, 780.51270032,\n",
      "       863.92123596, 695.31521813, 799.46781486, 876.53041534,\n",
      "       724.65218252, 743.8072481 , 770.72811344, 722.73099485,\n",
      "       733.79990542, 728.76976372, 866.57003017, 887.52255784,\n",
      "       896.85451539, 877.50053638])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([775.79215333, 766.04648599, 824.39341901, 762.38914573,\n",
      "       826.6395924 , 817.13219304, 796.22790491, 846.83172639,\n",
      "       857.07268611, 815.69791161, 875.33317673, 794.92274497,\n",
      "       836.89239776, 867.16387476, 826.78026331, 841.22937628,\n",
      "       774.8583325 , 851.97983988, 839.78847353, 806.18137957])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([669.98055191, 714.36151018, 834.70411715, 785.46547122,\n",
      "       690.58578351, 723.47617913, 745.1619075 , 627.8142917 ,\n",
      "       709.36185236, 750.69673545, 800.72938966, 880.17979644,\n",
      "       873.23872791, 793.98585857, 722.50694506, 884.27854574,\n",
      "       835.09507914, 790.9940369 , 784.06169243, 794.03170193,\n",
      "       782.43241217, 755.51672945, 747.93387285, 773.69529753,\n",
      "       760.13137483, 743.52554413, 780.65476182, 780.51270032,\n",
      "       863.92123596, 695.31521813])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([799.46781486, 876.53041534, 724.65218252, 743.8072481 ,\n",
      "       770.72811344, 722.73099485, 733.79990542, 728.76976372,\n",
      "       866.57003017, 887.52255784, 896.85451539, 877.50053638,\n",
      "       775.79215333, 766.04648599, 824.39341901, 762.38914573,\n",
      "       826.6395924 , 817.13219304, 796.22790491, 846.83172639])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 862.21052577,  857.87276556,  893.84910538,  914.7797677 ,\n",
      "        956.00910157,  950.98553127,  869.84354752,  923.32683661,\n",
      "        967.6534664 ,  975.40865419, 1003.22990125, 1003.55157614,\n",
      "        982.53631582,  949.02568453,  858.44878802,  927.72946126,\n",
      "        957.00092337,  961.84957712,  887.94565068,  866.15942091,\n",
      "        972.96430377,  964.76263466,  985.10560512,  967.39478882,\n",
      "        956.53369828, 1001.37762455,  949.18365111,  952.55314568,\n",
      "        935.12600406,  926.72522669])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 933.22378088,  963.24465376,  941.21957118, 1015.54262546,\n",
      "        944.97319267,  940.49716249,  938.69094632, 1029.24446491,\n",
      "       1007.91049914, 1029.7894881 ,  921.02414826,  957.69581689,\n",
      "       1019.86652719,  997.83420854, 1033.61470903,  975.19803339,\n",
      "       1041.13401513, 1123.42793916, 1016.12489876,  960.41402604])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 997.83420854, 1033.61470903,  975.19803339, 1041.13401513,\n",
      "       1123.42793916, 1016.12489876,  960.41402604, 1046.97405111,\n",
      "       1092.88908797,  997.59030213,  960.7578993 ,  964.83221813,\n",
      "        914.84459202,  914.65709275,  917.36114995,  928.99988218,\n",
      "        971.59260492, 1002.54363929, 1077.50206276,  981.41467242,\n",
      "       1031.38843481, 1008.36789473,  996.67797377, 1034.79060041,\n",
      "        961.82867089, 1001.79836244, 1019.2776188 , 1035.55514077,\n",
      "       1033.78454565, 1139.54181843])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1032.6695729 ,  979.85710706,  970.66424996,  971.55787089,\n",
      "        969.39897322, 1059.79104685, 1111.27315424, 1024.00153111,\n",
      "        971.80503587, 1026.55935047, 1002.37166937, 1044.2419652 ,\n",
      "       1047.00466984,  956.61527463, 1073.01406201, 1146.43226916,\n",
      "       1041.31184508, 1005.12645319, 1037.76355434, 1054.73861892])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 981.41467242, 1031.38843481, 1008.36789473,  996.67797377,\n",
      "       1034.79060041,  961.82867089, 1001.79836244, 1019.2776188 ,\n",
      "       1035.55514077, 1033.78454565, 1139.54181843, 1032.6695729 ,\n",
      "        979.85710706,  970.66424996,  971.55787089,  969.39897322,\n",
      "       1059.79104685, 1111.27315424, 1024.00153111,  971.80503587,\n",
      "       1026.55935047, 1002.37166937, 1044.2419652 , 1047.00466984,\n",
      "        956.61527463, 1073.01406201, 1146.43226916, 1041.31184508,\n",
      "       1005.12645319, 1037.76355434])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1054.73861892, 1074.97384986, 1150.99978669, 1151.27744666,\n",
      "       1079.38867994, 1032.6191415 , 1050.25584071, 1097.33429888,\n",
      "       1205.78161584, 1133.46971939, 1128.59352223, 1127.96863513,\n",
      "       1184.90842814, 1277.93386638, 1159.17684299, 1102.26127385,\n",
      "       1172.30186416, 1186.23552455, 1242.01460042, 1205.17205015])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1012.69305322,  979.26171637,  929.55501488,  868.02206953,\n",
      "        922.48195382,  889.64371938,  902.27492431, 1036.70076886,\n",
      "        959.19804806,  973.71324439,  902.93070215,  900.14532383,\n",
      "       1000.23202533,  921.42712426,  998.73708587,  990.95733222,\n",
      "        911.4989099 ,  943.0991758 ,  982.24411303,  975.88649061,\n",
      "        862.21052577,  857.87276556,  893.84910538,  914.7797677 ,\n",
      "        956.00910157,  950.98553127,  869.84354752,  923.32683661,\n",
      "        967.6534664 ,  975.40865419])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1003.22990125, 1003.55157614,  982.53631582,  949.02568453,\n",
      "        858.44878802,  927.72946126,  957.00092337,  961.84957712,\n",
      "        887.94565068,  866.15942091,  972.96430377,  964.76263466,\n",
      "        985.10560512,  967.39478882,  956.53369828, 1001.37762455,\n",
      "        949.18365111,  952.55314568,  935.12600406,  926.72522669])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 923.32683661,  967.6534664 ,  975.40865419, 1003.22990125,\n",
      "       1003.55157614,  982.53631582,  949.02568453,  858.44878802,\n",
      "        927.72946126,  957.00092337,  961.84957712,  887.94565068,\n",
      "        866.15942091,  972.96430377,  964.76263466,  985.10560512,\n",
      "        967.39478882,  956.53369828, 1001.37762455,  949.18365111,\n",
      "        952.55314568,  935.12600406,  926.72522669,  933.22378088,\n",
      "        963.24465376,  941.21957118, 1015.54262546,  944.97319267,\n",
      "        940.49716249,  938.69094632])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1029.24446491, 1007.91049914, 1029.7894881 ,  921.02414826,\n",
      "        957.69581689, 1019.86652719,  997.83420854, 1033.61470903,\n",
      "        975.19803339, 1041.13401513, 1123.42793916, 1016.12489876,\n",
      "        960.41402604, 1046.97405111, 1092.88908797,  997.59030213,\n",
      "        960.7578993 ,  964.83221813,  914.84459202,  914.65709275])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 914.7797677 ,  956.00910157,  950.98553127,  869.84354752,\n",
      "        923.32683661,  967.6534664 ,  975.40865419, 1003.22990125,\n",
      "       1003.55157614,  982.53631582,  949.02568453,  858.44878802,\n",
      "        927.72946126,  957.00092337,  961.84957712,  887.94565068,\n",
      "        866.15942091,  972.96430377,  964.76263466,  985.10560512,\n",
      "        967.39478882,  956.53369828, 1001.37762455,  949.18365111,\n",
      "        952.55314568,  935.12600406,  926.72522669,  933.22378088,\n",
      "        963.24465376,  941.21957118])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1015.54262546,  944.97319267,  940.49716249,  938.69094632,\n",
      "       1029.24446491, 1007.91049914, 1029.7894881 ,  921.02414826,\n",
      "        957.69581689, 1019.86652719,  997.83420854, 1033.61470903,\n",
      "        975.19803339, 1041.13401513, 1123.42793916, 1016.12489876,\n",
      "        960.41402604, 1046.97405111, 1092.88908797,  997.59030213])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([815.69791161, 875.33317673, 794.92274497, 836.89239776,\n",
      "       867.16387476, 826.78026331, 841.22937628, 774.8583325 ,\n",
      "       851.97983988, 839.78847353, 806.18137957, 876.3270062 ,\n",
      "       822.22514043, 755.48010844, 729.86087559, 832.20711867,\n",
      "       784.38456467, 902.87694654, 773.59564939, 889.41788188,\n",
      "       887.39322146, 850.96512158, 820.95104652, 858.00387261,\n",
      "       855.81095587, 906.99625232, 882.58263097, 867.8616934 ,\n",
      "       841.9761503 , 846.99786451])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([871.15717441, 780.21953936, 757.60855562, 864.58609949,\n",
      "       881.58160417, 859.3996499 , 862.91979102, 883.65609382,\n",
      "       848.05515331, 819.54479791, 861.61104418, 823.77193688,\n",
      "       871.634727  , 794.34800497, 890.0528615 , 916.1347314 ,\n",
      "       901.84355457, 934.63290199, 984.66471466, 970.16030028])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 963.24465376,  941.21957118, 1015.54262546,  944.97319267,\n",
      "        940.49716249,  938.69094632, 1029.24446491, 1007.91049914,\n",
      "       1029.7894881 ,  921.02414826,  957.69581689, 1019.86652719,\n",
      "        997.83420854, 1033.61470903,  975.19803339, 1041.13401513,\n",
      "       1123.42793916, 1016.12489876,  960.41402604, 1046.97405111,\n",
      "       1092.88908797,  997.59030213,  960.7578993 ,  964.83221813,\n",
      "        914.84459202,  914.65709275,  917.36114995,  928.99988218,\n",
      "        971.59260492, 1002.54363929])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1077.50206276,  981.41467242, 1031.38843481, 1008.36789473,\n",
      "        996.67797377, 1034.79060041,  961.82867089, 1001.79836244,\n",
      "       1019.2776188 , 1035.55514077, 1033.78454565, 1139.54181843,\n",
      "       1032.6695729 ,  979.85710706,  970.66424996,  971.55787089,\n",
      "        969.39897322, 1059.79104685, 1111.27315424, 1024.00153111])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1001.79836244, 1019.2776188 , 1035.55514077, 1033.78454565,\n",
      "       1139.54181843, 1032.6695729 ,  979.85710706,  970.66424996,\n",
      "        971.55787089,  969.39897322, 1059.79104685, 1111.27315424,\n",
      "       1024.00153111,  971.80503587, 1026.55935047, 1002.37166937,\n",
      "       1044.2419652 , 1047.00466984,  956.61527463, 1073.01406201,\n",
      "       1146.43226916, 1041.31184508, 1005.12645319, 1037.76355434,\n",
      "       1054.73861892, 1074.97384986, 1150.99978669, 1151.27744666,\n",
      "       1079.38867994, 1032.6191415 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1050.25584071, 1097.33429888, 1205.78161584, 1133.46971939,\n",
      "       1128.59352223, 1127.96863513, 1184.90842814, 1277.93386638,\n",
      "       1159.17684299, 1102.26127385, 1172.30186416, 1186.23552455,\n",
      "       1242.01460042, 1205.17205015, 1136.09652267, 1161.34722065,\n",
      "       1204.85291596, 1204.81004501, 1186.39807387, 1208.3644474 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([820.95104652, 858.00387261, 855.81095587, 906.99625232,\n",
      "       882.58263097, 867.8616934 , 841.9761503 , 846.99786451,\n",
      "       871.15717441, 780.21953936, 757.60855562, 864.58609949,\n",
      "       881.58160417, 859.3996499 , 862.91979102, 883.65609382,\n",
      "       848.05515331, 819.54479791, 861.61104418, 823.77193688,\n",
      "       871.634727  , 794.34800497, 890.0528615 , 916.1347314 ,\n",
      "       901.84355457, 934.63290199, 984.66471466, 970.16030028,\n",
      "       818.00754688, 784.76478819])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 821.16300342,  868.15554952,  909.64502365,  861.98607758,\n",
      "        884.23519565,  854.64268169,  844.90353406,  807.12054261,\n",
      "        815.2239179 ,  802.85865266,  814.48982293,  923.60552682,\n",
      "        869.34745536, 1012.69305322,  979.26171637,  929.55501488,\n",
      "        868.02206953,  922.48195382,  889.64371938,  902.27492431])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([887.39322146, 850.96512158, 820.95104652, 858.00387261,\n",
      "       855.81095587, 906.99625232, 882.58263097, 867.8616934 ,\n",
      "       841.9761503 , 846.99786451, 871.15717441, 780.21953936,\n",
      "       757.60855562, 864.58609949, 881.58160417, 859.3996499 ,\n",
      "       862.91979102, 883.65609382, 848.05515331, 819.54479791,\n",
      "       861.61104418, 823.77193688, 871.634727  , 794.34800497,\n",
      "       890.0528615 , 916.1347314 , 901.84355457, 934.63290199,\n",
      "       984.66471466, 970.16030028])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 818.00754688,  784.76478819,  821.16300342,  868.15554952,\n",
      "        909.64502365,  861.98607758,  884.23519565,  854.64268169,\n",
      "        844.90353406,  807.12054261,  815.2239179 ,  802.85865266,\n",
      "        814.48982293,  923.60552682,  869.34745536, 1012.69305322,\n",
      "        979.26171637,  929.55501488,  868.02206953,  922.48195382])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 950.98553127,  869.84354752,  923.32683661,  967.6534664 ,\n",
      "        975.40865419, 1003.22990125, 1003.55157614,  982.53631582,\n",
      "        949.02568453,  858.44878802,  927.72946126,  957.00092337,\n",
      "        961.84957712,  887.94565068,  866.15942091,  972.96430377,\n",
      "        964.76263466,  985.10560512,  967.39478882,  956.53369828,\n",
      "       1001.37762455,  949.18365111,  952.55314568,  935.12600406,\n",
      "        926.72522669,  933.22378088,  963.24465376,  941.21957118,\n",
      "       1015.54262546,  944.97319267])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 940.49716249,  938.69094632, 1029.24446491, 1007.91049914,\n",
      "       1029.7894881 ,  921.02414826,  957.69581689, 1019.86652719,\n",
      "        997.83420854, 1033.61470903,  975.19803339, 1041.13401513,\n",
      "       1123.42793916, 1016.12489876,  960.41402604, 1046.97405111,\n",
      "       1092.88908797,  997.59030213,  960.7578993 ,  964.83221813])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 972.96430377,  964.76263466,  985.10560512,  967.39478882,\n",
      "        956.53369828, 1001.37762455,  949.18365111,  952.55314568,\n",
      "        935.12600406,  926.72522669,  933.22378088,  963.24465376,\n",
      "        941.21957118, 1015.54262546,  944.97319267,  940.49716249,\n",
      "        938.69094632, 1029.24446491, 1007.91049914, 1029.7894881 ,\n",
      "        921.02414826,  957.69581689, 1019.86652719,  997.83420854,\n",
      "       1033.61470903,  975.19803339, 1041.13401513, 1123.42793916,\n",
      "       1016.12489876,  960.41402604])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1046.97405111, 1092.88908797,  997.59030213,  960.7578993 ,\n",
      "        964.83221813,  914.84459202,  914.65709275,  917.36114995,\n",
      "        928.99988218,  971.59260492, 1002.54363929, 1077.50206276,\n",
      "        981.41467242, 1031.38843481, 1008.36789473,  996.67797377,\n",
      "       1034.79060041,  961.82867089, 1001.79836244, 1019.2776188 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 959.19804806,  973.71324439,  902.93070215,  900.14532383,\n",
      "       1000.23202533,  921.42712426,  998.73708587,  990.95733222,\n",
      "        911.4989099 ,  943.0991758 ,  982.24411303,  975.88649061,\n",
      "        862.21052577,  857.87276556,  893.84910538,  914.7797677 ,\n",
      "        956.00910157,  950.98553127,  869.84354752,  923.32683661,\n",
      "        967.6534664 ,  975.40865419, 1003.22990125, 1003.55157614,\n",
      "        982.53631582,  949.02568453,  858.44878802,  927.72946126,\n",
      "        957.00092337,  961.84957712])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 887.94565068,  866.15942091,  972.96430377,  964.76263466,\n",
      "        985.10560512,  967.39478882,  956.53369828, 1001.37762455,\n",
      "        949.18365111,  952.55314568,  935.12600406,  926.72522669,\n",
      "        933.22378088,  963.24465376,  941.21957118, 1015.54262546,\n",
      "        944.97319267,  940.49716249,  938.69094632, 1029.24446491])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([689.61478723, 656.32769369, 668.64987017, 719.83064084,\n",
      "       758.57681546, 687.1679321 , 743.60502328, 802.44119057,\n",
      "       771.56266707, 828.12820822, 726.32636326, 651.49920215,\n",
      "       608.68186599, 758.13233494, 784.85025629, 742.43778409,\n",
      "       744.95265199, 681.04995163, 830.47041928, 788.31397118,\n",
      "       749.23453274, 775.92944027, 776.84226997, 761.63590883,\n",
      "       706.47664247, 747.91110009, 842.84008478, 850.5771681 ,\n",
      "       857.88656147, 759.24070059])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([692.35079504, 719.83979052, 745.37251598, 804.07326495,\n",
      "       689.79394523, 795.39479652, 773.4935943 , 733.44333496,\n",
      "       696.48647192, 651.50446061, 760.04488904, 774.24036403,\n",
      "       698.7297109 , 669.98055191, 714.36151018, 834.70411715,\n",
      "       785.46547122, 690.58578351, 723.47617913, 745.1619075 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([633.39966005, 675.71884524, 605.40908925, 650.71947409,\n",
      "       727.41228097, 557.18612905, 562.01216424, 655.97105619,\n",
      "       650.10028912, 668.16176824, 631.85717443, 648.09903429,\n",
      "       649.27642821, 712.67236658, 696.27975741, 686.1849128 ,\n",
      "       649.98371354, 631.18617055, 631.07502274, 673.75730796,\n",
      "       651.98129603, 673.73626744, 777.07324188, 758.2234969 ,\n",
      "       680.18442364, 729.54997166, 678.89672873, 568.29571713,\n",
      "       581.78740582, 555.83498193])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([615.91695795, 665.67546333, 761.52671654, 733.19317991,\n",
      "       684.17926491, 723.09299308, 592.89210242, 653.62634842,\n",
      "       724.79417847, 630.92834603, 717.67629451, 728.46205325,\n",
      "       681.62189532, 718.02118063, 820.5333744 , 755.39247769,\n",
      "       721.11223575, 683.36485669, 648.91690775, 722.89139063])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([718.86986728, 732.00333529, 767.83519502, 707.24881753,\n",
      "       689.61478723, 656.32769369, 668.64987017, 719.83064084,\n",
      "       758.57681546, 687.1679321 , 743.60502328, 802.44119057,\n",
      "       771.56266707, 828.12820822, 726.32636326, 651.49920215,\n",
      "       608.68186599, 758.13233494, 784.85025629, 742.43778409,\n",
      "       744.95265199, 681.04995163, 830.47041928, 788.31397118,\n",
      "       749.23453274, 775.92944027, 776.84226997, 761.63590883,\n",
      "       706.47664247, 747.91110009])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([842.84008478, 850.5771681 , 857.88656147, 759.24070059,\n",
      "       692.35079504, 719.83979052, 745.37251598, 804.07326495,\n",
      "       689.79394523, 795.39479652, 773.4935943 , 733.44333496,\n",
      "       696.48647192, 651.50446061, 760.04488904, 774.24036403,\n",
      "       698.7297109 , 669.98055191, 714.36151018, 834.70411715])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 996.67797377, 1034.79060041,  961.82867089, 1001.79836244,\n",
      "       1019.2776188 , 1035.55514077, 1033.78454565, 1139.54181843,\n",
      "       1032.6695729 ,  979.85710706,  970.66424996,  971.55787089,\n",
      "        969.39897322, 1059.79104685, 1111.27315424, 1024.00153111,\n",
      "        971.80503587, 1026.55935047, 1002.37166937, 1044.2419652 ,\n",
      "       1047.00466984,  956.61527463, 1073.01406201, 1146.43226916,\n",
      "       1041.31184508, 1005.12645319, 1037.76355434, 1054.73861892,\n",
      "       1074.97384986, 1150.99978669])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1151.27744666, 1079.38867994, 1032.6191415 , 1050.25584071,\n",
      "       1097.33429888, 1205.78161584, 1133.46971939, 1128.59352223,\n",
      "       1127.96863513, 1184.90842814, 1277.93386638, 1159.17684299,\n",
      "       1102.26127385, 1172.30186416, 1186.23552455, 1242.01460042,\n",
      "       1205.17205015, 1136.09652267, 1161.34722065, 1204.85291596])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 961.84957712,  887.94565068,  866.15942091,  972.96430377,\n",
      "        964.76263466,  985.10560512,  967.39478882,  956.53369828,\n",
      "       1001.37762455,  949.18365111,  952.55314568,  935.12600406,\n",
      "        926.72522669,  933.22378088,  963.24465376,  941.21957118,\n",
      "       1015.54262546,  944.97319267,  940.49716249,  938.69094632,\n",
      "       1029.24446491, 1007.91049914, 1029.7894881 ,  921.02414826,\n",
      "        957.69581689, 1019.86652719,  997.83420854, 1033.61470903,\n",
      "        975.19803339, 1041.13401513])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1123.42793916, 1016.12489876,  960.41402604, 1046.97405111,\n",
      "       1092.88908797,  997.59030213,  960.7578993 ,  964.83221813,\n",
      "        914.84459202,  914.65709275,  917.36114995,  928.99988218,\n",
      "        971.59260492, 1002.54363929, 1077.50206276,  981.41467242,\n",
      "       1031.38843481, 1008.36789473,  996.67797377, 1034.79060041])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 967.6534664 ,  975.40865419, 1003.22990125, 1003.55157614,\n",
      "        982.53631582,  949.02568453,  858.44878802,  927.72946126,\n",
      "        957.00092337,  961.84957712,  887.94565068,  866.15942091,\n",
      "        972.96430377,  964.76263466,  985.10560512,  967.39478882,\n",
      "        956.53369828, 1001.37762455,  949.18365111,  952.55314568,\n",
      "        935.12600406,  926.72522669,  933.22378088,  963.24465376,\n",
      "        941.21957118, 1015.54262546,  944.97319267,  940.49716249,\n",
      "        938.69094632, 1029.24446491])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1007.91049914, 1029.7894881 ,  921.02414826,  957.69581689,\n",
      "       1019.86652719,  997.83420854, 1033.61470903,  975.19803339,\n",
      "       1041.13401513, 1123.42793916, 1016.12489876,  960.41402604,\n",
      "       1046.97405111, 1092.88908797,  997.59030213,  960.7578993 ,\n",
      "        964.83221813,  914.84459202,  914.65709275,  917.36114995])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 823.77193688,  871.634727  ,  794.34800497,  890.0528615 ,\n",
      "        916.1347314 ,  901.84355457,  934.63290199,  984.66471466,\n",
      "        970.16030028,  818.00754688,  784.76478819,  821.16300342,\n",
      "        868.15554952,  909.64502365,  861.98607758,  884.23519565,\n",
      "        854.64268169,  844.90353406,  807.12054261,  815.2239179 ,\n",
      "        802.85865266,  814.48982293,  923.60552682,  869.34745536,\n",
      "       1012.69305322,  979.26171637,  929.55501488,  868.02206953,\n",
      "        922.48195382,  889.64371938])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 902.27492431, 1036.70076886,  959.19804806,  973.71324439,\n",
      "        902.93070215,  900.14532383, 1000.23202533,  921.42712426,\n",
      "        998.73708587,  990.95733222,  911.4989099 ,  943.0991758 ,\n",
      "        982.24411303,  975.88649061,  862.21052577,  857.87276556,\n",
      "        893.84910538,  914.7797677 ,  956.00910157,  950.98553127])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 923.60552682,  869.34745536, 1012.69305322,  979.26171637,\n",
      "        929.55501488,  868.02206953,  922.48195382,  889.64371938,\n",
      "        902.27492431, 1036.70076886,  959.19804806,  973.71324439,\n",
      "        902.93070215,  900.14532383, 1000.23202533,  921.42712426,\n",
      "        998.73708587,  990.95733222,  911.4989099 ,  943.0991758 ,\n",
      "        982.24411303,  975.88649061,  862.21052577,  857.87276556,\n",
      "        893.84910538,  914.7797677 ,  956.00910157,  950.98553127,\n",
      "        869.84354752,  923.32683661])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 967.6534664 ,  975.40865419, 1003.22990125, 1003.55157614,\n",
      "        982.53631582,  949.02568453,  858.44878802,  927.72946126,\n",
      "        957.00092337,  961.84957712,  887.94565068,  866.15942091,\n",
      "        972.96430377,  964.76263466,  985.10560512,  967.39478882,\n",
      "        956.53369828, 1001.37762455,  949.18365111,  952.55314568])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1026.55935047, 1002.37166937, 1044.2419652 , 1047.00466984,\n",
      "        956.61527463, 1073.01406201, 1146.43226916, 1041.31184508,\n",
      "       1005.12645319, 1037.76355434, 1054.73861892, 1074.97384986,\n",
      "       1150.99978669, 1151.27744666, 1079.38867994, 1032.6191415 ,\n",
      "       1050.25584071, 1097.33429888, 1205.78161584, 1133.46971939,\n",
      "       1128.59352223, 1127.96863513, 1184.90842814, 1277.93386638,\n",
      "       1159.17684299, 1102.26127385, 1172.30186416, 1186.23552455,\n",
      "       1242.01460042, 1205.17205015])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1136.09652267, 1161.34722065, 1204.85291596, 1204.81004501,\n",
      "       1186.39807387, 1208.3644474 , 1225.51802637, 1240.98008352,\n",
      "       1210.89558316, 1156.64225   , 1153.90637439, 1211.43040122,\n",
      "       1134.72817155, 1155.09827511, 1138.12562284, 1146.38320567,\n",
      "       1216.96420579, 1191.58212951, 1166.49606169, 1092.18467862])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 970.66424996,  971.55787089,  969.39897322, 1059.79104685,\n",
      "       1111.27315424, 1024.00153111,  971.80503587, 1026.55935047,\n",
      "       1002.37166937, 1044.2419652 , 1047.00466984,  956.61527463,\n",
      "       1073.01406201, 1146.43226916, 1041.31184508, 1005.12645319,\n",
      "       1037.76355434, 1054.73861892, 1074.97384986, 1150.99978669,\n",
      "       1151.27744666, 1079.38867994, 1032.6191415 , 1050.25584071,\n",
      "       1097.33429888, 1205.78161584, 1133.46971939, 1128.59352223,\n",
      "       1127.96863513, 1184.90842814])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1277.93386638, 1159.17684299, 1102.26127385, 1172.30186416,\n",
      "       1186.23552455, 1242.01460042, 1205.17205015, 1136.09652267,\n",
      "       1161.34722065, 1204.85291596, 1204.81004501, 1186.39807387,\n",
      "       1208.3644474 , 1225.51802637, 1240.98008352, 1210.89558316,\n",
      "       1156.64225   , 1153.90637439, 1211.43040122, 1134.72817155])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1139.54181843, 1032.6695729 ,  979.85710706,  970.66424996,\n",
      "        971.55787089,  969.39897322, 1059.79104685, 1111.27315424,\n",
      "       1024.00153111,  971.80503587, 1026.55935047, 1002.37166937,\n",
      "       1044.2419652 , 1047.00466984,  956.61527463, 1073.01406201,\n",
      "       1146.43226916, 1041.31184508, 1005.12645319, 1037.76355434,\n",
      "       1054.73861892, 1074.97384986, 1150.99978669, 1151.27744666,\n",
      "       1079.38867994, 1032.6191415 , 1050.25584071, 1097.33429888,\n",
      "       1205.78161584, 1133.46971939])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1128.59352223, 1127.96863513, 1184.90842814, 1277.93386638,\n",
      "       1159.17684299, 1102.26127385, 1172.30186416, 1186.23552455,\n",
      "       1242.01460042, 1205.17205015, 1136.09652267, 1161.34722065,\n",
      "       1204.85291596, 1204.81004501, 1186.39807387, 1208.3644474 ,\n",
      "       1225.51802637, 1240.98008352, 1210.89558316, 1156.64225   ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1024.00153111,  971.80503587, 1026.55935047, 1002.37166937,\n",
      "       1044.2419652 , 1047.00466984,  956.61527463, 1073.01406201,\n",
      "       1146.43226916, 1041.31184508, 1005.12645319, 1037.76355434,\n",
      "       1054.73861892, 1074.97384986, 1150.99978669, 1151.27744666,\n",
      "       1079.38867994, 1032.6191415 , 1050.25584071, 1097.33429888,\n",
      "       1205.78161584, 1133.46971939, 1128.59352223, 1127.96863513,\n",
      "       1184.90842814, 1277.93386638, 1159.17684299, 1102.26127385,\n",
      "       1172.30186416, 1186.23552455])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1242.01460042, 1205.17205015, 1136.09652267, 1161.34722065,\n",
      "       1204.85291596, 1204.81004501, 1186.39807387, 1208.3644474 ,\n",
      "       1225.51802637, 1240.98008352, 1210.89558316, 1156.64225   ,\n",
      "       1153.90637439, 1211.43040122, 1134.72817155, 1155.09827511,\n",
      "       1138.12562284, 1146.38320567, 1216.96420579, 1191.58212951])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([690.58578351, 723.47617913, 745.1619075 , 627.8142917 ,\n",
      "       709.36185236, 750.69673545, 800.72938966, 880.17979644,\n",
      "       873.23872791, 793.98585857, 722.50694506, 884.27854574,\n",
      "       835.09507914, 790.9940369 , 784.06169243, 794.03170193,\n",
      "       782.43241217, 755.51672945, 747.93387285, 773.69529753,\n",
      "       760.13137483, 743.52554413, 780.65476182, 780.51270032,\n",
      "       863.92123596, 695.31521813, 799.46781486, 876.53041534,\n",
      "       724.65218252, 743.8072481 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([770.72811344, 722.73099485, 733.79990542, 728.76976372,\n",
      "       866.57003017, 887.52255784, 896.85451539, 877.50053638,\n",
      "       775.79215333, 766.04648599, 824.39341901, 762.38914573,\n",
      "       826.6395924 , 817.13219304, 796.22790491, 846.83172639,\n",
      "       857.07268611, 815.69791161, 875.33317673, 794.92274497])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 921.42712426,  998.73708587,  990.95733222,  911.4989099 ,\n",
      "        943.0991758 ,  982.24411303,  975.88649061,  862.21052577,\n",
      "        857.87276556,  893.84910538,  914.7797677 ,  956.00910157,\n",
      "        950.98553127,  869.84354752,  923.32683661,  967.6534664 ,\n",
      "        975.40865419, 1003.22990125, 1003.55157614,  982.53631582,\n",
      "        949.02568453,  858.44878802,  927.72946126,  957.00092337,\n",
      "        961.84957712,  887.94565068,  866.15942091,  972.96430377,\n",
      "        964.76263466,  985.10560512])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 967.39478882,  956.53369828, 1001.37762455,  949.18365111,\n",
      "        952.55314568,  935.12600406,  926.72522669,  933.22378088,\n",
      "        963.24465376,  941.21957118, 1015.54262546,  944.97319267,\n",
      "        940.49716249,  938.69094632, 1029.24446491, 1007.91049914,\n",
      "       1029.7894881 ,  921.02414826,  957.69581689, 1019.86652719])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 971.55787089,  969.39897322, 1059.79104685, 1111.27315424,\n",
      "       1024.00153111,  971.80503587, 1026.55935047, 1002.37166937,\n",
      "       1044.2419652 , 1047.00466984,  956.61527463, 1073.01406201,\n",
      "       1146.43226916, 1041.31184508, 1005.12645319, 1037.76355434,\n",
      "       1054.73861892, 1074.97384986, 1150.99978669, 1151.27744666,\n",
      "       1079.38867994, 1032.6191415 , 1050.25584071, 1097.33429888,\n",
      "       1205.78161584, 1133.46971939, 1128.59352223, 1127.96863513,\n",
      "       1184.90842814, 1277.93386638])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1159.17684299, 1102.26127385, 1172.30186416, 1186.23552455,\n",
      "       1242.01460042, 1205.17205015, 1136.09652267, 1161.34722065,\n",
      "       1204.85291596, 1204.81004501, 1186.39807387, 1208.3644474 ,\n",
      "       1225.51802637, 1240.98008352, 1210.89558316, 1156.64225   ,\n",
      "       1153.90637439, 1211.43040122, 1134.72817155, 1155.09827511])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1032.6695729 ,  979.85710706,  970.66424996,  971.55787089,\n",
      "        969.39897322, 1059.79104685, 1111.27315424, 1024.00153111,\n",
      "        971.80503587, 1026.55935047, 1002.37166937, 1044.2419652 ,\n",
      "       1047.00466984,  956.61527463, 1073.01406201, 1146.43226916,\n",
      "       1041.31184508, 1005.12645319, 1037.76355434, 1054.73861892,\n",
      "       1074.97384986, 1150.99978669, 1151.27744666, 1079.38867994,\n",
      "       1032.6191415 , 1050.25584071, 1097.33429888, 1205.78161584,\n",
      "       1133.46971939, 1128.59352223])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1127.96863513, 1184.90842814, 1277.93386638, 1159.17684299,\n",
      "       1102.26127385, 1172.30186416, 1186.23552455, 1242.01460042,\n",
      "       1205.17205015, 1136.09652267, 1161.34722065, 1204.85291596,\n",
      "       1204.81004501, 1186.39807387, 1208.3644474 , 1225.51802637,\n",
      "       1240.98008352, 1210.89558316, 1156.64225   , 1153.90637439])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 949.18365111,  952.55314568,  935.12600406,  926.72522669,\n",
      "        933.22378088,  963.24465376,  941.21957118, 1015.54262546,\n",
      "        944.97319267,  940.49716249,  938.69094632, 1029.24446491,\n",
      "       1007.91049914, 1029.7894881 ,  921.02414826,  957.69581689,\n",
      "       1019.86652719,  997.83420854, 1033.61470903,  975.19803339,\n",
      "       1041.13401513, 1123.42793916, 1016.12489876,  960.41402604,\n",
      "       1046.97405111, 1092.88908797,  997.59030213,  960.7578993 ,\n",
      "        964.83221813,  914.84459202])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 914.65709275,  917.36114995,  928.99988218,  971.59260492,\n",
      "       1002.54363929, 1077.50206276,  981.41467242, 1031.38843481,\n",
      "       1008.36789473,  996.67797377, 1034.79060041,  961.82867089,\n",
      "       1001.79836244, 1019.2776188 , 1035.55514077, 1033.78454565,\n",
      "       1139.54181843, 1032.6695729 ,  979.85710706,  970.66424996])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 975.19803339, 1041.13401513, 1123.42793916, 1016.12489876,\n",
      "        960.41402604, 1046.97405111, 1092.88908797,  997.59030213,\n",
      "        960.7578993 ,  964.83221813,  914.84459202,  914.65709275,\n",
      "        917.36114995,  928.99988218,  971.59260492, 1002.54363929,\n",
      "       1077.50206276,  981.41467242, 1031.38843481, 1008.36789473,\n",
      "        996.67797377, 1034.79060041,  961.82867089, 1001.79836244,\n",
      "       1019.2776188 , 1035.55514077, 1033.78454565, 1139.54181843,\n",
      "       1032.6695729 ,  979.85710706])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 970.66424996,  971.55787089,  969.39897322, 1059.79104685,\n",
      "       1111.27315424, 1024.00153111,  971.80503587, 1026.55935047,\n",
      "       1002.37166937, 1044.2419652 , 1047.00466984,  956.61527463,\n",
      "       1073.01406201, 1146.43226916, 1041.31184508, 1005.12645319,\n",
      "       1037.76355434, 1054.73861892, 1074.97384986, 1150.99978669])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 926.72522669,  933.22378088,  963.24465376,  941.21957118,\n",
      "       1015.54262546,  944.97319267,  940.49716249,  938.69094632,\n",
      "       1029.24446491, 1007.91049914, 1029.7894881 ,  921.02414826,\n",
      "        957.69581689, 1019.86652719,  997.83420854, 1033.61470903,\n",
      "        975.19803339, 1041.13401513, 1123.42793916, 1016.12489876,\n",
      "        960.41402604, 1046.97405111, 1092.88908797,  997.59030213,\n",
      "        960.7578993 ,  964.83221813,  914.84459202,  914.65709275,\n",
      "        917.36114995,  928.99988218])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 971.59260492, 1002.54363929, 1077.50206276,  981.41467242,\n",
      "       1031.38843481, 1008.36789473,  996.67797377, 1034.79060041,\n",
      "        961.82867089, 1001.79836244, 1019.2776188 , 1035.55514077,\n",
      "       1033.78454565, 1139.54181843, 1032.6695729 ,  979.85710706,\n",
      "        970.66424996,  971.55787089,  969.39897322, 1059.79104685])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([653.7286504 , 580.21410777, 603.71020602, 655.75754555,\n",
      "       552.61491167, 614.86465122, 596.29610199, 638.34407776,\n",
      "       596.98361388, 521.09639918, 505.46555349, 589.70267609,\n",
      "       636.73604882, 589.67698049, 644.77612963, 561.24317062,\n",
      "       596.79057361, 568.82052745, 576.8558281 , 622.05026979,\n",
      "       594.06184224, 602.02666574, 680.36988082, 633.39966005,\n",
      "       675.71884524, 605.40908925, 650.71947409, 727.41228097,\n",
      "       557.18612905, 562.01216424])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([655.97105619, 650.10028912, 668.16176824, 631.85717443,\n",
      "       648.09903429, 649.27642821, 712.67236658, 696.27975741,\n",
      "       686.1849128 , 649.98371354, 631.18617055, 631.07502274,\n",
      "       673.75730796, 651.98129603, 673.73626744, 777.07324188,\n",
      "       758.2234969 , 680.18442364, 729.54997166, 678.89672873])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([796.22790491, 846.83172639, 857.07268611, 815.69791161,\n",
      "       875.33317673, 794.92274497, 836.89239776, 867.16387476,\n",
      "       826.78026331, 841.22937628, 774.8583325 , 851.97983988,\n",
      "       839.78847353, 806.18137957, 876.3270062 , 822.22514043,\n",
      "       755.48010844, 729.86087559, 832.20711867, 784.38456467,\n",
      "       902.87694654, 773.59564939, 889.41788188, 887.39322146,\n",
      "       850.96512158, 820.95104652, 858.00387261, 855.81095587,\n",
      "       906.99625232, 882.58263097])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([867.8616934 , 841.9761503 , 846.99786451, 871.15717441,\n",
      "       780.21953936, 757.60855562, 864.58609949, 881.58160417,\n",
      "       859.3996499 , 862.91979102, 883.65609382, 848.05515331,\n",
      "       819.54479791, 861.61104418, 823.77193688, 871.634727  ,\n",
      "       794.34800497, 890.0528615 , 916.1347314 , 901.84355457])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1000.23202533,  921.42712426,  998.73708587,  990.95733222,\n",
      "        911.4989099 ,  943.0991758 ,  982.24411303,  975.88649061,\n",
      "        862.21052577,  857.87276556,  893.84910538,  914.7797677 ,\n",
      "        956.00910157,  950.98553127,  869.84354752,  923.32683661,\n",
      "        967.6534664 ,  975.40865419, 1003.22990125, 1003.55157614,\n",
      "        982.53631582,  949.02568453,  858.44878802,  927.72946126,\n",
      "        957.00092337,  961.84957712,  887.94565068,  866.15942091,\n",
      "        972.96430377,  964.76263466])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 985.10560512,  967.39478882,  956.53369828, 1001.37762455,\n",
      "        949.18365111,  952.55314568,  935.12600406,  926.72522669,\n",
      "        933.22378088,  963.24465376,  941.21957118, 1015.54262546,\n",
      "        944.97319267,  940.49716249,  938.69094632, 1029.24446491,\n",
      "       1007.91049914, 1029.7894881 ,  921.02414826,  957.69581689])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 933.22378088,  963.24465376,  941.21957118, 1015.54262546,\n",
      "        944.97319267,  940.49716249,  938.69094632, 1029.24446491,\n",
      "       1007.91049914, 1029.7894881 ,  921.02414826,  957.69581689,\n",
      "       1019.86652719,  997.83420854, 1033.61470903,  975.19803339,\n",
      "       1041.13401513, 1123.42793916, 1016.12489876,  960.41402604,\n",
      "       1046.97405111, 1092.88908797,  997.59030213,  960.7578993 ,\n",
      "        964.83221813,  914.84459202,  914.65709275,  917.36114995,\n",
      "        928.99988218,  971.59260492])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1002.54363929, 1077.50206276,  981.41467242, 1031.38843481,\n",
      "       1008.36789473,  996.67797377, 1034.79060041,  961.82867089,\n",
      "       1001.79836244, 1019.2776188 , 1035.55514077, 1033.78454565,\n",
      "       1139.54181843, 1032.6695729 ,  979.85710706,  970.66424996,\n",
      "        971.55787089,  969.39897322, 1059.79104685, 1111.27315424])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([668.16176824, 631.85717443, 648.09903429, 649.27642821,\n",
      "       712.67236658, 696.27975741, 686.1849128 , 649.98371354,\n",
      "       631.18617055, 631.07502274, 673.75730796, 651.98129603,\n",
      "       673.73626744, 777.07324188, 758.2234969 , 680.18442364,\n",
      "       729.54997166, 678.89672873, 568.29571713, 581.78740582,\n",
      "       555.83498193, 615.91695795, 665.67546333, 761.52671654,\n",
      "       733.19317991, 684.17926491, 723.09299308, 592.89210242,\n",
      "       653.62634842, 724.79417847])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([630.92834603, 717.67629451, 728.46205325, 681.62189532,\n",
      "       718.02118063, 820.5333744 , 755.39247769, 721.11223575,\n",
      "       683.36485669, 648.91690775, 722.89139063, 674.15797097,\n",
      "       690.34147284, 679.89555162, 718.86986728, 732.00333529,\n",
      "       767.83519502, 707.24881753, 689.61478723, 656.32769369])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1151.27744666, 1079.38867994, 1032.6191415 , 1050.25584071,\n",
      "       1097.33429888, 1205.78161584, 1133.46971939, 1128.59352223,\n",
      "       1127.96863513, 1184.90842814, 1277.93386638, 1159.17684299,\n",
      "       1102.26127385, 1172.30186416, 1186.23552455, 1242.01460042,\n",
      "       1205.17205015, 1136.09652267, 1161.34722065, 1204.85291596,\n",
      "       1204.81004501, 1186.39807387, 1208.3644474 , 1225.51802637,\n",
      "       1240.98008352, 1210.89558316, 1156.64225   , 1153.90637439,\n",
      "       1211.43040122, 1134.72817155])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1155.09827511, 1138.12562284, 1146.38320567, 1216.96420579,\n",
      "       1191.58212951, 1166.49606169, 1092.18467862, 1167.0925951 ,\n",
      "       1203.52277271, 1283.56378373, 1195.20997238, 1174.88613873,\n",
      "       1182.27617431, 1250.21616117, 1196.58470937, 1184.87301985,\n",
      "       1206.17887719, 1193.51503893, 1155.23528584, 1173.69540801])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1015.54262546,  944.97319267,  940.49716249,  938.69094632,\n",
      "       1029.24446491, 1007.91049914, 1029.7894881 ,  921.02414826,\n",
      "        957.69581689, 1019.86652719,  997.83420854, 1033.61470903,\n",
      "        975.19803339, 1041.13401513, 1123.42793916, 1016.12489876,\n",
      "        960.41402604, 1046.97405111, 1092.88908797,  997.59030213,\n",
      "        960.7578993 ,  964.83221813,  914.84459202,  914.65709275,\n",
      "        917.36114995,  928.99988218,  971.59260492, 1002.54363929,\n",
      "       1077.50206276,  981.41467242])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1031.38843481, 1008.36789473,  996.67797377, 1034.79060041,\n",
      "        961.82867089, 1001.79836244, 1019.2776188 , 1035.55514077,\n",
      "       1033.78454565, 1139.54181843, 1032.6695729 ,  979.85710706,\n",
      "        970.66424996,  971.55787089,  969.39897322, 1059.79104685,\n",
      "       1111.27315424, 1024.00153111,  971.80503587, 1026.55935047])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([794.92274497, 836.89239776, 867.16387476, 826.78026331,\n",
      "       841.22937628, 774.8583325 , 851.97983988, 839.78847353,\n",
      "       806.18137957, 876.3270062 , 822.22514043, 755.48010844,\n",
      "       729.86087559, 832.20711867, 784.38456467, 902.87694654,\n",
      "       773.59564939, 889.41788188, 887.39322146, 850.96512158,\n",
      "       820.95104652, 858.00387261, 855.81095587, 906.99625232,\n",
      "       882.58263097, 867.8616934 , 841.9761503 , 846.99786451,\n",
      "       871.15717441, 780.21953936])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([757.60855562, 864.58609949, 881.58160417, 859.3996499 ,\n",
      "       862.91979102, 883.65609382, 848.05515331, 819.54479791,\n",
      "       861.61104418, 823.77193688, 871.634727  , 794.34800497,\n",
      "       890.0528615 , 916.1347314 , 901.84355457, 934.63290199,\n",
      "       984.66471466, 970.16030028, 818.00754688, 784.76478819])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 854.64268169,  844.90353406,  807.12054261,  815.2239179 ,\n",
      "        802.85865266,  814.48982293,  923.60552682,  869.34745536,\n",
      "       1012.69305322,  979.26171637,  929.55501488,  868.02206953,\n",
      "        922.48195382,  889.64371938,  902.27492431, 1036.70076886,\n",
      "        959.19804806,  973.71324439,  902.93070215,  900.14532383,\n",
      "       1000.23202533,  921.42712426,  998.73708587,  990.95733222,\n",
      "        911.4989099 ,  943.0991758 ,  982.24411303,  975.88649061,\n",
      "        862.21052577,  857.87276556])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 893.84910538,  914.7797677 ,  956.00910157,  950.98553127,\n",
      "        869.84354752,  923.32683661,  967.6534664 ,  975.40865419,\n",
      "       1003.22990125, 1003.55157614,  982.53631582,  949.02568453,\n",
      "        858.44878802,  927.72946126,  957.00092337,  961.84957712,\n",
      "        887.94565068,  866.15942091,  972.96430377,  964.76263466])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1128.59352223, 1127.96863513, 1184.90842814, 1277.93386638,\n",
      "       1159.17684299, 1102.26127385, 1172.30186416, 1186.23552455,\n",
      "       1242.01460042, 1205.17205015, 1136.09652267, 1161.34722065,\n",
      "       1204.85291596, 1204.81004501, 1186.39807387, 1208.3644474 ,\n",
      "       1225.51802637, 1240.98008352, 1210.89558316, 1156.64225   ,\n",
      "       1153.90637439, 1211.43040122, 1134.72817155, 1155.09827511,\n",
      "       1138.12562284, 1146.38320567, 1216.96420579, 1191.58212951,\n",
      "       1166.49606169, 1092.18467862])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1167.0925951 , 1203.52277271, 1283.56378373, 1195.20997238,\n",
      "       1174.88613873, 1182.27617431, 1250.21616117, 1196.58470937,\n",
      "       1184.87301985, 1206.17887719, 1193.51503893, 1155.23528584,\n",
      "       1173.69540801, 1230.05262276, 1146.91276357, 1164.0737293 ,\n",
      "       1142.68370924, 1222.57762927, 1126.8284362 , 1123.38586604])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1019.2776188 , 1035.55514077, 1033.78454565, 1139.54181843,\n",
      "       1032.6695729 ,  979.85710706,  970.66424996,  971.55787089,\n",
      "        969.39897322, 1059.79104685, 1111.27315424, 1024.00153111,\n",
      "        971.80503587, 1026.55935047, 1002.37166937, 1044.2419652 ,\n",
      "       1047.00466984,  956.61527463, 1073.01406201, 1146.43226916,\n",
      "       1041.31184508, 1005.12645319, 1037.76355434, 1054.73861892,\n",
      "       1074.97384986, 1150.99978669, 1151.27744666, 1079.38867994,\n",
      "       1032.6191415 , 1050.25584071])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1097.33429888, 1205.78161584, 1133.46971939, 1128.59352223,\n",
      "       1127.96863513, 1184.90842814, 1277.93386638, 1159.17684299,\n",
      "       1102.26127385, 1172.30186416, 1186.23552455, 1242.01460042,\n",
      "       1205.17205015, 1136.09652267, 1161.34722065, 1204.85291596,\n",
      "       1204.81004501, 1186.39807387, 1208.3644474 , 1225.51802637])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 928.99988218,  971.59260492, 1002.54363929, 1077.50206276,\n",
      "        981.41467242, 1031.38843481, 1008.36789473,  996.67797377,\n",
      "       1034.79060041,  961.82867089, 1001.79836244, 1019.2776188 ,\n",
      "       1035.55514077, 1033.78454565, 1139.54181843, 1032.6695729 ,\n",
      "        979.85710706,  970.66424996,  971.55787089,  969.39897322,\n",
      "       1059.79104685, 1111.27315424, 1024.00153111,  971.80503587,\n",
      "       1026.55935047, 1002.37166937, 1044.2419652 , 1047.00466984,\n",
      "        956.61527463, 1073.01406201])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1146.43226916, 1041.31184508, 1005.12645319, 1037.76355434,\n",
      "       1054.73861892, 1074.97384986, 1150.99978669, 1151.27744666,\n",
      "       1079.38867994, 1032.6191415 , 1050.25584071, 1097.33429888,\n",
      "       1205.78161584, 1133.46971939, 1128.59352223, 1127.96863513,\n",
      "       1184.90842814, 1277.93386638, 1159.17684299, 1102.26127385])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 944.97319267,  940.49716249,  938.69094632, 1029.24446491,\n",
      "       1007.91049914, 1029.7894881 ,  921.02414826,  957.69581689,\n",
      "       1019.86652719,  997.83420854, 1033.61470903,  975.19803339,\n",
      "       1041.13401513, 1123.42793916, 1016.12489876,  960.41402604,\n",
      "       1046.97405111, 1092.88908797,  997.59030213,  960.7578993 ,\n",
      "        964.83221813,  914.84459202,  914.65709275,  917.36114995,\n",
      "        928.99988218,  971.59260492, 1002.54363929, 1077.50206276,\n",
      "        981.41467242, 1031.38843481])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1008.36789473,  996.67797377, 1034.79060041,  961.82867089,\n",
      "       1001.79836244, 1019.2776188 , 1035.55514077, 1033.78454565,\n",
      "       1139.54181843, 1032.6695729 ,  979.85710706,  970.66424996,\n",
      "        971.55787089,  969.39897322, 1059.79104685, 1111.27315424,\n",
      "       1024.00153111,  971.80503587, 1026.55935047, 1002.37166937])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 941.21957118, 1015.54262546,  944.97319267,  940.49716249,\n",
      "        938.69094632, 1029.24446491, 1007.91049914, 1029.7894881 ,\n",
      "        921.02414826,  957.69581689, 1019.86652719,  997.83420854,\n",
      "       1033.61470903,  975.19803339, 1041.13401513, 1123.42793916,\n",
      "       1016.12489876,  960.41402604, 1046.97405111, 1092.88908797,\n",
      "        997.59030213,  960.7578993 ,  964.83221813,  914.84459202,\n",
      "        914.65709275,  917.36114995,  928.99988218,  971.59260492,\n",
      "       1002.54363929, 1077.50206276])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 981.41467242, 1031.38843481, 1008.36789473,  996.67797377,\n",
      "       1034.79060041,  961.82867089, 1001.79836244, 1019.2776188 ,\n",
      "       1035.55514077, 1033.78454565, 1139.54181843, 1032.6695729 ,\n",
      "        979.85710706,  970.66424996,  971.55787089,  969.39897322,\n",
      "       1059.79104685, 1111.27315424, 1024.00153111,  971.80503587])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1007.91049914, 1029.7894881 ,  921.02414826,  957.69581689,\n",
      "       1019.86652719,  997.83420854, 1033.61470903,  975.19803339,\n",
      "       1041.13401513, 1123.42793916, 1016.12489876,  960.41402604,\n",
      "       1046.97405111, 1092.88908797,  997.59030213,  960.7578993 ,\n",
      "        964.83221813,  914.84459202,  914.65709275,  917.36114995,\n",
      "        928.99988218,  971.59260492, 1002.54363929, 1077.50206276,\n",
      "        981.41467242, 1031.38843481, 1008.36789473,  996.67797377,\n",
      "       1034.79060041,  961.82867089])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1001.79836244, 1019.2776188 , 1035.55514077, 1033.78454565,\n",
      "       1139.54181843, 1032.6695729 ,  979.85710706,  970.66424996,\n",
      "        971.55787089,  969.39897322, 1059.79104685, 1111.27315424,\n",
      "       1024.00153111,  971.80503587, 1026.55935047, 1002.37166937,\n",
      "       1044.2419652 , 1047.00466984,  956.61527463, 1073.01406201])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1035.55514077, 1033.78454565, 1139.54181843, 1032.6695729 ,\n",
      "        979.85710706,  970.66424996,  971.55787089,  969.39897322,\n",
      "       1059.79104685, 1111.27315424, 1024.00153111,  971.80503587,\n",
      "       1026.55935047, 1002.37166937, 1044.2419652 , 1047.00466984,\n",
      "        956.61527463, 1073.01406201, 1146.43226916, 1041.31184508,\n",
      "       1005.12645319, 1037.76355434, 1054.73861892, 1074.97384986,\n",
      "       1150.99978669, 1151.27744666, 1079.38867994, 1032.6191415 ,\n",
      "       1050.25584071, 1097.33429888])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1205.78161584, 1133.46971939, 1128.59352223, 1127.96863513,\n",
      "       1184.90842814, 1277.93386638, 1159.17684299, 1102.26127385,\n",
      "       1172.30186416, 1186.23552455, 1242.01460042, 1205.17205015,\n",
      "       1136.09652267, 1161.34722065, 1204.85291596, 1204.81004501,\n",
      "       1186.39807387, 1208.3644474 , 1225.51802637, 1240.98008352])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1127.96863513, 1184.90842814, 1277.93386638, 1159.17684299,\n",
      "       1102.26127385, 1172.30186416, 1186.23552455, 1242.01460042,\n",
      "       1205.17205015, 1136.09652267, 1161.34722065, 1204.85291596,\n",
      "       1204.81004501, 1186.39807387, 1208.3644474 , 1225.51802637,\n",
      "       1240.98008352, 1210.89558316, 1156.64225   , 1153.90637439,\n",
      "       1211.43040122, 1134.72817155, 1155.09827511, 1138.12562284,\n",
      "       1146.38320567, 1216.96420579, 1191.58212951, 1166.49606169,\n",
      "       1092.18467862, 1167.0925951 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1203.52277271, 1283.56378373, 1195.20997238, 1174.88613873,\n",
      "       1182.27617431, 1250.21616117, 1196.58470937, 1184.87301985,\n",
      "       1206.17887719, 1193.51503893, 1155.23528584, 1173.69540801,\n",
      "       1230.05262276, 1146.91276357, 1164.0737293 , 1142.68370924,\n",
      "       1222.57762927, 1126.8284362 , 1123.38586604, 1184.52339898])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1079.38867994, 1032.6191415 , 1050.25584071, 1097.33429888,\n",
      "       1205.78161584, 1133.46971939, 1128.59352223, 1127.96863513,\n",
      "       1184.90842814, 1277.93386638, 1159.17684299, 1102.26127385,\n",
      "       1172.30186416, 1186.23552455, 1242.01460042, 1205.17205015,\n",
      "       1136.09652267, 1161.34722065, 1204.85291596, 1204.81004501,\n",
      "       1186.39807387, 1208.3644474 , 1225.51802637, 1240.98008352,\n",
      "       1210.89558316, 1156.64225   , 1153.90637439, 1211.43040122,\n",
      "       1134.72817155, 1155.09827511])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1138.12562284, 1146.38320567, 1216.96420579, 1191.58212951,\n",
      "       1166.49606169, 1092.18467862, 1167.0925951 , 1203.52277271,\n",
      "       1283.56378373, 1195.20997238, 1174.88613873, 1182.27617431,\n",
      "       1250.21616117, 1196.58470937, 1184.87301985, 1206.17887719,\n",
      "       1193.51503893, 1155.23528584, 1173.69540801, 1230.05262276])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1172.30186416, 1186.23552455, 1242.01460042, 1205.17205015,\n",
      "       1136.09652267, 1161.34722065, 1204.85291596, 1204.81004501,\n",
      "       1186.39807387, 1208.3644474 , 1225.51802637, 1240.98008352,\n",
      "       1210.89558316, 1156.64225   , 1153.90637439, 1211.43040122,\n",
      "       1134.72817155, 1155.09827511, 1138.12562284, 1146.38320567,\n",
      "       1216.96420579, 1191.58212951, 1166.49606169, 1092.18467862,\n",
      "       1167.0925951 , 1203.52277271, 1283.56378373, 1195.20997238,\n",
      "       1174.88613873, 1182.27617431])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1250.21616117, 1196.58470937, 1184.87301985, 1206.17887719,\n",
      "       1193.51503893, 1155.23528584, 1173.69540801, 1230.05262276,\n",
      "       1146.91276357, 1164.0737293 , 1142.68370924, 1222.57762927,\n",
      "       1126.8284362 , 1123.38586604, 1184.52339898, 1266.06107518,\n",
      "       1214.66957735, 1161.19412261, 1269.16771977, 1154.08837153])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 861.98607758,  884.23519565,  854.64268169,  844.90353406,\n",
      "        807.12054261,  815.2239179 ,  802.85865266,  814.48982293,\n",
      "        923.60552682,  869.34745536, 1012.69305322,  979.26171637,\n",
      "        929.55501488,  868.02206953,  922.48195382,  889.64371938,\n",
      "        902.27492431, 1036.70076886,  959.19804806,  973.71324439,\n",
      "        902.93070215,  900.14532383, 1000.23202533,  921.42712426,\n",
      "        998.73708587,  990.95733222,  911.4989099 ,  943.0991758 ,\n",
      "        982.24411303,  975.88649061])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 862.21052577,  857.87276556,  893.84910538,  914.7797677 ,\n",
      "        956.00910157,  950.98553127,  869.84354752,  923.32683661,\n",
      "        967.6534664 ,  975.40865419, 1003.22990125, 1003.55157614,\n",
      "        982.53631582,  949.02568453,  858.44878802,  927.72946126,\n",
      "        957.00092337,  961.84957712,  887.94565068,  866.15942091])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1034.79060041,  961.82867089, 1001.79836244, 1019.2776188 ,\n",
      "       1035.55514077, 1033.78454565, 1139.54181843, 1032.6695729 ,\n",
      "        979.85710706,  970.66424996,  971.55787089,  969.39897322,\n",
      "       1059.79104685, 1111.27315424, 1024.00153111,  971.80503587,\n",
      "       1026.55935047, 1002.37166937, 1044.2419652 , 1047.00466984,\n",
      "        956.61527463, 1073.01406201, 1146.43226916, 1041.31184508,\n",
      "       1005.12645319, 1037.76355434, 1054.73861892, 1074.97384986,\n",
      "       1150.99978669, 1151.27744666])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1079.38867994, 1032.6191415 , 1050.25584071, 1097.33429888,\n",
      "       1205.78161584, 1133.46971939, 1128.59352223, 1127.96863513,\n",
      "       1184.90842814, 1277.93386638, 1159.17684299, 1102.26127385,\n",
      "       1172.30186416, 1186.23552455, 1242.01460042, 1205.17205015,\n",
      "       1136.09652267, 1161.34722065, 1204.85291596, 1204.81004501])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1242.01460042, 1205.17205015, 1136.09652267, 1161.34722065,\n",
      "       1204.85291596, 1204.81004501, 1186.39807387, 1208.3644474 ,\n",
      "       1225.51802637, 1240.98008352, 1210.89558316, 1156.64225   ,\n",
      "       1153.90637439, 1211.43040122, 1134.72817155, 1155.09827511,\n",
      "       1138.12562284, 1146.38320567, 1216.96420579, 1191.58212951,\n",
      "       1166.49606169, 1092.18467862, 1167.0925951 , 1203.52277271,\n",
      "       1283.56378373, 1195.20997238, 1174.88613873, 1182.27617431,\n",
      "       1250.21616117, 1196.58470937])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1184.87301985, 1206.17887719, 1193.51503893, 1155.23528584,\n",
      "       1173.69540801, 1230.05262276, 1146.91276357, 1164.0737293 ,\n",
      "       1142.68370924, 1222.57762927, 1126.8284362 , 1123.38586604,\n",
      "       1184.52339898, 1266.06107518, 1214.66957735, 1161.19412261,\n",
      "       1269.16771977, 1154.08837153, 1053.80799635, 1161.52993384])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 857.87276556,  893.84910538,  914.7797677 ,  956.00910157,\n",
      "        950.98553127,  869.84354752,  923.32683661,  967.6534664 ,\n",
      "        975.40865419, 1003.22990125, 1003.55157614,  982.53631582,\n",
      "        949.02568453,  858.44878802,  927.72946126,  957.00092337,\n",
      "        961.84957712,  887.94565068,  866.15942091,  972.96430377,\n",
      "        964.76263466,  985.10560512,  967.39478882,  956.53369828,\n",
      "       1001.37762455,  949.18365111,  952.55314568,  935.12600406,\n",
      "        926.72522669,  933.22378088])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 963.24465376,  941.21957118, 1015.54262546,  944.97319267,\n",
      "        940.49716249,  938.69094632, 1029.24446491, 1007.91049914,\n",
      "       1029.7894881 ,  921.02414826,  957.69581689, 1019.86652719,\n",
      "        997.83420854, 1033.61470903,  975.19803339, 1041.13401513,\n",
      "       1123.42793916, 1016.12489876,  960.41402604, 1046.97405111])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1074.97384986, 1150.99978669, 1151.27744666, 1079.38867994,\n",
      "       1032.6191415 , 1050.25584071, 1097.33429888, 1205.78161584,\n",
      "       1133.46971939, 1128.59352223, 1127.96863513, 1184.90842814,\n",
      "       1277.93386638, 1159.17684299, 1102.26127385, 1172.30186416,\n",
      "       1186.23552455, 1242.01460042, 1205.17205015, 1136.09652267,\n",
      "       1161.34722065, 1204.85291596, 1204.81004501, 1186.39807387,\n",
      "       1208.3644474 , 1225.51802637, 1240.98008352, 1210.89558316,\n",
      "       1156.64225   , 1153.90637439])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1211.43040122, 1134.72817155, 1155.09827511, 1138.12562284,\n",
      "       1146.38320567, 1216.96420579, 1191.58212951, 1166.49606169,\n",
      "       1092.18467862, 1167.0925951 , 1203.52277271, 1283.56378373,\n",
      "       1195.20997238, 1174.88613873, 1182.27617431, 1250.21616117,\n",
      "       1196.58470937, 1184.87301985, 1206.17887719, 1193.51503893])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1050.25584071, 1097.33429888, 1205.78161584, 1133.46971939,\n",
      "       1128.59352223, 1127.96863513, 1184.90842814, 1277.93386638,\n",
      "       1159.17684299, 1102.26127385, 1172.30186416, 1186.23552455,\n",
      "       1242.01460042, 1205.17205015, 1136.09652267, 1161.34722065,\n",
      "       1204.85291596, 1204.81004501, 1186.39807387, 1208.3644474 ,\n",
      "       1225.51802637, 1240.98008352, 1210.89558316, 1156.64225   ,\n",
      "       1153.90637439, 1211.43040122, 1134.72817155, 1155.09827511,\n",
      "       1138.12562284, 1146.38320567])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1216.96420579, 1191.58212951, 1166.49606169, 1092.18467862,\n",
      "       1167.0925951 , 1203.52277271, 1283.56378373, 1195.20997238,\n",
      "       1174.88613873, 1182.27617431, 1250.21616117, 1196.58470937,\n",
      "       1184.87301985, 1206.17887719, 1193.51503893, 1155.23528584,\n",
      "       1173.69540801, 1230.05262276, 1146.91276357, 1164.0737293 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1041.31184508, 1005.12645319, 1037.76355434, 1054.73861892,\n",
      "       1074.97384986, 1150.99978669, 1151.27744666, 1079.38867994,\n",
      "       1032.6191415 , 1050.25584071, 1097.33429888, 1205.78161584,\n",
      "       1133.46971939, 1128.59352223, 1127.96863513, 1184.90842814,\n",
      "       1277.93386638, 1159.17684299, 1102.26127385, 1172.30186416,\n",
      "       1186.23552455, 1242.01460042, 1205.17205015, 1136.09652267,\n",
      "       1161.34722065, 1204.85291596, 1204.81004501, 1186.39807387,\n",
      "       1208.3644474 , 1225.51802637])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1240.98008352, 1210.89558316, 1156.64225   , 1153.90637439,\n",
      "       1211.43040122, 1134.72817155, 1155.09827511, 1138.12562284,\n",
      "       1146.38320567, 1216.96420579, 1191.58212951, 1166.49606169,\n",
      "       1092.18467862, 1167.0925951 , 1203.52277271, 1283.56378373,\n",
      "       1195.20997238, 1174.88613873, 1182.27617431, 1250.21616117])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 979.26171637,  929.55501488,  868.02206953,  922.48195382,\n",
      "        889.64371938,  902.27492431, 1036.70076886,  959.19804806,\n",
      "        973.71324439,  902.93070215,  900.14532383, 1000.23202533,\n",
      "        921.42712426,  998.73708587,  990.95733222,  911.4989099 ,\n",
      "        943.0991758 ,  982.24411303,  975.88649061,  862.21052577,\n",
      "        857.87276556,  893.84910538,  914.7797677 ,  956.00910157,\n",
      "        950.98553127,  869.84354752,  923.32683661,  967.6534664 ,\n",
      "        975.40865419, 1003.22990125])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1003.55157614,  982.53631582,  949.02568453,  858.44878802,\n",
      "        927.72946126,  957.00092337,  961.84957712,  887.94565068,\n",
      "        866.15942091,  972.96430377,  964.76263466,  985.10560512,\n",
      "        967.39478882,  956.53369828, 1001.37762455,  949.18365111,\n",
      "        952.55314568,  935.12600406,  926.72522669,  933.22378088])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 815.2239179 ,  802.85865266,  814.48982293,  923.60552682,\n",
      "        869.34745536, 1012.69305322,  979.26171637,  929.55501488,\n",
      "        868.02206953,  922.48195382,  889.64371938,  902.27492431,\n",
      "       1036.70076886,  959.19804806,  973.71324439,  902.93070215,\n",
      "        900.14532383, 1000.23202533,  921.42712426,  998.73708587,\n",
      "        990.95733222,  911.4989099 ,  943.0991758 ,  982.24411303,\n",
      "        975.88649061,  862.21052577,  857.87276556,  893.84910538,\n",
      "        914.7797677 ,  956.00910157])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 950.98553127,  869.84354752,  923.32683661,  967.6534664 ,\n",
      "        975.40865419, 1003.22990125, 1003.55157614,  982.53631582,\n",
      "        949.02568453,  858.44878802,  927.72946126,  957.00092337,\n",
      "        961.84957712,  887.94565068,  866.15942091,  972.96430377,\n",
      "        964.76263466,  985.10560512,  967.39478882,  956.53369828])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1102.26127385, 1172.30186416, 1186.23552455, 1242.01460042,\n",
      "       1205.17205015, 1136.09652267, 1161.34722065, 1204.85291596,\n",
      "       1204.81004501, 1186.39807387, 1208.3644474 , 1225.51802637,\n",
      "       1240.98008352, 1210.89558316, 1156.64225   , 1153.90637439,\n",
      "       1211.43040122, 1134.72817155, 1155.09827511, 1138.12562284,\n",
      "       1146.38320567, 1216.96420579, 1191.58212951, 1166.49606169,\n",
      "       1092.18467862, 1167.0925951 , 1203.52277271, 1283.56378373,\n",
      "       1195.20997238, 1174.88613873])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1182.27617431, 1250.21616117, 1196.58470937, 1184.87301985,\n",
      "       1206.17887719, 1193.51503893, 1155.23528584, 1173.69540801,\n",
      "       1230.05262276, 1146.91276357, 1164.0737293 , 1142.68370924,\n",
      "       1222.57762927, 1126.8284362 , 1123.38586604, 1184.52339898,\n",
      "       1266.06107518, 1214.66957735, 1161.19412261, 1269.16771977])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 969.39897322, 1059.79104685, 1111.27315424, 1024.00153111,\n",
      "        971.80503587, 1026.55935047, 1002.37166937, 1044.2419652 ,\n",
      "       1047.00466984,  956.61527463, 1073.01406201, 1146.43226916,\n",
      "       1041.31184508, 1005.12645319, 1037.76355434, 1054.73861892,\n",
      "       1074.97384986, 1150.99978669, 1151.27744666, 1079.38867994,\n",
      "       1032.6191415 , 1050.25584071, 1097.33429888, 1205.78161584,\n",
      "       1133.46971939, 1128.59352223, 1127.96863513, 1184.90842814,\n",
      "       1277.93386638, 1159.17684299])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1102.26127385, 1172.30186416, 1186.23552455, 1242.01460042,\n",
      "       1205.17205015, 1136.09652267, 1161.34722065, 1204.85291596,\n",
      "       1204.81004501, 1186.39807387, 1208.3644474 , 1225.51802637,\n",
      "       1240.98008352, 1210.89558316, 1156.64225   , 1153.90637439,\n",
      "       1211.43040122, 1134.72817155, 1155.09827511, 1138.12562284])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([614.86465122, 596.29610199, 638.34407776, 596.98361388,\n",
      "       521.09639918, 505.46555349, 589.70267609, 636.73604882,\n",
      "       589.67698049, 644.77612963, 561.24317062, 596.79057361,\n",
      "       568.82052745, 576.8558281 , 622.05026979, 594.06184224,\n",
      "       602.02666574, 680.36988082, 633.39966005, 675.71884524,\n",
      "       605.40908925, 650.71947409, 727.41228097, 557.18612905,\n",
      "       562.01216424, 655.97105619, 650.10028912, 668.16176824,\n",
      "       631.85717443, 648.09903429])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([649.27642821, 712.67236658, 696.27975741, 686.1849128 ,\n",
      "       649.98371354, 631.18617055, 631.07502274, 673.75730796,\n",
      "       651.98129603, 673.73626744, 777.07324188, 758.2234969 ,\n",
      "       680.18442364, 729.54997166, 678.89672873, 568.29571713,\n",
      "       581.78740582, 555.83498193, 615.91695795, 665.67546333])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1133.46971939, 1128.59352223, 1127.96863513, 1184.90842814,\n",
      "       1277.93386638, 1159.17684299, 1102.26127385, 1172.30186416,\n",
      "       1186.23552455, 1242.01460042, 1205.17205015, 1136.09652267,\n",
      "       1161.34722065, 1204.85291596, 1204.81004501, 1186.39807387,\n",
      "       1208.3644474 , 1225.51802637, 1240.98008352, 1210.89558316,\n",
      "       1156.64225   , 1153.90637439, 1211.43040122, 1134.72817155,\n",
      "       1155.09827511, 1138.12562284, 1146.38320567, 1216.96420579,\n",
      "       1191.58212951, 1166.49606169])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1092.18467862, 1167.0925951 , 1203.52277271, 1283.56378373,\n",
      "       1195.20997238, 1174.88613873, 1182.27617431, 1250.21616117,\n",
      "       1196.58470937, 1184.87301985, 1206.17887719, 1193.51503893,\n",
      "       1155.23528584, 1173.69540801, 1230.05262276, 1146.91276357,\n",
      "       1164.0737293 , 1142.68370924, 1222.57762927, 1126.8284362 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 858.44878802,  927.72946126,  957.00092337,  961.84957712,\n",
      "        887.94565068,  866.15942091,  972.96430377,  964.76263466,\n",
      "        985.10560512,  967.39478882,  956.53369828, 1001.37762455,\n",
      "        949.18365111,  952.55314568,  935.12600406,  926.72522669,\n",
      "        933.22378088,  963.24465376,  941.21957118, 1015.54262546,\n",
      "        944.97319267,  940.49716249,  938.69094632, 1029.24446491,\n",
      "       1007.91049914, 1029.7894881 ,  921.02414826,  957.69581689,\n",
      "       1019.86652719,  997.83420854])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1033.61470903,  975.19803339, 1041.13401513, 1123.42793916,\n",
      "       1016.12489876,  960.41402604, 1046.97405111, 1092.88908797,\n",
      "        997.59030213,  960.7578993 ,  964.83221813,  914.84459202,\n",
      "        914.65709275,  917.36114995,  928.99988218,  971.59260492,\n",
      "       1002.54363929, 1077.50206276,  981.41467242, 1031.38843481])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 964.83221813,  914.84459202,  914.65709275,  917.36114995,\n",
      "        928.99988218,  971.59260492, 1002.54363929, 1077.50206276,\n",
      "        981.41467242, 1031.38843481, 1008.36789473,  996.67797377,\n",
      "       1034.79060041,  961.82867089, 1001.79836244, 1019.2776188 ,\n",
      "       1035.55514077, 1033.78454565, 1139.54181843, 1032.6695729 ,\n",
      "        979.85710706,  970.66424996,  971.55787089,  969.39897322,\n",
      "       1059.79104685, 1111.27315424, 1024.00153111,  971.80503587,\n",
      "       1026.55935047, 1002.37166937])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1044.2419652 , 1047.00466984,  956.61527463, 1073.01406201,\n",
      "       1146.43226916, 1041.31184508, 1005.12645319, 1037.76355434,\n",
      "       1054.73861892, 1074.97384986, 1150.99978669, 1151.27744666,\n",
      "       1079.38867994, 1032.6191415 , 1050.25584071, 1097.33429888,\n",
      "       1205.78161584, 1133.46971939, 1128.59352223, 1127.96863513])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1204.85291596, 1204.81004501, 1186.39807387, 1208.3644474 ,\n",
      "       1225.51802637, 1240.98008352, 1210.89558316, 1156.64225   ,\n",
      "       1153.90637439, 1211.43040122, 1134.72817155, 1155.09827511,\n",
      "       1138.12562284, 1146.38320567, 1216.96420579, 1191.58212951,\n",
      "       1166.49606169, 1092.18467862, 1167.0925951 , 1203.52277271,\n",
      "       1283.56378373, 1195.20997238, 1174.88613873, 1182.27617431,\n",
      "       1250.21616117, 1196.58470937, 1184.87301985, 1206.17887719,\n",
      "       1193.51503893, 1155.23528584])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1173.69540801, 1230.05262276, 1146.91276357, 1164.0737293 ,\n",
      "       1142.68370924, 1222.57762927, 1126.8284362 , 1123.38586604,\n",
      "       1184.52339898, 1266.06107518, 1214.66957735, 1161.19412261,\n",
      "       1269.16771977, 1154.08837153, 1053.80799635, 1161.52993384,\n",
      "       1167.57896682, 1129.95184813, 1174.24243267, 1186.96621446])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1033.78454565, 1139.54181843, 1032.6695729 ,  979.85710706,\n",
      "        970.66424996,  971.55787089,  969.39897322, 1059.79104685,\n",
      "       1111.27315424, 1024.00153111,  971.80503587, 1026.55935047,\n",
      "       1002.37166937, 1044.2419652 , 1047.00466984,  956.61527463,\n",
      "       1073.01406201, 1146.43226916, 1041.31184508, 1005.12645319,\n",
      "       1037.76355434, 1054.73861892, 1074.97384986, 1150.99978669,\n",
      "       1151.27744666, 1079.38867994, 1032.6191415 , 1050.25584071,\n",
      "       1097.33429888, 1205.78161584])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1133.46971939, 1128.59352223, 1127.96863513, 1184.90842814,\n",
      "       1277.93386638, 1159.17684299, 1102.26127385, 1172.30186416,\n",
      "       1186.23552455, 1242.01460042, 1205.17205015, 1136.09652267,\n",
      "       1161.34722065, 1204.85291596, 1204.81004501, 1186.39807387,\n",
      "       1208.3644474 , 1225.51802637, 1240.98008352, 1210.89558316])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([728.76976372, 866.57003017, 887.52255784, 896.85451539,\n",
      "       877.50053638, 775.79215333, 766.04648599, 824.39341901,\n",
      "       762.38914573, 826.6395924 , 817.13219304, 796.22790491,\n",
      "       846.83172639, 857.07268611, 815.69791161, 875.33317673,\n",
      "       794.92274497, 836.89239776, 867.16387476, 826.78026331,\n",
      "       841.22937628, 774.8583325 , 851.97983988, 839.78847353,\n",
      "       806.18137957, 876.3270062 , 822.22514043, 755.48010844,\n",
      "       729.86087559, 832.20711867])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([784.38456467, 902.87694654, 773.59564939, 889.41788188,\n",
      "       887.39322146, 850.96512158, 820.95104652, 858.00387261,\n",
      "       855.81095587, 906.99625232, 882.58263097, 867.8616934 ,\n",
      "       841.9761503 , 846.99786451, 871.15717441, 780.21953936,\n",
      "       757.60855562, 864.58609949, 881.58160417, 859.3996499 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([743.8072481 , 770.72811344, 722.73099485, 733.79990542,\n",
      "       728.76976372, 866.57003017, 887.52255784, 896.85451539,\n",
      "       877.50053638, 775.79215333, 766.04648599, 824.39341901,\n",
      "       762.38914573, 826.6395924 , 817.13219304, 796.22790491,\n",
      "       846.83172639, 857.07268611, 815.69791161, 875.33317673,\n",
      "       794.92274497, 836.89239776, 867.16387476, 826.78026331,\n",
      "       841.22937628, 774.8583325 , 851.97983988, 839.78847353,\n",
      "       806.18137957, 876.3270062 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([822.22514043, 755.48010844, 729.86087559, 832.20711867,\n",
      "       784.38456467, 902.87694654, 773.59564939, 889.41788188,\n",
      "       887.39322146, 850.96512158, 820.95104652, 858.00387261,\n",
      "       855.81095587, 906.99625232, 882.58263097, 867.8616934 ,\n",
      "       841.9761503 , 846.99786451, 871.15717441, 780.21953936])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 940.49716249,  938.69094632, 1029.24446491, 1007.91049914,\n",
      "       1029.7894881 ,  921.02414826,  957.69581689, 1019.86652719,\n",
      "        997.83420854, 1033.61470903,  975.19803339, 1041.13401513,\n",
      "       1123.42793916, 1016.12489876,  960.41402604, 1046.97405111,\n",
      "       1092.88908797,  997.59030213,  960.7578993 ,  964.83221813,\n",
      "        914.84459202,  914.65709275,  917.36114995,  928.99988218,\n",
      "        971.59260492, 1002.54363929, 1077.50206276,  981.41467242,\n",
      "       1031.38843481, 1008.36789473])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 996.67797377, 1034.79060041,  961.82867089, 1001.79836244,\n",
      "       1019.2776188 , 1035.55514077, 1033.78454565, 1139.54181843,\n",
      "       1032.6695729 ,  979.85710706,  970.66424996,  971.55787089,\n",
      "        969.39897322, 1059.79104685, 1111.27315424, 1024.00153111,\n",
      "        971.80503587, 1026.55935047, 1002.37166937, 1044.2419652 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 957.69581689, 1019.86652719,  997.83420854, 1033.61470903,\n",
      "        975.19803339, 1041.13401513, 1123.42793916, 1016.12489876,\n",
      "        960.41402604, 1046.97405111, 1092.88908797,  997.59030213,\n",
      "        960.7578993 ,  964.83221813,  914.84459202,  914.65709275,\n",
      "        917.36114995,  928.99988218,  971.59260492, 1002.54363929,\n",
      "       1077.50206276,  981.41467242, 1031.38843481, 1008.36789473,\n",
      "        996.67797377, 1034.79060041,  961.82867089, 1001.79836244,\n",
      "       1019.2776188 , 1035.55514077])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1033.78454565, 1139.54181843, 1032.6695729 ,  979.85710706,\n",
      "        970.66424996,  971.55787089,  969.39897322, 1059.79104685,\n",
      "       1111.27315424, 1024.00153111,  971.80503587, 1026.55935047,\n",
      "       1002.37166937, 1044.2419652 , 1047.00466984,  956.61527463,\n",
      "       1073.01406201, 1146.43226916, 1041.31184508, 1005.12645319])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1046.97405111, 1092.88908797,  997.59030213,  960.7578993 ,\n",
      "        964.83221813,  914.84459202,  914.65709275,  917.36114995,\n",
      "        928.99988218,  971.59260492, 1002.54363929, 1077.50206276,\n",
      "        981.41467242, 1031.38843481, 1008.36789473,  996.67797377,\n",
      "       1034.79060041,  961.82867089, 1001.79836244, 1019.2776188 ,\n",
      "       1035.55514077, 1033.78454565, 1139.54181843, 1032.6695729 ,\n",
      "        979.85710706,  970.66424996,  971.55787089,  969.39897322,\n",
      "       1059.79104685, 1111.27315424])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1024.00153111,  971.80503587, 1026.55935047, 1002.37166937,\n",
      "       1044.2419652 , 1047.00466984,  956.61527463, 1073.01406201,\n",
      "       1146.43226916, 1041.31184508, 1005.12645319, 1037.76355434,\n",
      "       1054.73861892, 1074.97384986, 1150.99978669, 1151.27744666,\n",
      "       1079.38867994, 1032.6191415 , 1050.25584071, 1097.33429888])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1047.00466984,  956.61527463, 1073.01406201, 1146.43226916,\n",
      "       1041.31184508, 1005.12645319, 1037.76355434, 1054.73861892,\n",
      "       1074.97384986, 1150.99978669, 1151.27744666, 1079.38867994,\n",
      "       1032.6191415 , 1050.25584071, 1097.33429888, 1205.78161584,\n",
      "       1133.46971939, 1128.59352223, 1127.96863513, 1184.90842814,\n",
      "       1277.93386638, 1159.17684299, 1102.26127385, 1172.30186416,\n",
      "       1186.23552455, 1242.01460042, 1205.17205015, 1136.09652267,\n",
      "       1161.34722065, 1204.85291596])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1204.81004501, 1186.39807387, 1208.3644474 , 1225.51802637,\n",
      "       1240.98008352, 1210.89558316, 1156.64225   , 1153.90637439,\n",
      "       1211.43040122, 1134.72817155, 1155.09827511, 1138.12562284,\n",
      "       1146.38320567, 1216.96420579, 1191.58212951, 1166.49606169,\n",
      "       1092.18467862, 1167.0925951 , 1203.52277271, 1283.56378373])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 922.48195382,  889.64371938,  902.27492431, 1036.70076886,\n",
      "        959.19804806,  973.71324439,  902.93070215,  900.14532383,\n",
      "       1000.23202533,  921.42712426,  998.73708587,  990.95733222,\n",
      "        911.4989099 ,  943.0991758 ,  982.24411303,  975.88649061,\n",
      "        862.21052577,  857.87276556,  893.84910538,  914.7797677 ,\n",
      "        956.00910157,  950.98553127,  869.84354752,  923.32683661,\n",
      "        967.6534664 ,  975.40865419, 1003.22990125, 1003.55157614,\n",
      "        982.53631582,  949.02568453])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 858.44878802,  927.72946126,  957.00092337,  961.84957712,\n",
      "        887.94565068,  866.15942091,  972.96430377,  964.76263466,\n",
      "        985.10560512,  967.39478882,  956.53369828, 1001.37762455,\n",
      "        949.18365111,  952.55314568,  935.12600406,  926.72522669,\n",
      "        933.22378088,  963.24465376,  941.21957118, 1015.54262546])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1044.2419652 , 1047.00466984,  956.61527463, 1073.01406201,\n",
      "       1146.43226916, 1041.31184508, 1005.12645319, 1037.76355434,\n",
      "       1054.73861892, 1074.97384986, 1150.99978669, 1151.27744666,\n",
      "       1079.38867994, 1032.6191415 , 1050.25584071, 1097.33429888,\n",
      "       1205.78161584, 1133.46971939, 1128.59352223, 1127.96863513,\n",
      "       1184.90842814, 1277.93386638, 1159.17684299, 1102.26127385,\n",
      "       1172.30186416, 1186.23552455, 1242.01460042, 1205.17205015,\n",
      "       1136.09652267, 1161.34722065])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1204.85291596, 1204.81004501, 1186.39807387, 1208.3644474 ,\n",
      "       1225.51802637, 1240.98008352, 1210.89558316, 1156.64225   ,\n",
      "       1153.90637439, 1211.43040122, 1134.72817155, 1155.09827511,\n",
      "       1138.12562284, 1146.38320567, 1216.96420579, 1191.58212951,\n",
      "       1166.49606169, 1092.18467862, 1167.0925951 , 1203.52277271])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1138.12562284, 1146.38320567, 1216.96420579, 1191.58212951,\n",
      "       1166.49606169, 1092.18467862, 1167.0925951 , 1203.52277271,\n",
      "       1283.56378373, 1195.20997238, 1174.88613873, 1182.27617431,\n",
      "       1250.21616117, 1196.58470937, 1184.87301985, 1206.17887719,\n",
      "       1193.51503893, 1155.23528584, 1173.69540801, 1230.05262276,\n",
      "       1146.91276357, 1164.0737293 , 1142.68370924, 1222.57762927,\n",
      "       1126.8284362 , 1123.38586604, 1184.52339898, 1266.06107518,\n",
      "       1214.66957735, 1161.19412261])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1269.16771977, 1154.08837153, 1053.80799635, 1161.52993384,\n",
      "       1167.57896682, 1129.95184813, 1174.24243267, 1186.96621446,\n",
      "       1142.64760783, 1090.36430471, 1180.46179222, 1213.31148268,\n",
      "       1178.18182074, 1260.71384385, 1155.78393818, 1077.27025203,\n",
      "       1102.0871485 , 1149.52269827, 1180.35512515, 1165.76588431])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 861.61104418,  823.77193688,  871.634727  ,  794.34800497,\n",
      "        890.0528615 ,  916.1347314 ,  901.84355457,  934.63290199,\n",
      "        984.66471466,  970.16030028,  818.00754688,  784.76478819,\n",
      "        821.16300342,  868.15554952,  909.64502365,  861.98607758,\n",
      "        884.23519565,  854.64268169,  844.90353406,  807.12054261,\n",
      "        815.2239179 ,  802.85865266,  814.48982293,  923.60552682,\n",
      "        869.34745536, 1012.69305322,  979.26171637,  929.55501488,\n",
      "        868.02206953,  922.48195382])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 889.64371938,  902.27492431, 1036.70076886,  959.19804806,\n",
      "        973.71324439,  902.93070215,  900.14532383, 1000.23202533,\n",
      "        921.42712426,  998.73708587,  990.95733222,  911.4989099 ,\n",
      "        943.0991758 ,  982.24411303,  975.88649061,  862.21052577,\n",
      "        857.87276556,  893.84910538,  914.7797677 ,  956.00910157])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([717.67629451, 728.46205325, 681.62189532, 718.02118063,\n",
      "       820.5333744 , 755.39247769, 721.11223575, 683.36485669,\n",
      "       648.91690775, 722.89139063, 674.15797097, 690.34147284,\n",
      "       679.89555162, 718.86986728, 732.00333529, 767.83519502,\n",
      "       707.24881753, 689.61478723, 656.32769369, 668.64987017,\n",
      "       719.83064084, 758.57681546, 687.1679321 , 743.60502328,\n",
      "       802.44119057, 771.56266707, 828.12820822, 726.32636326,\n",
      "       651.49920215, 608.68186599])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([758.13233494, 784.85025629, 742.43778409, 744.95265199,\n",
      "       681.04995163, 830.47041928, 788.31397118, 749.23453274,\n",
      "       775.92944027, 776.84226997, 761.63590883, 706.47664247,\n",
      "       747.91110009, 842.84008478, 850.5771681 , 857.88656147,\n",
      "       759.24070059, 692.35079504, 719.83979052, 745.37251598])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1186.39807387, 1208.3644474 , 1225.51802637, 1240.98008352,\n",
      "       1210.89558316, 1156.64225   , 1153.90637439, 1211.43040122,\n",
      "       1134.72817155, 1155.09827511, 1138.12562284, 1146.38320567,\n",
      "       1216.96420579, 1191.58212951, 1166.49606169, 1092.18467862,\n",
      "       1167.0925951 , 1203.52277271, 1283.56378373, 1195.20997238,\n",
      "       1174.88613873, 1182.27617431, 1250.21616117, 1196.58470937,\n",
      "       1184.87301985, 1206.17887719, 1193.51503893, 1155.23528584,\n",
      "       1173.69540801, 1230.05262276])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1146.91276357, 1164.0737293 , 1142.68370924, 1222.57762927,\n",
      "       1126.8284362 , 1123.38586604, 1184.52339898, 1266.06107518,\n",
      "       1214.66957735, 1161.19412261, 1269.16771977, 1154.08837153,\n",
      "       1053.80799635, 1161.52993384, 1167.57896682, 1129.95184813,\n",
      "       1174.24243267, 1186.96621446, 1142.64760783, 1090.36430471])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1003.55157614,  982.53631582,  949.02568453,  858.44878802,\n",
      "        927.72946126,  957.00092337,  961.84957712,  887.94565068,\n",
      "        866.15942091,  972.96430377,  964.76263466,  985.10560512,\n",
      "        967.39478882,  956.53369828, 1001.37762455,  949.18365111,\n",
      "        952.55314568,  935.12600406,  926.72522669,  933.22378088,\n",
      "        963.24465376,  941.21957118, 1015.54262546,  944.97319267,\n",
      "        940.49716249,  938.69094632, 1029.24446491, 1007.91049914,\n",
      "       1029.7894881 ,  921.02414826])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 957.69581689, 1019.86652719,  997.83420854, 1033.61470903,\n",
      "        975.19803339, 1041.13401513, 1123.42793916, 1016.12489876,\n",
      "        960.41402604, 1046.97405111, 1092.88908797,  997.59030213,\n",
      "        960.7578993 ,  964.83221813,  914.84459202,  914.65709275,\n",
      "        917.36114995,  928.99988218,  971.59260492, 1002.54363929])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([834.70411715, 785.46547122, 690.58578351, 723.47617913,\n",
      "       745.1619075 , 627.8142917 , 709.36185236, 750.69673545,\n",
      "       800.72938966, 880.17979644, 873.23872791, 793.98585857,\n",
      "       722.50694506, 884.27854574, 835.09507914, 790.9940369 ,\n",
      "       784.06169243, 794.03170193, 782.43241217, 755.51672945,\n",
      "       747.93387285, 773.69529753, 760.13137483, 743.52554413,\n",
      "       780.65476182, 780.51270032, 863.92123596, 695.31521813,\n",
      "       799.46781486, 876.53041534])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([724.65218252, 743.8072481 , 770.72811344, 722.73099485,\n",
      "       733.79990542, 728.76976372, 866.57003017, 887.52255784,\n",
      "       896.85451539, 877.50053638, 775.79215333, 766.04648599,\n",
      "       824.39341901, 762.38914573, 826.6395924 , 817.13219304,\n",
      "       796.22790491, 846.83172639, 857.07268611, 815.69791161])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1146.43226916, 1041.31184508, 1005.12645319, 1037.76355434,\n",
      "       1054.73861892, 1074.97384986, 1150.99978669, 1151.27744666,\n",
      "       1079.38867994, 1032.6191415 , 1050.25584071, 1097.33429888,\n",
      "       1205.78161584, 1133.46971939, 1128.59352223, 1127.96863513,\n",
      "       1184.90842814, 1277.93386638, 1159.17684299, 1102.26127385,\n",
      "       1172.30186416, 1186.23552455, 1242.01460042, 1205.17205015,\n",
      "       1136.09652267, 1161.34722065, 1204.85291596, 1204.81004501,\n",
      "       1186.39807387, 1208.3644474 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1225.51802637, 1240.98008352, 1210.89558316, 1156.64225   ,\n",
      "       1153.90637439, 1211.43040122, 1134.72817155, 1155.09827511,\n",
      "       1138.12562284, 1146.38320567, 1216.96420579, 1191.58212951,\n",
      "       1166.49606169, 1092.18467862, 1167.0925951 , 1203.52277271,\n",
      "       1283.56378373, 1195.20997238, 1174.88613873, 1182.27617431])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([780.65476182, 780.51270032, 863.92123596, 695.31521813,\n",
      "       799.46781486, 876.53041534, 724.65218252, 743.8072481 ,\n",
      "       770.72811344, 722.73099485, 733.79990542, 728.76976372,\n",
      "       866.57003017, 887.52255784, 896.85451539, 877.50053638,\n",
      "       775.79215333, 766.04648599, 824.39341901, 762.38914573,\n",
      "       826.6395924 , 817.13219304, 796.22790491, 846.83172639,\n",
      "       857.07268611, 815.69791161, 875.33317673, 794.92274497,\n",
      "       836.89239776, 867.16387476])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([826.78026331, 841.22937628, 774.8583325 , 851.97983988,\n",
      "       839.78847353, 806.18137957, 876.3270062 , 822.22514043,\n",
      "       755.48010844, 729.86087559, 832.20711867, 784.38456467,\n",
      "       902.87694654, 773.59564939, 889.41788188, 887.39322146,\n",
      "       850.96512158, 820.95104652, 858.00387261, 855.81095587])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1111.27315424, 1024.00153111,  971.80503587, 1026.55935047,\n",
      "       1002.37166937, 1044.2419652 , 1047.00466984,  956.61527463,\n",
      "       1073.01406201, 1146.43226916, 1041.31184508, 1005.12645319,\n",
      "       1037.76355434, 1054.73861892, 1074.97384986, 1150.99978669,\n",
      "       1151.27744666, 1079.38867994, 1032.6191415 , 1050.25584071,\n",
      "       1097.33429888, 1205.78161584, 1133.46971939, 1128.59352223,\n",
      "       1127.96863513, 1184.90842814, 1277.93386638, 1159.17684299,\n",
      "       1102.26127385, 1172.30186416])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1186.23552455, 1242.01460042, 1205.17205015, 1136.09652267,\n",
      "       1161.34722065, 1204.85291596, 1204.81004501, 1186.39807387,\n",
      "       1208.3644474 , 1225.51802637, 1240.98008352, 1210.89558316,\n",
      "       1156.64225   , 1153.90637439, 1211.43040122, 1134.72817155,\n",
      "       1155.09827511, 1138.12562284, 1146.38320567, 1216.96420579])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1240.98008352, 1210.89558316, 1156.64225   , 1153.90637439,\n",
      "       1211.43040122, 1134.72817155, 1155.09827511, 1138.12562284,\n",
      "       1146.38320567, 1216.96420579, 1191.58212951, 1166.49606169,\n",
      "       1092.18467862, 1167.0925951 , 1203.52277271, 1283.56378373,\n",
      "       1195.20997238, 1174.88613873, 1182.27617431, 1250.21616117,\n",
      "       1196.58470937, 1184.87301985, 1206.17887719, 1193.51503893,\n",
      "       1155.23528584, 1173.69540801, 1230.05262276, 1146.91276357,\n",
      "       1164.0737293 , 1142.68370924])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1222.57762927, 1126.8284362 , 1123.38586604, 1184.52339898,\n",
      "       1266.06107518, 1214.66957735, 1161.19412261, 1269.16771977,\n",
      "       1154.08837153, 1053.80799635, 1161.52993384, 1167.57896682,\n",
      "       1129.95184813, 1174.24243267, 1186.96621446, 1142.64760783,\n",
      "       1090.36430471, 1180.46179222, 1213.31148268, 1178.18182074])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([857.88656147, 759.24070059, 692.35079504, 719.83979052,\n",
      "       745.37251598, 804.07326495, 689.79394523, 795.39479652,\n",
      "       773.4935943 , 733.44333496, 696.48647192, 651.50446061,\n",
      "       760.04488904, 774.24036403, 698.7297109 , 669.98055191,\n",
      "       714.36151018, 834.70411715, 785.46547122, 690.58578351,\n",
      "       723.47617913, 745.1619075 , 627.8142917 , 709.36185236,\n",
      "       750.69673545, 800.72938966, 880.17979644, 873.23872791,\n",
      "       793.98585857, 722.50694506])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([884.27854574, 835.09507914, 790.9940369 , 784.06169243,\n",
      "       794.03170193, 782.43241217, 755.51672945, 747.93387285,\n",
      "       773.69529753, 760.13137483, 743.52554413, 780.65476182,\n",
      "       780.51270032, 863.92123596, 695.31521813, 799.46781486,\n",
      "       876.53041534, 724.65218252, 743.8072481 , 770.72811344])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1142.68370924, 1222.57762927, 1126.8284362 , 1123.38586604,\n",
      "       1184.52339898, 1266.06107518, 1214.66957735, 1161.19412261,\n",
      "       1269.16771977, 1154.08837153, 1053.80799635, 1161.52993384,\n",
      "       1167.57896682, 1129.95184813, 1174.24243267, 1186.96621446,\n",
      "       1142.64760783, 1090.36430471, 1180.46179222, 1213.31148268,\n",
      "       1178.18182074, 1260.71384385, 1155.78393818, 1077.27025203,\n",
      "       1102.0871485 , 1149.52269827, 1180.35512515, 1165.76588431,\n",
      "       1187.36011924, 1119.93837473])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1222.3145851 , 1200.14197303, 1235.4790309 , 1220.45414496,\n",
      "       1213.51266018, 1220.77975695, 1217.94490589, 1226.27908317,\n",
      "       1265.20996166, 1224.93114943, 1229.45285265, 1198.54177021,\n",
      "       1259.40176692, 1185.32401861, 1268.59401946, 1225.15858003,\n",
      "       1123.93523893, 1168.53758729, 1157.02002074, 1219.95678964])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1005.12645319, 1037.76355434, 1054.73861892, 1074.97384986,\n",
      "       1150.99978669, 1151.27744666, 1079.38867994, 1032.6191415 ,\n",
      "       1050.25584071, 1097.33429888, 1205.78161584, 1133.46971939,\n",
      "       1128.59352223, 1127.96863513, 1184.90842814, 1277.93386638,\n",
      "       1159.17684299, 1102.26127385, 1172.30186416, 1186.23552455,\n",
      "       1242.01460042, 1205.17205015, 1136.09652267, 1161.34722065,\n",
      "       1204.85291596, 1204.81004501, 1186.39807387, 1208.3644474 ,\n",
      "       1225.51802637, 1240.98008352])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1210.89558316, 1156.64225   , 1153.90637439, 1211.43040122,\n",
      "       1134.72817155, 1155.09827511, 1138.12562284, 1146.38320567,\n",
      "       1216.96420579, 1191.58212951, 1166.49606169, 1092.18467862,\n",
      "       1167.0925951 , 1203.52277271, 1283.56378373, 1195.20997238,\n",
      "       1174.88613873, 1182.27617431, 1250.21616117, 1196.58470937])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 979.85710706,  970.66424996,  971.55787089,  969.39897322,\n",
      "       1059.79104685, 1111.27315424, 1024.00153111,  971.80503587,\n",
      "       1026.55935047, 1002.37166937, 1044.2419652 , 1047.00466984,\n",
      "        956.61527463, 1073.01406201, 1146.43226916, 1041.31184508,\n",
      "       1005.12645319, 1037.76355434, 1054.73861892, 1074.97384986,\n",
      "       1150.99978669, 1151.27744666, 1079.38867994, 1032.6191415 ,\n",
      "       1050.25584071, 1097.33429888, 1205.78161584, 1133.46971939,\n",
      "       1128.59352223, 1127.96863513])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1184.90842814, 1277.93386638, 1159.17684299, 1102.26127385,\n",
      "       1172.30186416, 1186.23552455, 1242.01460042, 1205.17205015,\n",
      "       1136.09652267, 1161.34722065, 1204.85291596, 1204.81004501,\n",
      "       1186.39807387, 1208.3644474 , 1225.51802637, 1240.98008352,\n",
      "       1210.89558316, 1156.64225   , 1153.90637439, 1211.43040122])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1146.38320567, 1216.96420579, 1191.58212951, 1166.49606169,\n",
      "       1092.18467862, 1167.0925951 , 1203.52277271, 1283.56378373,\n",
      "       1195.20997238, 1174.88613873, 1182.27617431, 1250.21616117,\n",
      "       1196.58470937, 1184.87301985, 1206.17887719, 1193.51503893,\n",
      "       1155.23528584, 1173.69540801, 1230.05262276, 1146.91276357,\n",
      "       1164.0737293 , 1142.68370924, 1222.57762927, 1126.8284362 ,\n",
      "       1123.38586604, 1184.52339898, 1266.06107518, 1214.66957735,\n",
      "       1161.19412261, 1269.16771977])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1154.08837153, 1053.80799635, 1161.52993384, 1167.57896682,\n",
      "       1129.95184813, 1174.24243267, 1186.96621446, 1142.64760783,\n",
      "       1090.36430471, 1180.46179222, 1213.31148268, 1178.18182074,\n",
      "       1260.71384385, 1155.78393818, 1077.27025203, 1102.0871485 ,\n",
      "       1149.52269827, 1180.35512515, 1165.76588431, 1187.36011924])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1029.24446491, 1007.91049914, 1029.7894881 ,  921.02414826,\n",
      "        957.69581689, 1019.86652719,  997.83420854, 1033.61470903,\n",
      "        975.19803339, 1041.13401513, 1123.42793916, 1016.12489876,\n",
      "        960.41402604, 1046.97405111, 1092.88908797,  997.59030213,\n",
      "        960.7578993 ,  964.83221813,  914.84459202,  914.65709275,\n",
      "        917.36114995,  928.99988218,  971.59260492, 1002.54363929,\n",
      "       1077.50206276,  981.41467242, 1031.38843481, 1008.36789473,\n",
      "        996.67797377, 1034.79060041])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 961.82867089, 1001.79836244, 1019.2776188 , 1035.55514077,\n",
      "       1033.78454565, 1139.54181843, 1032.6695729 ,  979.85710706,\n",
      "        970.66424996,  971.55787089,  969.39897322, 1059.79104685,\n",
      "       1111.27315424, 1024.00153111,  971.80503587, 1026.55935047,\n",
      "       1002.37166937, 1044.2419652 , 1047.00466984,  956.61527463])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1077.50206276,  981.41467242, 1031.38843481, 1008.36789473,\n",
      "        996.67797377, 1034.79060041,  961.82867089, 1001.79836244,\n",
      "       1019.2776188 , 1035.55514077, 1033.78454565, 1139.54181843,\n",
      "       1032.6695729 ,  979.85710706,  970.66424996,  971.55787089,\n",
      "        969.39897322, 1059.79104685, 1111.27315424, 1024.00153111,\n",
      "        971.80503587, 1026.55935047, 1002.37166937, 1044.2419652 ,\n",
      "       1047.00466984,  956.61527463, 1073.01406201, 1146.43226916,\n",
      "       1041.31184508, 1005.12645319])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1037.76355434, 1054.73861892, 1074.97384986, 1150.99978669,\n",
      "       1151.27744666, 1079.38867994, 1032.6191415 , 1050.25584071,\n",
      "       1097.33429888, 1205.78161584, 1133.46971939, 1128.59352223,\n",
      "       1127.96863513, 1184.90842814, 1277.93386638, 1159.17684299,\n",
      "       1102.26127385, 1172.30186416, 1186.23552455, 1242.01460042])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1225.51802637, 1240.98008352, 1210.89558316, 1156.64225   ,\n",
      "       1153.90637439, 1211.43040122, 1134.72817155, 1155.09827511,\n",
      "       1138.12562284, 1146.38320567, 1216.96420579, 1191.58212951,\n",
      "       1166.49606169, 1092.18467862, 1167.0925951 , 1203.52277271,\n",
      "       1283.56378373, 1195.20997238, 1174.88613873, 1182.27617431,\n",
      "       1250.21616117, 1196.58470937, 1184.87301985, 1206.17887719,\n",
      "       1193.51503893, 1155.23528584, 1173.69540801, 1230.05262276,\n",
      "       1146.91276357, 1164.0737293 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1142.68370924, 1222.57762927, 1126.8284362 , 1123.38586604,\n",
      "       1184.52339898, 1266.06107518, 1214.66957735, 1161.19412261,\n",
      "       1269.16771977, 1154.08837153, 1053.80799635, 1161.52993384,\n",
      "       1167.57896682, 1129.95184813, 1174.24243267, 1186.96621446,\n",
      "       1142.64760783, 1090.36430471, 1180.46179222, 1213.31148268])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1153.90637439, 1211.43040122, 1134.72817155, 1155.09827511,\n",
      "       1138.12562284, 1146.38320567, 1216.96420579, 1191.58212951,\n",
      "       1166.49606169, 1092.18467862, 1167.0925951 , 1203.52277271,\n",
      "       1283.56378373, 1195.20997238, 1174.88613873, 1182.27617431,\n",
      "       1250.21616117, 1196.58470937, 1184.87301985, 1206.17887719,\n",
      "       1193.51503893, 1155.23528584, 1173.69540801, 1230.05262276,\n",
      "       1146.91276357, 1164.0737293 , 1142.68370924, 1222.57762927,\n",
      "       1126.8284362 , 1123.38586604])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1184.52339898, 1266.06107518, 1214.66957735, 1161.19412261,\n",
      "       1269.16771977, 1154.08837153, 1053.80799635, 1161.52993384,\n",
      "       1167.57896682, 1129.95184813, 1174.24243267, 1186.96621446,\n",
      "       1142.64760783, 1090.36430471, 1180.46179222, 1213.31148268,\n",
      "       1178.18182074, 1260.71384385, 1155.78393818, 1077.27025203])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([889.41788188, 887.39322146, 850.96512158, 820.95104652,\n",
      "       858.00387261, 855.81095587, 906.99625232, 882.58263097,\n",
      "       867.8616934 , 841.9761503 , 846.99786451, 871.15717441,\n",
      "       780.21953936, 757.60855562, 864.58609949, 881.58160417,\n",
      "       859.3996499 , 862.91979102, 883.65609382, 848.05515331,\n",
      "       819.54479791, 861.61104418, 823.77193688, 871.634727  ,\n",
      "       794.34800497, 890.0528615 , 916.1347314 , 901.84355457,\n",
      "       934.63290199, 984.66471466])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 970.16030028,  818.00754688,  784.76478819,  821.16300342,\n",
      "        868.15554952,  909.64502365,  861.98607758,  884.23519565,\n",
      "        854.64268169,  844.90353406,  807.12054261,  815.2239179 ,\n",
      "        802.85865266,  814.48982293,  923.60552682,  869.34745536,\n",
      "       1012.69305322,  979.26171637,  929.55501488,  868.02206953])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([718.02118063, 820.5333744 , 755.39247769, 721.11223575,\n",
      "       683.36485669, 648.91690775, 722.89139063, 674.15797097,\n",
      "       690.34147284, 679.89555162, 718.86986728, 732.00333529,\n",
      "       767.83519502, 707.24881753, 689.61478723, 656.32769369,\n",
      "       668.64987017, 719.83064084, 758.57681546, 687.1679321 ,\n",
      "       743.60502328, 802.44119057, 771.56266707, 828.12820822,\n",
      "       726.32636326, 651.49920215, 608.68186599, 758.13233494,\n",
      "       784.85025629, 742.43778409])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([744.95265199, 681.04995163, 830.47041928, 788.31397118,\n",
      "       749.23453274, 775.92944027, 776.84226997, 761.63590883,\n",
      "       706.47664247, 747.91110009, 842.84008478, 850.5771681 ,\n",
      "       857.88656147, 759.24070059, 692.35079504, 719.83979052,\n",
      "       745.37251598, 804.07326495, 689.79394523, 795.39479652])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([857.07268611, 815.69791161, 875.33317673, 794.92274497,\n",
      "       836.89239776, 867.16387476, 826.78026331, 841.22937628,\n",
      "       774.8583325 , 851.97983988, 839.78847353, 806.18137957,\n",
      "       876.3270062 , 822.22514043, 755.48010844, 729.86087559,\n",
      "       832.20711867, 784.38456467, 902.87694654, 773.59564939,\n",
      "       889.41788188, 887.39322146, 850.96512158, 820.95104652,\n",
      "       858.00387261, 855.81095587, 906.99625232, 882.58263097,\n",
      "       867.8616934 , 841.9761503 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([846.99786451, 871.15717441, 780.21953936, 757.60855562,\n",
      "       864.58609949, 881.58160417, 859.3996499 , 862.91979102,\n",
      "       883.65609382, 848.05515331, 819.54479791, 861.61104418,\n",
      "       823.77193688, 871.634727  , 794.34800497, 890.0528615 ,\n",
      "       916.1347314 , 901.84355457, 934.63290199, 984.66471466])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1029.7894881 ,  921.02414826,  957.69581689, 1019.86652719,\n",
      "        997.83420854, 1033.61470903,  975.19803339, 1041.13401513,\n",
      "       1123.42793916, 1016.12489876,  960.41402604, 1046.97405111,\n",
      "       1092.88908797,  997.59030213,  960.7578993 ,  964.83221813,\n",
      "        914.84459202,  914.65709275,  917.36114995,  928.99988218,\n",
      "        971.59260492, 1002.54363929, 1077.50206276,  981.41467242,\n",
      "       1031.38843481, 1008.36789473,  996.67797377, 1034.79060041,\n",
      "        961.82867089, 1001.79836244])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1019.2776188 , 1035.55514077, 1033.78454565, 1139.54181843,\n",
      "       1032.6695729 ,  979.85710706,  970.66424996,  971.55787089,\n",
      "        969.39897322, 1059.79104685, 1111.27315424, 1024.00153111,\n",
      "        971.80503587, 1026.55935047, 1002.37166937, 1044.2419652 ,\n",
      "       1047.00466984,  956.61527463, 1073.01406201, 1146.43226916])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1073.01406201, 1146.43226916, 1041.31184508, 1005.12645319,\n",
      "       1037.76355434, 1054.73861892, 1074.97384986, 1150.99978669,\n",
      "       1151.27744666, 1079.38867994, 1032.6191415 , 1050.25584071,\n",
      "       1097.33429888, 1205.78161584, 1133.46971939, 1128.59352223,\n",
      "       1127.96863513, 1184.90842814, 1277.93386638, 1159.17684299,\n",
      "       1102.26127385, 1172.30186416, 1186.23552455, 1242.01460042,\n",
      "       1205.17205015, 1136.09652267, 1161.34722065, 1204.85291596,\n",
      "       1204.81004501, 1186.39807387])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1208.3644474 , 1225.51802637, 1240.98008352, 1210.89558316,\n",
      "       1156.64225   , 1153.90637439, 1211.43040122, 1134.72817155,\n",
      "       1155.09827511, 1138.12562284, 1146.38320567, 1216.96420579,\n",
      "       1191.58212951, 1166.49606169, 1092.18467862, 1167.0925951 ,\n",
      "       1203.52277271, 1283.56378373, 1195.20997238, 1174.88613873])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 964.76263466,  985.10560512,  967.39478882,  956.53369828,\n",
      "       1001.37762455,  949.18365111,  952.55314568,  935.12600406,\n",
      "        926.72522669,  933.22378088,  963.24465376,  941.21957118,\n",
      "       1015.54262546,  944.97319267,  940.49716249,  938.69094632,\n",
      "       1029.24446491, 1007.91049914, 1029.7894881 ,  921.02414826,\n",
      "        957.69581689, 1019.86652719,  997.83420854, 1033.61470903,\n",
      "        975.19803339, 1041.13401513, 1123.42793916, 1016.12489876,\n",
      "        960.41402604, 1046.97405111])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1092.88908797,  997.59030213,  960.7578993 ,  964.83221813,\n",
      "        914.84459202,  914.65709275,  917.36114995,  928.99988218,\n",
      "        971.59260492, 1002.54363929, 1077.50206276,  981.41467242,\n",
      "       1031.38843481, 1008.36789473,  996.67797377, 1034.79060041,\n",
      "        961.82867089, 1001.79836244, 1019.2776188 , 1035.55514077])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 956.53369828, 1001.37762455,  949.18365111,  952.55314568,\n",
      "        935.12600406,  926.72522669,  933.22378088,  963.24465376,\n",
      "        941.21957118, 1015.54262546,  944.97319267,  940.49716249,\n",
      "        938.69094632, 1029.24446491, 1007.91049914, 1029.7894881 ,\n",
      "        921.02414826,  957.69581689, 1019.86652719,  997.83420854,\n",
      "       1033.61470903,  975.19803339, 1041.13401513, 1123.42793916,\n",
      "       1016.12489876,  960.41402604, 1046.97405111, 1092.88908797,\n",
      "        997.59030213,  960.7578993 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 964.83221813,  914.84459202,  914.65709275,  917.36114995,\n",
      "        928.99988218,  971.59260492, 1002.54363929, 1077.50206276,\n",
      "        981.41467242, 1031.38843481, 1008.36789473,  996.67797377,\n",
      "       1034.79060041,  961.82867089, 1001.79836244, 1019.2776188 ,\n",
      "       1035.55514077, 1033.78454565, 1139.54181843, 1032.6695729 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 985.10560512,  967.39478882,  956.53369828, 1001.37762455,\n",
      "        949.18365111,  952.55314568,  935.12600406,  926.72522669,\n",
      "        933.22378088,  963.24465376,  941.21957118, 1015.54262546,\n",
      "        944.97319267,  940.49716249,  938.69094632, 1029.24446491,\n",
      "       1007.91049914, 1029.7894881 ,  921.02414826,  957.69581689,\n",
      "       1019.86652719,  997.83420854, 1033.61470903,  975.19803339,\n",
      "       1041.13401513, 1123.42793916, 1016.12489876,  960.41402604,\n",
      "       1046.97405111, 1092.88908797])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 997.59030213,  960.7578993 ,  964.83221813,  914.84459202,\n",
      "        914.65709275,  917.36114995,  928.99988218,  971.59260492,\n",
      "       1002.54363929, 1077.50206276,  981.41467242, 1031.38843481,\n",
      "       1008.36789473,  996.67797377, 1034.79060041,  961.82867089,\n",
      "       1001.79836244, 1019.2776188 , 1035.55514077, 1033.78454565])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1161.19412261, 1269.16771977, 1154.08837153, 1053.80799635,\n",
      "       1161.52993384, 1167.57896682, 1129.95184813, 1174.24243267,\n",
      "       1186.96621446, 1142.64760783, 1090.36430471, 1180.46179222,\n",
      "       1213.31148268, 1178.18182074, 1260.71384385, 1155.78393818,\n",
      "       1077.27025203, 1102.0871485 , 1149.52269827, 1180.35512515,\n",
      "       1165.76588431, 1187.36011924, 1119.93837473, 1222.3145851 ,\n",
      "       1200.14197303, 1235.4790309 , 1220.45414496, 1213.51266018,\n",
      "       1220.77975695, 1217.94490589])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1226.27908317, 1265.20996166, 1224.93114943, 1229.45285265,\n",
      "       1198.54177021, 1259.40176692, 1185.32401861, 1268.59401946,\n",
      "       1225.15858003, 1123.93523893, 1168.53758729, 1157.02002074,\n",
      "       1219.95678964, 1177.3895382 , 1161.08282259, 1090.95462499,\n",
      "       1132.33430313, 1061.19468042, 1065.20021327, 1195.39265367])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 971.80503587, 1026.55935047, 1002.37166937, 1044.2419652 ,\n",
      "       1047.00466984,  956.61527463, 1073.01406201, 1146.43226916,\n",
      "       1041.31184508, 1005.12645319, 1037.76355434, 1054.73861892,\n",
      "       1074.97384986, 1150.99978669, 1151.27744666, 1079.38867994,\n",
      "       1032.6191415 , 1050.25584071, 1097.33429888, 1205.78161584,\n",
      "       1133.46971939, 1128.59352223, 1127.96863513, 1184.90842814,\n",
      "       1277.93386638, 1159.17684299, 1102.26127385, 1172.30186416,\n",
      "       1186.23552455, 1242.01460042])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1205.17205015, 1136.09652267, 1161.34722065, 1204.85291596,\n",
      "       1204.81004501, 1186.39807387, 1208.3644474 , 1225.51802637,\n",
      "       1240.98008352, 1210.89558316, 1156.64225   , 1153.90637439,\n",
      "       1211.43040122, 1134.72817155, 1155.09827511, 1138.12562284,\n",
      "       1146.38320567, 1216.96420579, 1191.58212951, 1166.49606169])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1155.23528584, 1173.69540801, 1230.05262276, 1146.91276357,\n",
      "       1164.0737293 , 1142.68370924, 1222.57762927, 1126.8284362 ,\n",
      "       1123.38586604, 1184.52339898, 1266.06107518, 1214.66957735,\n",
      "       1161.19412261, 1269.16771977, 1154.08837153, 1053.80799635,\n",
      "       1161.52993384, 1167.57896682, 1129.95184813, 1174.24243267,\n",
      "       1186.96621446, 1142.64760783, 1090.36430471, 1180.46179222,\n",
      "       1213.31148268, 1178.18182074, 1260.71384385, 1155.78393818,\n",
      "       1077.27025203, 1102.0871485 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1149.52269827, 1180.35512515, 1165.76588431, 1187.36011924,\n",
      "       1119.93837473, 1222.3145851 , 1200.14197303, 1235.4790309 ,\n",
      "       1220.45414496, 1213.51266018, 1220.77975695, 1217.94490589,\n",
      "       1226.27908317, 1265.20996166, 1224.93114943, 1229.45285265,\n",
      "       1198.54177021, 1259.40176692, 1185.32401861, 1268.59401946])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1213.31148268, 1178.18182074, 1260.71384385, 1155.78393818,\n",
      "       1077.27025203, 1102.0871485 , 1149.52269827, 1180.35512515,\n",
      "       1165.76588431, 1187.36011924, 1119.93837473, 1222.3145851 ,\n",
      "       1200.14197303, 1235.4790309 , 1220.45414496, 1213.51266018,\n",
      "       1220.77975695, 1217.94490589, 1226.27908317, 1265.20996166,\n",
      "       1224.93114943, 1229.45285265, 1198.54177021, 1259.40176692,\n",
      "       1185.32401861, 1268.59401946, 1225.15858003, 1123.93523893,\n",
      "       1168.53758729, 1157.02002074])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1219.95678964, 1177.3895382 , 1161.08282259, 1090.95462499,\n",
      "       1132.33430313, 1061.19468042, 1065.20021327, 1195.39265367,\n",
      "       1248.97313025, 1240.25792575, 1166.21782504, 1182.54630196,\n",
      "       1200.76216394, 1146.92494919, 1256.28649583, 1279.43022998,\n",
      "       1219.86918036, 1209.6165152 , 1221.40755224, 1114.1366431 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1092.88908797,  997.59030213,  960.7578993 ,  964.83221813,\n",
      "        914.84459202,  914.65709275,  917.36114995,  928.99988218,\n",
      "        971.59260492, 1002.54363929, 1077.50206276,  981.41467242,\n",
      "       1031.38843481, 1008.36789473,  996.67797377, 1034.79060041,\n",
      "        961.82867089, 1001.79836244, 1019.2776188 , 1035.55514077,\n",
      "       1033.78454565, 1139.54181843, 1032.6695729 ,  979.85710706,\n",
      "        970.66424996,  971.55787089,  969.39897322, 1059.79104685,\n",
      "       1111.27315424, 1024.00153111])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 971.80503587, 1026.55935047, 1002.37166937, 1044.2419652 ,\n",
      "       1047.00466984,  956.61527463, 1073.01406201, 1146.43226916,\n",
      "       1041.31184508, 1005.12645319, 1037.76355434, 1054.73861892,\n",
      "       1074.97384986, 1150.99978669, 1151.27744666, 1079.38867994,\n",
      "       1032.6191415 , 1050.25584071, 1097.33429888, 1205.78161584])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 871.634727  ,  794.34800497,  890.0528615 ,  916.1347314 ,\n",
      "        901.84355457,  934.63290199,  984.66471466,  970.16030028,\n",
      "        818.00754688,  784.76478819,  821.16300342,  868.15554952,\n",
      "        909.64502365,  861.98607758,  884.23519565,  854.64268169,\n",
      "        844.90353406,  807.12054261,  815.2239179 ,  802.85865266,\n",
      "        814.48982293,  923.60552682,  869.34745536, 1012.69305322,\n",
      "        979.26171637,  929.55501488,  868.02206953,  922.48195382,\n",
      "        889.64371938,  902.27492431])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1036.70076886,  959.19804806,  973.71324439,  902.93070215,\n",
      "        900.14532383, 1000.23202533,  921.42712426,  998.73708587,\n",
      "        990.95733222,  911.4989099 ,  943.0991758 ,  982.24411303,\n",
      "        975.88649061,  862.21052577,  857.87276556,  893.84910538,\n",
      "        914.7797677 ,  956.00910157,  950.98553127,  869.84354752])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 982.53631582,  949.02568453,  858.44878802,  927.72946126,\n",
      "        957.00092337,  961.84957712,  887.94565068,  866.15942091,\n",
      "        972.96430377,  964.76263466,  985.10560512,  967.39478882,\n",
      "        956.53369828, 1001.37762455,  949.18365111,  952.55314568,\n",
      "        935.12600406,  926.72522669,  933.22378088,  963.24465376,\n",
      "        941.21957118, 1015.54262546,  944.97319267,  940.49716249,\n",
      "        938.69094632, 1029.24446491, 1007.91049914, 1029.7894881 ,\n",
      "        921.02414826,  957.69581689])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1019.86652719,  997.83420854, 1033.61470903,  975.19803339,\n",
      "       1041.13401513, 1123.42793916, 1016.12489876,  960.41402604,\n",
      "       1046.97405111, 1092.88908797,  997.59030213,  960.7578993 ,\n",
      "        964.83221813,  914.84459202,  914.65709275,  917.36114995,\n",
      "        928.99988218,  971.59260492, 1002.54363929, 1077.50206276])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1205.78161584, 1133.46971939, 1128.59352223, 1127.96863513,\n",
      "       1184.90842814, 1277.93386638, 1159.17684299, 1102.26127385,\n",
      "       1172.30186416, 1186.23552455, 1242.01460042, 1205.17205015,\n",
      "       1136.09652267, 1161.34722065, 1204.85291596, 1204.81004501,\n",
      "       1186.39807387, 1208.3644474 , 1225.51802637, 1240.98008352,\n",
      "       1210.89558316, 1156.64225   , 1153.90637439, 1211.43040122,\n",
      "       1134.72817155, 1155.09827511, 1138.12562284, 1146.38320567,\n",
      "       1216.96420579, 1191.58212951])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1166.49606169, 1092.18467862, 1167.0925951 , 1203.52277271,\n",
      "       1283.56378373, 1195.20997238, 1174.88613873, 1182.27617431,\n",
      "       1250.21616117, 1196.58470937, 1184.87301985, 1206.17887719,\n",
      "       1193.51503893, 1155.23528584, 1173.69540801, 1230.05262276,\n",
      "       1146.91276357, 1164.0737293 , 1142.68370924, 1222.57762927])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 935.12600406,  926.72522669,  933.22378088,  963.24465376,\n",
      "        941.21957118, 1015.54262546,  944.97319267,  940.49716249,\n",
      "        938.69094632, 1029.24446491, 1007.91049914, 1029.7894881 ,\n",
      "        921.02414826,  957.69581689, 1019.86652719,  997.83420854,\n",
      "       1033.61470903,  975.19803339, 1041.13401513, 1123.42793916,\n",
      "       1016.12489876,  960.41402604, 1046.97405111, 1092.88908797,\n",
      "        997.59030213,  960.7578993 ,  964.83221813,  914.84459202,\n",
      "        914.65709275,  917.36114995])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 928.99988218,  971.59260492, 1002.54363929, 1077.50206276,\n",
      "        981.41467242, 1031.38843481, 1008.36789473,  996.67797377,\n",
      "       1034.79060041,  961.82867089, 1001.79836244, 1019.2776188 ,\n",
      "       1035.55514077, 1033.78454565, 1139.54181843, 1032.6695729 ,\n",
      "        979.85710706,  970.66424996,  971.55787089,  969.39897322])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1193.51503893, 1155.23528584, 1173.69540801, 1230.05262276,\n",
      "       1146.91276357, 1164.0737293 , 1142.68370924, 1222.57762927,\n",
      "       1126.8284362 , 1123.38586604, 1184.52339898, 1266.06107518,\n",
      "       1214.66957735, 1161.19412261, 1269.16771977, 1154.08837153,\n",
      "       1053.80799635, 1161.52993384, 1167.57896682, 1129.95184813,\n",
      "       1174.24243267, 1186.96621446, 1142.64760783, 1090.36430471,\n",
      "       1180.46179222, 1213.31148268, 1178.18182074, 1260.71384385,\n",
      "       1155.78393818, 1077.27025203])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1102.0871485 , 1149.52269827, 1180.35512515, 1165.76588431,\n",
      "       1187.36011924, 1119.93837473, 1222.3145851 , 1200.14197303,\n",
      "       1235.4790309 , 1220.45414496, 1213.51266018, 1220.77975695,\n",
      "       1217.94490589, 1226.27908317, 1265.20996166, 1224.93114943,\n",
      "       1229.45285265, 1198.54177021, 1259.40176692, 1185.32401861])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1174.24243267, 1186.96621446, 1142.64760783, 1090.36430471,\n",
      "       1180.46179222, 1213.31148268, 1178.18182074, 1260.71384385,\n",
      "       1155.78393818, 1077.27025203, 1102.0871485 , 1149.52269827,\n",
      "       1180.35512515, 1165.76588431, 1187.36011924, 1119.93837473,\n",
      "       1222.3145851 , 1200.14197303, 1235.4790309 , 1220.45414496,\n",
      "       1213.51266018, 1220.77975695, 1217.94490589, 1226.27908317,\n",
      "       1265.20996166, 1224.93114943, 1229.45285265, 1198.54177021,\n",
      "       1259.40176692, 1185.32401861])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1268.59401946, 1225.15858003, 1123.93523893, 1168.53758729,\n",
      "       1157.02002074, 1219.95678964, 1177.3895382 , 1161.08282259,\n",
      "       1090.95462499, 1132.33430313, 1061.19468042, 1065.20021327,\n",
      "       1195.39265367, 1248.97313025, 1240.25792575, 1166.21782504,\n",
      "       1182.54630196, 1200.76216394, 1146.92494919, 1256.28649583])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1184.87301985, 1206.17887719, 1193.51503893, 1155.23528584,\n",
      "       1173.69540801, 1230.05262276, 1146.91276357, 1164.0737293 ,\n",
      "       1142.68370924, 1222.57762927, 1126.8284362 , 1123.38586604,\n",
      "       1184.52339898, 1266.06107518, 1214.66957735, 1161.19412261,\n",
      "       1269.16771977, 1154.08837153, 1053.80799635, 1161.52993384,\n",
      "       1167.57896682, 1129.95184813, 1174.24243267, 1186.96621446,\n",
      "       1142.64760783, 1090.36430471, 1180.46179222, 1213.31148268,\n",
      "       1178.18182074, 1260.71384385])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1155.78393818, 1077.27025203, 1102.0871485 , 1149.52269827,\n",
      "       1180.35512515, 1165.76588431, 1187.36011924, 1119.93837473,\n",
      "       1222.3145851 , 1200.14197303, 1235.4790309 , 1220.45414496,\n",
      "       1213.51266018, 1220.77975695, 1217.94490589, 1226.27908317,\n",
      "       1265.20996166, 1224.93114943, 1229.45285265, 1198.54177021])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 914.84459202,  914.65709275,  917.36114995,  928.99988218,\n",
      "        971.59260492, 1002.54363929, 1077.50206276,  981.41467242,\n",
      "       1031.38843481, 1008.36789473,  996.67797377, 1034.79060041,\n",
      "        961.82867089, 1001.79836244, 1019.2776188 , 1035.55514077,\n",
      "       1033.78454565, 1139.54181843, 1032.6695729 ,  979.85710706,\n",
      "        970.66424996,  971.55787089,  969.39897322, 1059.79104685,\n",
      "       1111.27315424, 1024.00153111,  971.80503587, 1026.55935047,\n",
      "       1002.37166937, 1044.2419652 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1047.00466984,  956.61527463, 1073.01406201, 1146.43226916,\n",
      "       1041.31184508, 1005.12645319, 1037.76355434, 1054.73861892,\n",
      "       1074.97384986, 1150.99978669, 1151.27744666, 1079.38867994,\n",
      "       1032.6191415 , 1050.25584071, 1097.33429888, 1205.78161584,\n",
      "       1133.46971939, 1128.59352223, 1127.96863513, 1184.90842814])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1283.56378373, 1195.20997238, 1174.88613873, 1182.27617431,\n",
      "       1250.21616117, 1196.58470937, 1184.87301985, 1206.17887719,\n",
      "       1193.51503893, 1155.23528584, 1173.69540801, 1230.05262276,\n",
      "       1146.91276357, 1164.0737293 , 1142.68370924, 1222.57762927,\n",
      "       1126.8284362 , 1123.38586604, 1184.52339898, 1266.06107518,\n",
      "       1214.66957735, 1161.19412261, 1269.16771977, 1154.08837153,\n",
      "       1053.80799635, 1161.52993384, 1167.57896682, 1129.95184813,\n",
      "       1174.24243267, 1186.96621446])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1142.64760783, 1090.36430471, 1180.46179222, 1213.31148268,\n",
      "       1178.18182074, 1260.71384385, 1155.78393818, 1077.27025203,\n",
      "       1102.0871485 , 1149.52269827, 1180.35512515, 1165.76588431,\n",
      "       1187.36011924, 1119.93837473, 1222.3145851 , 1200.14197303,\n",
      "       1235.4790309 , 1220.45414496, 1213.51266018, 1220.77975695])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1166.49606169, 1092.18467862, 1167.0925951 , 1203.52277271,\n",
      "       1283.56378373, 1195.20997238, 1174.88613873, 1182.27617431,\n",
      "       1250.21616117, 1196.58470937, 1184.87301985, 1206.17887719,\n",
      "       1193.51503893, 1155.23528584, 1173.69540801, 1230.05262276,\n",
      "       1146.91276357, 1164.0737293 , 1142.68370924, 1222.57762927,\n",
      "       1126.8284362 , 1123.38586604, 1184.52339898, 1266.06107518,\n",
      "       1214.66957735, 1161.19412261, 1269.16771977, 1154.08837153,\n",
      "       1053.80799635, 1161.52993384])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1167.57896682, 1129.95184813, 1174.24243267, 1186.96621446,\n",
      "       1142.64760783, 1090.36430471, 1180.46179222, 1213.31148268,\n",
      "       1178.18182074, 1260.71384385, 1155.78393818, 1077.27025203,\n",
      "       1102.0871485 , 1149.52269827, 1180.35512515, 1165.76588431,\n",
      "       1187.36011924, 1119.93837473, 1222.3145851 , 1200.14197303])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1186.23552455, 1242.01460042, 1205.17205015, 1136.09652267,\n",
      "       1161.34722065, 1204.85291596, 1204.81004501, 1186.39807387,\n",
      "       1208.3644474 , 1225.51802637, 1240.98008352, 1210.89558316,\n",
      "       1156.64225   , 1153.90637439, 1211.43040122, 1134.72817155,\n",
      "       1155.09827511, 1138.12562284, 1146.38320567, 1216.96420579,\n",
      "       1191.58212951, 1166.49606169, 1092.18467862, 1167.0925951 ,\n",
      "       1203.52277271, 1283.56378373, 1195.20997238, 1174.88613873,\n",
      "       1182.27617431, 1250.21616117])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1196.58470937, 1184.87301985, 1206.17887719, 1193.51503893,\n",
      "       1155.23528584, 1173.69540801, 1230.05262276, 1146.91276357,\n",
      "       1164.0737293 , 1142.68370924, 1222.57762927, 1126.8284362 ,\n",
      "       1123.38586604, 1184.52339898, 1266.06107518, 1214.66957735,\n",
      "       1161.19412261, 1269.16771977, 1154.08837153, 1053.80799635])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1041.13401513, 1123.42793916, 1016.12489876,  960.41402604,\n",
      "       1046.97405111, 1092.88908797,  997.59030213,  960.7578993 ,\n",
      "        964.83221813,  914.84459202,  914.65709275,  917.36114995,\n",
      "        928.99988218,  971.59260492, 1002.54363929, 1077.50206276,\n",
      "        981.41467242, 1031.38843481, 1008.36789473,  996.67797377,\n",
      "       1034.79060041,  961.82867089, 1001.79836244, 1019.2776188 ,\n",
      "       1035.55514077, 1033.78454565, 1139.54181843, 1032.6695729 ,\n",
      "        979.85710706,  970.66424996])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 971.55787089,  969.39897322, 1059.79104685, 1111.27315424,\n",
      "       1024.00153111,  971.80503587, 1026.55935047, 1002.37166937,\n",
      "       1044.2419652 , 1047.00466984,  956.61527463, 1073.01406201,\n",
      "       1146.43226916, 1041.31184508, 1005.12645319, 1037.76355434,\n",
      "       1054.73861892, 1074.97384986, 1150.99978669, 1151.27744666])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 961.82867089, 1001.79836244, 1019.2776188 , 1035.55514077,\n",
      "       1033.78454565, 1139.54181843, 1032.6695729 ,  979.85710706,\n",
      "        970.66424996,  971.55787089,  969.39897322, 1059.79104685,\n",
      "       1111.27315424, 1024.00153111,  971.80503587, 1026.55935047,\n",
      "       1002.37166937, 1044.2419652 , 1047.00466984,  956.61527463,\n",
      "       1073.01406201, 1146.43226916, 1041.31184508, 1005.12645319,\n",
      "       1037.76355434, 1054.73861892, 1074.97384986, 1150.99978669,\n",
      "       1151.27744666, 1079.38867994])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1032.6191415 , 1050.25584071, 1097.33429888, 1205.78161584,\n",
      "       1133.46971939, 1128.59352223, 1127.96863513, 1184.90842814,\n",
      "       1277.93386638, 1159.17684299, 1102.26127385, 1172.30186416,\n",
      "       1186.23552455, 1242.01460042, 1205.17205015, 1136.09652267,\n",
      "       1161.34722065, 1204.85291596, 1204.81004501, 1186.39807387])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1077.27025203, 1102.0871485 , 1149.52269827, 1180.35512515,\n",
      "       1165.76588431, 1187.36011924, 1119.93837473, 1222.3145851 ,\n",
      "       1200.14197303, 1235.4790309 , 1220.45414496, 1213.51266018,\n",
      "       1220.77975695, 1217.94490589, 1226.27908317, 1265.20996166,\n",
      "       1224.93114943, 1229.45285265, 1198.54177021, 1259.40176692,\n",
      "       1185.32401861, 1268.59401946, 1225.15858003, 1123.93523893,\n",
      "       1168.53758729, 1157.02002074, 1219.95678964, 1177.3895382 ,\n",
      "       1161.08282259, 1090.95462499])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1132.33430313, 1061.19468042, 1065.20021327, 1195.39265367,\n",
      "       1248.97313025, 1240.25792575, 1166.21782504, 1182.54630196,\n",
      "       1200.76216394, 1146.92494919, 1256.28649583, 1279.43022998,\n",
      "       1219.86918036, 1209.6165152 , 1221.40755224, 1114.1366431 ,\n",
      "       1156.9080603 , 1167.58371861, 1147.26345236, 1176.87866827])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1167.57896682, 1129.95184813, 1174.24243267, 1186.96621446,\n",
      "       1142.64760783, 1090.36430471, 1180.46179222, 1213.31148268,\n",
      "       1178.18182074, 1260.71384385, 1155.78393818, 1077.27025203,\n",
      "       1102.0871485 , 1149.52269827, 1180.35512515, 1165.76588431,\n",
      "       1187.36011924, 1119.93837473, 1222.3145851 , 1200.14197303,\n",
      "       1235.4790309 , 1220.45414496, 1213.51266018, 1220.77975695,\n",
      "       1217.94490589, 1226.27908317, 1265.20996166, 1224.93114943,\n",
      "       1229.45285265, 1198.54177021])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1259.40176692, 1185.32401861, 1268.59401946, 1225.15858003,\n",
      "       1123.93523893, 1168.53758729, 1157.02002074, 1219.95678964,\n",
      "       1177.3895382 , 1161.08282259, 1090.95462499, 1132.33430313,\n",
      "       1061.19468042, 1065.20021327, 1195.39265367, 1248.97313025,\n",
      "       1240.25792575, 1166.21782504, 1182.54630196, 1200.76216394])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1216.96420579, 1191.58212951, 1166.49606169, 1092.18467862,\n",
      "       1167.0925951 , 1203.52277271, 1283.56378373, 1195.20997238,\n",
      "       1174.88613873, 1182.27617431, 1250.21616117, 1196.58470937,\n",
      "       1184.87301985, 1206.17887719, 1193.51503893, 1155.23528584,\n",
      "       1173.69540801, 1230.05262276, 1146.91276357, 1164.0737293 ,\n",
      "       1142.68370924, 1222.57762927, 1126.8284362 , 1123.38586604,\n",
      "       1184.52339898, 1266.06107518, 1214.66957735, 1161.19412261,\n",
      "       1269.16771977, 1154.08837153])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1053.80799635, 1161.52993384, 1167.57896682, 1129.95184813,\n",
      "       1174.24243267, 1186.96621446, 1142.64760783, 1090.36430471,\n",
      "       1180.46179222, 1213.31148268, 1178.18182074, 1260.71384385,\n",
      "       1155.78393818, 1077.27025203, 1102.0871485 , 1149.52269827,\n",
      "       1180.35512515, 1165.76588431, 1187.36011924, 1119.93837473])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1156.64225   , 1153.90637439, 1211.43040122, 1134.72817155,\n",
      "       1155.09827511, 1138.12562284, 1146.38320567, 1216.96420579,\n",
      "       1191.58212951, 1166.49606169, 1092.18467862, 1167.0925951 ,\n",
      "       1203.52277271, 1283.56378373, 1195.20997238, 1174.88613873,\n",
      "       1182.27617431, 1250.21616117, 1196.58470937, 1184.87301985,\n",
      "       1206.17887719, 1193.51503893, 1155.23528584, 1173.69540801,\n",
      "       1230.05262276, 1146.91276357, 1164.0737293 , 1142.68370924,\n",
      "       1222.57762927, 1126.8284362 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1123.38586604, 1184.52339898, 1266.06107518, 1214.66957735,\n",
      "       1161.19412261, 1269.16771977, 1154.08837153, 1053.80799635,\n",
      "       1161.52993384, 1167.57896682, 1129.95184813, 1174.24243267,\n",
      "       1186.96621446, 1142.64760783, 1090.36430471, 1180.46179222,\n",
      "       1213.31148268, 1178.18182074, 1260.71384385, 1155.78393818])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1129.95184813, 1174.24243267, 1186.96621446, 1142.64760783,\n",
      "       1090.36430471, 1180.46179222, 1213.31148268, 1178.18182074,\n",
      "       1260.71384385, 1155.78393818, 1077.27025203, 1102.0871485 ,\n",
      "       1149.52269827, 1180.35512515, 1165.76588431, 1187.36011924,\n",
      "       1119.93837473, 1222.3145851 , 1200.14197303, 1235.4790309 ,\n",
      "       1220.45414496, 1213.51266018, 1220.77975695, 1217.94490589,\n",
      "       1226.27908317, 1265.20996166, 1224.93114943, 1229.45285265,\n",
      "       1198.54177021, 1259.40176692])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1185.32401861, 1268.59401946, 1225.15858003, 1123.93523893,\n",
      "       1168.53758729, 1157.02002074, 1219.95678964, 1177.3895382 ,\n",
      "       1161.08282259, 1090.95462499, 1132.33430313, 1061.19468042,\n",
      "       1065.20021327, 1195.39265367, 1248.97313025, 1240.25792575,\n",
      "       1166.21782504, 1182.54630196, 1200.76216394, 1146.92494919])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1016.12489876,  960.41402604, 1046.97405111, 1092.88908797,\n",
      "        997.59030213,  960.7578993 ,  964.83221813,  914.84459202,\n",
      "        914.65709275,  917.36114995,  928.99988218,  971.59260492,\n",
      "       1002.54363929, 1077.50206276,  981.41467242, 1031.38843481,\n",
      "       1008.36789473,  996.67797377, 1034.79060041,  961.82867089,\n",
      "       1001.79836244, 1019.2776188 , 1035.55514077, 1033.78454565,\n",
      "       1139.54181843, 1032.6695729 ,  979.85710706,  970.66424996,\n",
      "        971.55787089,  969.39897322])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1059.79104685, 1111.27315424, 1024.00153111,  971.80503587,\n",
      "       1026.55935047, 1002.37166937, 1044.2419652 , 1047.00466984,\n",
      "        956.61527463, 1073.01406201, 1146.43226916, 1041.31184508,\n",
      "       1005.12645319, 1037.76355434, 1054.73861892, 1074.97384986,\n",
      "       1150.99978669, 1151.27744666, 1079.38867994, 1032.6191415 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1154.08837153, 1053.80799635, 1161.52993384, 1167.57896682,\n",
      "       1129.95184813, 1174.24243267, 1186.96621446, 1142.64760783,\n",
      "       1090.36430471, 1180.46179222, 1213.31148268, 1178.18182074,\n",
      "       1260.71384385, 1155.78393818, 1077.27025203, 1102.0871485 ,\n",
      "       1149.52269827, 1180.35512515, 1165.76588431, 1187.36011924,\n",
      "       1119.93837473, 1222.3145851 , 1200.14197303, 1235.4790309 ,\n",
      "       1220.45414496, 1213.51266018, 1220.77975695, 1217.94490589,\n",
      "       1226.27908317, 1265.20996166])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1224.93114943, 1229.45285265, 1198.54177021, 1259.40176692,\n",
      "       1185.32401861, 1268.59401946, 1225.15858003, 1123.93523893,\n",
      "       1168.53758729, 1157.02002074, 1219.95678964, 1177.3895382 ,\n",
      "       1161.08282259, 1090.95462499, 1132.33430313, 1061.19468042,\n",
      "       1065.20021327, 1195.39265367, 1248.97313025, 1240.25792575])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1173.69540801, 1230.05262276, 1146.91276357, 1164.0737293 ,\n",
      "       1142.68370924, 1222.57762927, 1126.8284362 , 1123.38586604,\n",
      "       1184.52339898, 1266.06107518, 1214.66957735, 1161.19412261,\n",
      "       1269.16771977, 1154.08837153, 1053.80799635, 1161.52993384,\n",
      "       1167.57896682, 1129.95184813, 1174.24243267, 1186.96621446,\n",
      "       1142.64760783, 1090.36430471, 1180.46179222, 1213.31148268,\n",
      "       1178.18182074, 1260.71384385, 1155.78393818, 1077.27025203,\n",
      "       1102.0871485 , 1149.52269827])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1180.35512515, 1165.76588431, 1187.36011924, 1119.93837473,\n",
      "       1222.3145851 , 1200.14197303, 1235.4790309 , 1220.45414496,\n",
      "       1213.51266018, 1220.77975695, 1217.94490589, 1226.27908317,\n",
      "       1265.20996166, 1224.93114943, 1229.45285265, 1198.54177021,\n",
      "       1259.40176692, 1185.32401861, 1268.59401946, 1225.15858003])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([563.99484285, 636.27975237, 653.7286504 , 580.21410777,\n",
      "       603.71020602, 655.75754555, 552.61491167, 614.86465122,\n",
      "       596.29610199, 638.34407776, 596.98361388, 521.09639918,\n",
      "       505.46555349, 589.70267609, 636.73604882, 589.67698049,\n",
      "       644.77612963, 561.24317062, 596.79057361, 568.82052745,\n",
      "       576.8558281 , 622.05026979, 594.06184224, 602.02666574,\n",
      "       680.36988082, 633.39966005, 675.71884524, 605.40908925,\n",
      "       650.71947409, 727.41228097])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([557.18612905, 562.01216424, 655.97105619, 650.10028912,\n",
      "       668.16176824, 631.85717443, 648.09903429, 649.27642821,\n",
      "       712.67236658, 696.27975741, 686.1849128 , 649.98371354,\n",
      "       631.18617055, 631.07502274, 673.75730796, 651.98129603,\n",
      "       673.73626744, 777.07324188, 758.2234969 , 680.18442364])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1142.64760783, 1090.36430471, 1180.46179222, 1213.31148268,\n",
      "       1178.18182074, 1260.71384385, 1155.78393818, 1077.27025203,\n",
      "       1102.0871485 , 1149.52269827, 1180.35512515, 1165.76588431,\n",
      "       1187.36011924, 1119.93837473, 1222.3145851 , 1200.14197303,\n",
      "       1235.4790309 , 1220.45414496, 1213.51266018, 1220.77975695,\n",
      "       1217.94490589, 1226.27908317, 1265.20996166, 1224.93114943,\n",
      "       1229.45285265, 1198.54177021, 1259.40176692, 1185.32401861,\n",
      "       1268.59401946, 1225.15858003])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1123.93523893, 1168.53758729, 1157.02002074, 1219.95678964,\n",
      "       1177.3895382 , 1161.08282259, 1090.95462499, 1132.33430313,\n",
      "       1061.19468042, 1065.20021327, 1195.39265367, 1248.97313025,\n",
      "       1240.25792575, 1166.21782504, 1182.54630196, 1200.76216394,\n",
      "       1146.92494919, 1256.28649583, 1279.43022998, 1219.86918036])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1182.27617431, 1250.21616117, 1196.58470937, 1184.87301985,\n",
      "       1206.17887719, 1193.51503893, 1155.23528584, 1173.69540801,\n",
      "       1230.05262276, 1146.91276357, 1164.0737293 , 1142.68370924,\n",
      "       1222.57762927, 1126.8284362 , 1123.38586604, 1184.52339898,\n",
      "       1266.06107518, 1214.66957735, 1161.19412261, 1269.16771977,\n",
      "       1154.08837153, 1053.80799635, 1161.52993384, 1167.57896682,\n",
      "       1129.95184813, 1174.24243267, 1186.96621446, 1142.64760783,\n",
      "       1090.36430471, 1180.46179222])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1213.31148268, 1178.18182074, 1260.71384385, 1155.78393818,\n",
      "       1077.27025203, 1102.0871485 , 1149.52269827, 1180.35512515,\n",
      "       1165.76588431, 1187.36011924, 1119.93837473, 1222.3145851 ,\n",
      "       1200.14197303, 1235.4790309 , 1220.45414496, 1213.51266018,\n",
      "       1220.77975695, 1217.94490589, 1226.27908317, 1265.20996166])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 911.4989099 ,  943.0991758 ,  982.24411303,  975.88649061,\n",
      "        862.21052577,  857.87276556,  893.84910538,  914.7797677 ,\n",
      "        956.00910157,  950.98553127,  869.84354752,  923.32683661,\n",
      "        967.6534664 ,  975.40865419, 1003.22990125, 1003.55157614,\n",
      "        982.53631582,  949.02568453,  858.44878802,  927.72946126,\n",
      "        957.00092337,  961.84957712,  887.94565068,  866.15942091,\n",
      "        972.96430377,  964.76263466,  985.10560512,  967.39478882,\n",
      "        956.53369828, 1001.37762455])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 949.18365111,  952.55314568,  935.12600406,  926.72522669,\n",
      "        933.22378088,  963.24465376,  941.21957118, 1015.54262546,\n",
      "        944.97319267,  940.49716249,  938.69094632, 1029.24446491,\n",
      "       1007.91049914, 1029.7894881 ,  921.02414826,  957.69581689,\n",
      "       1019.86652719,  997.83420854, 1033.61470903,  975.19803339])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1268.59401946, 1225.15858003, 1123.93523893, 1168.53758729,\n",
      "       1157.02002074, 1219.95678964, 1177.3895382 , 1161.08282259,\n",
      "       1090.95462499, 1132.33430313, 1061.19468042, 1065.20021327,\n",
      "       1195.39265367, 1248.97313025, 1240.25792575, 1166.21782504,\n",
      "       1182.54630196, 1200.76216394, 1146.92494919, 1256.28649583,\n",
      "       1279.43022998, 1219.86918036, 1209.6165152 , 1221.40755224,\n",
      "       1114.1366431 , 1156.9080603 , 1167.58371861, 1147.26345236,\n",
      "       1176.87866827, 1295.60874377])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1285.39830312, 1208.39997965, 1238.54035826, 1300.79793746,\n",
      "       1298.33604137, 1249.36545257, 1196.01140311, 1245.95065635,\n",
      "       1257.53178452, 1280.48604928, 1279.16542224, 1296.79142958,\n",
      "       1227.31536253, 1287.8747474 , 1267.99229358, 1343.20395591,\n",
      "       1244.3351939 , 1312.33941447, 1281.18593196, 1212.85492579])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1214.66957735, 1161.19412261, 1269.16771977, 1154.08837153,\n",
      "       1053.80799635, 1161.52993384, 1167.57896682, 1129.95184813,\n",
      "       1174.24243267, 1186.96621446, 1142.64760783, 1090.36430471,\n",
      "       1180.46179222, 1213.31148268, 1178.18182074, 1260.71384385,\n",
      "       1155.78393818, 1077.27025203, 1102.0871485 , 1149.52269827,\n",
      "       1180.35512515, 1165.76588431, 1187.36011924, 1119.93837473,\n",
      "       1222.3145851 , 1200.14197303, 1235.4790309 , 1220.45414496,\n",
      "       1213.51266018, 1220.77975695])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1217.94490589, 1226.27908317, 1265.20996166, 1224.93114943,\n",
      "       1229.45285265, 1198.54177021, 1259.40176692, 1185.32401861,\n",
      "       1268.59401946, 1225.15858003, 1123.93523893, 1168.53758729,\n",
      "       1157.02002074, 1219.95678964, 1177.3895382 , 1161.08282259,\n",
      "       1090.95462499, 1132.33430313, 1061.19468042, 1065.20021327])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 967.39478882,  956.53369828, 1001.37762455,  949.18365111,\n",
      "        952.55314568,  935.12600406,  926.72522669,  933.22378088,\n",
      "        963.24465376,  941.21957118, 1015.54262546,  944.97319267,\n",
      "        940.49716249,  938.69094632, 1029.24446491, 1007.91049914,\n",
      "       1029.7894881 ,  921.02414826,  957.69581689, 1019.86652719,\n",
      "        997.83420854, 1033.61470903,  975.19803339, 1041.13401513,\n",
      "       1123.42793916, 1016.12489876,  960.41402604, 1046.97405111,\n",
      "       1092.88908797,  997.59030213])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 960.7578993 ,  964.83221813,  914.84459202,  914.65709275,\n",
      "        917.36114995,  928.99988218,  971.59260492, 1002.54363929,\n",
      "       1077.50206276,  981.41467242, 1031.38843481, 1008.36789473,\n",
      "        996.67797377, 1034.79060041,  961.82867089, 1001.79836244,\n",
      "       1019.2776188 , 1035.55514077, 1033.78454565, 1139.54181843])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1184.52339898, 1266.06107518, 1214.66957735, 1161.19412261,\n",
      "       1269.16771977, 1154.08837153, 1053.80799635, 1161.52993384,\n",
      "       1167.57896682, 1129.95184813, 1174.24243267, 1186.96621446,\n",
      "       1142.64760783, 1090.36430471, 1180.46179222, 1213.31148268,\n",
      "       1178.18182074, 1260.71384385, 1155.78393818, 1077.27025203,\n",
      "       1102.0871485 , 1149.52269827, 1180.35512515, 1165.76588431,\n",
      "       1187.36011924, 1119.93837473, 1222.3145851 , 1200.14197303,\n",
      "       1235.4790309 , 1220.45414496])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1213.51266018, 1220.77975695, 1217.94490589, 1226.27908317,\n",
      "       1265.20996166, 1224.93114943, 1229.45285265, 1198.54177021,\n",
      "       1259.40176692, 1185.32401861, 1268.59401946, 1225.15858003,\n",
      "       1123.93523893, 1168.53758729, 1157.02002074, 1219.95678964,\n",
      "       1177.3895382 , 1161.08282259, 1090.95462499, 1132.33430313])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1155.78393818, 1077.27025203, 1102.0871485 , 1149.52269827,\n",
      "       1180.35512515, 1165.76588431, 1187.36011924, 1119.93837473,\n",
      "       1222.3145851 , 1200.14197303, 1235.4790309 , 1220.45414496,\n",
      "       1213.51266018, 1220.77975695, 1217.94490589, 1226.27908317,\n",
      "       1265.20996166, 1224.93114943, 1229.45285265, 1198.54177021,\n",
      "       1259.40176692, 1185.32401861, 1268.59401946, 1225.15858003,\n",
      "       1123.93523893, 1168.53758729, 1157.02002074, 1219.95678964,\n",
      "       1177.3895382 , 1161.08282259])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1090.95462499, 1132.33430313, 1061.19468042, 1065.20021327,\n",
      "       1195.39265367, 1248.97313025, 1240.25792575, 1166.21782504,\n",
      "       1182.54630196, 1200.76216394, 1146.92494919, 1256.28649583,\n",
      "       1279.43022998, 1219.86918036, 1209.6165152 , 1221.40755224,\n",
      "       1114.1366431 , 1156.9080603 , 1167.58371861, 1147.26345236])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1259.40176692, 1185.32401861, 1268.59401946, 1225.15858003,\n",
      "       1123.93523893, 1168.53758729, 1157.02002074, 1219.95678964,\n",
      "       1177.3895382 , 1161.08282259, 1090.95462499, 1132.33430313,\n",
      "       1061.19468042, 1065.20021327, 1195.39265367, 1248.97313025,\n",
      "       1240.25792575, 1166.21782504, 1182.54630196, 1200.76216394,\n",
      "       1146.92494919, 1256.28649583, 1279.43022998, 1219.86918036,\n",
      "       1209.6165152 , 1221.40755224, 1114.1366431 , 1156.9080603 ,\n",
      "       1167.58371861, 1147.26345236])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1176.87866827, 1295.60874377, 1285.39830312, 1208.39997965,\n",
      "       1238.54035826, 1300.79793746, 1298.33604137, 1249.36545257,\n",
      "       1196.01140311, 1245.95065635, 1257.53178452, 1280.48604928,\n",
      "       1279.16542224, 1296.79142958, 1227.31536253, 1287.8747474 ,\n",
      "       1267.99229358, 1343.20395591, 1244.3351939 , 1312.33941447])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 916.1347314 ,  901.84355457,  934.63290199,  984.66471466,\n",
      "        970.16030028,  818.00754688,  784.76478819,  821.16300342,\n",
      "        868.15554952,  909.64502365,  861.98607758,  884.23519565,\n",
      "        854.64268169,  844.90353406,  807.12054261,  815.2239179 ,\n",
      "        802.85865266,  814.48982293,  923.60552682,  869.34745536,\n",
      "       1012.69305322,  979.26171637,  929.55501488,  868.02206953,\n",
      "        922.48195382,  889.64371938,  902.27492431, 1036.70076886,\n",
      "        959.19804806,  973.71324439])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 902.93070215,  900.14532383, 1000.23202533,  921.42712426,\n",
      "        998.73708587,  990.95733222,  911.4989099 ,  943.0991758 ,\n",
      "        982.24411303,  975.88649061,  862.21052577,  857.87276556,\n",
      "        893.84910538,  914.7797677 ,  956.00910157,  950.98553127,\n",
      "        869.84354752,  923.32683661,  967.6534664 ,  975.40865419])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1164.0737293 , 1142.68370924, 1222.57762927, 1126.8284362 ,\n",
      "       1123.38586604, 1184.52339898, 1266.06107518, 1214.66957735,\n",
      "       1161.19412261, 1269.16771977, 1154.08837153, 1053.80799635,\n",
      "       1161.52993384, 1167.57896682, 1129.95184813, 1174.24243267,\n",
      "       1186.96621446, 1142.64760783, 1090.36430471, 1180.46179222,\n",
      "       1213.31148268, 1178.18182074, 1260.71384385, 1155.78393818,\n",
      "       1077.27025203, 1102.0871485 , 1149.52269827, 1180.35512515,\n",
      "       1165.76588431, 1187.36011924])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1119.93837473, 1222.3145851 , 1200.14197303, 1235.4790309 ,\n",
      "       1220.45414496, 1213.51266018, 1220.77975695, 1217.94490589,\n",
      "       1226.27908317, 1265.20996166, 1224.93114943, 1229.45285265,\n",
      "       1198.54177021, 1259.40176692, 1185.32401861, 1268.59401946,\n",
      "       1225.15858003, 1123.93523893, 1168.53758729, 1157.02002074])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1195.20997238, 1174.88613873, 1182.27617431, 1250.21616117,\n",
      "       1196.58470937, 1184.87301985, 1206.17887719, 1193.51503893,\n",
      "       1155.23528584, 1173.69540801, 1230.05262276, 1146.91276357,\n",
      "       1164.0737293 , 1142.68370924, 1222.57762927, 1126.8284362 ,\n",
      "       1123.38586604, 1184.52339898, 1266.06107518, 1214.66957735,\n",
      "       1161.19412261, 1269.16771977, 1154.08837153, 1053.80799635,\n",
      "       1161.52993384, 1167.57896682, 1129.95184813, 1174.24243267,\n",
      "       1186.96621446, 1142.64760783])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1090.36430471, 1180.46179222, 1213.31148268, 1178.18182074,\n",
      "       1260.71384385, 1155.78393818, 1077.27025203, 1102.0871485 ,\n",
      "       1149.52269827, 1180.35512515, 1165.76588431, 1187.36011924,\n",
      "       1119.93837473, 1222.3145851 , 1200.14197303, 1235.4790309 ,\n",
      "       1220.45414496, 1213.51266018, 1220.77975695, 1217.94490589])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1002.54363929, 1077.50206276,  981.41467242, 1031.38843481,\n",
      "       1008.36789473,  996.67797377, 1034.79060041,  961.82867089,\n",
      "       1001.79836244, 1019.2776188 , 1035.55514077, 1033.78454565,\n",
      "       1139.54181843, 1032.6695729 ,  979.85710706,  970.66424996,\n",
      "        971.55787089,  969.39897322, 1059.79104685, 1111.27315424,\n",
      "       1024.00153111,  971.80503587, 1026.55935047, 1002.37166937,\n",
      "       1044.2419652 , 1047.00466984,  956.61527463, 1073.01406201,\n",
      "       1146.43226916, 1041.31184508])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1005.12645319, 1037.76355434, 1054.73861892, 1074.97384986,\n",
      "       1150.99978669, 1151.27744666, 1079.38867994, 1032.6191415 ,\n",
      "       1050.25584071, 1097.33429888, 1205.78161584, 1133.46971939,\n",
      "       1128.59352223, 1127.96863513, 1184.90842814, 1277.93386638,\n",
      "       1159.17684299, 1102.26127385, 1172.30186416, 1186.23552455])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1032.6191415 , 1050.25584071, 1097.33429888, 1205.78161584,\n",
      "       1133.46971939, 1128.59352223, 1127.96863513, 1184.90842814,\n",
      "       1277.93386638, 1159.17684299, 1102.26127385, 1172.30186416,\n",
      "       1186.23552455, 1242.01460042, 1205.17205015, 1136.09652267,\n",
      "       1161.34722065, 1204.85291596, 1204.81004501, 1186.39807387,\n",
      "       1208.3644474 , 1225.51802637, 1240.98008352, 1210.89558316,\n",
      "       1156.64225   , 1153.90637439, 1211.43040122, 1134.72817155,\n",
      "       1155.09827511, 1138.12562284])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1146.38320567, 1216.96420579, 1191.58212951, 1166.49606169,\n",
      "       1092.18467862, 1167.0925951 , 1203.52277271, 1283.56378373,\n",
      "       1195.20997238, 1174.88613873, 1182.27617431, 1250.21616117,\n",
      "       1196.58470937, 1184.87301985, 1206.17887719, 1193.51503893,\n",
      "       1155.23528584, 1173.69540801, 1230.05262276, 1146.91276357])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1208.3644474 , 1225.51802637, 1240.98008352, 1210.89558316,\n",
      "       1156.64225   , 1153.90637439, 1211.43040122, 1134.72817155,\n",
      "       1155.09827511, 1138.12562284, 1146.38320567, 1216.96420579,\n",
      "       1191.58212951, 1166.49606169, 1092.18467862, 1167.0925951 ,\n",
      "       1203.52277271, 1283.56378373, 1195.20997238, 1174.88613873,\n",
      "       1182.27617431, 1250.21616117, 1196.58470937, 1184.87301985,\n",
      "       1206.17887719, 1193.51503893, 1155.23528584, 1173.69540801,\n",
      "       1230.05262276, 1146.91276357])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1164.0737293 , 1142.68370924, 1222.57762927, 1126.8284362 ,\n",
      "       1123.38586604, 1184.52339898, 1266.06107518, 1214.66957735,\n",
      "       1161.19412261, 1269.16771977, 1154.08837153, 1053.80799635,\n",
      "       1161.52993384, 1167.57896682, 1129.95184813, 1174.24243267,\n",
      "       1186.96621446, 1142.64760783, 1090.36430471, 1180.46179222])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1219.95678964, 1177.3895382 , 1161.08282259, 1090.95462499,\n",
      "       1132.33430313, 1061.19468042, 1065.20021327, 1195.39265367,\n",
      "       1248.97313025, 1240.25792575, 1166.21782504, 1182.54630196,\n",
      "       1200.76216394, 1146.92494919, 1256.28649583, 1279.43022998,\n",
      "       1219.86918036, 1209.6165152 , 1221.40755224, 1114.1366431 ,\n",
      "       1156.9080603 , 1167.58371861, 1147.26345236, 1176.87866827,\n",
      "       1295.60874377, 1285.39830312, 1208.39997965, 1238.54035826,\n",
      "       1300.79793746, 1298.33604137])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1249.36545257, 1196.01140311, 1245.95065635, 1257.53178452,\n",
      "       1280.48604928, 1279.16542224, 1296.79142958, 1227.31536253,\n",
      "       1287.8747474 , 1267.99229358, 1343.20395591, 1244.3351939 ,\n",
      "       1312.33941447, 1281.18593196, 1212.85492579, 1200.65028141,\n",
      "       1209.82890306, 1251.02415764, 1273.27157451, 1224.26355311])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1031.38843481, 1008.36789473,  996.67797377, 1034.79060041,\n",
      "        961.82867089, 1001.79836244, 1019.2776188 , 1035.55514077,\n",
      "       1033.78454565, 1139.54181843, 1032.6695729 ,  979.85710706,\n",
      "        970.66424996,  971.55787089,  969.39897322, 1059.79104685,\n",
      "       1111.27315424, 1024.00153111,  971.80503587, 1026.55935047,\n",
      "       1002.37166937, 1044.2419652 , 1047.00466984,  956.61527463,\n",
      "       1073.01406201, 1146.43226916, 1041.31184508, 1005.12645319,\n",
      "       1037.76355434, 1054.73861892])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1074.97384986, 1150.99978669, 1151.27744666, 1079.38867994,\n",
      "       1032.6191415 , 1050.25584071, 1097.33429888, 1205.78161584,\n",
      "       1133.46971939, 1128.59352223, 1127.96863513, 1184.90842814,\n",
      "       1277.93386638, 1159.17684299, 1102.26127385, 1172.30186416,\n",
      "       1186.23552455, 1242.01460042, 1205.17205015, 1136.09652267])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 971.59260492, 1002.54363929, 1077.50206276,  981.41467242,\n",
      "       1031.38843481, 1008.36789473,  996.67797377, 1034.79060041,\n",
      "        961.82867089, 1001.79836244, 1019.2776188 , 1035.55514077,\n",
      "       1033.78454565, 1139.54181843, 1032.6695729 ,  979.85710706,\n",
      "        970.66424996,  971.55787089,  969.39897322, 1059.79104685,\n",
      "       1111.27315424, 1024.00153111,  971.80503587, 1026.55935047,\n",
      "       1002.37166937, 1044.2419652 , 1047.00466984,  956.61527463,\n",
      "       1073.01406201, 1146.43226916])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1041.31184508, 1005.12645319, 1037.76355434, 1054.73861892,\n",
      "       1074.97384986, 1150.99978669, 1151.27744666, 1079.38867994,\n",
      "       1032.6191415 , 1050.25584071, 1097.33429888, 1205.78161584,\n",
      "       1133.46971939, 1128.59352223, 1127.96863513, 1184.90842814,\n",
      "       1277.93386638, 1159.17684299, 1102.26127385, 1172.30186416])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1174.88613873, 1182.27617431, 1250.21616117, 1196.58470937,\n",
      "       1184.87301985, 1206.17887719, 1193.51503893, 1155.23528584,\n",
      "       1173.69540801, 1230.05262276, 1146.91276357, 1164.0737293 ,\n",
      "       1142.68370924, 1222.57762927, 1126.8284362 , 1123.38586604,\n",
      "       1184.52339898, 1266.06107518, 1214.66957735, 1161.19412261,\n",
      "       1269.16771977, 1154.08837153, 1053.80799635, 1161.52993384,\n",
      "       1167.57896682, 1129.95184813, 1174.24243267, 1186.96621446,\n",
      "       1142.64760783, 1090.36430471])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1180.46179222, 1213.31148268, 1178.18182074, 1260.71384385,\n",
      "       1155.78393818, 1077.27025203, 1102.0871485 , 1149.52269827,\n",
      "       1180.35512515, 1165.76588431, 1187.36011924, 1119.93837473,\n",
      "       1222.3145851 , 1200.14197303, 1235.4790309 , 1220.45414496,\n",
      "       1213.51266018, 1220.77975695, 1217.94490589, 1226.27908317])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1037.76355434, 1054.73861892, 1074.97384986, 1150.99978669,\n",
      "       1151.27744666, 1079.38867994, 1032.6191415 , 1050.25584071,\n",
      "       1097.33429888, 1205.78161584, 1133.46971939, 1128.59352223,\n",
      "       1127.96863513, 1184.90842814, 1277.93386638, 1159.17684299,\n",
      "       1102.26127385, 1172.30186416, 1186.23552455, 1242.01460042,\n",
      "       1205.17205015, 1136.09652267, 1161.34722065, 1204.85291596,\n",
      "       1204.81004501, 1186.39807387, 1208.3644474 , 1225.51802637,\n",
      "       1240.98008352, 1210.89558316])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1156.64225   , 1153.90637439, 1211.43040122, 1134.72817155,\n",
      "       1155.09827511, 1138.12562284, 1146.38320567, 1216.96420579,\n",
      "       1191.58212951, 1166.49606169, 1092.18467862, 1167.0925951 ,\n",
      "       1203.52277271, 1283.56378373, 1195.20997238, 1174.88613873,\n",
      "       1182.27617431, 1250.21616117, 1196.58470937, 1184.87301985])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1159.17684299, 1102.26127385, 1172.30186416, 1186.23552455,\n",
      "       1242.01460042, 1205.17205015, 1136.09652267, 1161.34722065,\n",
      "       1204.85291596, 1204.81004501, 1186.39807387, 1208.3644474 ,\n",
      "       1225.51802637, 1240.98008352, 1210.89558316, 1156.64225   ,\n",
      "       1153.90637439, 1211.43040122, 1134.72817155, 1155.09827511,\n",
      "       1138.12562284, 1146.38320567, 1216.96420579, 1191.58212951,\n",
      "       1166.49606169, 1092.18467862, 1167.0925951 , 1203.52277271,\n",
      "       1283.56378373, 1195.20997238])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1174.88613873, 1182.27617431, 1250.21616117, 1196.58470937,\n",
      "       1184.87301985, 1206.17887719, 1193.51503893, 1155.23528584,\n",
      "       1173.69540801, 1230.05262276, 1146.91276357, 1164.0737293 ,\n",
      "       1142.68370924, 1222.57762927, 1126.8284362 , 1123.38586604,\n",
      "       1184.52339898, 1266.06107518, 1214.66957735, 1161.19412261])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1195.39265367, 1248.97313025, 1240.25792575, 1166.21782504,\n",
      "       1182.54630196, 1200.76216394, 1146.92494919, 1256.28649583,\n",
      "       1279.43022998, 1219.86918036, 1209.6165152 , 1221.40755224,\n",
      "       1114.1366431 , 1156.9080603 , 1167.58371861, 1147.26345236,\n",
      "       1176.87866827, 1295.60874377, 1285.39830312, 1208.39997965,\n",
      "       1238.54035826, 1300.79793746, 1298.33604137, 1249.36545257,\n",
      "       1196.01140311, 1245.95065635, 1257.53178452, 1280.48604928,\n",
      "       1279.16542224, 1296.79142958])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1227.31536253, 1287.8747474 , 1267.99229358, 1343.20395591,\n",
      "       1244.3351939 , 1312.33941447, 1281.18593196, 1212.85492579,\n",
      "       1200.65028141, 1209.82890306, 1251.02415764, 1273.27157451,\n",
      "       1224.26355311, 1230.12515941, 1347.48992924, 1375.06385378,\n",
      "       1310.02570673, 1264.10167232, 1256.72678598, 1281.80209639])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1150.99978669, 1151.27744666, 1079.38867994, 1032.6191415 ,\n",
      "       1050.25584071, 1097.33429888, 1205.78161584, 1133.46971939,\n",
      "       1128.59352223, 1127.96863513, 1184.90842814, 1277.93386638,\n",
      "       1159.17684299, 1102.26127385, 1172.30186416, 1186.23552455,\n",
      "       1242.01460042, 1205.17205015, 1136.09652267, 1161.34722065,\n",
      "       1204.85291596, 1204.81004501, 1186.39807387, 1208.3644474 ,\n",
      "       1225.51802637, 1240.98008352, 1210.89558316, 1156.64225   ,\n",
      "       1153.90637439, 1211.43040122])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1134.72817155, 1155.09827511, 1138.12562284, 1146.38320567,\n",
      "       1216.96420579, 1191.58212951, 1166.49606169, 1092.18467862,\n",
      "       1167.0925951 , 1203.52277271, 1283.56378373, 1195.20997238,\n",
      "       1174.88613873, 1182.27617431, 1250.21616117, 1196.58470937,\n",
      "       1184.87301985, 1206.17887719, 1193.51503893, 1155.23528584])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1061.19468042, 1065.20021327, 1195.39265367, 1248.97313025,\n",
      "       1240.25792575, 1166.21782504, 1182.54630196, 1200.76216394,\n",
      "       1146.92494919, 1256.28649583, 1279.43022998, 1219.86918036,\n",
      "       1209.6165152 , 1221.40755224, 1114.1366431 , 1156.9080603 ,\n",
      "       1167.58371861, 1147.26345236, 1176.87866827, 1295.60874377,\n",
      "       1285.39830312, 1208.39997965, 1238.54035826, 1300.79793746,\n",
      "       1298.33604137, 1249.36545257, 1196.01140311, 1245.95065635,\n",
      "       1257.53178452, 1280.48604928])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1279.16542224, 1296.79142958, 1227.31536253, 1287.8747474 ,\n",
      "       1267.99229358, 1343.20395591, 1244.3351939 , 1312.33941447,\n",
      "       1281.18593196, 1212.85492579, 1200.65028141, 1209.82890306,\n",
      "       1251.02415764, 1273.27157451, 1224.26355311, 1230.12515941,\n",
      "       1347.48992924, 1375.06385378, 1310.02570673, 1264.10167232])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1019.86652719,  997.83420854, 1033.61470903,  975.19803339,\n",
      "       1041.13401513, 1123.42793916, 1016.12489876,  960.41402604,\n",
      "       1046.97405111, 1092.88908797,  997.59030213,  960.7578993 ,\n",
      "        964.83221813,  914.84459202,  914.65709275,  917.36114995,\n",
      "        928.99988218,  971.59260492, 1002.54363929, 1077.50206276,\n",
      "        981.41467242, 1031.38843481, 1008.36789473,  996.67797377,\n",
      "       1034.79060041,  961.82867089, 1001.79836244, 1019.2776188 ,\n",
      "       1035.55514077, 1033.78454565])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1139.54181843, 1032.6695729 ,  979.85710706,  970.66424996,\n",
      "        971.55787089,  969.39897322, 1059.79104685, 1111.27315424,\n",
      "       1024.00153111,  971.80503587, 1026.55935047, 1002.37166937,\n",
      "       1044.2419652 , 1047.00466984,  956.61527463, 1073.01406201,\n",
      "       1146.43226916, 1041.31184508, 1005.12645319, 1037.76355434])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 889.64371938,  902.27492431, 1036.70076886,  959.19804806,\n",
      "        973.71324439,  902.93070215,  900.14532383, 1000.23202533,\n",
      "        921.42712426,  998.73708587,  990.95733222,  911.4989099 ,\n",
      "        943.0991758 ,  982.24411303,  975.88649061,  862.21052577,\n",
      "        857.87276556,  893.84910538,  914.7797677 ,  956.00910157,\n",
      "        950.98553127,  869.84354752,  923.32683661,  967.6534664 ,\n",
      "        975.40865419, 1003.22990125, 1003.55157614,  982.53631582,\n",
      "        949.02568453,  858.44878802])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 927.72946126,  957.00092337,  961.84957712,  887.94565068,\n",
      "        866.15942091,  972.96430377,  964.76263466,  985.10560512,\n",
      "        967.39478882,  956.53369828, 1001.37762455,  949.18365111,\n",
      "        952.55314568,  935.12600406,  926.72522669,  933.22378088,\n",
      "        963.24465376,  941.21957118, 1015.54262546,  944.97319267])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1092.18467862, 1167.0925951 , 1203.52277271, 1283.56378373,\n",
      "       1195.20997238, 1174.88613873, 1182.27617431, 1250.21616117,\n",
      "       1196.58470937, 1184.87301985, 1206.17887719, 1193.51503893,\n",
      "       1155.23528584, 1173.69540801, 1230.05262276, 1146.91276357,\n",
      "       1164.0737293 , 1142.68370924, 1222.57762927, 1126.8284362 ,\n",
      "       1123.38586604, 1184.52339898, 1266.06107518, 1214.66957735,\n",
      "       1161.19412261, 1269.16771977, 1154.08837153, 1053.80799635,\n",
      "       1161.52993384, 1167.57896682])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1129.95184813, 1174.24243267, 1186.96621446, 1142.64760783,\n",
      "       1090.36430471, 1180.46179222, 1213.31148268, 1178.18182074,\n",
      "       1260.71384385, 1155.78393818, 1077.27025203, 1102.0871485 ,\n",
      "       1149.52269827, 1180.35512515, 1165.76588431, 1187.36011924,\n",
      "       1119.93837473, 1222.3145851 , 1200.14197303, 1235.4790309 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([533.76513254, 439.37788128, 437.09888259, 592.31584485,\n",
      "       669.82746742, 592.01663525, 597.07230042, 597.51240164,\n",
      "       732.10605797, 694.37287539, 600.22696169, 529.54083307,\n",
      "       475.68450536, 549.19229931, 538.22986094, 490.72208051,\n",
      "       514.5088088 , 508.05592927, 639.58053533, 658.89585885,\n",
      "       607.11396555, 662.96302643, 623.20154476, 551.67016265,\n",
      "       648.06221241, 647.35774585, 555.63289089, 563.85025964,\n",
      "       543.66405584, 506.29295049])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([609.75057181, 707.14155238, 571.44576343, 604.4266225 ,\n",
      "       577.85684727, 565.12944302, 561.70663192, 546.92000432,\n",
      "       587.34387768, 563.28894392, 603.15946382, 609.79642481,\n",
      "       597.68857407, 561.29017164, 563.99484285, 636.27975237,\n",
      "       653.7286504 , 580.21410777, 603.71020602, 655.75754555])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1196.58470937, 1184.87301985, 1206.17887719, 1193.51503893,\n",
      "       1155.23528584, 1173.69540801, 1230.05262276, 1146.91276357,\n",
      "       1164.0737293 , 1142.68370924, 1222.57762927, 1126.8284362 ,\n",
      "       1123.38586604, 1184.52339898, 1266.06107518, 1214.66957735,\n",
      "       1161.19412261, 1269.16771977, 1154.08837153, 1053.80799635,\n",
      "       1161.52993384, 1167.57896682, 1129.95184813, 1174.24243267,\n",
      "       1186.96621446, 1142.64760783, 1090.36430471, 1180.46179222,\n",
      "       1213.31148268, 1178.18182074])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1260.71384385, 1155.78393818, 1077.27025203, 1102.0871485 ,\n",
      "       1149.52269827, 1180.35512515, 1165.76588431, 1187.36011924,\n",
      "       1119.93837473, 1222.3145851 , 1200.14197303, 1235.4790309 ,\n",
      "       1220.45414496, 1213.51266018, 1220.77975695, 1217.94490589,\n",
      "       1226.27908317, 1265.20996166, 1224.93114943, 1229.45285265])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1147.26345236, 1176.87866827, 1295.60874377, 1285.39830312,\n",
      "       1208.39997965, 1238.54035826, 1300.79793746, 1298.33604137,\n",
      "       1249.36545257, 1196.01140311, 1245.95065635, 1257.53178452,\n",
      "       1280.48604928, 1279.16542224, 1296.79142958, 1227.31536253,\n",
      "       1287.8747474 , 1267.99229358, 1343.20395591, 1244.3351939 ,\n",
      "       1312.33941447, 1281.18593196, 1212.85492579, 1200.65028141,\n",
      "       1209.82890306, 1251.02415764, 1273.27157451, 1224.26355311,\n",
      "       1230.12515941, 1347.48992924])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1375.06385378, 1310.02570673, 1264.10167232, 1256.72678598,\n",
      "       1281.80209639, 1211.59839264, 1217.24591029, 1159.56808269,\n",
      "       1234.58683511, 1287.97014211, 1243.51458296, 1324.96513129,\n",
      "       1227.54195237, 1162.62515194, 1235.21876337, 1310.6568578 ,\n",
      "       1365.64444403, 1277.39691922, 1312.16121294, 1284.40235693])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([780.21953936, 757.60855562, 864.58609949, 881.58160417,\n",
      "       859.3996499 , 862.91979102, 883.65609382, 848.05515331,\n",
      "       819.54479791, 861.61104418, 823.77193688, 871.634727  ,\n",
      "       794.34800497, 890.0528615 , 916.1347314 , 901.84355457,\n",
      "       934.63290199, 984.66471466, 970.16030028, 818.00754688,\n",
      "       784.76478819, 821.16300342, 868.15554952, 909.64502365,\n",
      "       861.98607758, 884.23519565, 854.64268169, 844.90353406,\n",
      "       807.12054261, 815.2239179 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 802.85865266,  814.48982293,  923.60552682,  869.34745536,\n",
      "       1012.69305322,  979.26171637,  929.55501488,  868.02206953,\n",
      "        922.48195382,  889.64371938,  902.27492431, 1036.70076886,\n",
      "        959.19804806,  973.71324439,  902.93070215,  900.14532383,\n",
      "       1000.23202533,  921.42712426,  998.73708587,  990.95733222])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1279.43022998, 1219.86918036, 1209.6165152 , 1221.40755224,\n",
      "       1114.1366431 , 1156.9080603 , 1167.58371861, 1147.26345236,\n",
      "       1176.87866827, 1295.60874377, 1285.39830312, 1208.39997965,\n",
      "       1238.54035826, 1300.79793746, 1298.33604137, 1249.36545257,\n",
      "       1196.01140311, 1245.95065635, 1257.53178452, 1280.48604928,\n",
      "       1279.16542224, 1296.79142958, 1227.31536253, 1287.8747474 ,\n",
      "       1267.99229358, 1343.20395591, 1244.3351939 , 1312.33941447,\n",
      "       1281.18593196, 1212.85492579])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1200.65028141, 1209.82890306, 1251.02415764, 1273.27157451,\n",
      "       1224.26355311, 1230.12515941, 1347.48992924, 1375.06385378,\n",
      "       1310.02570673, 1264.10167232, 1256.72678598, 1281.80209639,\n",
      "       1211.59839264, 1217.24591029, 1159.56808269, 1234.58683511,\n",
      "       1287.97014211, 1243.51458296, 1324.96513129, 1227.54195237])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([745.37251598, 804.07326495, 689.79394523, 795.39479652,\n",
      "       773.4935943 , 733.44333496, 696.48647192, 651.50446061,\n",
      "       760.04488904, 774.24036403, 698.7297109 , 669.98055191,\n",
      "       714.36151018, 834.70411715, 785.46547122, 690.58578351,\n",
      "       723.47617913, 745.1619075 , 627.8142917 , 709.36185236,\n",
      "       750.69673545, 800.72938966, 880.17979644, 873.23872791,\n",
      "       793.98585857, 722.50694506, 884.27854574, 835.09507914,\n",
      "       790.9940369 , 784.06169243])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([794.03170193, 782.43241217, 755.51672945, 747.93387285,\n",
      "       773.69529753, 760.13137483, 743.52554413, 780.65476182,\n",
      "       780.51270032, 863.92123596, 695.31521813, 799.46781486,\n",
      "       876.53041534, 724.65218252, 743.8072481 , 770.72811344,\n",
      "       722.73099485, 733.79990542, 728.76976372, 866.57003017])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1161.52993384, 1167.57896682, 1129.95184813, 1174.24243267,\n",
      "       1186.96621446, 1142.64760783, 1090.36430471, 1180.46179222,\n",
      "       1213.31148268, 1178.18182074, 1260.71384385, 1155.78393818,\n",
      "       1077.27025203, 1102.0871485 , 1149.52269827, 1180.35512515,\n",
      "       1165.76588431, 1187.36011924, 1119.93837473, 1222.3145851 ,\n",
      "       1200.14197303, 1235.4790309 , 1220.45414496, 1213.51266018,\n",
      "       1220.77975695, 1217.94490589, 1226.27908317, 1265.20996166,\n",
      "       1224.93114943, 1229.45285265])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1198.54177021, 1259.40176692, 1185.32401861, 1268.59401946,\n",
      "       1225.15858003, 1123.93523893, 1168.53758729, 1157.02002074,\n",
      "       1219.95678964, 1177.3895382 , 1161.08282259, 1090.95462499,\n",
      "       1132.33430313, 1061.19468042, 1065.20021327, 1195.39265367,\n",
      "       1248.97313025, 1240.25792575, 1166.21782504, 1182.54630196])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1161.34722065, 1204.85291596, 1204.81004501, 1186.39807387,\n",
      "       1208.3644474 , 1225.51802637, 1240.98008352, 1210.89558316,\n",
      "       1156.64225   , 1153.90637439, 1211.43040122, 1134.72817155,\n",
      "       1155.09827511, 1138.12562284, 1146.38320567, 1216.96420579,\n",
      "       1191.58212951, 1166.49606169, 1092.18467862, 1167.0925951 ,\n",
      "       1203.52277271, 1283.56378373, 1195.20997238, 1174.88613873,\n",
      "       1182.27617431, 1250.21616117, 1196.58470937, 1184.87301985,\n",
      "       1206.17887719, 1193.51503893])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1155.23528584, 1173.69540801, 1230.05262276, 1146.91276357,\n",
      "       1164.0737293 , 1142.68370924, 1222.57762927, 1126.8284362 ,\n",
      "       1123.38586604, 1184.52339898, 1266.06107518, 1214.66957735,\n",
      "       1161.19412261, 1269.16771977, 1154.08837153, 1053.80799635,\n",
      "       1161.52993384, 1167.57896682, 1129.95184813, 1174.24243267])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1126.8284362 , 1123.38586604, 1184.52339898, 1266.06107518,\n",
      "       1214.66957735, 1161.19412261, 1269.16771977, 1154.08837153,\n",
      "       1053.80799635, 1161.52993384, 1167.57896682, 1129.95184813,\n",
      "       1174.24243267, 1186.96621446, 1142.64760783, 1090.36430471,\n",
      "       1180.46179222, 1213.31148268, 1178.18182074, 1260.71384385,\n",
      "       1155.78393818, 1077.27025203, 1102.0871485 , 1149.52269827,\n",
      "       1180.35512515, 1165.76588431, 1187.36011924, 1119.93837473,\n",
      "       1222.3145851 , 1200.14197303])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1235.4790309 , 1220.45414496, 1213.51266018, 1220.77975695,\n",
      "       1217.94490589, 1226.27908317, 1265.20996166, 1224.93114943,\n",
      "       1229.45285265, 1198.54177021, 1259.40176692, 1185.32401861,\n",
      "       1268.59401946, 1225.15858003, 1123.93523893, 1168.53758729,\n",
      "       1157.02002074, 1219.95678964, 1177.3895382 , 1161.08282259])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1217.94490589, 1226.27908317, 1265.20996166, 1224.93114943,\n",
      "       1229.45285265, 1198.54177021, 1259.40176692, 1185.32401861,\n",
      "       1268.59401946, 1225.15858003, 1123.93523893, 1168.53758729,\n",
      "       1157.02002074, 1219.95678964, 1177.3895382 , 1161.08282259,\n",
      "       1090.95462499, 1132.33430313, 1061.19468042, 1065.20021327,\n",
      "       1195.39265367, 1248.97313025, 1240.25792575, 1166.21782504,\n",
      "       1182.54630196, 1200.76216394, 1146.92494919, 1256.28649583,\n",
      "       1279.43022998, 1219.86918036])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1209.6165152 , 1221.40755224, 1114.1366431 , 1156.9080603 ,\n",
      "       1167.58371861, 1147.26345236, 1176.87866827, 1295.60874377,\n",
      "       1285.39830312, 1208.39997965, 1238.54035826, 1300.79793746,\n",
      "       1298.33604137, 1249.36545257, 1196.01140311, 1245.95065635,\n",
      "       1257.53178452, 1280.48604928, 1279.16542224, 1296.79142958])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1196.01140311, 1245.95065635, 1257.53178452, 1280.48604928,\n",
      "       1279.16542224, 1296.79142958, 1227.31536253, 1287.8747474 ,\n",
      "       1267.99229358, 1343.20395591, 1244.3351939 , 1312.33941447,\n",
      "       1281.18593196, 1212.85492579, 1200.65028141, 1209.82890306,\n",
      "       1251.02415764, 1273.27157451, 1224.26355311, 1230.12515941,\n",
      "       1347.48992924, 1375.06385378, 1310.02570673, 1264.10167232,\n",
      "       1256.72678598, 1281.80209639, 1211.59839264, 1217.24591029,\n",
      "       1159.56808269, 1234.58683511])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1287.97014211, 1243.51458296, 1324.96513129, 1227.54195237,\n",
      "       1162.62515194, 1235.21876337, 1310.6568578 , 1365.64444403,\n",
      "       1277.39691922, 1312.16121294, 1284.40235693, 1258.24029309,\n",
      "       1306.30410473, 1317.29103874, 1205.5095401 , 1310.95265197,\n",
      "       1365.37550819, 1272.40133942, 1283.12485345, 1304.96118896])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1123.93523893, 1168.53758729, 1157.02002074, 1219.95678964,\n",
      "       1177.3895382 , 1161.08282259, 1090.95462499, 1132.33430313,\n",
      "       1061.19468042, 1065.20021327, 1195.39265367, 1248.97313025,\n",
      "       1240.25792575, 1166.21782504, 1182.54630196, 1200.76216394,\n",
      "       1146.92494919, 1256.28649583, 1279.43022998, 1219.86918036,\n",
      "       1209.6165152 , 1221.40755224, 1114.1366431 , 1156.9080603 ,\n",
      "       1167.58371861, 1147.26345236, 1176.87866827, 1295.60874377,\n",
      "       1285.39830312, 1208.39997965])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1238.54035826, 1300.79793746, 1298.33604137, 1249.36545257,\n",
      "       1196.01140311, 1245.95065635, 1257.53178452, 1280.48604928,\n",
      "       1279.16542224, 1296.79142958, 1227.31536253, 1287.8747474 ,\n",
      "       1267.99229358, 1343.20395591, 1244.3351939 , 1312.33941447,\n",
      "       1281.18593196, 1212.85492579, 1200.65028141, 1209.82890306])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1166.21782504, 1182.54630196, 1200.76216394, 1146.92494919,\n",
      "       1256.28649583, 1279.43022998, 1219.86918036, 1209.6165152 ,\n",
      "       1221.40755224, 1114.1366431 , 1156.9080603 , 1167.58371861,\n",
      "       1147.26345236, 1176.87866827, 1295.60874377, 1285.39830312,\n",
      "       1208.39997965, 1238.54035826, 1300.79793746, 1298.33604137,\n",
      "       1249.36545257, 1196.01140311, 1245.95065635, 1257.53178452,\n",
      "       1280.48604928, 1279.16542224, 1296.79142958, 1227.31536253,\n",
      "       1287.8747474 , 1267.99229358])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1343.20395591, 1244.3351939 , 1312.33941447, 1281.18593196,\n",
      "       1212.85492579, 1200.65028141, 1209.82890306, 1251.02415764,\n",
      "       1273.27157451, 1224.26355311, 1230.12515941, 1347.48992924,\n",
      "       1375.06385378, 1310.02570673, 1264.10167232, 1256.72678598,\n",
      "       1281.80209639, 1211.59839264, 1217.24591029, 1159.56808269])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1180.46179222, 1213.31148268, 1178.18182074, 1260.71384385,\n",
      "       1155.78393818, 1077.27025203, 1102.0871485 , 1149.52269827,\n",
      "       1180.35512515, 1165.76588431, 1187.36011924, 1119.93837473,\n",
      "       1222.3145851 , 1200.14197303, 1235.4790309 , 1220.45414496,\n",
      "       1213.51266018, 1220.77975695, 1217.94490589, 1226.27908317,\n",
      "       1265.20996166, 1224.93114943, 1229.45285265, 1198.54177021,\n",
      "       1259.40176692, 1185.32401861, 1268.59401946, 1225.15858003,\n",
      "       1123.93523893, 1168.53758729])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1157.02002074, 1219.95678964, 1177.3895382 , 1161.08282259,\n",
      "       1090.95462499, 1132.33430313, 1061.19468042, 1065.20021327,\n",
      "       1195.39265367, 1248.97313025, 1240.25792575, 1166.21782504,\n",
      "       1182.54630196, 1200.76216394, 1146.92494919, 1256.28649583,\n",
      "       1279.43022998, 1219.86918036, 1209.6165152 , 1221.40755224])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1168.53758729, 1157.02002074, 1219.95678964, 1177.3895382 ,\n",
      "       1161.08282259, 1090.95462499, 1132.33430313, 1061.19468042,\n",
      "       1065.20021327, 1195.39265367, 1248.97313025, 1240.25792575,\n",
      "       1166.21782504, 1182.54630196, 1200.76216394, 1146.92494919,\n",
      "       1256.28649583, 1279.43022998, 1219.86918036, 1209.6165152 ,\n",
      "       1221.40755224, 1114.1366431 , 1156.9080603 , 1167.58371861,\n",
      "       1147.26345236, 1176.87866827, 1295.60874377, 1285.39830312,\n",
      "       1208.39997965, 1238.54035826])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1300.79793746, 1298.33604137, 1249.36545257, 1196.01140311,\n",
      "       1245.95065635, 1257.53178452, 1280.48604928, 1279.16542224,\n",
      "       1296.79142958, 1227.31536253, 1287.8747474 , 1267.99229358,\n",
      "       1343.20395591, 1244.3351939 , 1312.33941447, 1281.18593196,\n",
      "       1212.85492579, 1200.65028141, 1209.82890306, 1251.02415764])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1149.52269827, 1180.35512515, 1165.76588431, 1187.36011924,\n",
      "       1119.93837473, 1222.3145851 , 1200.14197303, 1235.4790309 ,\n",
      "       1220.45414496, 1213.51266018, 1220.77975695, 1217.94490589,\n",
      "       1226.27908317, 1265.20996166, 1224.93114943, 1229.45285265,\n",
      "       1198.54177021, 1259.40176692, 1185.32401861, 1268.59401946,\n",
      "       1225.15858003, 1123.93523893, 1168.53758729, 1157.02002074,\n",
      "       1219.95678964, 1177.3895382 , 1161.08282259, 1090.95462499,\n",
      "       1132.33430313, 1061.19468042])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1065.20021327, 1195.39265367, 1248.97313025, 1240.25792575,\n",
      "       1166.21782504, 1182.54630196, 1200.76216394, 1146.92494919,\n",
      "       1256.28649583, 1279.43022998, 1219.86918036, 1209.6165152 ,\n",
      "       1221.40755224, 1114.1366431 , 1156.9080603 , 1167.58371861,\n",
      "       1147.26345236, 1176.87866827, 1295.60874377, 1285.39830312])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1279.16542224, 1296.79142958, 1227.31536253, 1287.8747474 ,\n",
      "       1267.99229358, 1343.20395591, 1244.3351939 , 1312.33941447,\n",
      "       1281.18593196, 1212.85492579, 1200.65028141, 1209.82890306,\n",
      "       1251.02415764, 1273.27157451, 1224.26355311, 1230.12515941,\n",
      "       1347.48992924, 1375.06385378, 1310.02570673, 1264.10167232,\n",
      "       1256.72678598, 1281.80209639, 1211.59839264, 1217.24591029,\n",
      "       1159.56808269, 1234.58683511, 1287.97014211, 1243.51458296,\n",
      "       1324.96513129, 1227.54195237])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1162.62515194, 1235.21876337, 1310.6568578 , 1365.64444403,\n",
      "       1277.39691922, 1312.16121294, 1284.40235693, 1258.24029309,\n",
      "       1306.30410473, 1317.29103874, 1205.5095401 , 1310.95265197,\n",
      "       1365.37550819, 1272.40133942, 1283.12485345, 1304.96118896,\n",
      "       1298.22951149, 1254.67251479, 1231.80960079, 1259.72667505])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1220.45414496, 1213.51266018, 1220.77975695, 1217.94490589,\n",
      "       1226.27908317, 1265.20996166, 1224.93114943, 1229.45285265,\n",
      "       1198.54177021, 1259.40176692, 1185.32401861, 1268.59401946,\n",
      "       1225.15858003, 1123.93523893, 1168.53758729, 1157.02002074,\n",
      "       1219.95678964, 1177.3895382 , 1161.08282259, 1090.95462499,\n",
      "       1132.33430313, 1061.19468042, 1065.20021327, 1195.39265367,\n",
      "       1248.97313025, 1240.25792575, 1166.21782504, 1182.54630196,\n",
      "       1200.76216394, 1146.92494919])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1256.28649583, 1279.43022998, 1219.86918036, 1209.6165152 ,\n",
      "       1221.40755224, 1114.1366431 , 1156.9080603 , 1167.58371861,\n",
      "       1147.26345236, 1176.87866827, 1295.60874377, 1285.39830312,\n",
      "       1208.39997965, 1238.54035826, 1300.79793746, 1298.33604137,\n",
      "       1249.36545257, 1196.01140311, 1245.95065635, 1257.53178452])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 862.91979102,  883.65609382,  848.05515331,  819.54479791,\n",
      "        861.61104418,  823.77193688,  871.634727  ,  794.34800497,\n",
      "        890.0528615 ,  916.1347314 ,  901.84355457,  934.63290199,\n",
      "        984.66471466,  970.16030028,  818.00754688,  784.76478819,\n",
      "        821.16300342,  868.15554952,  909.64502365,  861.98607758,\n",
      "        884.23519565,  854.64268169,  844.90353406,  807.12054261,\n",
      "        815.2239179 ,  802.85865266,  814.48982293,  923.60552682,\n",
      "        869.34745536, 1012.69305322])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 979.26171637,  929.55501488,  868.02206953,  922.48195382,\n",
      "        889.64371938,  902.27492431, 1036.70076886,  959.19804806,\n",
      "        973.71324439,  902.93070215,  900.14532383, 1000.23202533,\n",
      "        921.42712426,  998.73708587,  990.95733222,  911.4989099 ,\n",
      "        943.0991758 ,  982.24411303,  975.88649061,  862.21052577])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1165.76588431, 1187.36011924, 1119.93837473, 1222.3145851 ,\n",
      "       1200.14197303, 1235.4790309 , 1220.45414496, 1213.51266018,\n",
      "       1220.77975695, 1217.94490589, 1226.27908317, 1265.20996166,\n",
      "       1224.93114943, 1229.45285265, 1198.54177021, 1259.40176692,\n",
      "       1185.32401861, 1268.59401946, 1225.15858003, 1123.93523893,\n",
      "       1168.53758729, 1157.02002074, 1219.95678964, 1177.3895382 ,\n",
      "       1161.08282259, 1090.95462499, 1132.33430313, 1061.19468042,\n",
      "       1065.20021327, 1195.39265367])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1248.97313025, 1240.25792575, 1166.21782504, 1182.54630196,\n",
      "       1200.76216394, 1146.92494919, 1256.28649583, 1279.43022998,\n",
      "       1219.86918036, 1209.6165152 , 1221.40755224, 1114.1366431 ,\n",
      "       1156.9080603 , 1167.58371861, 1147.26345236, 1176.87866827,\n",
      "       1295.60874377, 1285.39830312, 1208.39997965, 1238.54035826])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1219.86918036, 1209.6165152 , 1221.40755224, 1114.1366431 ,\n",
      "       1156.9080603 , 1167.58371861, 1147.26345236, 1176.87866827,\n",
      "       1295.60874377, 1285.39830312, 1208.39997965, 1238.54035826,\n",
      "       1300.79793746, 1298.33604137, 1249.36545257, 1196.01140311,\n",
      "       1245.95065635, 1257.53178452, 1280.48604928, 1279.16542224,\n",
      "       1296.79142958, 1227.31536253, 1287.8747474 , 1267.99229358,\n",
      "       1343.20395591, 1244.3351939 , 1312.33941447, 1281.18593196,\n",
      "       1212.85492579, 1200.65028141])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1209.82890306, 1251.02415764, 1273.27157451, 1224.26355311,\n",
      "       1230.12515941, 1347.48992924, 1375.06385378, 1310.02570673,\n",
      "       1264.10167232, 1256.72678598, 1281.80209639, 1211.59839264,\n",
      "       1217.24591029, 1159.56808269, 1234.58683511, 1287.97014211,\n",
      "       1243.51458296, 1324.96513129, 1227.54195237, 1162.62515194])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1136.09652267, 1161.34722065, 1204.85291596, 1204.81004501,\n",
      "       1186.39807387, 1208.3644474 , 1225.51802637, 1240.98008352,\n",
      "       1210.89558316, 1156.64225   , 1153.90637439, 1211.43040122,\n",
      "       1134.72817155, 1155.09827511, 1138.12562284, 1146.38320567,\n",
      "       1216.96420579, 1191.58212951, 1166.49606169, 1092.18467862,\n",
      "       1167.0925951 , 1203.52277271, 1283.56378373, 1195.20997238,\n",
      "       1174.88613873, 1182.27617431, 1250.21616117, 1196.58470937,\n",
      "       1184.87301985, 1206.17887719])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1193.51503893, 1155.23528584, 1173.69540801, 1230.05262276,\n",
      "       1146.91276357, 1164.0737293 , 1142.68370924, 1222.57762927,\n",
      "       1126.8284362 , 1123.38586604, 1184.52339898, 1266.06107518,\n",
      "       1214.66957735, 1161.19412261, 1269.16771977, 1154.08837153,\n",
      "       1053.80799635, 1161.52993384, 1167.57896682, 1129.95184813])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1210.89558316, 1156.64225   , 1153.90637439, 1211.43040122,\n",
      "       1134.72817155, 1155.09827511, 1138.12562284, 1146.38320567,\n",
      "       1216.96420579, 1191.58212951, 1166.49606169, 1092.18467862,\n",
      "       1167.0925951 , 1203.52277271, 1283.56378373, 1195.20997238,\n",
      "       1174.88613873, 1182.27617431, 1250.21616117, 1196.58470937,\n",
      "       1184.87301985, 1206.17887719, 1193.51503893, 1155.23528584,\n",
      "       1173.69540801, 1230.05262276, 1146.91276357, 1164.0737293 ,\n",
      "       1142.68370924, 1222.57762927])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1126.8284362 , 1123.38586604, 1184.52339898, 1266.06107518,\n",
      "       1214.66957735, 1161.19412261, 1269.16771977, 1154.08837153,\n",
      "       1053.80799635, 1161.52993384, 1167.57896682, 1129.95184813,\n",
      "       1174.24243267, 1186.96621446, 1142.64760783, 1090.36430471,\n",
      "       1180.46179222, 1213.31148268, 1178.18182074, 1260.71384385])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1212.85492579, 1200.65028141, 1209.82890306, 1251.02415764,\n",
      "       1273.27157451, 1224.26355311, 1230.12515941, 1347.48992924,\n",
      "       1375.06385378, 1310.02570673, 1264.10167232, 1256.72678598,\n",
      "       1281.80209639, 1211.59839264, 1217.24591029, 1159.56808269,\n",
      "       1234.58683511, 1287.97014211, 1243.51458296, 1324.96513129,\n",
      "       1227.54195237, 1162.62515194, 1235.21876337, 1310.6568578 ,\n",
      "       1365.64444403, 1277.39691922, 1312.16121294, 1284.40235693,\n",
      "       1258.24029309, 1306.30410473])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1317.29103874, 1205.5095401 , 1310.95265197, 1365.37550819,\n",
      "       1272.40133942, 1283.12485345, 1304.96118896, 1298.22951149,\n",
      "       1254.67251479, 1231.80960079, 1259.72667505, 1333.16528255,\n",
      "       1330.27488592, 1276.61798394, 1314.085094  , 1156.2090095 ,\n",
      "       1281.73607348, 1213.66539508, 1231.51440224, 1217.67272786])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1225.15858003, 1123.93523893, 1168.53758729, 1157.02002074,\n",
      "       1219.95678964, 1177.3895382 , 1161.08282259, 1090.95462499,\n",
      "       1132.33430313, 1061.19468042, 1065.20021327, 1195.39265367,\n",
      "       1248.97313025, 1240.25792575, 1166.21782504, 1182.54630196,\n",
      "       1200.76216394, 1146.92494919, 1256.28649583, 1279.43022998,\n",
      "       1219.86918036, 1209.6165152 , 1221.40755224, 1114.1366431 ,\n",
      "       1156.9080603 , 1167.58371861, 1147.26345236, 1176.87866827,\n",
      "       1295.60874377, 1285.39830312])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1208.39997965, 1238.54035826, 1300.79793746, 1298.33604137,\n",
      "       1249.36545257, 1196.01140311, 1245.95065635, 1257.53178452,\n",
      "       1280.48604928, 1279.16542224, 1296.79142958, 1227.31536253,\n",
      "       1287.8747474 , 1267.99229358, 1343.20395591, 1244.3351939 ,\n",
      "       1312.33941447, 1281.18593196, 1212.85492579, 1200.65028141])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1167.0925951 , 1203.52277271, 1283.56378373, 1195.20997238,\n",
      "       1174.88613873, 1182.27617431, 1250.21616117, 1196.58470937,\n",
      "       1184.87301985, 1206.17887719, 1193.51503893, 1155.23528584,\n",
      "       1173.69540801, 1230.05262276, 1146.91276357, 1164.0737293 ,\n",
      "       1142.68370924, 1222.57762927, 1126.8284362 , 1123.38586604,\n",
      "       1184.52339898, 1266.06107518, 1214.66957735, 1161.19412261,\n",
      "       1269.16771977, 1154.08837153, 1053.80799635, 1161.52993384,\n",
      "       1167.57896682, 1129.95184813])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1174.24243267, 1186.96621446, 1142.64760783, 1090.36430471,\n",
      "       1180.46179222, 1213.31148268, 1178.18182074, 1260.71384385,\n",
      "       1155.78393818, 1077.27025203, 1102.0871485 , 1149.52269827,\n",
      "       1180.35512515, 1165.76588431, 1187.36011924, 1119.93837473,\n",
      "       1222.3145851 , 1200.14197303, 1235.4790309 , 1220.45414496])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 890.0528615 ,  916.1347314 ,  901.84355457,  934.63290199,\n",
      "        984.66471466,  970.16030028,  818.00754688,  784.76478819,\n",
      "        821.16300342,  868.15554952,  909.64502365,  861.98607758,\n",
      "        884.23519565,  854.64268169,  844.90353406,  807.12054261,\n",
      "        815.2239179 ,  802.85865266,  814.48982293,  923.60552682,\n",
      "        869.34745536, 1012.69305322,  979.26171637,  929.55501488,\n",
      "        868.02206953,  922.48195382,  889.64371938,  902.27492431,\n",
      "       1036.70076886,  959.19804806])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 973.71324439,  902.93070215,  900.14532383, 1000.23202533,\n",
      "        921.42712426,  998.73708587,  990.95733222,  911.4989099 ,\n",
      "        943.0991758 ,  982.24411303,  975.88649061,  862.21052577,\n",
      "        857.87276556,  893.84910538,  914.7797677 ,  956.00910157,\n",
      "        950.98553127,  869.84354752,  923.32683661,  967.6534664 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1235.4790309 , 1220.45414496, 1213.51266018, 1220.77975695,\n",
      "       1217.94490589, 1226.27908317, 1265.20996166, 1224.93114943,\n",
      "       1229.45285265, 1198.54177021, 1259.40176692, 1185.32401861,\n",
      "       1268.59401946, 1225.15858003, 1123.93523893, 1168.53758729,\n",
      "       1157.02002074, 1219.95678964, 1177.3895382 , 1161.08282259,\n",
      "       1090.95462499, 1132.33430313, 1061.19468042, 1065.20021327,\n",
      "       1195.39265367, 1248.97313025, 1240.25792575, 1166.21782504,\n",
      "       1182.54630196, 1200.76216394])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1146.92494919, 1256.28649583, 1279.43022998, 1219.86918036,\n",
      "       1209.6165152 , 1221.40755224, 1114.1366431 , 1156.9080603 ,\n",
      "       1167.58371861, 1147.26345236, 1176.87866827, 1295.60874377,\n",
      "       1285.39830312, 1208.39997965, 1238.54035826, 1300.79793746,\n",
      "       1298.33604137, 1249.36545257, 1196.01140311, 1245.95065635])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1178.18182074, 1260.71384385, 1155.78393818, 1077.27025203,\n",
      "       1102.0871485 , 1149.52269827, 1180.35512515, 1165.76588431,\n",
      "       1187.36011924, 1119.93837473, 1222.3145851 , 1200.14197303,\n",
      "       1235.4790309 , 1220.45414496, 1213.51266018, 1220.77975695,\n",
      "       1217.94490589, 1226.27908317, 1265.20996166, 1224.93114943,\n",
      "       1229.45285265, 1198.54177021, 1259.40176692, 1185.32401861,\n",
      "       1268.59401946, 1225.15858003, 1123.93523893, 1168.53758729,\n",
      "       1157.02002074, 1219.95678964])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1177.3895382 , 1161.08282259, 1090.95462499, 1132.33430313,\n",
      "       1061.19468042, 1065.20021327, 1195.39265367, 1248.97313025,\n",
      "       1240.25792575, 1166.21782504, 1182.54630196, 1200.76216394,\n",
      "       1146.92494919, 1256.28649583, 1279.43022998, 1219.86918036,\n",
      "       1209.6165152 , 1221.40755224, 1114.1366431 , 1156.9080603 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 917.36114995,  928.99988218,  971.59260492, 1002.54363929,\n",
      "       1077.50206276,  981.41467242, 1031.38843481, 1008.36789473,\n",
      "        996.67797377, 1034.79060041,  961.82867089, 1001.79836244,\n",
      "       1019.2776188 , 1035.55514077, 1033.78454565, 1139.54181843,\n",
      "       1032.6695729 ,  979.85710706,  970.66424996,  971.55787089,\n",
      "        969.39897322, 1059.79104685, 1111.27315424, 1024.00153111,\n",
      "        971.80503587, 1026.55935047, 1002.37166937, 1044.2419652 ,\n",
      "       1047.00466984,  956.61527463])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1073.01406201, 1146.43226916, 1041.31184508, 1005.12645319,\n",
      "       1037.76355434, 1054.73861892, 1074.97384986, 1150.99978669,\n",
      "       1151.27744666, 1079.38867994, 1032.6191415 , 1050.25584071,\n",
      "       1097.33429888, 1205.78161584, 1133.46971939, 1128.59352223,\n",
      "       1127.96863513, 1184.90842814, 1277.93386638, 1159.17684299])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1296.79142958, 1227.31536253, 1287.8747474 , 1267.99229358,\n",
      "       1343.20395591, 1244.3351939 , 1312.33941447, 1281.18593196,\n",
      "       1212.85492579, 1200.65028141, 1209.82890306, 1251.02415764,\n",
      "       1273.27157451, 1224.26355311, 1230.12515941, 1347.48992924,\n",
      "       1375.06385378, 1310.02570673, 1264.10167232, 1256.72678598,\n",
      "       1281.80209639, 1211.59839264, 1217.24591029, 1159.56808269,\n",
      "       1234.58683511, 1287.97014211, 1243.51458296, 1324.96513129,\n",
      "       1227.54195237, 1162.62515194])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1235.21876337, 1310.6568578 , 1365.64444403, 1277.39691922,\n",
      "       1312.16121294, 1284.40235693, 1258.24029309, 1306.30410473,\n",
      "       1317.29103874, 1205.5095401 , 1310.95265197, 1365.37550819,\n",
      "       1272.40133942, 1283.12485345, 1304.96118896, 1298.22951149,\n",
      "       1254.67251479, 1231.80960079, 1259.72667505, 1333.16528255])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1230.12515941, 1347.48992924, 1375.06385378, 1310.02570673,\n",
      "       1264.10167232, 1256.72678598, 1281.80209639, 1211.59839264,\n",
      "       1217.24591029, 1159.56808269, 1234.58683511, 1287.97014211,\n",
      "       1243.51458296, 1324.96513129, 1227.54195237, 1162.62515194,\n",
      "       1235.21876337, 1310.6568578 , 1365.64444403, 1277.39691922,\n",
      "       1312.16121294, 1284.40235693, 1258.24029309, 1306.30410473,\n",
      "       1317.29103874, 1205.5095401 , 1310.95265197, 1365.37550819,\n",
      "       1272.40133942, 1283.12485345])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1304.96118896, 1298.22951149, 1254.67251479, 1231.80960079,\n",
      "       1259.72667505, 1333.16528255, 1330.27488592, 1276.61798394,\n",
      "       1314.085094  , 1156.2090095 , 1281.73607348, 1213.66539508,\n",
      "       1231.51440224, 1217.67272786, 1196.53079239, 1314.36976499,\n",
      "       1285.83712486, 1300.15992366, 1295.77968745, 1316.27883188])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1295.60874377, 1285.39830312, 1208.39997965, 1238.54035826,\n",
      "       1300.79793746, 1298.33604137, 1249.36545257, 1196.01140311,\n",
      "       1245.95065635, 1257.53178452, 1280.48604928, 1279.16542224,\n",
      "       1296.79142958, 1227.31536253, 1287.8747474 , 1267.99229358,\n",
      "       1343.20395591, 1244.3351939 , 1312.33941447, 1281.18593196,\n",
      "       1212.85492579, 1200.65028141, 1209.82890306, 1251.02415764,\n",
      "       1273.27157451, 1224.26355311, 1230.12515941, 1347.48992924,\n",
      "       1375.06385378, 1310.02570673])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1264.10167232, 1256.72678598, 1281.80209639, 1211.59839264,\n",
      "       1217.24591029, 1159.56808269, 1234.58683511, 1287.97014211,\n",
      "       1243.51458296, 1324.96513129, 1227.54195237, 1162.62515194,\n",
      "       1235.21876337, 1310.6568578 , 1365.64444403, 1277.39691922,\n",
      "       1312.16121294, 1284.40235693, 1258.24029309, 1306.30410473])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1177.3895382 , 1161.08282259, 1090.95462499, 1132.33430313,\n",
      "       1061.19468042, 1065.20021327, 1195.39265367, 1248.97313025,\n",
      "       1240.25792575, 1166.21782504, 1182.54630196, 1200.76216394,\n",
      "       1146.92494919, 1256.28649583, 1279.43022998, 1219.86918036,\n",
      "       1209.6165152 , 1221.40755224, 1114.1366431 , 1156.9080603 ,\n",
      "       1167.58371861, 1147.26345236, 1176.87866827, 1295.60874377,\n",
      "       1285.39830312, 1208.39997965, 1238.54035826, 1300.79793746,\n",
      "       1298.33604137, 1249.36545257])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1196.01140311, 1245.95065635, 1257.53178452, 1280.48604928,\n",
      "       1279.16542224, 1296.79142958, 1227.31536253, 1287.8747474 ,\n",
      "       1267.99229358, 1343.20395591, 1244.3351939 , 1312.33941447,\n",
      "       1281.18593196, 1212.85492579, 1200.65028141, 1209.82890306,\n",
      "       1251.02415764, 1273.27157451, 1224.26355311, 1230.12515941])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1053.80799635, 1161.52993384, 1167.57896682, 1129.95184813,\n",
      "       1174.24243267, 1186.96621446, 1142.64760783, 1090.36430471,\n",
      "       1180.46179222, 1213.31148268, 1178.18182074, 1260.71384385,\n",
      "       1155.78393818, 1077.27025203, 1102.0871485 , 1149.52269827,\n",
      "       1180.35512515, 1165.76588431, 1187.36011924, 1119.93837473,\n",
      "       1222.3145851 , 1200.14197303, 1235.4790309 , 1220.45414496,\n",
      "       1213.51266018, 1220.77975695, 1217.94490589, 1226.27908317,\n",
      "       1265.20996166, 1224.93114943])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1229.45285265, 1198.54177021, 1259.40176692, 1185.32401861,\n",
      "       1268.59401946, 1225.15858003, 1123.93523893, 1168.53758729,\n",
      "       1157.02002074, 1219.95678964, 1177.3895382 , 1161.08282259,\n",
      "       1090.95462499, 1132.33430313, 1061.19468042, 1065.20021327,\n",
      "       1195.39265367, 1248.97313025, 1240.25792575, 1166.21782504])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1251.02415764, 1273.27157451, 1224.26355311, 1230.12515941,\n",
      "       1347.48992924, 1375.06385378, 1310.02570673, 1264.10167232,\n",
      "       1256.72678598, 1281.80209639, 1211.59839264, 1217.24591029,\n",
      "       1159.56808269, 1234.58683511, 1287.97014211, 1243.51458296,\n",
      "       1324.96513129, 1227.54195237, 1162.62515194, 1235.21876337,\n",
      "       1310.6568578 , 1365.64444403, 1277.39691922, 1312.16121294,\n",
      "       1284.40235693, 1258.24029309, 1306.30410473, 1317.29103874,\n",
      "       1205.5095401 , 1310.95265197])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1365.37550819, 1272.40133942, 1283.12485345, 1304.96118896,\n",
      "       1298.22951149, 1254.67251479, 1231.80960079, 1259.72667505,\n",
      "       1333.16528255, 1330.27488592, 1276.61798394, 1314.085094  ,\n",
      "       1156.2090095 , 1281.73607348, 1213.66539508, 1231.51440224,\n",
      "       1217.67272786, 1196.53079239, 1314.36976499, 1285.83712486])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 938.69094632, 1029.24446491, 1007.91049914, 1029.7894881 ,\n",
      "        921.02414826,  957.69581689, 1019.86652719,  997.83420854,\n",
      "       1033.61470903,  975.19803339, 1041.13401513, 1123.42793916,\n",
      "       1016.12489876,  960.41402604, 1046.97405111, 1092.88908797,\n",
      "        997.59030213,  960.7578993 ,  964.83221813,  914.84459202,\n",
      "        914.65709275,  917.36114995,  928.99988218,  971.59260492,\n",
      "       1002.54363929, 1077.50206276,  981.41467242, 1031.38843481,\n",
      "       1008.36789473,  996.67797377])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1034.79060041,  961.82867089, 1001.79836244, 1019.2776188 ,\n",
      "       1035.55514077, 1033.78454565, 1139.54181843, 1032.6695729 ,\n",
      "        979.85710706,  970.66424996,  971.55787089,  969.39897322,\n",
      "       1059.79104685, 1111.27315424, 1024.00153111,  971.80503587,\n",
      "       1026.55935047, 1002.37166937, 1044.2419652 , 1047.00466984])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1184.90842814, 1277.93386638, 1159.17684299, 1102.26127385,\n",
      "       1172.30186416, 1186.23552455, 1242.01460042, 1205.17205015,\n",
      "       1136.09652267, 1161.34722065, 1204.85291596, 1204.81004501,\n",
      "       1186.39807387, 1208.3644474 , 1225.51802637, 1240.98008352,\n",
      "       1210.89558316, 1156.64225   , 1153.90637439, 1211.43040122,\n",
      "       1134.72817155, 1155.09827511, 1138.12562284, 1146.38320567,\n",
      "       1216.96420579, 1191.58212951, 1166.49606169, 1092.18467862,\n",
      "       1167.0925951 , 1203.52277271])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1283.56378373, 1195.20997238, 1174.88613873, 1182.27617431,\n",
      "       1250.21616117, 1196.58470937, 1184.87301985, 1206.17887719,\n",
      "       1193.51503893, 1155.23528584, 1173.69540801, 1230.05262276,\n",
      "       1146.91276357, 1164.0737293 , 1142.68370924, 1222.57762927,\n",
      "       1126.8284362 , 1123.38586604, 1184.52339898, 1266.06107518])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1220.77975695, 1217.94490589, 1226.27908317, 1265.20996166,\n",
      "       1224.93114943, 1229.45285265, 1198.54177021, 1259.40176692,\n",
      "       1185.32401861, 1268.59401946, 1225.15858003, 1123.93523893,\n",
      "       1168.53758729, 1157.02002074, 1219.95678964, 1177.3895382 ,\n",
      "       1161.08282259, 1090.95462499, 1132.33430313, 1061.19468042,\n",
      "       1065.20021327, 1195.39265367, 1248.97313025, 1240.25792575,\n",
      "       1166.21782504, 1182.54630196, 1200.76216394, 1146.92494919,\n",
      "       1256.28649583, 1279.43022998])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1219.86918036, 1209.6165152 , 1221.40755224, 1114.1366431 ,\n",
      "       1156.9080603 , 1167.58371861, 1147.26345236, 1176.87866827,\n",
      "       1295.60874377, 1285.39830312, 1208.39997965, 1238.54035826,\n",
      "       1300.79793746, 1298.33604137, 1249.36545257, 1196.01140311,\n",
      "       1245.95065635, 1257.53178452, 1280.48604928, 1279.16542224])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1312.33941447, 1281.18593196, 1212.85492579, 1200.65028141,\n",
      "       1209.82890306, 1251.02415764, 1273.27157451, 1224.26355311,\n",
      "       1230.12515941, 1347.48992924, 1375.06385378, 1310.02570673,\n",
      "       1264.10167232, 1256.72678598, 1281.80209639, 1211.59839264,\n",
      "       1217.24591029, 1159.56808269, 1234.58683511, 1287.97014211,\n",
      "       1243.51458296, 1324.96513129, 1227.54195237, 1162.62515194,\n",
      "       1235.21876337, 1310.6568578 , 1365.64444403, 1277.39691922,\n",
      "       1312.16121294, 1284.40235693])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1258.24029309, 1306.30410473, 1317.29103874, 1205.5095401 ,\n",
      "       1310.95265197, 1365.37550819, 1272.40133942, 1283.12485345,\n",
      "       1304.96118896, 1298.22951149, 1254.67251479, 1231.80960079,\n",
      "       1259.72667505, 1333.16528255, 1330.27488592, 1276.61798394,\n",
      "       1314.085094  , 1156.2090095 , 1281.73607348, 1213.66539508])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1114.1366431 , 1156.9080603 , 1167.58371861, 1147.26345236,\n",
      "       1176.87866827, 1295.60874377, 1285.39830312, 1208.39997965,\n",
      "       1238.54035826, 1300.79793746, 1298.33604137, 1249.36545257,\n",
      "       1196.01140311, 1245.95065635, 1257.53178452, 1280.48604928,\n",
      "       1279.16542224, 1296.79142958, 1227.31536253, 1287.8747474 ,\n",
      "       1267.99229358, 1343.20395591, 1244.3351939 , 1312.33941447,\n",
      "       1281.18593196, 1212.85492579, 1200.65028141, 1209.82890306,\n",
      "       1251.02415764, 1273.27157451])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1224.26355311, 1230.12515941, 1347.48992924, 1375.06385378,\n",
      "       1310.02570673, 1264.10167232, 1256.72678598, 1281.80209639,\n",
      "       1211.59839264, 1217.24591029, 1159.56808269, 1234.58683511,\n",
      "       1287.97014211, 1243.51458296, 1324.96513129, 1227.54195237,\n",
      "       1162.62515194, 1235.21876337, 1310.6568578 , 1365.64444403])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1267.99229358, 1343.20395591, 1244.3351939 , 1312.33941447,\n",
      "       1281.18593196, 1212.85492579, 1200.65028141, 1209.82890306,\n",
      "       1251.02415764, 1273.27157451, 1224.26355311, 1230.12515941,\n",
      "       1347.48992924, 1375.06385378, 1310.02570673, 1264.10167232,\n",
      "       1256.72678598, 1281.80209639, 1211.59839264, 1217.24591029,\n",
      "       1159.56808269, 1234.58683511, 1287.97014211, 1243.51458296,\n",
      "       1324.96513129, 1227.54195237, 1162.62515194, 1235.21876337,\n",
      "       1310.6568578 , 1365.64444403])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1277.39691922, 1312.16121294, 1284.40235693, 1258.24029309,\n",
      "       1306.30410473, 1317.29103874, 1205.5095401 , 1310.95265197,\n",
      "       1365.37550819, 1272.40133942, 1283.12485345, 1304.96118896,\n",
      "       1298.22951149, 1254.67251479, 1231.80960079, 1259.72667505,\n",
      "       1333.16528255, 1330.27488592, 1276.61798394, 1314.085094  ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1256.72678598, 1281.80209639, 1211.59839264, 1217.24591029,\n",
      "       1159.56808269, 1234.58683511, 1287.97014211, 1243.51458296,\n",
      "       1324.96513129, 1227.54195237, 1162.62515194, 1235.21876337,\n",
      "       1310.6568578 , 1365.64444403, 1277.39691922, 1312.16121294,\n",
      "       1284.40235693, 1258.24029309, 1306.30410473, 1317.29103874,\n",
      "       1205.5095401 , 1310.95265197, 1365.37550819, 1272.40133942,\n",
      "       1283.12485345, 1304.96118896, 1298.22951149, 1254.67251479,\n",
      "       1231.80960079, 1259.72667505])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1333.16528255, 1330.27488592, 1276.61798394, 1314.085094  ,\n",
      "       1156.2090095 , 1281.73607348, 1213.66539508, 1231.51440224,\n",
      "       1217.67272786, 1196.53079239, 1314.36976499, 1285.83712486,\n",
      "       1300.15992366, 1295.77968745, 1316.27883188, 1307.81557175,\n",
      "       1234.09948608, 1318.68039141, 1289.74344268, 1212.14524777])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 819.54479791,  861.61104418,  823.77193688,  871.634727  ,\n",
      "        794.34800497,  890.0528615 ,  916.1347314 ,  901.84355457,\n",
      "        934.63290199,  984.66471466,  970.16030028,  818.00754688,\n",
      "        784.76478819,  821.16300342,  868.15554952,  909.64502365,\n",
      "        861.98607758,  884.23519565,  854.64268169,  844.90353406,\n",
      "        807.12054261,  815.2239179 ,  802.85865266,  814.48982293,\n",
      "        923.60552682,  869.34745536, 1012.69305322,  979.26171637,\n",
      "        929.55501488,  868.02206953])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 922.48195382,  889.64371938,  902.27492431, 1036.70076886,\n",
      "        959.19804806,  973.71324439,  902.93070215,  900.14532383,\n",
      "       1000.23202533,  921.42712426,  998.73708587,  990.95733222,\n",
      "        911.4989099 ,  943.0991758 ,  982.24411303,  975.88649061,\n",
      "        862.21052577,  857.87276556,  893.84910538,  914.7797677 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1280.48604928, 1279.16542224, 1296.79142958, 1227.31536253,\n",
      "       1287.8747474 , 1267.99229358, 1343.20395591, 1244.3351939 ,\n",
      "       1312.33941447, 1281.18593196, 1212.85492579, 1200.65028141,\n",
      "       1209.82890306, 1251.02415764, 1273.27157451, 1224.26355311,\n",
      "       1230.12515941, 1347.48992924, 1375.06385378, 1310.02570673,\n",
      "       1264.10167232, 1256.72678598, 1281.80209639, 1211.59839264,\n",
      "       1217.24591029, 1159.56808269, 1234.58683511, 1287.97014211,\n",
      "       1243.51458296, 1324.96513129])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1227.54195237, 1162.62515194, 1235.21876337, 1310.6568578 ,\n",
      "       1365.64444403, 1277.39691922, 1312.16121294, 1284.40235693,\n",
      "       1258.24029309, 1306.30410473, 1317.29103874, 1205.5095401 ,\n",
      "       1310.95265197, 1365.37550819, 1272.40133942, 1283.12485345,\n",
      "       1304.96118896, 1298.22951149, 1254.67251479, 1231.80960079])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1310.6568578 , 1365.64444403, 1277.39691922, 1312.16121294,\n",
      "       1284.40235693, 1258.24029309, 1306.30410473, 1317.29103874,\n",
      "       1205.5095401 , 1310.95265197, 1365.37550819, 1272.40133942,\n",
      "       1283.12485345, 1304.96118896, 1298.22951149, 1254.67251479,\n",
      "       1231.80960079, 1259.72667505, 1333.16528255, 1330.27488592,\n",
      "       1276.61798394, 1314.085094  , 1156.2090095 , 1281.73607348,\n",
      "       1213.66539508, 1231.51440224, 1217.67272786, 1196.53079239,\n",
      "       1314.36976499, 1285.83712486])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1300.15992366, 1295.77968745, 1316.27883188, 1307.81557175,\n",
      "       1234.09948608, 1318.68039141, 1289.74344268, 1212.14524777,\n",
      "       1243.50409428, 1360.53893262, 1372.95605223, 1307.19100233,\n",
      "       1282.26642324, 1280.82367109, 1400.90086855, 1368.69029468,\n",
      "       1343.50420286, 1372.03403984, 1345.81936368, 1306.9290217 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1211.59839264, 1217.24591029, 1159.56808269, 1234.58683511,\n",
      "       1287.97014211, 1243.51458296, 1324.96513129, 1227.54195237,\n",
      "       1162.62515194, 1235.21876337, 1310.6568578 , 1365.64444403,\n",
      "       1277.39691922, 1312.16121294, 1284.40235693, 1258.24029309,\n",
      "       1306.30410473, 1317.29103874, 1205.5095401 , 1310.95265197,\n",
      "       1365.37550819, 1272.40133942, 1283.12485345, 1304.96118896,\n",
      "       1298.22951149, 1254.67251479, 1231.80960079, 1259.72667505,\n",
      "       1333.16528255, 1330.27488592])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1276.61798394, 1314.085094  , 1156.2090095 , 1281.73607348,\n",
      "       1213.66539508, 1231.51440224, 1217.67272786, 1196.53079239,\n",
      "       1314.36976499, 1285.83712486, 1300.15992366, 1295.77968745,\n",
      "       1316.27883188, 1307.81557175, 1234.09948608, 1318.68039141,\n",
      "       1289.74344268, 1212.14524777, 1243.50409428, 1360.53893262])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1161.08282259, 1090.95462499, 1132.33430313, 1061.19468042,\n",
      "       1065.20021327, 1195.39265367, 1248.97313025, 1240.25792575,\n",
      "       1166.21782504, 1182.54630196, 1200.76216394, 1146.92494919,\n",
      "       1256.28649583, 1279.43022998, 1219.86918036, 1209.6165152 ,\n",
      "       1221.40755224, 1114.1366431 , 1156.9080603 , 1167.58371861,\n",
      "       1147.26345236, 1176.87866827, 1295.60874377, 1285.39830312,\n",
      "       1208.39997965, 1238.54035826, 1300.79793746, 1298.33604137,\n",
      "       1249.36545257, 1196.01140311])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1245.95065635, 1257.53178452, 1280.48604928, 1279.16542224,\n",
      "       1296.79142958, 1227.31536253, 1287.8747474 , 1267.99229358,\n",
      "       1343.20395591, 1244.3351939 , 1312.33941447, 1281.18593196,\n",
      "       1212.85492579, 1200.65028141, 1209.82890306, 1251.02415764,\n",
      "       1273.27157451, 1224.26355311, 1230.12515941, 1347.48992924])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1343.20395591, 1244.3351939 , 1312.33941447, 1281.18593196,\n",
      "       1212.85492579, 1200.65028141, 1209.82890306, 1251.02415764,\n",
      "       1273.27157451, 1224.26355311, 1230.12515941, 1347.48992924,\n",
      "       1375.06385378, 1310.02570673, 1264.10167232, 1256.72678598,\n",
      "       1281.80209639, 1211.59839264, 1217.24591029, 1159.56808269,\n",
      "       1234.58683511, 1287.97014211, 1243.51458296, 1324.96513129,\n",
      "       1227.54195237, 1162.62515194, 1235.21876337, 1310.6568578 ,\n",
      "       1365.64444403, 1277.39691922])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1312.16121294, 1284.40235693, 1258.24029309, 1306.30410473,\n",
      "       1317.29103874, 1205.5095401 , 1310.95265197, 1365.37550819,\n",
      "       1272.40133942, 1283.12485345, 1304.96118896, 1298.22951149,\n",
      "       1254.67251479, 1231.80960079, 1259.72667505, 1333.16528255,\n",
      "       1330.27488592, 1276.61798394, 1314.085094  , 1156.2090095 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1317.29103874, 1205.5095401 , 1310.95265197, 1365.37550819,\n",
      "       1272.40133942, 1283.12485345, 1304.96118896, 1298.22951149,\n",
      "       1254.67251479, 1231.80960079, 1259.72667505, 1333.16528255,\n",
      "       1330.27488592, 1276.61798394, 1314.085094  , 1156.2090095 ,\n",
      "       1281.73607348, 1213.66539508, 1231.51440224, 1217.67272786,\n",
      "       1196.53079239, 1314.36976499, 1285.83712486, 1300.15992366,\n",
      "       1295.77968745, 1316.27883188, 1307.81557175, 1234.09948608,\n",
      "       1318.68039141, 1289.74344268])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1212.14524777, 1243.50409428, 1360.53893262, 1372.95605223,\n",
      "       1307.19100233, 1282.26642324, 1280.82367109, 1400.90086855,\n",
      "       1368.69029468, 1343.50420286, 1372.03403984, 1345.81936368,\n",
      "       1306.9290217 , 1298.4278982 , 1304.96516988, 1310.0966879 ,\n",
      "       1350.30219671, 1334.28085345, 1357.28465431, 1330.91134593])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1002.37166937, 1044.2419652 , 1047.00466984,  956.61527463,\n",
      "       1073.01406201, 1146.43226916, 1041.31184508, 1005.12645319,\n",
      "       1037.76355434, 1054.73861892, 1074.97384986, 1150.99978669,\n",
      "       1151.27744666, 1079.38867994, 1032.6191415 , 1050.25584071,\n",
      "       1097.33429888, 1205.78161584, 1133.46971939, 1128.59352223,\n",
      "       1127.96863513, 1184.90842814, 1277.93386638, 1159.17684299,\n",
      "       1102.26127385, 1172.30186416, 1186.23552455, 1242.01460042,\n",
      "       1205.17205015, 1136.09652267])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1161.34722065, 1204.85291596, 1204.81004501, 1186.39807387,\n",
      "       1208.3644474 , 1225.51802637, 1240.98008352, 1210.89558316,\n",
      "       1156.64225   , 1153.90637439, 1211.43040122, 1134.72817155,\n",
      "       1155.09827511, 1138.12562284, 1146.38320567, 1216.96420579,\n",
      "       1191.58212951, 1166.49606169, 1092.18467862, 1167.0925951 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1222.3145851 , 1200.14197303, 1235.4790309 , 1220.45414496,\n",
      "       1213.51266018, 1220.77975695, 1217.94490589, 1226.27908317,\n",
      "       1265.20996166, 1224.93114943, 1229.45285265, 1198.54177021,\n",
      "       1259.40176692, 1185.32401861, 1268.59401946, 1225.15858003,\n",
      "       1123.93523893, 1168.53758729, 1157.02002074, 1219.95678964,\n",
      "       1177.3895382 , 1161.08282259, 1090.95462499, 1132.33430313,\n",
      "       1061.19468042, 1065.20021327, 1195.39265367, 1248.97313025,\n",
      "       1240.25792575, 1166.21782504])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1182.54630196, 1200.76216394, 1146.92494919, 1256.28649583,\n",
      "       1279.43022998, 1219.86918036, 1209.6165152 , 1221.40755224,\n",
      "       1114.1366431 , 1156.9080603 , 1167.58371861, 1147.26345236,\n",
      "       1176.87866827, 1295.60874377, 1285.39830312, 1208.39997965,\n",
      "       1238.54035826, 1300.79793746, 1298.33604137, 1249.36545257])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1134.72817155, 1155.09827511, 1138.12562284, 1146.38320567,\n",
      "       1216.96420579, 1191.58212951, 1166.49606169, 1092.18467862,\n",
      "       1167.0925951 , 1203.52277271, 1283.56378373, 1195.20997238,\n",
      "       1174.88613873, 1182.27617431, 1250.21616117, 1196.58470937,\n",
      "       1184.87301985, 1206.17887719, 1193.51503893, 1155.23528584,\n",
      "       1173.69540801, 1230.05262276, 1146.91276357, 1164.0737293 ,\n",
      "       1142.68370924, 1222.57762927, 1126.8284362 , 1123.38586604,\n",
      "       1184.52339898, 1266.06107518])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1214.66957735, 1161.19412261, 1269.16771977, 1154.08837153,\n",
      "       1053.80799635, 1161.52993384, 1167.57896682, 1129.95184813,\n",
      "       1174.24243267, 1186.96621446, 1142.64760783, 1090.36430471,\n",
      "       1180.46179222, 1213.31148268, 1178.18182074, 1260.71384385,\n",
      "       1155.78393818, 1077.27025203, 1102.0871485 , 1149.52269827])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1277.93386638, 1159.17684299, 1102.26127385, 1172.30186416,\n",
      "       1186.23552455, 1242.01460042, 1205.17205015, 1136.09652267,\n",
      "       1161.34722065, 1204.85291596, 1204.81004501, 1186.39807387,\n",
      "       1208.3644474 , 1225.51802637, 1240.98008352, 1210.89558316,\n",
      "       1156.64225   , 1153.90637439, 1211.43040122, 1134.72817155,\n",
      "       1155.09827511, 1138.12562284, 1146.38320567, 1216.96420579,\n",
      "       1191.58212951, 1166.49606169, 1092.18467862, 1167.0925951 ,\n",
      "       1203.52277271, 1283.56378373])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1195.20997238, 1174.88613873, 1182.27617431, 1250.21616117,\n",
      "       1196.58470937, 1184.87301985, 1206.17887719, 1193.51503893,\n",
      "       1155.23528584, 1173.69540801, 1230.05262276, 1146.91276357,\n",
      "       1164.0737293 , 1142.68370924, 1222.57762927, 1126.8284362 ,\n",
      "       1123.38586604, 1184.52339898, 1266.06107518, 1214.66957735])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1156.9080603 , 1167.58371861, 1147.26345236, 1176.87866827,\n",
      "       1295.60874377, 1285.39830312, 1208.39997965, 1238.54035826,\n",
      "       1300.79793746, 1298.33604137, 1249.36545257, 1196.01140311,\n",
      "       1245.95065635, 1257.53178452, 1280.48604928, 1279.16542224,\n",
      "       1296.79142958, 1227.31536253, 1287.8747474 , 1267.99229358,\n",
      "       1343.20395591, 1244.3351939 , 1312.33941447, 1281.18593196,\n",
      "       1212.85492579, 1200.65028141, 1209.82890306, 1251.02415764,\n",
      "       1273.27157451, 1224.26355311])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1230.12515941, 1347.48992924, 1375.06385378, 1310.02570673,\n",
      "       1264.10167232, 1256.72678598, 1281.80209639, 1211.59839264,\n",
      "       1217.24591029, 1159.56808269, 1234.58683511, 1287.97014211,\n",
      "       1243.51458296, 1324.96513129, 1227.54195237, 1162.62515194,\n",
      "       1235.21876337, 1310.6568578 , 1365.64444403, 1277.39691922])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1264.10167232, 1256.72678598, 1281.80209639, 1211.59839264,\n",
      "       1217.24591029, 1159.56808269, 1234.58683511, 1287.97014211,\n",
      "       1243.51458296, 1324.96513129, 1227.54195237, 1162.62515194,\n",
      "       1235.21876337, 1310.6568578 , 1365.64444403, 1277.39691922,\n",
      "       1312.16121294, 1284.40235693, 1258.24029309, 1306.30410473,\n",
      "       1317.29103874, 1205.5095401 , 1310.95265197, 1365.37550819,\n",
      "       1272.40133942, 1283.12485345, 1304.96118896, 1298.22951149,\n",
      "       1254.67251479, 1231.80960079])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1259.72667505, 1333.16528255, 1330.27488592, 1276.61798394,\n",
      "       1314.085094  , 1156.2090095 , 1281.73607348, 1213.66539508,\n",
      "       1231.51440224, 1217.67272786, 1196.53079239, 1314.36976499,\n",
      "       1285.83712486, 1300.15992366, 1295.77968745, 1316.27883188,\n",
      "       1307.81557175, 1234.09948608, 1318.68039141, 1289.74344268])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1245.95065635, 1257.53178452, 1280.48604928, 1279.16542224,\n",
      "       1296.79142958, 1227.31536253, 1287.8747474 , 1267.99229358,\n",
      "       1343.20395591, 1244.3351939 , 1312.33941447, 1281.18593196,\n",
      "       1212.85492579, 1200.65028141, 1209.82890306, 1251.02415764,\n",
      "       1273.27157451, 1224.26355311, 1230.12515941, 1347.48992924,\n",
      "       1375.06385378, 1310.02570673, 1264.10167232, 1256.72678598,\n",
      "       1281.80209639, 1211.59839264, 1217.24591029, 1159.56808269,\n",
      "       1234.58683511, 1287.97014211])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1243.51458296, 1324.96513129, 1227.54195237, 1162.62515194,\n",
      "       1235.21876337, 1310.6568578 , 1365.64444403, 1277.39691922,\n",
      "       1312.16121294, 1284.40235693, 1258.24029309, 1306.30410473,\n",
      "       1317.29103874, 1205.5095401 , 1310.95265197, 1365.37550819,\n",
      "       1272.40133942, 1283.12485345, 1304.96118896, 1298.22951149])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1180.35512515, 1165.76588431, 1187.36011924, 1119.93837473,\n",
      "       1222.3145851 , 1200.14197303, 1235.4790309 , 1220.45414496,\n",
      "       1213.51266018, 1220.77975695, 1217.94490589, 1226.27908317,\n",
      "       1265.20996166, 1224.93114943, 1229.45285265, 1198.54177021,\n",
      "       1259.40176692, 1185.32401861, 1268.59401946, 1225.15858003,\n",
      "       1123.93523893, 1168.53758729, 1157.02002074, 1219.95678964,\n",
      "       1177.3895382 , 1161.08282259, 1090.95462499, 1132.33430313,\n",
      "       1061.19468042, 1065.20021327])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1195.39265367, 1248.97313025, 1240.25792575, 1166.21782504,\n",
      "       1182.54630196, 1200.76216394, 1146.92494919, 1256.28649583,\n",
      "       1279.43022998, 1219.86918036, 1209.6165152 , 1221.40755224,\n",
      "       1114.1366431 , 1156.9080603 , 1167.58371861, 1147.26345236,\n",
      "       1176.87866827, 1295.60874377, 1285.39830312, 1208.39997965])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1200.65028141, 1209.82890306, 1251.02415764, 1273.27157451,\n",
      "       1224.26355311, 1230.12515941, 1347.48992924, 1375.06385378,\n",
      "       1310.02570673, 1264.10167232, 1256.72678598, 1281.80209639,\n",
      "       1211.59839264, 1217.24591029, 1159.56808269, 1234.58683511,\n",
      "       1287.97014211, 1243.51458296, 1324.96513129, 1227.54195237,\n",
      "       1162.62515194, 1235.21876337, 1310.6568578 , 1365.64444403,\n",
      "       1277.39691922, 1312.16121294, 1284.40235693, 1258.24029309,\n",
      "       1306.30410473, 1317.29103874])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1205.5095401 , 1310.95265197, 1365.37550819, 1272.40133942,\n",
      "       1283.12485345, 1304.96118896, 1298.22951149, 1254.67251479,\n",
      "       1231.80960079, 1259.72667505, 1333.16528255, 1330.27488592,\n",
      "       1276.61798394, 1314.085094  , 1156.2090095 , 1281.73607348,\n",
      "       1213.66539508, 1231.51440224, 1217.67272786, 1196.53079239])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1324.96513129, 1227.54195237, 1162.62515194, 1235.21876337,\n",
      "       1310.6568578 , 1365.64444403, 1277.39691922, 1312.16121294,\n",
      "       1284.40235693, 1258.24029309, 1306.30410473, 1317.29103874,\n",
      "       1205.5095401 , 1310.95265197, 1365.37550819, 1272.40133942,\n",
      "       1283.12485345, 1304.96118896, 1298.22951149, 1254.67251479,\n",
      "       1231.80960079, 1259.72667505, 1333.16528255, 1330.27488592,\n",
      "       1276.61798394, 1314.085094  , 1156.2090095 , 1281.73607348,\n",
      "       1213.66539508, 1231.51440224])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1217.67272786, 1196.53079239, 1314.36976499, 1285.83712486,\n",
      "       1300.15992366, 1295.77968745, 1316.27883188, 1307.81557175,\n",
      "       1234.09948608, 1318.68039141, 1289.74344268, 1212.14524777,\n",
      "       1243.50409428, 1360.53893262, 1372.95605223, 1307.19100233,\n",
      "       1282.26642324, 1280.82367109, 1400.90086855, 1368.69029468])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1132.33430313, 1061.19468042, 1065.20021327, 1195.39265367,\n",
      "       1248.97313025, 1240.25792575, 1166.21782504, 1182.54630196,\n",
      "       1200.76216394, 1146.92494919, 1256.28649583, 1279.43022998,\n",
      "       1219.86918036, 1209.6165152 , 1221.40755224, 1114.1366431 ,\n",
      "       1156.9080603 , 1167.58371861, 1147.26345236, 1176.87866827,\n",
      "       1295.60874377, 1285.39830312, 1208.39997965, 1238.54035826,\n",
      "       1300.79793746, 1298.33604137, 1249.36545257, 1196.01140311,\n",
      "       1245.95065635, 1257.53178452])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1280.48604928, 1279.16542224, 1296.79142958, 1227.31536253,\n",
      "       1287.8747474 , 1267.99229358, 1343.20395591, 1244.3351939 ,\n",
      "       1312.33941447, 1281.18593196, 1212.85492579, 1200.65028141,\n",
      "       1209.82890306, 1251.02415764, 1273.27157451, 1224.26355311,\n",
      "       1230.12515941, 1347.48992924, 1375.06385378, 1310.02570673])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1284.40235693, 1258.24029309, 1306.30410473, 1317.29103874,\n",
      "       1205.5095401 , 1310.95265197, 1365.37550819, 1272.40133942,\n",
      "       1283.12485345, 1304.96118896, 1298.22951149, 1254.67251479,\n",
      "       1231.80960079, 1259.72667505, 1333.16528255, 1330.27488592,\n",
      "       1276.61798394, 1314.085094  , 1156.2090095 , 1281.73607348,\n",
      "       1213.66539508, 1231.51440224, 1217.67272786, 1196.53079239,\n",
      "       1314.36976499, 1285.83712486, 1300.15992366, 1295.77968745,\n",
      "       1316.27883188, 1307.81557175])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1234.09948608, 1318.68039141, 1289.74344268, 1212.14524777,\n",
      "       1243.50409428, 1360.53893262, 1372.95605223, 1307.19100233,\n",
      "       1282.26642324, 1280.82367109, 1400.90086855, 1368.69029468,\n",
      "       1343.50420286, 1372.03403984, 1345.81936368, 1306.9290217 ,\n",
      "       1298.4278982 , 1304.96516988, 1310.0966879 , 1350.30219671])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1235.21876337, 1310.6568578 , 1365.64444403, 1277.39691922,\n",
      "       1312.16121294, 1284.40235693, 1258.24029309, 1306.30410473,\n",
      "       1317.29103874, 1205.5095401 , 1310.95265197, 1365.37550819,\n",
      "       1272.40133942, 1283.12485345, 1304.96118896, 1298.22951149,\n",
      "       1254.67251479, 1231.80960079, 1259.72667505, 1333.16528255,\n",
      "       1330.27488592, 1276.61798394, 1314.085094  , 1156.2090095 ,\n",
      "       1281.73607348, 1213.66539508, 1231.51440224, 1217.67272786,\n",
      "       1196.53079239, 1314.36976499])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1285.83712486, 1300.15992366, 1295.77968745, 1316.27883188,\n",
      "       1307.81557175, 1234.09948608, 1318.68039141, 1289.74344268,\n",
      "       1212.14524777, 1243.50409428, 1360.53893262, 1372.95605223,\n",
      "       1307.19100233, 1282.26642324, 1280.82367109, 1400.90086855,\n",
      "       1368.69029468, 1343.50420286, 1372.03403984, 1345.81936368])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1234.58683511, 1287.97014211, 1243.51458296, 1324.96513129,\n",
      "       1227.54195237, 1162.62515194, 1235.21876337, 1310.6568578 ,\n",
      "       1365.64444403, 1277.39691922, 1312.16121294, 1284.40235693,\n",
      "       1258.24029309, 1306.30410473, 1317.29103874, 1205.5095401 ,\n",
      "       1310.95265197, 1365.37550819, 1272.40133942, 1283.12485345,\n",
      "       1304.96118896, 1298.22951149, 1254.67251479, 1231.80960079,\n",
      "       1259.72667505, 1333.16528255, 1330.27488592, 1276.61798394,\n",
      "       1314.085094  , 1156.2090095 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1281.73607348, 1213.66539508, 1231.51440224, 1217.67272786,\n",
      "       1196.53079239, 1314.36976499, 1285.83712486, 1300.15992366,\n",
      "       1295.77968745, 1316.27883188, 1307.81557175, 1234.09948608,\n",
      "       1318.68039141, 1289.74344268, 1212.14524777, 1243.50409428,\n",
      "       1360.53893262, 1372.95605223, 1307.19100233, 1282.26642324])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1230.05262276, 1146.91276357, 1164.0737293 , 1142.68370924,\n",
      "       1222.57762927, 1126.8284362 , 1123.38586604, 1184.52339898,\n",
      "       1266.06107518, 1214.66957735, 1161.19412261, 1269.16771977,\n",
      "       1154.08837153, 1053.80799635, 1161.52993384, 1167.57896682,\n",
      "       1129.95184813, 1174.24243267, 1186.96621446, 1142.64760783,\n",
      "       1090.36430471, 1180.46179222, 1213.31148268, 1178.18182074,\n",
      "       1260.71384385, 1155.78393818, 1077.27025203, 1102.0871485 ,\n",
      "       1149.52269827, 1180.35512515])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1165.76588431, 1187.36011924, 1119.93837473, 1222.3145851 ,\n",
      "       1200.14197303, 1235.4790309 , 1220.45414496, 1213.51266018,\n",
      "       1220.77975695, 1217.94490589, 1226.27908317, 1265.20996166,\n",
      "       1224.93114943, 1229.45285265, 1198.54177021, 1259.40176692,\n",
      "       1185.32401861, 1268.59401946, 1225.15858003, 1123.93523893])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1287.8747474 , 1267.99229358, 1343.20395591, 1244.3351939 ,\n",
      "       1312.33941447, 1281.18593196, 1212.85492579, 1200.65028141,\n",
      "       1209.82890306, 1251.02415764, 1273.27157451, 1224.26355311,\n",
      "       1230.12515941, 1347.48992924, 1375.06385378, 1310.02570673,\n",
      "       1264.10167232, 1256.72678598, 1281.80209639, 1211.59839264,\n",
      "       1217.24591029, 1159.56808269, 1234.58683511, 1287.97014211,\n",
      "       1243.51458296, 1324.96513129, 1227.54195237, 1162.62515194,\n",
      "       1235.21876337, 1310.6568578 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1365.64444403, 1277.39691922, 1312.16121294, 1284.40235693,\n",
      "       1258.24029309, 1306.30410473, 1317.29103874, 1205.5095401 ,\n",
      "       1310.95265197, 1365.37550819, 1272.40133942, 1283.12485345,\n",
      "       1304.96118896, 1298.22951149, 1254.67251479, 1231.80960079,\n",
      "       1259.72667505, 1333.16528255, 1330.27488592, 1276.61798394])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1209.6165152 , 1221.40755224, 1114.1366431 , 1156.9080603 ,\n",
      "       1167.58371861, 1147.26345236, 1176.87866827, 1295.60874377,\n",
      "       1285.39830312, 1208.39997965, 1238.54035826, 1300.79793746,\n",
      "       1298.33604137, 1249.36545257, 1196.01140311, 1245.95065635,\n",
      "       1257.53178452, 1280.48604928, 1279.16542224, 1296.79142958,\n",
      "       1227.31536253, 1287.8747474 , 1267.99229358, 1343.20395591,\n",
      "       1244.3351939 , 1312.33941447, 1281.18593196, 1212.85492579,\n",
      "       1200.65028141, 1209.82890306])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1251.02415764, 1273.27157451, 1224.26355311, 1230.12515941,\n",
      "       1347.48992924, 1375.06385378, 1310.02570673, 1264.10167232,\n",
      "       1256.72678598, 1281.80209639, 1211.59839264, 1217.24591029,\n",
      "       1159.56808269, 1234.58683511, 1287.97014211, 1243.51458296,\n",
      "       1324.96513129, 1227.54195237, 1162.62515194, 1235.21876337])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1222.57762927, 1126.8284362 , 1123.38586604, 1184.52339898,\n",
      "       1266.06107518, 1214.66957735, 1161.19412261, 1269.16771977,\n",
      "       1154.08837153, 1053.80799635, 1161.52993384, 1167.57896682,\n",
      "       1129.95184813, 1174.24243267, 1186.96621446, 1142.64760783,\n",
      "       1090.36430471, 1180.46179222, 1213.31148268, 1178.18182074,\n",
      "       1260.71384385, 1155.78393818, 1077.27025203, 1102.0871485 ,\n",
      "       1149.52269827, 1180.35512515, 1165.76588431, 1187.36011924,\n",
      "       1119.93837473, 1222.3145851 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1200.14197303, 1235.4790309 , 1220.45414496, 1213.51266018,\n",
      "       1220.77975695, 1217.94490589, 1226.27908317, 1265.20996166,\n",
      "       1224.93114943, 1229.45285265, 1198.54177021, 1259.40176692,\n",
      "       1185.32401861, 1268.59401946, 1225.15858003, 1123.93523893,\n",
      "       1168.53758729, 1157.02002074, 1219.95678964, 1177.3895382 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1226.27908317, 1265.20996166, 1224.93114943, 1229.45285265,\n",
      "       1198.54177021, 1259.40176692, 1185.32401861, 1268.59401946,\n",
      "       1225.15858003, 1123.93523893, 1168.53758729, 1157.02002074,\n",
      "       1219.95678964, 1177.3895382 , 1161.08282259, 1090.95462499,\n",
      "       1132.33430313, 1061.19468042, 1065.20021327, 1195.39265367,\n",
      "       1248.97313025, 1240.25792575, 1166.21782504, 1182.54630196,\n",
      "       1200.76216394, 1146.92494919, 1256.28649583, 1279.43022998,\n",
      "       1219.86918036, 1209.6165152 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1221.40755224, 1114.1366431 , 1156.9080603 , 1167.58371861,\n",
      "       1147.26345236, 1176.87866827, 1295.60874377, 1285.39830312,\n",
      "       1208.39997965, 1238.54035826, 1300.79793746, 1298.33604137,\n",
      "       1249.36545257, 1196.01140311, 1245.95065635, 1257.53178452,\n",
      "       1280.48604928, 1279.16542224, 1296.79142958, 1227.31536253])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1249.36545257, 1196.01140311, 1245.95065635, 1257.53178452,\n",
      "       1280.48604928, 1279.16542224, 1296.79142958, 1227.31536253,\n",
      "       1287.8747474 , 1267.99229358, 1343.20395591, 1244.3351939 ,\n",
      "       1312.33941447, 1281.18593196, 1212.85492579, 1200.65028141,\n",
      "       1209.82890306, 1251.02415764, 1273.27157451, 1224.26355311,\n",
      "       1230.12515941, 1347.48992924, 1375.06385378, 1310.02570673,\n",
      "       1264.10167232, 1256.72678598, 1281.80209639, 1211.59839264,\n",
      "       1217.24591029, 1159.56808269])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1234.58683511, 1287.97014211, 1243.51458296, 1324.96513129,\n",
      "       1227.54195237, 1162.62515194, 1235.21876337, 1310.6568578 ,\n",
      "       1365.64444403, 1277.39691922, 1312.16121294, 1284.40235693,\n",
      "       1258.24029309, 1306.30410473, 1317.29103874, 1205.5095401 ,\n",
      "       1310.95265197, 1365.37550819, 1272.40133942, 1283.12485345])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1176.87866827, 1295.60874377, 1285.39830312, 1208.39997965,\n",
      "       1238.54035826, 1300.79793746, 1298.33604137, 1249.36545257,\n",
      "       1196.01140311, 1245.95065635, 1257.53178452, 1280.48604928,\n",
      "       1279.16542224, 1296.79142958, 1227.31536253, 1287.8747474 ,\n",
      "       1267.99229358, 1343.20395591, 1244.3351939 , 1312.33941447,\n",
      "       1281.18593196, 1212.85492579, 1200.65028141, 1209.82890306,\n",
      "       1251.02415764, 1273.27157451, 1224.26355311, 1230.12515941,\n",
      "       1347.48992924, 1375.06385378])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1310.02570673, 1264.10167232, 1256.72678598, 1281.80209639,\n",
      "       1211.59839264, 1217.24591029, 1159.56808269, 1234.58683511,\n",
      "       1287.97014211, 1243.51458296, 1324.96513129, 1227.54195237,\n",
      "       1162.62515194, 1235.21876337, 1310.6568578 , 1365.64444403,\n",
      "       1277.39691922, 1312.16121294, 1284.40235693, 1258.24029309])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1240.25792575, 1166.21782504, 1182.54630196, 1200.76216394,\n",
      "       1146.92494919, 1256.28649583, 1279.43022998, 1219.86918036,\n",
      "       1209.6165152 , 1221.40755224, 1114.1366431 , 1156.9080603 ,\n",
      "       1167.58371861, 1147.26345236, 1176.87866827, 1295.60874377,\n",
      "       1285.39830312, 1208.39997965, 1238.54035826, 1300.79793746,\n",
      "       1298.33604137, 1249.36545257, 1196.01140311, 1245.95065635,\n",
      "       1257.53178452, 1280.48604928, 1279.16542224, 1296.79142958,\n",
      "       1227.31536253, 1287.8747474 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1267.99229358, 1343.20395591, 1244.3351939 , 1312.33941447,\n",
      "       1281.18593196, 1212.85492579, 1200.65028141, 1209.82890306,\n",
      "       1251.02415764, 1273.27157451, 1224.26355311, 1230.12515941,\n",
      "       1347.48992924, 1375.06385378, 1310.02570673, 1264.10167232,\n",
      "       1256.72678598, 1281.80209639, 1211.59839264, 1217.24591029])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1298.33604137, 1249.36545257, 1196.01140311, 1245.95065635,\n",
      "       1257.53178452, 1280.48604928, 1279.16542224, 1296.79142958,\n",
      "       1227.31536253, 1287.8747474 , 1267.99229358, 1343.20395591,\n",
      "       1244.3351939 , 1312.33941447, 1281.18593196, 1212.85492579,\n",
      "       1200.65028141, 1209.82890306, 1251.02415764, 1273.27157451,\n",
      "       1224.26355311, 1230.12515941, 1347.48992924, 1375.06385378,\n",
      "       1310.02570673, 1264.10167232, 1256.72678598, 1281.80209639,\n",
      "       1211.59839264, 1217.24591029])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1159.56808269, 1234.58683511, 1287.97014211, 1243.51458296,\n",
      "       1324.96513129, 1227.54195237, 1162.62515194, 1235.21876337,\n",
      "       1310.6568578 , 1365.64444403, 1277.39691922, 1312.16121294,\n",
      "       1284.40235693, 1258.24029309, 1306.30410473, 1317.29103874,\n",
      "       1205.5095401 , 1310.95265197, 1365.37550819, 1272.40133942])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1238.54035826, 1300.79793746, 1298.33604137, 1249.36545257,\n",
      "       1196.01140311, 1245.95065635, 1257.53178452, 1280.48604928,\n",
      "       1279.16542224, 1296.79142958, 1227.31536253, 1287.8747474 ,\n",
      "       1267.99229358, 1343.20395591, 1244.3351939 , 1312.33941447,\n",
      "       1281.18593196, 1212.85492579, 1200.65028141, 1209.82890306,\n",
      "       1251.02415764, 1273.27157451, 1224.26355311, 1230.12515941,\n",
      "       1347.48992924, 1375.06385378, 1310.02570673, 1264.10167232,\n",
      "       1256.72678598, 1281.80209639])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1211.59839264, 1217.24591029, 1159.56808269, 1234.58683511,\n",
      "       1287.97014211, 1243.51458296, 1324.96513129, 1227.54195237,\n",
      "       1162.62515194, 1235.21876337, 1310.6568578 , 1365.64444403,\n",
      "       1277.39691922, 1312.16121294, 1284.40235693, 1258.24029309,\n",
      "       1306.30410473, 1317.29103874, 1205.5095401 , 1310.95265197])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1221.40755224, 1114.1366431 , 1156.9080603 , 1167.58371861,\n",
      "       1147.26345236, 1176.87866827, 1295.60874377, 1285.39830312,\n",
      "       1208.39997965, 1238.54035826, 1300.79793746, 1298.33604137,\n",
      "       1249.36545257, 1196.01140311, 1245.95065635, 1257.53178452,\n",
      "       1280.48604928, 1279.16542224, 1296.79142958, 1227.31536253,\n",
      "       1287.8747474 , 1267.99229358, 1343.20395591, 1244.3351939 ,\n",
      "       1312.33941447, 1281.18593196, 1212.85492579, 1200.65028141,\n",
      "       1209.82890306, 1251.02415764])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1273.27157451, 1224.26355311, 1230.12515941, 1347.48992924,\n",
      "       1375.06385378, 1310.02570673, 1264.10167232, 1256.72678598,\n",
      "       1281.80209639, 1211.59839264, 1217.24591029, 1159.56808269,\n",
      "       1234.58683511, 1287.97014211, 1243.51458296, 1324.96513129,\n",
      "       1227.54195237, 1162.62515194, 1235.21876337, 1310.6568578 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 960.7578993 ,  964.83221813,  914.84459202,  914.65709275,\n",
      "        917.36114995,  928.99988218,  971.59260492, 1002.54363929,\n",
      "       1077.50206276,  981.41467242, 1031.38843481, 1008.36789473,\n",
      "        996.67797377, 1034.79060041,  961.82867089, 1001.79836244,\n",
      "       1019.2776188 , 1035.55514077, 1033.78454565, 1139.54181843,\n",
      "       1032.6695729 ,  979.85710706,  970.66424996,  971.55787089,\n",
      "        969.39897322, 1059.79104685, 1111.27315424, 1024.00153111,\n",
      "        971.80503587, 1026.55935047])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1002.37166937, 1044.2419652 , 1047.00466984,  956.61527463,\n",
      "       1073.01406201, 1146.43226916, 1041.31184508, 1005.12645319,\n",
      "       1037.76355434, 1054.73861892, 1074.97384986, 1150.99978669,\n",
      "       1151.27744666, 1079.38867994, 1032.6191415 , 1050.25584071,\n",
      "       1097.33429888, 1205.78161584, 1133.46971939, 1128.59352223])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1157.02002074, 1219.95678964, 1177.3895382 , 1161.08282259,\n",
      "       1090.95462499, 1132.33430313, 1061.19468042, 1065.20021327,\n",
      "       1195.39265367, 1248.97313025, 1240.25792575, 1166.21782504,\n",
      "       1182.54630196, 1200.76216394, 1146.92494919, 1256.28649583,\n",
      "       1279.43022998, 1219.86918036, 1209.6165152 , 1221.40755224,\n",
      "       1114.1366431 , 1156.9080603 , 1167.58371861, 1147.26345236,\n",
      "       1176.87866827, 1295.60874377, 1285.39830312, 1208.39997965,\n",
      "       1238.54035826, 1300.79793746])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1298.33604137, 1249.36545257, 1196.01140311, 1245.95065635,\n",
      "       1257.53178452, 1280.48604928, 1279.16542224, 1296.79142958,\n",
      "       1227.31536253, 1287.8747474 , 1267.99229358, 1343.20395591,\n",
      "       1244.3351939 , 1312.33941447, 1281.18593196, 1212.85492579,\n",
      "       1200.65028141, 1209.82890306, 1251.02415764, 1273.27157451])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1248.97313025, 1240.25792575, 1166.21782504, 1182.54630196,\n",
      "       1200.76216394, 1146.92494919, 1256.28649583, 1279.43022998,\n",
      "       1219.86918036, 1209.6165152 , 1221.40755224, 1114.1366431 ,\n",
      "       1156.9080603 , 1167.58371861, 1147.26345236, 1176.87866827,\n",
      "       1295.60874377, 1285.39830312, 1208.39997965, 1238.54035826,\n",
      "       1300.79793746, 1298.33604137, 1249.36545257, 1196.01140311,\n",
      "       1245.95065635, 1257.53178452, 1280.48604928, 1279.16542224,\n",
      "       1296.79142958, 1227.31536253])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1287.8747474 , 1267.99229358, 1343.20395591, 1244.3351939 ,\n",
      "       1312.33941447, 1281.18593196, 1212.85492579, 1200.65028141,\n",
      "       1209.82890306, 1251.02415764, 1273.27157451, 1224.26355311,\n",
      "       1230.12515941, 1347.48992924, 1375.06385378, 1310.02570673,\n",
      "       1264.10167232, 1256.72678598, 1281.80209639, 1211.59839264])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1365.64444403, 1277.39691922, 1312.16121294, 1284.40235693,\n",
      "       1258.24029309, 1306.30410473, 1317.29103874, 1205.5095401 ,\n",
      "       1310.95265197, 1365.37550819, 1272.40133942, 1283.12485345,\n",
      "       1304.96118896, 1298.22951149, 1254.67251479, 1231.80960079,\n",
      "       1259.72667505, 1333.16528255, 1330.27488592, 1276.61798394,\n",
      "       1314.085094  , 1156.2090095 , 1281.73607348, 1213.66539508,\n",
      "       1231.51440224, 1217.67272786, 1196.53079239, 1314.36976499,\n",
      "       1285.83712486, 1300.15992366])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1295.77968745, 1316.27883188, 1307.81557175, 1234.09948608,\n",
      "       1318.68039141, 1289.74344268, 1212.14524777, 1243.50409428,\n",
      "       1360.53893262, 1372.95605223, 1307.19100233, 1282.26642324,\n",
      "       1280.82367109, 1400.90086855, 1368.69029468, 1343.50420286,\n",
      "       1372.03403984, 1345.81936368, 1306.9290217 , 1298.4278982 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1205.17205015, 1136.09652267, 1161.34722065, 1204.85291596,\n",
      "       1204.81004501, 1186.39807387, 1208.3644474 , 1225.51802637,\n",
      "       1240.98008352, 1210.89558316, 1156.64225   , 1153.90637439,\n",
      "       1211.43040122, 1134.72817155, 1155.09827511, 1138.12562284,\n",
      "       1146.38320567, 1216.96420579, 1191.58212951, 1166.49606169,\n",
      "       1092.18467862, 1167.0925951 , 1203.52277271, 1283.56378373,\n",
      "       1195.20997238, 1174.88613873, 1182.27617431, 1250.21616117,\n",
      "       1196.58470937, 1184.87301985])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1206.17887719, 1193.51503893, 1155.23528584, 1173.69540801,\n",
      "       1230.05262276, 1146.91276357, 1164.0737293 , 1142.68370924,\n",
      "       1222.57762927, 1126.8284362 , 1123.38586604, 1184.52339898,\n",
      "       1266.06107518, 1214.66957735, 1161.19412261, 1269.16771977,\n",
      "       1154.08837153, 1053.80799635, 1161.52993384, 1167.57896682])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1159.56808269, 1234.58683511, 1287.97014211, 1243.51458296,\n",
      "       1324.96513129, 1227.54195237, 1162.62515194, 1235.21876337,\n",
      "       1310.6568578 , 1365.64444403, 1277.39691922, 1312.16121294,\n",
      "       1284.40235693, 1258.24029309, 1306.30410473, 1317.29103874,\n",
      "       1205.5095401 , 1310.95265197, 1365.37550819, 1272.40133942,\n",
      "       1283.12485345, 1304.96118896, 1298.22951149, 1254.67251479,\n",
      "       1231.80960079, 1259.72667505, 1333.16528255, 1330.27488592,\n",
      "       1276.61798394, 1314.085094  ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1156.2090095 , 1281.73607348, 1213.66539508, 1231.51440224,\n",
      "       1217.67272786, 1196.53079239, 1314.36976499, 1285.83712486,\n",
      "       1300.15992366, 1295.77968745, 1316.27883188, 1307.81557175,\n",
      "       1234.09948608, 1318.68039141, 1289.74344268, 1212.14524777,\n",
      "       1243.50409428, 1360.53893262, 1372.95605223, 1307.19100233])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1146.92494919, 1256.28649583, 1279.43022998, 1219.86918036,\n",
      "       1209.6165152 , 1221.40755224, 1114.1366431 , 1156.9080603 ,\n",
      "       1167.58371861, 1147.26345236, 1176.87866827, 1295.60874377,\n",
      "       1285.39830312, 1208.39997965, 1238.54035826, 1300.79793746,\n",
      "       1298.33604137, 1249.36545257, 1196.01140311, 1245.95065635,\n",
      "       1257.53178452, 1280.48604928, 1279.16542224, 1296.79142958,\n",
      "       1227.31536253, 1287.8747474 , 1267.99229358, 1343.20395591,\n",
      "       1244.3351939 , 1312.33941447])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1281.18593196, 1212.85492579, 1200.65028141, 1209.82890306,\n",
      "       1251.02415764, 1273.27157451, 1224.26355311, 1230.12515941,\n",
      "       1347.48992924, 1375.06385378, 1310.02570673, 1264.10167232,\n",
      "       1256.72678598, 1281.80209639, 1211.59839264, 1217.24591029,\n",
      "       1159.56808269, 1234.58683511, 1287.97014211, 1243.51458296])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1155.09827511, 1138.12562284, 1146.38320567, 1216.96420579,\n",
      "       1191.58212951, 1166.49606169, 1092.18467862, 1167.0925951 ,\n",
      "       1203.52277271, 1283.56378373, 1195.20997238, 1174.88613873,\n",
      "       1182.27617431, 1250.21616117, 1196.58470937, 1184.87301985,\n",
      "       1206.17887719, 1193.51503893, 1155.23528584, 1173.69540801,\n",
      "       1230.05262276, 1146.91276357, 1164.0737293 , 1142.68370924,\n",
      "       1222.57762927, 1126.8284362 , 1123.38586604, 1184.52339898,\n",
      "       1266.06107518, 1214.66957735])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1161.19412261, 1269.16771977, 1154.08837153, 1053.80799635,\n",
      "       1161.52993384, 1167.57896682, 1129.95184813, 1174.24243267,\n",
      "       1186.96621446, 1142.64760783, 1090.36430471, 1180.46179222,\n",
      "       1213.31148268, 1178.18182074, 1260.71384385, 1155.78393818,\n",
      "       1077.27025203, 1102.0871485 , 1149.52269827, 1180.35512515])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1146.91276357, 1164.0737293 , 1142.68370924, 1222.57762927,\n",
      "       1126.8284362 , 1123.38586604, 1184.52339898, 1266.06107518,\n",
      "       1214.66957735, 1161.19412261, 1269.16771977, 1154.08837153,\n",
      "       1053.80799635, 1161.52993384, 1167.57896682, 1129.95184813,\n",
      "       1174.24243267, 1186.96621446, 1142.64760783, 1090.36430471,\n",
      "       1180.46179222, 1213.31148268, 1178.18182074, 1260.71384385,\n",
      "       1155.78393818, 1077.27025203, 1102.0871485 , 1149.52269827,\n",
      "       1180.35512515, 1165.76588431])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1187.36011924, 1119.93837473, 1222.3145851 , 1200.14197303,\n",
      "       1235.4790309 , 1220.45414496, 1213.51266018, 1220.77975695,\n",
      "       1217.94490589, 1226.27908317, 1265.20996166, 1224.93114943,\n",
      "       1229.45285265, 1198.54177021, 1259.40176692, 1185.32401861,\n",
      "       1268.59401946, 1225.15858003, 1123.93523893, 1168.53758729])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1300.79793746, 1298.33604137, 1249.36545257, 1196.01140311,\n",
      "       1245.95065635, 1257.53178452, 1280.48604928, 1279.16542224,\n",
      "       1296.79142958, 1227.31536253, 1287.8747474 , 1267.99229358,\n",
      "       1343.20395591, 1244.3351939 , 1312.33941447, 1281.18593196,\n",
      "       1212.85492579, 1200.65028141, 1209.82890306, 1251.02415764,\n",
      "       1273.27157451, 1224.26355311, 1230.12515941, 1347.48992924,\n",
      "       1375.06385378, 1310.02570673, 1264.10167232, 1256.72678598,\n",
      "       1281.80209639, 1211.59839264])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1217.24591029, 1159.56808269, 1234.58683511, 1287.97014211,\n",
      "       1243.51458296, 1324.96513129, 1227.54195237, 1162.62515194,\n",
      "       1235.21876337, 1310.6568578 , 1365.64444403, 1277.39691922,\n",
      "       1312.16121294, 1284.40235693, 1258.24029309, 1306.30410473,\n",
      "       1317.29103874, 1205.5095401 , 1310.95265197, 1365.37550819])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1208.39997965, 1238.54035826, 1300.79793746, 1298.33604137,\n",
      "       1249.36545257, 1196.01140311, 1245.95065635, 1257.53178452,\n",
      "       1280.48604928, 1279.16542224, 1296.79142958, 1227.31536253,\n",
      "       1287.8747474 , 1267.99229358, 1343.20395591, 1244.3351939 ,\n",
      "       1312.33941447, 1281.18593196, 1212.85492579, 1200.65028141,\n",
      "       1209.82890306, 1251.02415764, 1273.27157451, 1224.26355311,\n",
      "       1230.12515941, 1347.48992924, 1375.06385378, 1310.02570673,\n",
      "       1264.10167232, 1256.72678598])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1281.80209639, 1211.59839264, 1217.24591029, 1159.56808269,\n",
      "       1234.58683511, 1287.97014211, 1243.51458296, 1324.96513129,\n",
      "       1227.54195237, 1162.62515194, 1235.21876337, 1310.6568578 ,\n",
      "       1365.64444403, 1277.39691922, 1312.16121294, 1284.40235693,\n",
      "       1258.24029309, 1306.30410473, 1317.29103874, 1205.5095401 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1375.06385378, 1310.02570673, 1264.10167232, 1256.72678598,\n",
      "       1281.80209639, 1211.59839264, 1217.24591029, 1159.56808269,\n",
      "       1234.58683511, 1287.97014211, 1243.51458296, 1324.96513129,\n",
      "       1227.54195237, 1162.62515194, 1235.21876337, 1310.6568578 ,\n",
      "       1365.64444403, 1277.39691922, 1312.16121294, 1284.40235693,\n",
      "       1258.24029309, 1306.30410473, 1317.29103874, 1205.5095401 ,\n",
      "       1310.95265197, 1365.37550819, 1272.40133942, 1283.12485345,\n",
      "       1304.96118896, 1298.22951149])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1254.67251479, 1231.80960079, 1259.72667505, 1333.16528255,\n",
      "       1330.27488592, 1276.61798394, 1314.085094  , 1156.2090095 ,\n",
      "       1281.73607348, 1213.66539508, 1231.51440224, 1217.67272786,\n",
      "       1196.53079239, 1314.36976499, 1285.83712486, 1300.15992366,\n",
      "       1295.77968745, 1316.27883188, 1307.81557175, 1234.09948608])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1211.43040122, 1134.72817155, 1155.09827511, 1138.12562284,\n",
      "       1146.38320567, 1216.96420579, 1191.58212951, 1166.49606169,\n",
      "       1092.18467862, 1167.0925951 , 1203.52277271, 1283.56378373,\n",
      "       1195.20997238, 1174.88613873, 1182.27617431, 1250.21616117,\n",
      "       1196.58470937, 1184.87301985, 1206.17887719, 1193.51503893,\n",
      "       1155.23528584, 1173.69540801, 1230.05262276, 1146.91276357,\n",
      "       1164.0737293 , 1142.68370924, 1222.57762927, 1126.8284362 ,\n",
      "       1123.38586604, 1184.52339898])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1266.06107518, 1214.66957735, 1161.19412261, 1269.16771977,\n",
      "       1154.08837153, 1053.80799635, 1161.52993384, 1167.57896682,\n",
      "       1129.95184813, 1174.24243267, 1186.96621446, 1142.64760783,\n",
      "       1090.36430471, 1180.46179222, 1213.31148268, 1178.18182074,\n",
      "       1260.71384385, 1155.78393818, 1077.27025203, 1102.0871485 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1256.28649583, 1279.43022998, 1219.86918036, 1209.6165152 ,\n",
      "       1221.40755224, 1114.1366431 , 1156.9080603 , 1167.58371861,\n",
      "       1147.26345236, 1176.87866827, 1295.60874377, 1285.39830312,\n",
      "       1208.39997965, 1238.54035826, 1300.79793746, 1298.33604137,\n",
      "       1249.36545257, 1196.01140311, 1245.95065635, 1257.53178452,\n",
      "       1280.48604928, 1279.16542224, 1296.79142958, 1227.31536253,\n",
      "       1287.8747474 , 1267.99229358, 1343.20395591, 1244.3351939 ,\n",
      "       1312.33941447, 1281.18593196])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1212.85492579, 1200.65028141, 1209.82890306, 1251.02415764,\n",
      "       1273.27157451, 1224.26355311, 1230.12515941, 1347.48992924,\n",
      "       1375.06385378, 1310.02570673, 1264.10167232, 1256.72678598,\n",
      "       1281.80209639, 1211.59839264, 1217.24591029, 1159.56808269,\n",
      "       1234.58683511, 1287.97014211, 1243.51458296, 1324.96513129])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1227.31536253, 1287.8747474 , 1267.99229358, 1343.20395591,\n",
      "       1244.3351939 , 1312.33941447, 1281.18593196, 1212.85492579,\n",
      "       1200.65028141, 1209.82890306, 1251.02415764, 1273.27157451,\n",
      "       1224.26355311, 1230.12515941, 1347.48992924, 1375.06385378,\n",
      "       1310.02570673, 1264.10167232, 1256.72678598, 1281.80209639,\n",
      "       1211.59839264, 1217.24591029, 1159.56808269, 1234.58683511,\n",
      "       1287.97014211, 1243.51458296, 1324.96513129, 1227.54195237,\n",
      "       1162.62515194, 1235.21876337])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1310.6568578 , 1365.64444403, 1277.39691922, 1312.16121294,\n",
      "       1284.40235693, 1258.24029309, 1306.30410473, 1317.29103874,\n",
      "       1205.5095401 , 1310.95265197, 1365.37550819, 1272.40133942,\n",
      "       1283.12485345, 1304.96118896, 1298.22951149, 1254.67251479,\n",
      "       1231.80960079, 1259.72667505, 1333.16528255, 1330.27488592])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1200.76216394, 1146.92494919, 1256.28649583, 1279.43022998,\n",
      "       1219.86918036, 1209.6165152 , 1221.40755224, 1114.1366431 ,\n",
      "       1156.9080603 , 1167.58371861, 1147.26345236, 1176.87866827,\n",
      "       1295.60874377, 1285.39830312, 1208.39997965, 1238.54035826,\n",
      "       1300.79793746, 1298.33604137, 1249.36545257, 1196.01140311,\n",
      "       1245.95065635, 1257.53178452, 1280.48604928, 1279.16542224,\n",
      "       1296.79142958, 1227.31536253, 1287.8747474 , 1267.99229358,\n",
      "       1343.20395591, 1244.3351939 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1312.33941447, 1281.18593196, 1212.85492579, 1200.65028141,\n",
      "       1209.82890306, 1251.02415764, 1273.27157451, 1224.26355311,\n",
      "       1230.12515941, 1347.48992924, 1375.06385378, 1310.02570673,\n",
      "       1264.10167232, 1256.72678598, 1281.80209639, 1211.59839264,\n",
      "       1217.24591029, 1159.56808269, 1234.58683511, 1287.97014211])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1186.96621446, 1142.64760783, 1090.36430471, 1180.46179222,\n",
      "       1213.31148268, 1178.18182074, 1260.71384385, 1155.78393818,\n",
      "       1077.27025203, 1102.0871485 , 1149.52269827, 1180.35512515,\n",
      "       1165.76588431, 1187.36011924, 1119.93837473, 1222.3145851 ,\n",
      "       1200.14197303, 1235.4790309 , 1220.45414496, 1213.51266018,\n",
      "       1220.77975695, 1217.94490589, 1226.27908317, 1265.20996166,\n",
      "       1224.93114943, 1229.45285265, 1198.54177021, 1259.40176692,\n",
      "       1185.32401861, 1268.59401946])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1225.15858003, 1123.93523893, 1168.53758729, 1157.02002074,\n",
      "       1219.95678964, 1177.3895382 , 1161.08282259, 1090.95462499,\n",
      "       1132.33430313, 1061.19468042, 1065.20021327, 1195.39265367,\n",
      "       1248.97313025, 1240.25792575, 1166.21782504, 1182.54630196,\n",
      "       1200.76216394, 1146.92494919, 1256.28649583, 1279.43022998])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1306.30410473, 1317.29103874, 1205.5095401 , 1310.95265197,\n",
      "       1365.37550819, 1272.40133942, 1283.12485345, 1304.96118896,\n",
      "       1298.22951149, 1254.67251479, 1231.80960079, 1259.72667505,\n",
      "       1333.16528255, 1330.27488592, 1276.61798394, 1314.085094  ,\n",
      "       1156.2090095 , 1281.73607348, 1213.66539508, 1231.51440224,\n",
      "       1217.67272786, 1196.53079239, 1314.36976499, 1285.83712486,\n",
      "       1300.15992366, 1295.77968745, 1316.27883188, 1307.81557175,\n",
      "       1234.09948608, 1318.68039141])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1289.74344268, 1212.14524777, 1243.50409428, 1360.53893262,\n",
      "       1372.95605223, 1307.19100233, 1282.26642324, 1280.82367109,\n",
      "       1400.90086855, 1368.69029468, 1343.50420286, 1372.03403984,\n",
      "       1345.81936368, 1306.9290217 , 1298.4278982 , 1304.96516988,\n",
      "       1310.0966879 , 1350.30219671, 1334.28085345, 1357.28465431])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1065.20021327, 1195.39265367, 1248.97313025, 1240.25792575,\n",
      "       1166.21782504, 1182.54630196, 1200.76216394, 1146.92494919,\n",
      "       1256.28649583, 1279.43022998, 1219.86918036, 1209.6165152 ,\n",
      "       1221.40755224, 1114.1366431 , 1156.9080603 , 1167.58371861,\n",
      "       1147.26345236, 1176.87866827, 1295.60874377, 1285.39830312,\n",
      "       1208.39997965, 1238.54035826, 1300.79793746, 1298.33604137,\n",
      "       1249.36545257, 1196.01140311, 1245.95065635, 1257.53178452,\n",
      "       1280.48604928, 1279.16542224])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1296.79142958, 1227.31536253, 1287.8747474 , 1267.99229358,\n",
      "       1343.20395591, 1244.3351939 , 1312.33941447, 1281.18593196,\n",
      "       1212.85492579, 1200.65028141, 1209.82890306, 1251.02415764,\n",
      "       1273.27157451, 1224.26355311, 1230.12515941, 1347.48992924,\n",
      "       1375.06385378, 1310.02570673, 1264.10167232, 1256.72678598])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 956.61527463, 1073.01406201, 1146.43226916, 1041.31184508,\n",
      "       1005.12645319, 1037.76355434, 1054.73861892, 1074.97384986,\n",
      "       1150.99978669, 1151.27744666, 1079.38867994, 1032.6191415 ,\n",
      "       1050.25584071, 1097.33429888, 1205.78161584, 1133.46971939,\n",
      "       1128.59352223, 1127.96863513, 1184.90842814, 1277.93386638,\n",
      "       1159.17684299, 1102.26127385, 1172.30186416, 1186.23552455,\n",
      "       1242.01460042, 1205.17205015, 1136.09652267, 1161.34722065,\n",
      "       1204.85291596, 1204.81004501])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1186.39807387, 1208.3644474 , 1225.51802637, 1240.98008352,\n",
      "       1210.89558316, 1156.64225   , 1153.90637439, 1211.43040122,\n",
      "       1134.72817155, 1155.09827511, 1138.12562284, 1146.38320567,\n",
      "       1216.96420579, 1191.58212951, 1166.49606169, 1092.18467862,\n",
      "       1167.0925951 , 1203.52277271, 1283.56378373, 1195.20997238])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1281.18593196, 1212.85492579, 1200.65028141, 1209.82890306,\n",
      "       1251.02415764, 1273.27157451, 1224.26355311, 1230.12515941,\n",
      "       1347.48992924, 1375.06385378, 1310.02570673, 1264.10167232,\n",
      "       1256.72678598, 1281.80209639, 1211.59839264, 1217.24591029,\n",
      "       1159.56808269, 1234.58683511, 1287.97014211, 1243.51458296,\n",
      "       1324.96513129, 1227.54195237, 1162.62515194, 1235.21876337,\n",
      "       1310.6568578 , 1365.64444403, 1277.39691922, 1312.16121294,\n",
      "       1284.40235693, 1258.24029309])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1306.30410473, 1317.29103874, 1205.5095401 , 1310.95265197,\n",
      "       1365.37550819, 1272.40133942, 1283.12485345, 1304.96118896,\n",
      "       1298.22951149, 1254.67251479, 1231.80960079, 1259.72667505,\n",
      "       1333.16528255, 1330.27488592, 1276.61798394, 1314.085094  ,\n",
      "       1156.2090095 , 1281.73607348, 1213.66539508, 1231.51440224])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 902.27492431, 1036.70076886,  959.19804806,  973.71324439,\n",
      "        902.93070215,  900.14532383, 1000.23202533,  921.42712426,\n",
      "        998.73708587,  990.95733222,  911.4989099 ,  943.0991758 ,\n",
      "        982.24411303,  975.88649061,  862.21052577,  857.87276556,\n",
      "        893.84910538,  914.7797677 ,  956.00910157,  950.98553127,\n",
      "        869.84354752,  923.32683661,  967.6534664 ,  975.40865419,\n",
      "       1003.22990125, 1003.55157614,  982.53631582,  949.02568453,\n",
      "        858.44878802,  927.72946126])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 957.00092337,  961.84957712,  887.94565068,  866.15942091,\n",
      "        972.96430377,  964.76263466,  985.10560512,  967.39478882,\n",
      "        956.53369828, 1001.37762455,  949.18365111,  952.55314568,\n",
      "        935.12600406,  926.72522669,  933.22378088,  963.24465376,\n",
      "        941.21957118, 1015.54262546,  944.97319267,  940.49716249])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1191.58212951, 1166.49606169, 1092.18467862, 1167.0925951 ,\n",
      "       1203.52277271, 1283.56378373, 1195.20997238, 1174.88613873,\n",
      "       1182.27617431, 1250.21616117, 1196.58470937, 1184.87301985,\n",
      "       1206.17887719, 1193.51503893, 1155.23528584, 1173.69540801,\n",
      "       1230.05262276, 1146.91276357, 1164.0737293 , 1142.68370924,\n",
      "       1222.57762927, 1126.8284362 , 1123.38586604, 1184.52339898,\n",
      "       1266.06107518, 1214.66957735, 1161.19412261, 1269.16771977,\n",
      "       1154.08837153, 1053.80799635])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1161.52993384, 1167.57896682, 1129.95184813, 1174.24243267,\n",
      "       1186.96621446, 1142.64760783, 1090.36430471, 1180.46179222,\n",
      "       1213.31148268, 1178.18182074, 1260.71384385, 1155.78393818,\n",
      "       1077.27025203, 1102.0871485 , 1149.52269827, 1180.35512515,\n",
      "       1165.76588431, 1187.36011924, 1119.93837473, 1222.3145851 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1244.3351939 , 1312.33941447, 1281.18593196, 1212.85492579,\n",
      "       1200.65028141, 1209.82890306, 1251.02415764, 1273.27157451,\n",
      "       1224.26355311, 1230.12515941, 1347.48992924, 1375.06385378,\n",
      "       1310.02570673, 1264.10167232, 1256.72678598, 1281.80209639,\n",
      "       1211.59839264, 1217.24591029, 1159.56808269, 1234.58683511,\n",
      "       1287.97014211, 1243.51458296, 1324.96513129, 1227.54195237,\n",
      "       1162.62515194, 1235.21876337, 1310.6568578 , 1365.64444403,\n",
      "       1277.39691922, 1312.16121294])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1284.40235693, 1258.24029309, 1306.30410473, 1317.29103874,\n",
      "       1205.5095401 , 1310.95265197, 1365.37550819, 1272.40133942,\n",
      "       1283.12485345, 1304.96118896, 1298.22951149, 1254.67251479,\n",
      "       1231.80960079, 1259.72667505, 1333.16528255, 1330.27488592,\n",
      "       1276.61798394, 1314.085094  , 1156.2090095 , 1281.73607348])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1260.71384385, 1155.78393818, 1077.27025203, 1102.0871485 ,\n",
      "       1149.52269827, 1180.35512515, 1165.76588431, 1187.36011924,\n",
      "       1119.93837473, 1222.3145851 , 1200.14197303, 1235.4790309 ,\n",
      "       1220.45414496, 1213.51266018, 1220.77975695, 1217.94490589,\n",
      "       1226.27908317, 1265.20996166, 1224.93114943, 1229.45285265,\n",
      "       1198.54177021, 1259.40176692, 1185.32401861, 1268.59401946,\n",
      "       1225.15858003, 1123.93523893, 1168.53758729, 1157.02002074,\n",
      "       1219.95678964, 1177.3895382 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1161.08282259, 1090.95462499, 1132.33430313, 1061.19468042,\n",
      "       1065.20021327, 1195.39265367, 1248.97313025, 1240.25792575,\n",
      "       1166.21782504, 1182.54630196, 1200.76216394, 1146.92494919,\n",
      "       1256.28649583, 1279.43022998, 1219.86918036, 1209.6165152 ,\n",
      "       1221.40755224, 1114.1366431 , 1156.9080603 , 1167.58371861])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1185.32401861, 1268.59401946, 1225.15858003, 1123.93523893,\n",
      "       1168.53758729, 1157.02002074, 1219.95678964, 1177.3895382 ,\n",
      "       1161.08282259, 1090.95462499, 1132.33430313, 1061.19468042,\n",
      "       1065.20021327, 1195.39265367, 1248.97313025, 1240.25792575,\n",
      "       1166.21782504, 1182.54630196, 1200.76216394, 1146.92494919,\n",
      "       1256.28649583, 1279.43022998, 1219.86918036, 1209.6165152 ,\n",
      "       1221.40755224, 1114.1366431 , 1156.9080603 , 1167.58371861,\n",
      "       1147.26345236, 1176.87866827])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1295.60874377, 1285.39830312, 1208.39997965, 1238.54035826,\n",
      "       1300.79793746, 1298.33604137, 1249.36545257, 1196.01140311,\n",
      "       1245.95065635, 1257.53178452, 1280.48604928, 1279.16542224,\n",
      "       1296.79142958, 1227.31536253, 1287.8747474 , 1267.99229358,\n",
      "       1343.20395591, 1244.3351939 , 1312.33941447, 1281.18593196])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1258.24029309, 1306.30410473, 1317.29103874, 1205.5095401 ,\n",
      "       1310.95265197, 1365.37550819, 1272.40133942, 1283.12485345,\n",
      "       1304.96118896, 1298.22951149, 1254.67251479, 1231.80960079,\n",
      "       1259.72667505, 1333.16528255, 1330.27488592, 1276.61798394,\n",
      "       1314.085094  , 1156.2090095 , 1281.73607348, 1213.66539508,\n",
      "       1231.51440224, 1217.67272786, 1196.53079239, 1314.36976499,\n",
      "       1285.83712486, 1300.15992366, 1295.77968745, 1316.27883188,\n",
      "       1307.81557175, 1234.09948608])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1318.68039141, 1289.74344268, 1212.14524777, 1243.50409428,\n",
      "       1360.53893262, 1372.95605223, 1307.19100233, 1282.26642324,\n",
      "       1280.82367109, 1400.90086855, 1368.69029468, 1343.50420286,\n",
      "       1372.03403984, 1345.81936368, 1306.9290217 , 1298.4278982 ,\n",
      "       1304.96516988, 1310.0966879 , 1350.30219671, 1334.28085345])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1204.81004501, 1186.39807387, 1208.3644474 , 1225.51802637,\n",
      "       1240.98008352, 1210.89558316, 1156.64225   , 1153.90637439,\n",
      "       1211.43040122, 1134.72817155, 1155.09827511, 1138.12562284,\n",
      "       1146.38320567, 1216.96420579, 1191.58212951, 1166.49606169,\n",
      "       1092.18467862, 1167.0925951 , 1203.52277271, 1283.56378373,\n",
      "       1195.20997238, 1174.88613873, 1182.27617431, 1250.21616117,\n",
      "       1196.58470937, 1184.87301985, 1206.17887719, 1193.51503893,\n",
      "       1155.23528584, 1173.69540801])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1230.05262276, 1146.91276357, 1164.0737293 , 1142.68370924,\n",
      "       1222.57762927, 1126.8284362 , 1123.38586604, 1184.52339898,\n",
      "       1266.06107518, 1214.66957735, 1161.19412261, 1269.16771977,\n",
      "       1154.08837153, 1053.80799635, 1161.52993384, 1167.57896682,\n",
      "       1129.95184813, 1174.24243267, 1186.96621446, 1142.64760783])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1213.51266018, 1220.77975695, 1217.94490589, 1226.27908317,\n",
      "       1265.20996166, 1224.93114943, 1229.45285265, 1198.54177021,\n",
      "       1259.40176692, 1185.32401861, 1268.59401946, 1225.15858003,\n",
      "       1123.93523893, 1168.53758729, 1157.02002074, 1219.95678964,\n",
      "       1177.3895382 , 1161.08282259, 1090.95462499, 1132.33430313,\n",
      "       1061.19468042, 1065.20021327, 1195.39265367, 1248.97313025,\n",
      "       1240.25792575, 1166.21782504, 1182.54630196, 1200.76216394,\n",
      "       1146.92494919, 1256.28649583])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1279.43022998, 1219.86918036, 1209.6165152 , 1221.40755224,\n",
      "       1114.1366431 , 1156.9080603 , 1167.58371861, 1147.26345236,\n",
      "       1176.87866827, 1295.60874377, 1285.39830312, 1208.39997965,\n",
      "       1238.54035826, 1300.79793746, 1298.33604137, 1249.36545257,\n",
      "       1196.01140311, 1245.95065635, 1257.53178452, 1280.48604928])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1224.26355311, 1230.12515941, 1347.48992924, 1375.06385378,\n",
      "       1310.02570673, 1264.10167232, 1256.72678598, 1281.80209639,\n",
      "       1211.59839264, 1217.24591029, 1159.56808269, 1234.58683511,\n",
      "       1287.97014211, 1243.51458296, 1324.96513129, 1227.54195237,\n",
      "       1162.62515194, 1235.21876337, 1310.6568578 , 1365.64444403,\n",
      "       1277.39691922, 1312.16121294, 1284.40235693, 1258.24029309,\n",
      "       1306.30410473, 1317.29103874, 1205.5095401 , 1310.95265197,\n",
      "       1365.37550819, 1272.40133942])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1283.12485345, 1304.96118896, 1298.22951149, 1254.67251479,\n",
      "       1231.80960079, 1259.72667505, 1333.16528255, 1330.27488592,\n",
      "       1276.61798394, 1314.085094  , 1156.2090095 , 1281.73607348,\n",
      "       1213.66539508, 1231.51440224, 1217.67272786, 1196.53079239,\n",
      "       1314.36976499, 1285.83712486, 1300.15992366, 1295.77968745])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1162.62515194, 1235.21876337, 1310.6568578 , 1365.64444403,\n",
      "       1277.39691922, 1312.16121294, 1284.40235693, 1258.24029309,\n",
      "       1306.30410473, 1317.29103874, 1205.5095401 , 1310.95265197,\n",
      "       1365.37550819, 1272.40133942, 1283.12485345, 1304.96118896,\n",
      "       1298.22951149, 1254.67251479, 1231.80960079, 1259.72667505,\n",
      "       1333.16528255, 1330.27488592, 1276.61798394, 1314.085094  ,\n",
      "       1156.2090095 , 1281.73607348, 1213.66539508, 1231.51440224,\n",
      "       1217.67272786, 1196.53079239])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1314.36976499, 1285.83712486, 1300.15992366, 1295.77968745,\n",
      "       1316.27883188, 1307.81557175, 1234.09948608, 1318.68039141,\n",
      "       1289.74344268, 1212.14524777, 1243.50409428, 1360.53893262,\n",
      "       1372.95605223, 1307.19100233, 1282.26642324, 1280.82367109,\n",
      "       1400.90086855, 1368.69029468, 1343.50420286, 1372.03403984])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([589.67698049, 644.77612963, 561.24317062, 596.79057361,\n",
      "       568.82052745, 576.8558281 , 622.05026979, 594.06184224,\n",
      "       602.02666574, 680.36988082, 633.39966005, 675.71884524,\n",
      "       605.40908925, 650.71947409, 727.41228097, 557.18612905,\n",
      "       562.01216424, 655.97105619, 650.10028912, 668.16176824,\n",
      "       631.85717443, 648.09903429, 649.27642821, 712.67236658,\n",
      "       696.27975741, 686.1849128 , 649.98371354, 631.18617055,\n",
      "       631.07502274, 673.75730796])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([651.98129603, 673.73626744, 777.07324188, 758.2234969 ,\n",
      "       680.18442364, 729.54997166, 678.89672873, 568.29571713,\n",
      "       581.78740582, 555.83498193, 615.91695795, 665.67546333,\n",
      "       761.52671654, 733.19317991, 684.17926491, 723.09299308,\n",
      "       592.89210242, 653.62634842, 724.79417847, 630.92834603])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1243.51458296, 1324.96513129, 1227.54195237, 1162.62515194,\n",
      "       1235.21876337, 1310.6568578 , 1365.64444403, 1277.39691922,\n",
      "       1312.16121294, 1284.40235693, 1258.24029309, 1306.30410473,\n",
      "       1317.29103874, 1205.5095401 , 1310.95265197, 1365.37550819,\n",
      "       1272.40133942, 1283.12485345, 1304.96118896, 1298.22951149,\n",
      "       1254.67251479, 1231.80960079, 1259.72667505, 1333.16528255,\n",
      "       1330.27488592, 1276.61798394, 1314.085094  , 1156.2090095 ,\n",
      "       1281.73607348, 1213.66539508])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1231.51440224, 1217.67272786, 1196.53079239, 1314.36976499,\n",
      "       1285.83712486, 1300.15992366, 1295.77968745, 1316.27883188,\n",
      "       1307.81557175, 1234.09948608, 1318.68039141, 1289.74344268,\n",
      "       1212.14524777, 1243.50409428, 1360.53893262, 1372.95605223,\n",
      "       1307.19100233, 1282.26642324, 1280.82367109, 1400.90086855])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1200.14197303, 1235.4790309 , 1220.45414496, 1213.51266018,\n",
      "       1220.77975695, 1217.94490589, 1226.27908317, 1265.20996166,\n",
      "       1224.93114943, 1229.45285265, 1198.54177021, 1259.40176692,\n",
      "       1185.32401861, 1268.59401946, 1225.15858003, 1123.93523893,\n",
      "       1168.53758729, 1157.02002074, 1219.95678964, 1177.3895382 ,\n",
      "       1161.08282259, 1090.95462499, 1132.33430313, 1061.19468042,\n",
      "       1065.20021327, 1195.39265367, 1248.97313025, 1240.25792575,\n",
      "       1166.21782504, 1182.54630196])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1200.76216394, 1146.92494919, 1256.28649583, 1279.43022998,\n",
      "       1219.86918036, 1209.6165152 , 1221.40755224, 1114.1366431 ,\n",
      "       1156.9080603 , 1167.58371861, 1147.26345236, 1176.87866827,\n",
      "       1295.60874377, 1285.39830312, 1208.39997965, 1238.54035826,\n",
      "       1300.79793746, 1298.33604137, 1249.36545257, 1196.01140311])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1347.48992924, 1375.06385378, 1310.02570673, 1264.10167232,\n",
      "       1256.72678598, 1281.80209639, 1211.59839264, 1217.24591029,\n",
      "       1159.56808269, 1234.58683511, 1287.97014211, 1243.51458296,\n",
      "       1324.96513129, 1227.54195237, 1162.62515194, 1235.21876337,\n",
      "       1310.6568578 , 1365.64444403, 1277.39691922, 1312.16121294,\n",
      "       1284.40235693, 1258.24029309, 1306.30410473, 1317.29103874,\n",
      "       1205.5095401 , 1310.95265197, 1365.37550819, 1272.40133942,\n",
      "       1283.12485345, 1304.96118896])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1298.22951149, 1254.67251479, 1231.80960079, 1259.72667505,\n",
      "       1333.16528255, 1330.27488592, 1276.61798394, 1314.085094  ,\n",
      "       1156.2090095 , 1281.73607348, 1213.66539508, 1231.51440224,\n",
      "       1217.67272786, 1196.53079239, 1314.36976499, 1285.83712486,\n",
      "       1300.15992366, 1295.77968745, 1316.27883188, 1307.81557175])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1265.20996166, 1224.93114943, 1229.45285265, 1198.54177021,\n",
      "       1259.40176692, 1185.32401861, 1268.59401946, 1225.15858003,\n",
      "       1123.93523893, 1168.53758729, 1157.02002074, 1219.95678964,\n",
      "       1177.3895382 , 1161.08282259, 1090.95462499, 1132.33430313,\n",
      "       1061.19468042, 1065.20021327, 1195.39265367, 1248.97313025,\n",
      "       1240.25792575, 1166.21782504, 1182.54630196, 1200.76216394,\n",
      "       1146.92494919, 1256.28649583, 1279.43022998, 1219.86918036,\n",
      "       1209.6165152 , 1221.40755224])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1114.1366431 , 1156.9080603 , 1167.58371861, 1147.26345236,\n",
      "       1176.87866827, 1295.60874377, 1285.39830312, 1208.39997965,\n",
      "       1238.54035826, 1300.79793746, 1298.33604137, 1249.36545257,\n",
      "       1196.01140311, 1245.95065635, 1257.53178452, 1280.48604928,\n",
      "       1279.16542224, 1296.79142958, 1227.31536253, 1287.8747474 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1266.06107518, 1214.66957735, 1161.19412261, 1269.16771977,\n",
      "       1154.08837153, 1053.80799635, 1161.52993384, 1167.57896682,\n",
      "       1129.95184813, 1174.24243267, 1186.96621446, 1142.64760783,\n",
      "       1090.36430471, 1180.46179222, 1213.31148268, 1178.18182074,\n",
      "       1260.71384385, 1155.78393818, 1077.27025203, 1102.0871485 ,\n",
      "       1149.52269827, 1180.35512515, 1165.76588431, 1187.36011924,\n",
      "       1119.93837473, 1222.3145851 , 1200.14197303, 1235.4790309 ,\n",
      "       1220.45414496, 1213.51266018])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1220.77975695, 1217.94490589, 1226.27908317, 1265.20996166,\n",
      "       1224.93114943, 1229.45285265, 1198.54177021, 1259.40176692,\n",
      "       1185.32401861, 1268.59401946, 1225.15858003, 1123.93523893,\n",
      "       1168.53758729, 1157.02002074, 1219.95678964, 1177.3895382 ,\n",
      "       1161.08282259, 1090.95462499, 1132.33430313, 1061.19468042])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1059.79104685, 1111.27315424, 1024.00153111,  971.80503587,\n",
      "       1026.55935047, 1002.37166937, 1044.2419652 , 1047.00466984,\n",
      "        956.61527463, 1073.01406201, 1146.43226916, 1041.31184508,\n",
      "       1005.12645319, 1037.76355434, 1054.73861892, 1074.97384986,\n",
      "       1150.99978669, 1151.27744666, 1079.38867994, 1032.6191415 ,\n",
      "       1050.25584071, 1097.33429888, 1205.78161584, 1133.46971939,\n",
      "       1128.59352223, 1127.96863513, 1184.90842814, 1277.93386638,\n",
      "       1159.17684299, 1102.26127385])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1172.30186416, 1186.23552455, 1242.01460042, 1205.17205015,\n",
      "       1136.09652267, 1161.34722065, 1204.85291596, 1204.81004501,\n",
      "       1186.39807387, 1208.3644474 , 1225.51802637, 1240.98008352,\n",
      "       1210.89558316, 1156.64225   , 1153.90637439, 1211.43040122,\n",
      "       1134.72817155, 1155.09827511, 1138.12562284, 1146.38320567])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1206.17887719, 1193.51503893, 1155.23528584, 1173.69540801,\n",
      "       1230.05262276, 1146.91276357, 1164.0737293 , 1142.68370924,\n",
      "       1222.57762927, 1126.8284362 , 1123.38586604, 1184.52339898,\n",
      "       1266.06107518, 1214.66957735, 1161.19412261, 1269.16771977,\n",
      "       1154.08837153, 1053.80799635, 1161.52993384, 1167.57896682,\n",
      "       1129.95184813, 1174.24243267, 1186.96621446, 1142.64760783,\n",
      "       1090.36430471, 1180.46179222, 1213.31148268, 1178.18182074,\n",
      "       1260.71384385, 1155.78393818])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1077.27025203, 1102.0871485 , 1149.52269827, 1180.35512515,\n",
      "       1165.76588431, 1187.36011924, 1119.93837473, 1222.3145851 ,\n",
      "       1200.14197303, 1235.4790309 , 1220.45414496, 1213.51266018,\n",
      "       1220.77975695, 1217.94490589, 1226.27908317, 1265.20996166,\n",
      "       1224.93114943, 1229.45285265, 1198.54177021, 1259.40176692])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1090.36430471, 1180.46179222, 1213.31148268, 1178.18182074,\n",
      "       1260.71384385, 1155.78393818, 1077.27025203, 1102.0871485 ,\n",
      "       1149.52269827, 1180.35512515, 1165.76588431, 1187.36011924,\n",
      "       1119.93837473, 1222.3145851 , 1200.14197303, 1235.4790309 ,\n",
      "       1220.45414496, 1213.51266018, 1220.77975695, 1217.94490589,\n",
      "       1226.27908317, 1265.20996166, 1224.93114943, 1229.45285265,\n",
      "       1198.54177021, 1259.40176692, 1185.32401861, 1268.59401946,\n",
      "       1225.15858003, 1123.93523893])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1168.53758729, 1157.02002074, 1219.95678964, 1177.3895382 ,\n",
      "       1161.08282259, 1090.95462499, 1132.33430313, 1061.19468042,\n",
      "       1065.20021327, 1195.39265367, 1248.97313025, 1240.25792575,\n",
      "       1166.21782504, 1182.54630196, 1200.76216394, 1146.92494919,\n",
      "       1256.28649583, 1279.43022998, 1219.86918036, 1209.6165152 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1209.82890306, 1251.02415764, 1273.27157451, 1224.26355311,\n",
      "       1230.12515941, 1347.48992924, 1375.06385378, 1310.02570673,\n",
      "       1264.10167232, 1256.72678598, 1281.80209639, 1211.59839264,\n",
      "       1217.24591029, 1159.56808269, 1234.58683511, 1287.97014211,\n",
      "       1243.51458296, 1324.96513129, 1227.54195237, 1162.62515194,\n",
      "       1235.21876337, 1310.6568578 , 1365.64444403, 1277.39691922,\n",
      "       1312.16121294, 1284.40235693, 1258.24029309, 1306.30410473,\n",
      "       1317.29103874, 1205.5095401 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1310.95265197, 1365.37550819, 1272.40133942, 1283.12485345,\n",
      "       1304.96118896, 1298.22951149, 1254.67251479, 1231.80960079,\n",
      "       1259.72667505, 1333.16528255, 1330.27488592, 1276.61798394,\n",
      "       1314.085094  , 1156.2090095 , 1281.73607348, 1213.66539508,\n",
      "       1231.51440224, 1217.67272786, 1196.53079239, 1314.36976499])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1008.36789473,  996.67797377, 1034.79060041,  961.82867089,\n",
      "       1001.79836244, 1019.2776188 , 1035.55514077, 1033.78454565,\n",
      "       1139.54181843, 1032.6695729 ,  979.85710706,  970.66424996,\n",
      "        971.55787089,  969.39897322, 1059.79104685, 1111.27315424,\n",
      "       1024.00153111,  971.80503587, 1026.55935047, 1002.37166937,\n",
      "       1044.2419652 , 1047.00466984,  956.61527463, 1073.01406201,\n",
      "       1146.43226916, 1041.31184508, 1005.12645319, 1037.76355434,\n",
      "       1054.73861892, 1074.97384986])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1150.99978669, 1151.27744666, 1079.38867994, 1032.6191415 ,\n",
      "       1050.25584071, 1097.33429888, 1205.78161584, 1133.46971939,\n",
      "       1128.59352223, 1127.96863513, 1184.90842814, 1277.93386638,\n",
      "       1159.17684299, 1102.26127385, 1172.30186416, 1186.23552455,\n",
      "       1242.01460042, 1205.17205015, 1136.09652267, 1161.34722065])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1090.95462499, 1132.33430313, 1061.19468042, 1065.20021327,\n",
      "       1195.39265367, 1248.97313025, 1240.25792575, 1166.21782504,\n",
      "       1182.54630196, 1200.76216394, 1146.92494919, 1256.28649583,\n",
      "       1279.43022998, 1219.86918036, 1209.6165152 , 1221.40755224,\n",
      "       1114.1366431 , 1156.9080603 , 1167.58371861, 1147.26345236,\n",
      "       1176.87866827, 1295.60874377, 1285.39830312, 1208.39997965,\n",
      "       1238.54035826, 1300.79793746, 1298.33604137, 1249.36545257,\n",
      "       1196.01140311, 1245.95065635])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1257.53178452, 1280.48604928, 1279.16542224, 1296.79142958,\n",
      "       1227.31536253, 1287.8747474 , 1267.99229358, 1343.20395591,\n",
      "       1244.3351939 , 1312.33941447, 1281.18593196, 1212.85492579,\n",
      "       1200.65028141, 1209.82890306, 1251.02415764, 1273.27157451,\n",
      "       1224.26355311, 1230.12515941, 1347.48992924, 1375.06385378])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1102.0871485 , 1149.52269827, 1180.35512515, 1165.76588431,\n",
      "       1187.36011924, 1119.93837473, 1222.3145851 , 1200.14197303,\n",
      "       1235.4790309 , 1220.45414496, 1213.51266018, 1220.77975695,\n",
      "       1217.94490589, 1226.27908317, 1265.20996166, 1224.93114943,\n",
      "       1229.45285265, 1198.54177021, 1259.40176692, 1185.32401861,\n",
      "       1268.59401946, 1225.15858003, 1123.93523893, 1168.53758729,\n",
      "       1157.02002074, 1219.95678964, 1177.3895382 , 1161.08282259,\n",
      "       1090.95462499, 1132.33430313])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1061.19468042, 1065.20021327, 1195.39265367, 1248.97313025,\n",
      "       1240.25792575, 1166.21782504, 1182.54630196, 1200.76216394,\n",
      "       1146.92494919, 1256.28649583, 1279.43022998, 1219.86918036,\n",
      "       1209.6165152 , 1221.40755224, 1114.1366431 , 1156.9080603 ,\n",
      "       1167.58371861, 1147.26345236, 1176.87866827, 1295.60874377])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1217.24591029, 1159.56808269, 1234.58683511, 1287.97014211,\n",
      "       1243.51458296, 1324.96513129, 1227.54195237, 1162.62515194,\n",
      "       1235.21876337, 1310.6568578 , 1365.64444403, 1277.39691922,\n",
      "       1312.16121294, 1284.40235693, 1258.24029309, 1306.30410473,\n",
      "       1317.29103874, 1205.5095401 , 1310.95265197, 1365.37550819,\n",
      "       1272.40133942, 1283.12485345, 1304.96118896, 1298.22951149,\n",
      "       1254.67251479, 1231.80960079, 1259.72667505, 1333.16528255,\n",
      "       1330.27488592, 1276.61798394])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1314.085094  , 1156.2090095 , 1281.73607348, 1213.66539508,\n",
      "       1231.51440224, 1217.67272786, 1196.53079239, 1314.36976499,\n",
      "       1285.83712486, 1300.15992366, 1295.77968745, 1316.27883188,\n",
      "       1307.81557175, 1234.09948608, 1318.68039141, 1289.74344268,\n",
      "       1212.14524777, 1243.50409428, 1360.53893262, 1372.95605223])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1312.16121294, 1284.40235693, 1258.24029309, 1306.30410473,\n",
      "       1317.29103874, 1205.5095401 , 1310.95265197, 1365.37550819,\n",
      "       1272.40133942, 1283.12485345, 1304.96118896, 1298.22951149,\n",
      "       1254.67251479, 1231.80960079, 1259.72667505, 1333.16528255,\n",
      "       1330.27488592, 1276.61798394, 1314.085094  , 1156.2090095 ,\n",
      "       1281.73607348, 1213.66539508, 1231.51440224, 1217.67272786,\n",
      "       1196.53079239, 1314.36976499, 1285.83712486, 1300.15992366,\n",
      "       1295.77968745, 1316.27883188])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1307.81557175, 1234.09948608, 1318.68039141, 1289.74344268,\n",
      "       1212.14524777, 1243.50409428, 1360.53893262, 1372.95605223,\n",
      "       1307.19100233, 1282.26642324, 1280.82367109, 1400.90086855,\n",
      "       1368.69029468, 1343.50420286, 1372.03403984, 1345.81936368,\n",
      "       1306.9290217 , 1298.4278982 , 1304.96516988, 1310.0966879 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1097.33429888, 1205.78161584, 1133.46971939, 1128.59352223,\n",
      "       1127.96863513, 1184.90842814, 1277.93386638, 1159.17684299,\n",
      "       1102.26127385, 1172.30186416, 1186.23552455, 1242.01460042,\n",
      "       1205.17205015, 1136.09652267, 1161.34722065, 1204.85291596,\n",
      "       1204.81004501, 1186.39807387, 1208.3644474 , 1225.51802637,\n",
      "       1240.98008352, 1210.89558316, 1156.64225   , 1153.90637439,\n",
      "       1211.43040122, 1134.72817155, 1155.09827511, 1138.12562284,\n",
      "       1146.38320567, 1216.96420579])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1191.58212951, 1166.49606169, 1092.18467862, 1167.0925951 ,\n",
      "       1203.52277271, 1283.56378373, 1195.20997238, 1174.88613873,\n",
      "       1182.27617431, 1250.21616117, 1196.58470937, 1184.87301985,\n",
      "       1206.17887719, 1193.51503893, 1155.23528584, 1173.69540801,\n",
      "       1230.05262276, 1146.91276357, 1164.0737293 , 1142.68370924])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1287.97014211, 1243.51458296, 1324.96513129, 1227.54195237,\n",
      "       1162.62515194, 1235.21876337, 1310.6568578 , 1365.64444403,\n",
      "       1277.39691922, 1312.16121294, 1284.40235693, 1258.24029309,\n",
      "       1306.30410473, 1317.29103874, 1205.5095401 , 1310.95265197,\n",
      "       1365.37550819, 1272.40133942, 1283.12485345, 1304.96118896,\n",
      "       1298.22951149, 1254.67251479, 1231.80960079, 1259.72667505,\n",
      "       1333.16528255, 1330.27488592, 1276.61798394, 1314.085094  ,\n",
      "       1156.2090095 , 1281.73607348])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1213.66539508, 1231.51440224, 1217.67272786, 1196.53079239,\n",
      "       1314.36976499, 1285.83712486, 1300.15992366, 1295.77968745,\n",
      "       1316.27883188, 1307.81557175, 1234.09948608, 1318.68039141,\n",
      "       1289.74344268, 1212.14524777, 1243.50409428, 1360.53893262,\n",
      "       1372.95605223, 1307.19100233, 1282.26642324, 1280.82367109])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1224.93114943, 1229.45285265, 1198.54177021, 1259.40176692,\n",
      "       1185.32401861, 1268.59401946, 1225.15858003, 1123.93523893,\n",
      "       1168.53758729, 1157.02002074, 1219.95678964, 1177.3895382 ,\n",
      "       1161.08282259, 1090.95462499, 1132.33430313, 1061.19468042,\n",
      "       1065.20021327, 1195.39265367, 1248.97313025, 1240.25792575,\n",
      "       1166.21782504, 1182.54630196, 1200.76216394, 1146.92494919,\n",
      "       1256.28649583, 1279.43022998, 1219.86918036, 1209.6165152 ,\n",
      "       1221.40755224, 1114.1366431 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1156.9080603 , 1167.58371861, 1147.26345236, 1176.87866827,\n",
      "       1295.60874377, 1285.39830312, 1208.39997965, 1238.54035826,\n",
      "       1300.79793746, 1298.33604137, 1249.36545257, 1196.01140311,\n",
      "       1245.95065635, 1257.53178452, 1280.48604928, 1279.16542224,\n",
      "       1296.79142958, 1227.31536253, 1287.8747474 , 1267.99229358])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1250.21616117, 1196.58470937, 1184.87301985, 1206.17887719,\n",
      "       1193.51503893, 1155.23528584, 1173.69540801, 1230.05262276,\n",
      "       1146.91276357, 1164.0737293 , 1142.68370924, 1222.57762927,\n",
      "       1126.8284362 , 1123.38586604, 1184.52339898, 1266.06107518,\n",
      "       1214.66957735, 1161.19412261, 1269.16771977, 1154.08837153,\n",
      "       1053.80799635, 1161.52993384, 1167.57896682, 1129.95184813,\n",
      "       1174.24243267, 1186.96621446, 1142.64760783, 1090.36430471,\n",
      "       1180.46179222, 1213.31148268])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1178.18182074, 1260.71384385, 1155.78393818, 1077.27025203,\n",
      "       1102.0871485 , 1149.52269827, 1180.35512515, 1165.76588431,\n",
      "       1187.36011924, 1119.93837473, 1222.3145851 , 1200.14197303,\n",
      "       1235.4790309 , 1220.45414496, 1213.51266018, 1220.77975695,\n",
      "       1217.94490589, 1226.27908317, 1265.20996166, 1224.93114943])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 921.02414826,  957.69581689, 1019.86652719,  997.83420854,\n",
      "       1033.61470903,  975.19803339, 1041.13401513, 1123.42793916,\n",
      "       1016.12489876,  960.41402604, 1046.97405111, 1092.88908797,\n",
      "        997.59030213,  960.7578993 ,  964.83221813,  914.84459202,\n",
      "        914.65709275,  917.36114995,  928.99988218,  971.59260492,\n",
      "       1002.54363929, 1077.50206276,  981.41467242, 1031.38843481,\n",
      "       1008.36789473,  996.67797377, 1034.79060041,  961.82867089,\n",
      "       1001.79836244, 1019.2776188 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1035.55514077, 1033.78454565, 1139.54181843, 1032.6695729 ,\n",
      "        979.85710706,  970.66424996,  971.55787089,  969.39897322,\n",
      "       1059.79104685, 1111.27315424, 1024.00153111,  971.80503587,\n",
      "       1026.55935047, 1002.37166937, 1044.2419652 , 1047.00466984,\n",
      "        956.61527463, 1073.01406201, 1146.43226916, 1041.31184508])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1257.53178452, 1280.48604928, 1279.16542224, 1296.79142958,\n",
      "       1227.31536253, 1287.8747474 , 1267.99229358, 1343.20395591,\n",
      "       1244.3351939 , 1312.33941447, 1281.18593196, 1212.85492579,\n",
      "       1200.65028141, 1209.82890306, 1251.02415764, 1273.27157451,\n",
      "       1224.26355311, 1230.12515941, 1347.48992924, 1375.06385378,\n",
      "       1310.02570673, 1264.10167232, 1256.72678598, 1281.80209639,\n",
      "       1211.59839264, 1217.24591029, 1159.56808269, 1234.58683511,\n",
      "       1287.97014211, 1243.51458296])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1324.96513129, 1227.54195237, 1162.62515194, 1235.21876337,\n",
      "       1310.6568578 , 1365.64444403, 1277.39691922, 1312.16121294,\n",
      "       1284.40235693, 1258.24029309, 1306.30410473, 1317.29103874,\n",
      "       1205.5095401 , 1310.95265197, 1365.37550819, 1272.40133942,\n",
      "       1283.12485345, 1304.96118896, 1298.22951149, 1254.67251479])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1281.80209639, 1211.59839264, 1217.24591029, 1159.56808269,\n",
      "       1234.58683511, 1287.97014211, 1243.51458296, 1324.96513129,\n",
      "       1227.54195237, 1162.62515194, 1235.21876337, 1310.6568578 ,\n",
      "       1365.64444403, 1277.39691922, 1312.16121294, 1284.40235693,\n",
      "       1258.24029309, 1306.30410473, 1317.29103874, 1205.5095401 ,\n",
      "       1310.95265197, 1365.37550819, 1272.40133942, 1283.12485345,\n",
      "       1304.96118896, 1298.22951149, 1254.67251479, 1231.80960079,\n",
      "       1259.72667505, 1333.16528255])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1330.27488592, 1276.61798394, 1314.085094  , 1156.2090095 ,\n",
      "       1281.73607348, 1213.66539508, 1231.51440224, 1217.67272786,\n",
      "       1196.53079239, 1314.36976499, 1285.83712486, 1300.15992366,\n",
      "       1295.77968745, 1316.27883188, 1307.81557175, 1234.09948608,\n",
      "       1318.68039141, 1289.74344268, 1212.14524777, 1243.50409428])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1187.36011924, 1119.93837473, 1222.3145851 , 1200.14197303,\n",
      "       1235.4790309 , 1220.45414496, 1213.51266018, 1220.77975695,\n",
      "       1217.94490589, 1226.27908317, 1265.20996166, 1224.93114943,\n",
      "       1229.45285265, 1198.54177021, 1259.40176692, 1185.32401861,\n",
      "       1268.59401946, 1225.15858003, 1123.93523893, 1168.53758729,\n",
      "       1157.02002074, 1219.95678964, 1177.3895382 , 1161.08282259,\n",
      "       1090.95462499, 1132.33430313, 1061.19468042, 1065.20021327,\n",
      "       1195.39265367, 1248.97313025])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1240.25792575, 1166.21782504, 1182.54630196, 1200.76216394,\n",
      "       1146.92494919, 1256.28649583, 1279.43022998, 1219.86918036,\n",
      "       1209.6165152 , 1221.40755224, 1114.1366431 , 1156.9080603 ,\n",
      "       1167.58371861, 1147.26345236, 1176.87866827, 1295.60874377,\n",
      "       1285.39830312, 1208.39997965, 1238.54035826, 1300.79793746])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1229.45285265, 1198.54177021, 1259.40176692, 1185.32401861,\n",
      "       1268.59401946, 1225.15858003, 1123.93523893, 1168.53758729,\n",
      "       1157.02002074, 1219.95678964, 1177.3895382 , 1161.08282259,\n",
      "       1090.95462499, 1132.33430313, 1061.19468042, 1065.20021327,\n",
      "       1195.39265367, 1248.97313025, 1240.25792575, 1166.21782504,\n",
      "       1182.54630196, 1200.76216394, 1146.92494919, 1256.28649583,\n",
      "       1279.43022998, 1219.86918036, 1209.6165152 , 1221.40755224,\n",
      "       1114.1366431 , 1156.9080603 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1167.58371861, 1147.26345236, 1176.87866827, 1295.60874377,\n",
      "       1285.39830312, 1208.39997965, 1238.54035826, 1300.79793746,\n",
      "       1298.33604137, 1249.36545257, 1196.01140311, 1245.95065635,\n",
      "       1257.53178452, 1280.48604928, 1279.16542224, 1296.79142958,\n",
      "       1227.31536253, 1287.8747474 , 1267.99229358, 1343.20395591])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1167.58371861, 1147.26345236, 1176.87866827, 1295.60874377,\n",
      "       1285.39830312, 1208.39997965, 1238.54035826, 1300.79793746,\n",
      "       1298.33604137, 1249.36545257, 1196.01140311, 1245.95065635,\n",
      "       1257.53178452, 1280.48604928, 1279.16542224, 1296.79142958,\n",
      "       1227.31536253, 1287.8747474 , 1267.99229358, 1343.20395591,\n",
      "       1244.3351939 , 1312.33941447, 1281.18593196, 1212.85492579,\n",
      "       1200.65028141, 1209.82890306, 1251.02415764, 1273.27157451,\n",
      "       1224.26355311, 1230.12515941])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1347.48992924, 1375.06385378, 1310.02570673, 1264.10167232,\n",
      "       1256.72678598, 1281.80209639, 1211.59839264, 1217.24591029,\n",
      "       1159.56808269, 1234.58683511, 1287.97014211, 1243.51458296,\n",
      "       1324.96513129, 1227.54195237, 1162.62515194, 1235.21876337,\n",
      "       1310.6568578 , 1365.64444403, 1277.39691922, 1312.16121294])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 957.00092337,  961.84957712,  887.94565068,  866.15942091,\n",
      "        972.96430377,  964.76263466,  985.10560512,  967.39478882,\n",
      "        956.53369828, 1001.37762455,  949.18365111,  952.55314568,\n",
      "        935.12600406,  926.72522669,  933.22378088,  963.24465376,\n",
      "        941.21957118, 1015.54262546,  944.97319267,  940.49716249,\n",
      "        938.69094632, 1029.24446491, 1007.91049914, 1029.7894881 ,\n",
      "        921.02414826,  957.69581689, 1019.86652719,  997.83420854,\n",
      "       1033.61470903,  975.19803339])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1041.13401513, 1123.42793916, 1016.12489876,  960.41402604,\n",
      "       1046.97405111, 1092.88908797,  997.59030213,  960.7578993 ,\n",
      "        964.83221813,  914.84459202,  914.65709275,  917.36114995,\n",
      "        928.99988218,  971.59260492, 1002.54363929, 1077.50206276,\n",
      "        981.41467242, 1031.38843481, 1008.36789473,  996.67797377])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1203.52277271, 1283.56378373, 1195.20997238, 1174.88613873,\n",
      "       1182.27617431, 1250.21616117, 1196.58470937, 1184.87301985,\n",
      "       1206.17887719, 1193.51503893, 1155.23528584, 1173.69540801,\n",
      "       1230.05262276, 1146.91276357, 1164.0737293 , 1142.68370924,\n",
      "       1222.57762927, 1126.8284362 , 1123.38586604, 1184.52339898,\n",
      "       1266.06107518, 1214.66957735, 1161.19412261, 1269.16771977,\n",
      "       1154.08837153, 1053.80799635, 1161.52993384, 1167.57896682,\n",
      "       1129.95184813, 1174.24243267])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1186.96621446, 1142.64760783, 1090.36430471, 1180.46179222,\n",
      "       1213.31148268, 1178.18182074, 1260.71384385, 1155.78393818,\n",
      "       1077.27025203, 1102.0871485 , 1149.52269827, 1180.35512515,\n",
      "       1165.76588431, 1187.36011924, 1119.93837473, 1222.3145851 ,\n",
      "       1200.14197303, 1235.4790309 , 1220.45414496, 1213.51266018])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 818.00754688,  784.76478819,  821.16300342,  868.15554952,\n",
      "        909.64502365,  861.98607758,  884.23519565,  854.64268169,\n",
      "        844.90353406,  807.12054261,  815.2239179 ,  802.85865266,\n",
      "        814.48982293,  923.60552682,  869.34745536, 1012.69305322,\n",
      "        979.26171637,  929.55501488,  868.02206953,  922.48195382,\n",
      "        889.64371938,  902.27492431, 1036.70076886,  959.19804806,\n",
      "        973.71324439,  902.93070215,  900.14532383, 1000.23202533,\n",
      "        921.42712426,  998.73708587])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 990.95733222,  911.4989099 ,  943.0991758 ,  982.24411303,\n",
      "        975.88649061,  862.21052577,  857.87276556,  893.84910538,\n",
      "        914.7797677 ,  956.00910157,  950.98553127,  869.84354752,\n",
      "        923.32683661,  967.6534664 ,  975.40865419, 1003.22990125,\n",
      "       1003.55157614,  982.53631582,  949.02568453,  858.44878802])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1054.73861892, 1074.97384986, 1150.99978669, 1151.27744666,\n",
      "       1079.38867994, 1032.6191415 , 1050.25584071, 1097.33429888,\n",
      "       1205.78161584, 1133.46971939, 1128.59352223, 1127.96863513,\n",
      "       1184.90842814, 1277.93386638, 1159.17684299, 1102.26127385,\n",
      "       1172.30186416, 1186.23552455, 1242.01460042, 1205.17205015,\n",
      "       1136.09652267, 1161.34722065, 1204.85291596, 1204.81004501,\n",
      "       1186.39807387, 1208.3644474 , 1225.51802637, 1240.98008352,\n",
      "       1210.89558316, 1156.64225   ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1153.90637439, 1211.43040122, 1134.72817155, 1155.09827511,\n",
      "       1138.12562284, 1146.38320567, 1216.96420579, 1191.58212951,\n",
      "       1166.49606169, 1092.18467862, 1167.0925951 , 1203.52277271,\n",
      "       1283.56378373, 1195.20997238, 1174.88613873, 1182.27617431,\n",
      "       1250.21616117, 1196.58470937, 1184.87301985, 1206.17887719])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([761.63590883, 706.47664247, 747.91110009, 842.84008478,\n",
      "       850.5771681 , 857.88656147, 759.24070059, 692.35079504,\n",
      "       719.83979052, 745.37251598, 804.07326495, 689.79394523,\n",
      "       795.39479652, 773.4935943 , 733.44333496, 696.48647192,\n",
      "       651.50446061, 760.04488904, 774.24036403, 698.7297109 ,\n",
      "       669.98055191, 714.36151018, 834.70411715, 785.46547122,\n",
      "       690.58578351, 723.47617913, 745.1619075 , 627.8142917 ,\n",
      "       709.36185236, 750.69673545])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([800.72938966, 880.17979644, 873.23872791, 793.98585857,\n",
      "       722.50694506, 884.27854574, 835.09507914, 790.9940369 ,\n",
      "       784.06169243, 794.03170193, 782.43241217, 755.51672945,\n",
      "       747.93387285, 773.69529753, 760.13137483, 743.52554413,\n",
      "       780.65476182, 780.51270032, 863.92123596, 695.31521813])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 866.15942091,  972.96430377,  964.76263466,  985.10560512,\n",
      "        967.39478882,  956.53369828, 1001.37762455,  949.18365111,\n",
      "        952.55314568,  935.12600406,  926.72522669,  933.22378088,\n",
      "        963.24465376,  941.21957118, 1015.54262546,  944.97319267,\n",
      "        940.49716249,  938.69094632, 1029.24446491, 1007.91049914,\n",
      "       1029.7894881 ,  921.02414826,  957.69581689, 1019.86652719,\n",
      "        997.83420854, 1033.61470903,  975.19803339, 1041.13401513,\n",
      "       1123.42793916, 1016.12489876])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 960.41402604, 1046.97405111, 1092.88908797,  997.59030213,\n",
      "        960.7578993 ,  964.83221813,  914.84459202,  914.65709275,\n",
      "        917.36114995,  928.99988218,  971.59260492, 1002.54363929,\n",
      "       1077.50206276,  981.41467242, 1031.38843481, 1008.36789473,\n",
      "        996.67797377, 1034.79060041,  961.82867089, 1001.79836244])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1119.93837473, 1222.3145851 , 1200.14197303, 1235.4790309 ,\n",
      "       1220.45414496, 1213.51266018, 1220.77975695, 1217.94490589,\n",
      "       1226.27908317, 1265.20996166, 1224.93114943, 1229.45285265,\n",
      "       1198.54177021, 1259.40176692, 1185.32401861, 1268.59401946,\n",
      "       1225.15858003, 1123.93523893, 1168.53758729, 1157.02002074,\n",
      "       1219.95678964, 1177.3895382 , 1161.08282259, 1090.95462499,\n",
      "       1132.33430313, 1061.19468042, 1065.20021327, 1195.39265367,\n",
      "       1248.97313025, 1240.25792575])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1166.21782504, 1182.54630196, 1200.76216394, 1146.92494919,\n",
      "       1256.28649583, 1279.43022998, 1219.86918036, 1209.6165152 ,\n",
      "       1221.40755224, 1114.1366431 , 1156.9080603 , 1167.58371861,\n",
      "       1147.26345236, 1176.87866827, 1295.60874377, 1285.39830312,\n",
      "       1208.39997965, 1238.54035826, 1300.79793746, 1298.33604137])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 884.23519565,  854.64268169,  844.90353406,  807.12054261,\n",
      "        815.2239179 ,  802.85865266,  814.48982293,  923.60552682,\n",
      "        869.34745536, 1012.69305322,  979.26171637,  929.55501488,\n",
      "        868.02206953,  922.48195382,  889.64371938,  902.27492431,\n",
      "       1036.70076886,  959.19804806,  973.71324439,  902.93070215,\n",
      "        900.14532383, 1000.23202533,  921.42712426,  998.73708587,\n",
      "        990.95733222,  911.4989099 ,  943.0991758 ,  982.24411303,\n",
      "        975.88649061,  862.21052577])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([ 857.87276556,  893.84910538,  914.7797677 ,  956.00910157,\n",
      "        950.98553127,  869.84354752,  923.32683661,  967.6534664 ,\n",
      "        975.40865419, 1003.22990125, 1003.55157614,  982.53631582,\n",
      "        949.02568453,  858.44878802,  927.72946126,  957.00092337,\n",
      "        961.84957712,  887.94565068,  866.15942091,  972.96430377])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1198.54177021, 1259.40176692, 1185.32401861, 1268.59401946,\n",
      "       1225.15858003, 1123.93523893, 1168.53758729, 1157.02002074,\n",
      "       1219.95678964, 1177.3895382 , 1161.08282259, 1090.95462499,\n",
      "       1132.33430313, 1061.19468042, 1065.20021327, 1195.39265367,\n",
      "       1248.97313025, 1240.25792575, 1166.21782504, 1182.54630196,\n",
      "       1200.76216394, 1146.92494919, 1256.28649583, 1279.43022998,\n",
      "       1219.86918036, 1209.6165152 , 1221.40755224, 1114.1366431 ,\n",
      "       1156.9080603 , 1167.58371861])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1147.26345236, 1176.87866827, 1295.60874377, 1285.39830312,\n",
      "       1208.39997965, 1238.54035826, 1300.79793746, 1298.33604137,\n",
      "       1249.36545257, 1196.01140311, 1245.95065635, 1257.53178452,\n",
      "       1280.48604928, 1279.16542224, 1296.79142958, 1227.31536253,\n",
      "       1287.8747474 , 1267.99229358, 1343.20395591, 1244.3351939 ])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1269.16771977, 1154.08837153, 1053.80799635, 1161.52993384,\n",
      "       1167.57896682, 1129.95184813, 1174.24243267, 1186.96621446,\n",
      "       1142.64760783, 1090.36430471, 1180.46179222, 1213.31148268,\n",
      "       1178.18182074, 1260.71384385, 1155.78393818, 1077.27025203,\n",
      "       1102.0871485 , 1149.52269827, 1180.35512515, 1165.76588431,\n",
      "       1187.36011924, 1119.93837473, 1222.3145851 , 1200.14197303,\n",
      "       1235.4790309 , 1220.45414496, 1213.51266018, 1220.77975695,\n",
      "       1217.94490589, 1226.27908317])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1265.20996166, 1224.93114943, 1229.45285265, 1198.54177021,\n",
      "       1259.40176692, 1185.32401861, 1268.59401946, 1225.15858003,\n",
      "       1123.93523893, 1168.53758729, 1157.02002074, 1219.95678964,\n",
      "       1177.3895382 , 1161.08282259, 1090.95462499, 1132.33430313,\n",
      "       1061.19468042, 1065.20021327, 1195.39265367, 1248.97313025])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1182.54630196, 1200.76216394, 1146.92494919, 1256.28649583,\n",
      "       1279.43022998, 1219.86918036, 1209.6165152 , 1221.40755224,\n",
      "       1114.1366431 , 1156.9080603 , 1167.58371861, 1147.26345236,\n",
      "       1176.87866827, 1295.60874377, 1285.39830312, 1208.39997965,\n",
      "       1238.54035826, 1300.79793746, 1298.33604137, 1249.36545257,\n",
      "       1196.01140311, 1245.95065635, 1257.53178452, 1280.48604928,\n",
      "       1279.16542224, 1296.79142958, 1227.31536253, 1287.8747474 ,\n",
      "       1267.99229358, 1343.20395591])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1244.3351939 , 1312.33941447, 1281.18593196, 1212.85492579,\n",
      "       1200.65028141, 1209.82890306, 1251.02415764, 1273.27157451,\n",
      "       1224.26355311, 1230.12515941, 1347.48992924, 1375.06385378,\n",
      "       1310.02570673, 1264.10167232, 1256.72678598, 1281.80209639,\n",
      "       1211.59839264, 1217.24591029, 1159.56808269, 1234.58683511])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1285.39830312, 1208.39997965, 1238.54035826, 1300.79793746,\n",
      "       1298.33604137, 1249.36545257, 1196.01140311, 1245.95065635,\n",
      "       1257.53178452, 1280.48604928, 1279.16542224, 1296.79142958,\n",
      "       1227.31536253, 1287.8747474 , 1267.99229358, 1343.20395591,\n",
      "       1244.3351939 , 1312.33941447, 1281.18593196, 1212.85492579,\n",
      "       1200.65028141, 1209.82890306, 1251.02415764, 1273.27157451,\n",
      "       1224.26355311, 1230.12515941, 1347.48992924, 1375.06385378,\n",
      "       1310.02570673, 1264.10167232])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1256.72678598, 1281.80209639, 1211.59839264, 1217.24591029,\n",
      "       1159.56808269, 1234.58683511, 1287.97014211, 1243.51458296,\n",
      "       1324.96513129, 1227.54195237, 1162.62515194, 1235.21876337,\n",
      "       1310.6568578 , 1365.64444403, 1277.39691922, 1312.16121294,\n",
      "       1284.40235693, 1258.24029309, 1306.30410473, 1317.29103874])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1273.27157451, 1224.26355311, 1230.12515941, 1347.48992924,\n",
      "       1375.06385378, 1310.02570673, 1264.10167232, 1256.72678598,\n",
      "       1281.80209639, 1211.59839264, 1217.24591029, 1159.56808269,\n",
      "       1234.58683511, 1287.97014211, 1243.51458296, 1324.96513129,\n",
      "       1227.54195237, 1162.62515194, 1235.21876337, 1310.6568578 ,\n",
      "       1365.64444403, 1277.39691922, 1312.16121294, 1284.40235693,\n",
      "       1258.24029309, 1306.30410473, 1317.29103874, 1205.5095401 ,\n",
      "       1310.95265197, 1365.37550819])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1272.40133942, 1283.12485345, 1304.96118896, 1298.22951149,\n",
      "       1254.67251479, 1231.80960079, 1259.72667505, 1333.16528255,\n",
      "       1330.27488592, 1276.61798394, 1314.085094  , 1156.2090095 ,\n",
      "       1281.73607348, 1213.66539508, 1231.51440224, 1217.67272786,\n",
      "       1196.53079239, 1314.36976499, 1285.83712486, 1300.15992366])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1227.54195237, 1162.62515194, 1235.21876337, 1310.6568578 ,\n",
      "       1365.64444403, 1277.39691922, 1312.16121294, 1284.40235693,\n",
      "       1258.24029309, 1306.30410473, 1317.29103874, 1205.5095401 ,\n",
      "       1310.95265197, 1365.37550819, 1272.40133942, 1283.12485345,\n",
      "       1304.96118896, 1298.22951149, 1254.67251479, 1231.80960079,\n",
      "       1259.72667505, 1333.16528255, 1330.27488592, 1276.61798394,\n",
      "       1314.085094  , 1156.2090095 , 1281.73607348, 1213.66539508,\n",
      "       1231.51440224, 1217.67272786])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1196.53079239, 1314.36976499, 1285.83712486, 1300.15992366,\n",
      "       1295.77968745, 1316.27883188, 1307.81557175, 1234.09948608,\n",
      "       1318.68039141, 1289.74344268, 1212.14524777, 1243.50409428,\n",
      "       1360.53893262, 1372.95605223, 1307.19100233, 1282.26642324,\n",
      "       1280.82367109, 1400.90086855, 1368.69029468, 1343.50420286])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([790.9940369 , 784.06169243, 794.03170193, 782.43241217,\n",
      "       755.51672945, 747.93387285, 773.69529753, 760.13137483,\n",
      "       743.52554413, 780.65476182, 780.51270032, 863.92123596,\n",
      "       695.31521813, 799.46781486, 876.53041534, 724.65218252,\n",
      "       743.8072481 , 770.72811344, 722.73099485, 733.79990542,\n",
      "       728.76976372, 866.57003017, 887.52255784, 896.85451539,\n",
      "       877.50053638, 775.79215333, 766.04648599, 824.39341901,\n",
      "       762.38914573, 826.6395924 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([817.13219304, 796.22790491, 846.83172639, 857.07268611,\n",
      "       815.69791161, 875.33317673, 794.92274497, 836.89239776,\n",
      "       867.16387476, 826.78026331, 841.22937628, 774.8583325 ,\n",
      "       851.97983988, 839.78847353, 806.18137957, 876.3270062 ,\n",
      "       822.22514043, 755.48010844, 729.86087559, 832.20711867])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1123.38586604, 1184.52339898, 1266.06107518, 1214.66957735,\n",
      "       1161.19412261, 1269.16771977, 1154.08837153, 1053.80799635,\n",
      "       1161.52993384, 1167.57896682, 1129.95184813, 1174.24243267,\n",
      "       1186.96621446, 1142.64760783, 1090.36430471, 1180.46179222,\n",
      "       1213.31148268, 1178.18182074, 1260.71384385, 1155.78393818,\n",
      "       1077.27025203, 1102.0871485 , 1149.52269827, 1180.35512515,\n",
      "       1165.76588431, 1187.36011924, 1119.93837473, 1222.3145851 ,\n",
      "       1200.14197303, 1235.4790309 ])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1220.45414496, 1213.51266018, 1220.77975695, 1217.94490589,\n",
      "       1226.27908317, 1265.20996166, 1224.93114943, 1229.45285265,\n",
      "       1198.54177021, 1259.40176692, 1185.32401861, 1268.59401946,\n",
      "       1225.15858003, 1123.93523893, 1168.53758729, 1157.02002074,\n",
      "       1219.95678964, 1177.3895382 , 1161.08282259, 1090.95462499])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1310.02570673, 1264.10167232, 1256.72678598, 1281.80209639,\n",
      "       1211.59839264, 1217.24591029, 1159.56808269, 1234.58683511,\n",
      "       1287.97014211, 1243.51458296, 1324.96513129, 1227.54195237,\n",
      "       1162.62515194, 1235.21876337, 1310.6568578 , 1365.64444403,\n",
      "       1277.39691922, 1312.16121294, 1284.40235693, 1258.24029309,\n",
      "       1306.30410473, 1317.29103874, 1205.5095401 , 1310.95265197,\n",
      "       1365.37550819, 1272.40133942, 1283.12485345, 1304.96118896,\n",
      "       1298.22951149, 1254.67251479])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1231.80960079, 1259.72667505, 1333.16528255, 1330.27488592,\n",
      "       1276.61798394, 1314.085094  , 1156.2090095 , 1281.73607348,\n",
      "       1213.66539508, 1231.51440224, 1217.67272786, 1196.53079239,\n",
      "       1314.36976499, 1285.83712486, 1300.15992366, 1295.77968745,\n",
      "       1316.27883188, 1307.81557175, 1234.09948608, 1318.68039141])>)\n",
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1277.39691922, 1312.16121294, 1284.40235693, 1258.24029309,\n",
      "       1306.30410473, 1317.29103874, 1205.5095401 , 1310.95265197,\n",
      "       1365.37550819, 1272.40133942, 1283.12485345, 1304.96118896,\n",
      "       1298.22951149, 1254.67251479, 1231.80960079, 1259.72667505,\n",
      "       1333.16528255, 1330.27488592, 1276.61798394, 1314.085094  ,\n",
      "       1156.2090095 , 1281.73607348, 1213.66539508, 1231.51440224,\n",
      "       1217.67272786, 1196.53079239, 1314.36976499, 1285.83712486,\n",
      "       1300.15992366, 1295.77968745])>, <tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
      "array([1316.27883188, 1307.81557175, 1234.09948608, 1318.68039141,\n",
      "       1289.74344268, 1212.14524777, 1243.50409428, 1360.53893262,\n",
      "       1372.95605223, 1307.19100233, 1282.26642324, 1280.82367109,\n",
      "       1400.90086855, 1368.69029468, 1343.50420286, 1372.03403984,\n",
      "       1345.81936368, 1306.9290217 , 1298.4278982 , 1304.96516988])>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1046"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for element in dataset:\n",
    "  print(element)\n",
    "  i = i + 1\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c41df3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 10:54:16.136896: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-28 10:54:16.138729: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-28 10:54:16.140104: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-28 10:54:16.423248: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-28 10:54:16.425309: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-28 10:54:16.426725: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.GlorotUniform()\n",
    "model = Sequential([\n",
    "#     Conv1D(filters=32, kernel_size=3, activation='relu', batch_input_shape=(window_size, 1)),\n",
    "    LSTM(32, return_sequences=True, input_shape=(window_size, 1), kernel_initializer=initializer),\n",
    "    LSTM(32),\n",
    "    Dense(nforecast)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b5537b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_ts = tf.convert_to_tensor(inp)\n",
    "t_ts = tf.convert_to_tensor(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c259daa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 10:54:16.887719: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-28 10:54:16.890027: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-28 10:54:16.891551: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-28 10:54:17.187421: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-28 10:54:17.190543: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-28 10:54:17.193120: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-28 10:54:18.905298: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-28 10:54:18.907560: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-28 10:54:18.909088: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-28 10:54:19.144013: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-28 10:54:19.146089: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-28 10:54:19.147484: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 0s - loss: 512377.3438 - mae: 627.4547 - mse: 512377.3438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 10:54:22.036468: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-28 10:54:22.038625: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-28 10:54:22.041523: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-28 10:54:22.360129: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-28 10:54:22.362044: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-28 10:54:22.363421: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 6s 58ms/step - loss: 512377.3438 - mae: 627.4547 - mse: 512377.3438 - val_loss: 1481735.0000 - val_mae: 1210.0447 - val_mse: 1481735.0000\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 510877.7812 - mae: 626.2590 - mse: 510877.7812 - val_loss: 1478230.2500 - val_mae: 1208.5948 - val_mse: 1478230.2500\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 509132.0000 - mae: 624.8660 - mse: 509132.0000 - val_loss: 1475168.1250 - val_mae: 1207.3268 - val_mse: 1475168.1250\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 507672.6562 - mae: 623.7111 - mse: 507672.6562 - val_loss: 1472591.7500 - val_mae: 1206.2592 - val_mse: 1472591.7500\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 1s 25ms/step - loss: 506381.3750 - mae: 622.6789 - mse: 506381.3750 - val_loss: 1470157.0000 - val_mae: 1205.2495 - val_mse: 1470157.0000\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 1s 25ms/step - loss: 505141.8125 - mae: 621.6816 - mse: 505141.8125 - val_loss: 1467757.5000 - val_mae: 1204.2538 - val_mse: 1467757.5000\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 503913.5312 - mae: 620.6904 - mse: 503913.5312 - val_loss: 1465366.0000 - val_mae: 1203.2604 - val_mse: 1465366.0000\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 502679.3750 - mae: 619.7001 - mse: 502679.3750 - val_loss: 1462978.5000 - val_mae: 1202.2677 - val_mse: 1462978.5000\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 1s 25ms/step - loss: 501440.2188 - mae: 618.7110 - mse: 501440.2188 - val_loss: 1460593.3750 - val_mae: 1201.2755 - val_mse: 1460593.3750\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 500217.3125 - mae: 617.7225 - mse: 500217.3125 - val_loss: 1458211.7500 - val_mae: 1200.2838 - val_mse: 1458211.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4e7547ef10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-8,\n",
    "    clipvalue=1.0\n",
    ")\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "\n",
    "# Set the training parameters\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mae\", \"mse\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(inp_ts, t_ts, validation_split=0.1, epochs=10, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faa00619-dd7d-4aa7-b9fc-1252dd820cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 30, 32)            4352      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                660       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,332\n",
      "Trainable params: 13,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7ff4911",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_forecast' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m forecast \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forecast\u001b[49m(model, series, window_size, batch_size)\n\u001b[1;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m forecast\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_forecast' is not defined"
     ]
    }
   ],
   "source": [
    "forecast = model_forecast(model, series, window_size, batch_size)\n",
    "results = forecast.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aeb277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forecast(model, series, window_size, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda w: w.batch(window_size))\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    forecast = model.predict(dataset)\n",
    "    return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548fa34-678a-45e4-bc76-05a7aa33d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler.plot_all(\n",
    "    series_lines = [\n",
    "        (time_data[0:window_size+nforecast], series[0:window_size+nforecast]),\n",
    "        (time_data[window_size:window_size+nforecast], series[window_size:window_size+nforecast])\n",
    "    ],\n",
    "    series_points = [(time_data[window_size+1:window_size+nforecast+1], results[0]),],\n",
    "    labels_lines = [\"series\", \"window\"],\n",
    "    labels_points = [\"forecast\"],\n",
    "    xy_label = [\"Time\", \"Value\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352a92c-99c9-4f49-a543-d96636e82d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398e68e-a46d-4ee8-9c59-1f164bf759f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[-2], results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72559054-0188-4f66-b874-0c314037b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler.plot_all(\n",
    "    series_lines = [\n",
    "        (time_data[-(window_size+nforecast):], series[-(window_size+nforecast):]),\n",
    "        (time_data[-(window_size+nforecast):-nforecast], series[-(window_size+nforecast):-nforecast])\n",
    "    ],\n",
    "    series_points = [(time_data[-nforecast:], results[-1]),],\n",
    "    labels_lines = [\"series\", \"window\"],\n",
    "    labels_points = [\"forecast\"],\n",
    "    xy_label = [\"Time\", \"Value\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db7667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_compra_comigo.data_handler import Visualizer\n",
    "\n",
    "visualizer = Visualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = visualizer.create_gif(\n",
    "    time_data=time_data[:365],\n",
    "    series=series[:365],\n",
    "    forecast=results[:365],\n",
    "    batch_size=batch_size,\n",
    "    window_size=window_size,\n",
    "    nforecast=nforecast,\n",
    "    gif_window=70\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3713c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[0].save('./tmp/k_nn.gif',\n",
    "             save_all = True, append_images = plots[1:100], \n",
    "             optimize = False, duration = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e2a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "plots = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d17b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = visualizer.create_gif(\n",
    "    time_data=time_data[-365:],\n",
    "    series=series[-365:],\n",
    "    forecast=results[-365:],\n",
    "    batch_size=batch_size,\n",
    "    window_size=window_size,\n",
    "    nforecast=nforecast,\n",
    "    gif_window=70\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54843fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[0].save('./tmp/k_nn_end.gif',\n",
    "             save_all = True, append_images = plots, \n",
    "             optimize = False, duration = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e139f1e-ca8d-4dee-bf89-de3ab31dc54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mse(y_true, y_pred).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49cdf0",
   "metadata": {},
   "source": [
    "## Lessons\n",
    "Most promising approach, will need to see how to automate the selection of these hyperparameters (and make it robust in an application)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff69eab7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import autokeras as ak\n",
    "import numpy as np\n",
    "\n",
    "predict_from = 1\n",
    "predict_until = nforecast\n",
    "lookback = window_size\n",
    "\n",
    "clf = ak.TimeseriesForecaster(\n",
    "    lookback=lookback,\n",
    "#     predict_from=predict_from,\n",
    "#     predict_until=predict_until,\n",
    "    max_trials=1,\n",
    "    objective=\"mse\",\n",
    ")\n",
    "# Train the TimeSeriesForecaster with train data\n",
    "clf.fit(\n",
    "    x=np.array(inputs),\n",
    "    y=np.array(targets),\n",
    "#     validation_data=(inputs, targets),\n",
    "#     batch_size=32,\n",
    "    epochs=1000,\n",
    ")\n",
    "# Predict with the best model(includes original training data).\n",
    "predictions = clf.predict(inputs)\n",
    "print(predictions.shape)\n",
    "# Evaluate the best model with testing data.\n",
    "print(clf.evaluate(predictions, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe26daa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autokeras as ak\n",
    "import numpy as np\n",
    "\n",
    "def generate_time_series_forecaster(window_size, n_forecast):\n",
    "    # Generate some dummy data\n",
    "    X_train = np.random.rand(100, window_size, 1)\n",
    "    y_train = np.random.rand(100, n_forecast, 1)\n",
    "    \n",
    "    # Initialize the time series regressor\n",
    "    input_node = ak.Input()\n",
    "    output_node = ak.TemporalConvBlock()(input_node)\n",
    "    output_node = ak.RegressionHead()(output_node)\n",
    "    regressor = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=10)\n",
    "    \n",
    "    # Search for the best model architecture\n",
    "    regressor.fit(X_train, y_train, epochs=10)\n",
    "    \n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ece18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series_forecaster(window_size, n_forecast):\n",
    "    # Generate some dummy data\n",
    "    X_train = np.random.rand(100, window_size, 1)\n",
    "    y_train = np.random.rand(100, n_forecast, 1)\n",
    "    \n",
    "    # Initialize the time series regressor\n",
    "    input_node = ak.Input()\n",
    "    output_node = ak.TCNBlock()(input_node)\n",
    "    output_node = ak.RegressionHead()(output_node)\n",
    "    regressor = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=10)\n",
    "    \n",
    "    # Search for the best model architecture\n",
    "    regressor.fit(X_train, y_train, epochs=10)\n",
    "    \n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b4c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series_forecaster(window_size, n_forecast):\n",
    "    # Generate some dummy data\n",
    "    X_train = np.random.rand(100, window_size, 1)\n",
    "    y_train = np.random.rand(100, n_forecast, 1)\n",
    "\n",
    "    # Initialize the time series forecaster\n",
    "    input_node = ak.Input()\n",
    "    output_node = ak.TimeseriesForecaster(output_dim=n_forecast)(input_node)\n",
    "    forecaster = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=10)\n",
    "\n",
    "    # Search for the best model architecture\n",
    "    forecaster.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "    return forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723513c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install autokeras --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f37b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series_forecaster(window_size, n_forecast):\n",
    "    # Generate some dummy data\n",
    "    X_train = np.random.rand(100, window_size, 1)\n",
    "    y_train = np.random.rand(100, n_forecast, 1)\n",
    "\n",
    "    # Initialize the time series forecaster\n",
    "    input_node = ak.Input()\n",
    "    output_node = ak.TimeseriesForecaster(block_type='dense', output_dim=n_forecast)(input_node)\n",
    "    forecaster = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=10)\n",
    "\n",
    "    # Search for the best model architecture\n",
    "    forecaster.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "    return forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dfa8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series_forecaster(window_size, n_forecast):\n",
    "    # Generate some dummy data\n",
    "    X_train = np.random.rand(100, window_size, 1)\n",
    "    y_train = np.random.rand(100, n_forecast, 1)\n",
    "\n",
    "    # Initialize the time series forecaster\n",
    "    input_node = ak.Input()\n",
    "    output_node = ak.AutoRegressor()(input_node)\n",
    "    forecaster = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=10)\n",
    "\n",
    "    # Search for the best model architecture\n",
    "    forecaster.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "    return forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56d8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.array(inputs)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae189e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.reshape(\n",
    "            i, (-1,window_size, 1)\n",
    "        ),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b4f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 14ms/step - loss: 702767.3750 - mean_squared_error: 702767.3750\n",
      "Epoch 789/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 699239.0000 - mean_squared_error: 699239.0000\n",
      "Epoch 790/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 699451.1875 - mean_squared_error: 699451.1875\n",
      "Epoch 791/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 710288.3750 - mean_squared_error: 710288.3750\n",
      "Epoch 792/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 704353.9375 - mean_squared_error: 704353.9375\n",
      "Epoch 793/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 698279.2500 - mean_squared_error: 698279.2500\n",
      "Epoch 794/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 700702.1250 - mean_squared_error: 700702.1250\n",
      "Epoch 795/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 699587.6875 - mean_squared_error: 699587.6875\n",
      "Epoch 796/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 695069.5000 - mean_squared_error: 695069.5000\n",
      "Epoch 797/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 690640.9375 - mean_squared_error: 690640.9375\n",
      "Epoch 798/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 695558.6875 - mean_squared_error: 695558.6875\n",
      "Epoch 799/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 696293.0625 - mean_squared_error: 696293.0625\n",
      "Epoch 800/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 693048.6875 - mean_squared_error: 693048.6875\n",
      "Epoch 801/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 694846.6250 - mean_squared_error: 694846.6250\n",
      "Epoch 802/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 699308.3750 - mean_squared_error: 699308.3750\n",
      "Epoch 803/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 694871.5000 - mean_squared_error: 694871.5000\n",
      "Epoch 804/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 693624.7500 - mean_squared_error: 693624.7500\n",
      "Epoch 805/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 693931.2500 - mean_squared_error: 693931.3125\n",
      "Epoch 806/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 688432.8125 - mean_squared_error: 688432.8125\n",
      "Epoch 807/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 690312.0000 - mean_squared_error: 690312.0000\n",
      "Epoch 808/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 691304.3125 - mean_squared_error: 691304.3125\n",
      "Epoch 809/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 687636.3750 - mean_squared_error: 687636.3750\n",
      "Epoch 810/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 693059.0625 - mean_squared_error: 693059.0625\n",
      "Epoch 811/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 691745.6250 - mean_squared_error: 691745.6250\n",
      "Epoch 812/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 683071.6250 - mean_squared_error: 683071.6250\n",
      "Epoch 813/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 687625.5625 - mean_squared_error: 687625.5625\n",
      "Epoch 814/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 688614.8125 - mean_squared_error: 688614.8125\n",
      "Epoch 815/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 685026.6875 - mean_squared_error: 685026.6875\n",
      "Epoch 816/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 686515.0625 - mean_squared_error: 686515.0625\n",
      "Epoch 817/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 686751.0625 - mean_squared_error: 686751.0625\n",
      "Epoch 818/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 682984.0625 - mean_squared_error: 682984.0625\n",
      "Epoch 819/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 679965.3750 - mean_squared_error: 679965.3750\n",
      "Epoch 820/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 685461.6250 - mean_squared_error: 685461.6250\n",
      "Epoch 821/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 684210.6875 - mean_squared_error: 684210.6875\n",
      "Epoch 822/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 679620.5625 - mean_squared_error: 679620.5625\n",
      "Epoch 823/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 682093.1250 - mean_squared_error: 682093.1875\n",
      "Epoch 824/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 679786.8125 - mean_squared_error: 679786.7500\n",
      "Epoch 825/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 683009.6250 - mean_squared_error: 683009.6250\n",
      "Epoch 826/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 681142.3750 - mean_squared_error: 681142.3750\n",
      "Epoch 827/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 682128.2500 - mean_squared_error: 682128.2500\n",
      "Epoch 828/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 679666.8125 - mean_squared_error: 679666.8750\n",
      "Epoch 829/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 678431.7500 - mean_squared_error: 678431.7500\n",
      "Epoch 830/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 676368.3750 - mean_squared_error: 676368.3750\n",
      "Epoch 831/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 680585.1250 - mean_squared_error: 680585.1250\n",
      "Epoch 832/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 678009.3125 - mean_squared_error: 678009.3125\n",
      "Epoch 833/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 679934.0625 - mean_squared_error: 679934.0625\n",
      "Epoch 834/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 674245.9375 - mean_squared_error: 674245.9375\n",
      "Epoch 835/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 678202.7500 - mean_squared_error: 678202.7500\n",
      "Epoch 836/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 673614.1250 - mean_squared_error: 673614.1250\n",
      "Epoch 837/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 680805.9375 - mean_squared_error: 680805.9375\n",
      "Epoch 838/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 668282.0000 - mean_squared_error: 668282.0625\n",
      "Epoch 839/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 667474.1875 - mean_squared_error: 667474.1875\n",
      "Epoch 840/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 672238.7500 - mean_squared_error: 672238.7500\n",
      "Epoch 841/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 674145.3125 - mean_squared_error: 674145.2500\n",
      "Epoch 842/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 676813.8750 - mean_squared_error: 676813.8750\n",
      "Epoch 843/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 669739.0625 - mean_squared_error: 669739.0625\n",
      "Epoch 844/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 671636.6250 - mean_squared_error: 671636.6250\n",
      "Epoch 845/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 671362.6250 - mean_squared_error: 671362.6250\n",
      "Epoch 846/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 667642.1250 - mean_squared_error: 667642.1250\n",
      "Epoch 847/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 668445.3750 - mean_squared_error: 668445.3750\n",
      "Epoch 848/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 672241.1875 - mean_squared_error: 672241.1875\n",
      "Epoch 849/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 660750.5000 - mean_squared_error: 660750.5000\n",
      "Epoch 850/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 666990.1875 - mean_squared_error: 666990.1875\n",
      "Epoch 851/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 665936.0000 - mean_squared_error: 665936.0000\n",
      "Epoch 852/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 664642.0000 - mean_squared_error: 664642.0000\n",
      "Epoch 853/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 661229.0000 - mean_squared_error: 661229.0000\n",
      "Epoch 854/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 665438.2500 - mean_squared_error: 665438.2500\n",
      "Epoch 855/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 667783.0000 - mean_squared_error: 667782.9375\n",
      "Epoch 856/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 659849.0625 - mean_squared_error: 659849.0625\n",
      "Epoch 857/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 666489.7500 - mean_squared_error: 666489.7500\n",
      "Epoch 858/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 663115.0000 - mean_squared_error: 663115.0000\n",
      "Epoch 859/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 667292.0000 - mean_squared_error: 667292.0000\n",
      "Epoch 860/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 660933.6875 - mean_squared_error: 660933.6875\n",
      "Epoch 861/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 661450.3750 - mean_squared_error: 661450.3750\n",
      "Epoch 862/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 662040.7500 - mean_squared_error: 662040.8125\n",
      "Epoch 863/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 660146.4375 - mean_squared_error: 660146.4375\n",
      "Epoch 864/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 660713.2500 - mean_squared_error: 660713.2500\n",
      "Epoch 865/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 664664.0625 - mean_squared_error: 664664.0625\n",
      "Epoch 866/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 649859.8125 - mean_squared_error: 649859.8125\n",
      "Epoch 867/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 657060.4375 - mean_squared_error: 657060.3750\n",
      "Epoch 868/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 655366.3750 - mean_squared_error: 655366.3750\n",
      "Epoch 869/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 653747.7500 - mean_squared_error: 653747.7500\n",
      "Epoch 870/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 662636.6875 - mean_squared_error: 662636.6875\n",
      "Epoch 871/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 655547.3125 - mean_squared_error: 655547.3125\n",
      "Epoch 872/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 648827.4375 - mean_squared_error: 648827.4375\n",
      "Epoch 873/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 648857.5625 - mean_squared_error: 648857.5625\n",
      "Epoch 874/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 646169.5625 - mean_squared_error: 646169.5625\n",
      "Epoch 875/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 651471.6250 - mean_squared_error: 651471.6250\n",
      "Epoch 876/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 652459.5625 - mean_squared_error: 652459.5625\n",
      "Epoch 877/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 647983.4375 - mean_squared_error: 647983.4375\n",
      "Epoch 878/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 652881.8125 - mean_squared_error: 652881.8125\n",
      "Epoch 879/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 653535.3125 - mean_squared_error: 653535.3125\n",
      "Epoch 880/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 651340.5000 - mean_squared_error: 651340.5000\n",
      "Epoch 881/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 650556.3125 - mean_squared_error: 650556.3125\n",
      "Epoch 882/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 646075.1250 - mean_squared_error: 646075.1250\n",
      "Epoch 883/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 649242.6875 - mean_squared_error: 649242.6875\n",
      "Epoch 884/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 644634.0625 - mean_squared_error: 644634.0625\n",
      "Epoch 885/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 649426.9375 - mean_squared_error: 649426.9375\n",
      "Epoch 886/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 643011.4375 - mean_squared_error: 643011.4375\n",
      "Epoch 887/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 656686.9375 - mean_squared_error: 656686.9375\n",
      "Epoch 888/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 639253.5625 - mean_squared_error: 639253.5625\n",
      "Epoch 889/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 642839.1875 - mean_squared_error: 642839.1875\n",
      "Epoch 890/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 642562.3125 - mean_squared_error: 642562.3125\n",
      "Epoch 891/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 642762.3125 - mean_squared_error: 642762.3125\n",
      "Epoch 892/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 646026.1875 - mean_squared_error: 646026.1875\n",
      "Epoch 893/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 641313.1875 - mean_squared_error: 641313.1875\n",
      "Epoch 894/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 642971.5625 - mean_squared_error: 642971.5625\n",
      "Epoch 895/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 639040.6250 - mean_squared_error: 639040.6250\n",
      "Epoch 896/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 635961.1250 - mean_squared_error: 635961.1250\n",
      "Epoch 897/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 641730.8125 - mean_squared_error: 641730.8125\n",
      "Epoch 898/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 638342.8750 - mean_squared_error: 638342.8750\n",
      "Epoch 899/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 637793.8125 - mean_squared_error: 637793.8125\n",
      "Epoch 900/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 638704.2500 - mean_squared_error: 638704.3125\n",
      "Epoch 901/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 638968.1250 - mean_squared_error: 638968.1250\n",
      "Epoch 902/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 641156.7500 - mean_squared_error: 641156.7500\n",
      "Epoch 903/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 640735.4375 - mean_squared_error: 640735.4375\n",
      "Epoch 904/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 635734.9375 - mean_squared_error: 635734.9375\n",
      "Epoch 905/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 636678.7500 - mean_squared_error: 636678.7500\n",
      "Epoch 906/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 635590.7500 - mean_squared_error: 635590.7500\n",
      "Epoch 907/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 633887.5625 - mean_squared_error: 633887.5625\n",
      "Epoch 908/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 641261.3125 - mean_squared_error: 641261.3125\n",
      "Epoch 909/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 631975.0625 - mean_squared_error: 631975.0625\n",
      "Epoch 910/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 633891.8125 - mean_squared_error: 633891.8125\n",
      "Epoch 911/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 633712.2500 - mean_squared_error: 633712.2500\n",
      "Epoch 912/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 632102.8750 - mean_squared_error: 632102.8750\n",
      "Epoch 913/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 633463.8125 - mean_squared_error: 633463.8125\n",
      "Epoch 914/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 625964.8750 - mean_squared_error: 625964.8750\n",
      "Epoch 915/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 632228.5625 - mean_squared_error: 632228.5625\n",
      "Epoch 916/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 630513.6875 - mean_squared_error: 630513.6875\n",
      "Epoch 917/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 627789.4375 - mean_squared_error: 627789.4375\n",
      "Epoch 918/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 633306.0625 - mean_squared_error: 633306.0625\n",
      "Epoch 919/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 629818.3750 - mean_squared_error: 629818.3750\n",
      "Epoch 920/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 632785.0000 - mean_squared_error: 632785.0000\n",
      "Epoch 921/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 629537.3750 - mean_squared_error: 629537.3750\n",
      "Epoch 922/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 627435.2500 - mean_squared_error: 627435.2500\n",
      "Epoch 923/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 624213.6875 - mean_squared_error: 624213.6875\n",
      "Epoch 924/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 632199.5625 - mean_squared_error: 632199.5625\n",
      "Epoch 925/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 627678.8125 - mean_squared_error: 627678.8125\n",
      "Epoch 926/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 624659.2500 - mean_squared_error: 624659.2500\n",
      "Epoch 927/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 628803.1875 - mean_squared_error: 628803.1875\n",
      "Epoch 928/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 625325.2500 - mean_squared_error: 625325.2500\n",
      "Epoch 929/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 627289.2500 - mean_squared_error: 627289.2500\n",
      "Epoch 930/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 624726.4375 - mean_squared_error: 624726.4375\n",
      "Epoch 931/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 616710.3125 - mean_squared_error: 616710.3125\n",
      "Epoch 932/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 619659.2500 - mean_squared_error: 619659.2500\n",
      "Epoch 933/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 621370.1250 - mean_squared_error: 621370.1250\n",
      "Epoch 934/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 623038.5625 - mean_squared_error: 623038.5625\n",
      "Epoch 935/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 627308.3125 - mean_squared_error: 627308.3125\n",
      "Epoch 936/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 619036.4375 - mean_squared_error: 619036.4375\n",
      "Epoch 937/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 615874.8750 - mean_squared_error: 615874.8750\n",
      "Epoch 938/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 616337.3750 - mean_squared_error: 616337.3750\n",
      "Epoch 939/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 621763.9375 - mean_squared_error: 621763.9375\n",
      "Epoch 940/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 620126.0000 - mean_squared_error: 620126.0000\n",
      "Epoch 941/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 617606.8125 - mean_squared_error: 617606.8125\n",
      "Epoch 942/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 621361.3125 - mean_squared_error: 621361.3125\n",
      "Epoch 943/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 619686.5000 - mean_squared_error: 619686.5000\n",
      "Epoch 944/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 614119.8125 - mean_squared_error: 614119.8125\n",
      "Epoch 945/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 615056.8750 - mean_squared_error: 615056.8750\n",
      "Epoch 946/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 617291.3125 - mean_squared_error: 617291.3125\n",
      "Epoch 947/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 613739.2500 - mean_squared_error: 613739.2500\n",
      "Epoch 948/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 616177.9375 - mean_squared_error: 616177.9375\n",
      "Epoch 949/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 617256.5000 - mean_squared_error: 617256.5000\n",
      "Epoch 950/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 616414.9375 - mean_squared_error: 616414.9375\n",
      "Epoch 951/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 616017.6250 - mean_squared_error: 616017.6250\n",
      "Epoch 952/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 614988.9375 - mean_squared_error: 614988.9375\n",
      "Epoch 953/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 612147.4375 - mean_squared_error: 612147.4375\n",
      "Epoch 954/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 610672.0000 - mean_squared_error: 610672.0000\n",
      "Epoch 955/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 604922.8750 - mean_squared_error: 604922.8750\n",
      "Epoch 956/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 610041.3125 - mean_squared_error: 610041.3125\n",
      "Epoch 957/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 610879.1875 - mean_squared_error: 610879.1875\n",
      "Epoch 958/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 612983.5625 - mean_squared_error: 612983.5625\n",
      "Epoch 959/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 609929.0625 - mean_squared_error: 609929.0625\n",
      "Epoch 960/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 607951.6875 - mean_squared_error: 607951.6875\n",
      "Epoch 961/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 604993.3125 - mean_squared_error: 604993.3125\n",
      "Epoch 962/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 603165.7500 - mean_squared_error: 603165.7500\n",
      "Epoch 963/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 608752.0000 - mean_squared_error: 608752.0000\n",
      "Epoch 964/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 605276.4375 - mean_squared_error: 605276.4375\n",
      "Epoch 965/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 603157.5000 - mean_squared_error: 603157.5000\n",
      "Epoch 966/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 605265.6875 - mean_squared_error: 605265.6875\n",
      "Epoch 967/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 605383.3750 - mean_squared_error: 605383.3750\n",
      "Epoch 968/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 606762.1250 - mean_squared_error: 606762.1250\n",
      "Epoch 969/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 603247.3750 - mean_squared_error: 603247.3750\n",
      "Epoch 970/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 602386.9375 - mean_squared_error: 602386.9375\n",
      "Epoch 971/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 598937.3125 - mean_squared_error: 598937.3125\n",
      "Epoch 972/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 601129.0000 - mean_squared_error: 601129.0000\n",
      "Epoch 973/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 602983.2500 - mean_squared_error: 602983.2500\n",
      "Epoch 974/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 602256.5000 - mean_squared_error: 602256.5000\n",
      "Epoch 975/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 593493.8125 - mean_squared_error: 593493.8125\n",
      "Epoch 976/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 595467.7500 - mean_squared_error: 595467.7500\n",
      "Epoch 977/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 596787.3750 - mean_squared_error: 596787.3125\n",
      "Epoch 978/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 595402.9375 - mean_squared_error: 595402.9375\n",
      "Epoch 979/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 595703.6875 - mean_squared_error: 595703.6875\n",
      "Epoch 980/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 595784.5000 - mean_squared_error: 595784.5000\n",
      "Epoch 981/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 592435.4375 - mean_squared_error: 592435.4375\n",
      "Epoch 982/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 598399.1250 - mean_squared_error: 598399.1250\n",
      "Epoch 983/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 596594.9375 - mean_squared_error: 596594.9375\n",
      "Epoch 984/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 594834.2500 - mean_squared_error: 594834.2500\n",
      "Epoch 985/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 597145.4375 - mean_squared_error: 597145.4375\n",
      "Epoch 986/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 591320.3750 - mean_squared_error: 591320.3750\n",
      "Epoch 987/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 593854.6875 - mean_squared_error: 593854.6875\n",
      "Epoch 988/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 593102.8750 - mean_squared_error: 593102.8750\n",
      "Epoch 989/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 594344.0000 - mean_squared_error: 594344.0000\n",
      "Epoch 990/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 588060.0000 - mean_squared_error: 588060.0000\n",
      "Epoch 991/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 593362.1250 - mean_squared_error: 593362.1250\n",
      "Epoch 992/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 591305.5000 - mean_squared_error: 591305.5000\n",
      "Epoch 993/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 586700.0000 - mean_squared_error: 586700.0000\n",
      "Epoch 994/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 589748.8750 - mean_squared_error: 589748.8750\n",
      "Epoch 995/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 587924.3125 - mean_squared_error: 587924.3125\n",
      "Epoch 996/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 589416.0625 - mean_squared_error: 589416.0625\n",
      "Epoch 997/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 587566.1250 - mean_squared_error: 587566.1250\n",
      "Epoch 998/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 584430.3750 - mean_squared_error: 584430.3750\n",
      "Epoch 999/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 582144.7500 - mean_squared_error: 582144.7500\n",
      "Epoch 1000/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 584981.2500 - mean_squared_error: 584981.3125\n",
      "Epoch 1001/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 584958.0625 - mean_squared_error: 584958.0625\n",
      "Epoch 1002/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 588141.8125 - mean_squared_error: 588141.8125\n",
      "Epoch 1003/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 588135.3750 - mean_squared_error: 588135.3750\n",
      "Epoch 1004/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 582007.4375 - mean_squared_error: 582007.4375\n",
      "Epoch 1005/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 582127.2500 - mean_squared_error: 582127.2500\n",
      "Epoch 1006/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 584523.3125 - mean_squared_error: 584523.3125\n",
      "Epoch 1007/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 582415.8125 - mean_squared_error: 582415.8125\n",
      "Epoch 1008/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 581979.0000 - mean_squared_error: 581979.0000\n",
      "Epoch 1009/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 586201.3125 - mean_squared_error: 586201.3125\n",
      "Epoch 1010/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 579346.5625 - mean_squared_error: 579346.5625\n",
      "Epoch 1011/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 582536.1250 - mean_squared_error: 582536.1250\n",
      "Epoch 1012/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 577533.2500 - mean_squared_error: 577533.2500\n",
      "Epoch 1013/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 581713.0000 - mean_squared_error: 581713.0000\n",
      "Epoch 1014/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 577859.8125 - mean_squared_error: 577859.8125\n",
      "Epoch 1015/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 579122.8125 - mean_squared_error: 579122.8125\n",
      "Epoch 1016/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 576488.8750 - mean_squared_error: 576488.8750\n",
      "Epoch 1017/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 580450.6250 - mean_squared_error: 580450.5625\n",
      "Epoch 1018/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 575681.4375 - mean_squared_error: 575681.4375\n",
      "Epoch 1019/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 580929.7500 - mean_squared_error: 580929.7500\n",
      "Epoch 1020/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 575974.9375 - mean_squared_error: 575974.9375\n",
      "Epoch 1021/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 566131.5625 - mean_squared_error: 566131.5625\n",
      "Epoch 1022/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 579360.2500 - mean_squared_error: 579360.2500\n",
      "Epoch 1023/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 570225.5625 - mean_squared_error: 570225.5625\n",
      "Epoch 1024/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 577312.0000 - mean_squared_error: 577312.0000\n",
      "Epoch 1025/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 572404.7500 - mean_squared_error: 572404.7500\n",
      "Epoch 1026/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 574845.6250 - mean_squared_error: 574845.6250\n",
      "Epoch 1027/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 573513.0000 - mean_squared_error: 573513.0000\n",
      "Epoch 1028/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 567018.9375 - mean_squared_error: 567018.9375\n",
      "Epoch 1029/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 573789.5625 - mean_squared_error: 573789.5625\n",
      "Epoch 1030/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 575815.3750 - mean_squared_error: 575815.3750\n",
      "Epoch 1031/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 573895.2500 - mean_squared_error: 573895.2500\n",
      "Epoch 1032/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 570666.5625 - mean_squared_error: 570666.5625\n",
      "Epoch 1033/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 574128.5000 - mean_squared_error: 574128.5000\n",
      "Epoch 1034/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 566734.0625 - mean_squared_error: 566734.0625\n",
      "Epoch 1035/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 571536.5625 - mean_squared_error: 571536.5625\n",
      "Epoch 1036/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 570116.6250 - mean_squared_error: 570116.6250\n",
      "Epoch 1037/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 573234.3125 - mean_squared_error: 573234.3125\n",
      "Epoch 1038/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 568084.5000 - mean_squared_error: 568084.5000\n",
      "Epoch 1039/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 565876.5000 - mean_squared_error: 565876.5000\n",
      "Epoch 1040/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 572453.1250 - mean_squared_error: 572453.1250\n",
      "Epoch 1041/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 566677.6875 - mean_squared_error: 566677.7500\n",
      "Epoch 1042/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 567477.6250 - mean_squared_error: 567477.6250\n",
      "Epoch 1043/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 567117.0625 - mean_squared_error: 567117.0625\n",
      "Epoch 1044/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 568464.2500 - mean_squared_error: 568464.2500\n",
      "Epoch 1045/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 566072.1875 - mean_squared_error: 566072.1875\n",
      "Epoch 1046/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 564373.6875 - mean_squared_error: 564373.6875\n",
      "Epoch 1047/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 564195.1250 - mean_squared_error: 564195.1250\n",
      "Epoch 1048/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 563932.2500 - mean_squared_error: 563932.3125\n",
      "Epoch 1049/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 560171.3750 - mean_squared_error: 560171.3750\n",
      "Epoch 1050/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 567952.3125 - mean_squared_error: 567952.3125\n",
      "Epoch 1051/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 566819.6250 - mean_squared_error: 566819.6250\n",
      "Epoch 1052/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 14ms/step - loss: 560620.0625 - mean_squared_error: 560620.0000\n",
      "Epoch 1053/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 559007.8125 - mean_squared_error: 559007.8125\n",
      "Epoch 1054/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 561944.4375 - mean_squared_error: 561944.4375\n",
      "Epoch 1055/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 563001.5625 - mean_squared_error: 563001.5625\n",
      "Epoch 1056/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 568649.3750 - mean_squared_error: 568649.3750\n",
      "Epoch 1057/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 562214.3125 - mean_squared_error: 562214.3125\n",
      "Epoch 1058/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 564752.2500 - mean_squared_error: 564752.2500\n",
      "Epoch 1059/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 559491.1875 - mean_squared_error: 559491.1875\n",
      "Epoch 1060/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 559528.1875 - mean_squared_error: 559528.1875\n",
      "Epoch 1061/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 555722.0625 - mean_squared_error: 555722.0625\n",
      "Epoch 1062/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 556557.0625 - mean_squared_error: 556557.0625\n",
      "Epoch 1063/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 555455.1875 - mean_squared_error: 555455.1875\n",
      "Epoch 1064/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 557340.5000 - mean_squared_error: 557340.5000\n",
      "Epoch 1065/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 558156.3750 - mean_squared_error: 558156.3750\n",
      "Epoch 1066/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 560941.1250 - mean_squared_error: 560941.1250\n",
      "Epoch 1067/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 559473.8125 - mean_squared_error: 559473.8125\n",
      "Epoch 1068/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 556202.8750 - mean_squared_error: 556202.8750\n",
      "Epoch 1069/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 552067.4375 - mean_squared_error: 552067.4375\n",
      "Epoch 1070/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 559904.4375 - mean_squared_error: 559904.4375\n",
      "Epoch 1071/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 554933.7500 - mean_squared_error: 554933.7500\n",
      "Epoch 1072/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 556891.3750 - mean_squared_error: 556891.3750\n",
      "Epoch 1073/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 551776.1250 - mean_squared_error: 551776.1250\n",
      "Epoch 1074/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 551887.3125 - mean_squared_error: 551887.3125\n",
      "Epoch 1075/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 554389.7500 - mean_squared_error: 554389.7500\n",
      "Epoch 1076/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 553975.8125 - mean_squared_error: 553975.8125\n",
      "Epoch 1077/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 549544.6250 - mean_squared_error: 549544.6250\n",
      "Epoch 1078/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 550567.5000 - mean_squared_error: 550567.5000\n",
      "Epoch 1079/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 551480.0625 - mean_squared_error: 551480.0625\n",
      "Epoch 1080/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 553068.6250 - mean_squared_error: 553068.6250\n",
      "Epoch 1081/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 547938.8125 - mean_squared_error: 547938.8125\n",
      "Epoch 1082/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 549830.3125 - mean_squared_error: 549830.3125\n",
      "Epoch 1083/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 543171.9375 - mean_squared_error: 543171.9375\n",
      "Epoch 1084/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 549732.5000 - mean_squared_error: 549732.5000\n",
      "Epoch 1085/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 547840.9375 - mean_squared_error: 547840.9375\n",
      "Epoch 1086/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 549199.8125 - mean_squared_error: 549199.8125\n",
      "Epoch 1087/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 551040.0000 - mean_squared_error: 551040.0000\n",
      "Epoch 1088/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 549144.9375 - mean_squared_error: 549145.0000\n",
      "Epoch 1089/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 542890.5000 - mean_squared_error: 542890.5000\n",
      "Epoch 1090/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 549612.3125 - mean_squared_error: 549612.3125\n",
      "Epoch 1091/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 551202.3125 - mean_squared_error: 551202.3125\n",
      "Epoch 1092/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 548199.0000 - mean_squared_error: 548199.0000\n",
      "Epoch 1093/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 538995.9375 - mean_squared_error: 538995.9375\n",
      "Epoch 1094/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 546367.8750 - mean_squared_error: 546367.8750\n",
      "Epoch 1095/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 542466.6250 - mean_squared_error: 542466.6250\n",
      "Epoch 1096/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 548941.9375 - mean_squared_error: 548941.9375\n",
      "Epoch 1097/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 541177.6875 - mean_squared_error: 541177.6875\n",
      "Epoch 1098/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 543550.8125 - mean_squared_error: 543550.8125\n",
      "Epoch 1099/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 547441.0625 - mean_squared_error: 547441.0625\n",
      "Epoch 1100/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 545222.8125 - mean_squared_error: 545222.8125\n",
      "Epoch 1101/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 541451.1875 - mean_squared_error: 541451.1875\n",
      "Epoch 1102/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 540078.8750 - mean_squared_error: 540078.8750\n",
      "Epoch 1103/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 540400.3125 - mean_squared_error: 540400.3125\n",
      "Epoch 1104/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 539560.3125 - mean_squared_error: 539560.3125\n",
      "Epoch 1105/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 543601.9375 - mean_squared_error: 543602.0000\n",
      "Epoch 1106/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 539354.0000 - mean_squared_error: 539354.0000\n",
      "Epoch 1107/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 541361.1250 - mean_squared_error: 541361.1250\n",
      "Epoch 1108/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 533480.3750 - mean_squared_error: 533480.3750\n",
      "Epoch 1109/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 529105.9375 - mean_squared_error: 529105.9375\n",
      "Epoch 1110/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 538427.6250 - mean_squared_error: 538427.6250\n",
      "Epoch 1111/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 537282.8125 - mean_squared_error: 537282.8125\n",
      "Epoch 1112/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 537223.2500 - mean_squared_error: 537223.2500\n",
      "Epoch 1113/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 532433.0625 - mean_squared_error: 532433.0625\n",
      "Epoch 1114/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 538962.9375 - mean_squared_error: 538962.9375\n",
      "Epoch 1115/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 534255.8125 - mean_squared_error: 534255.8125\n",
      "Epoch 1116/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 531169.1875 - mean_squared_error: 531169.1875\n",
      "Epoch 1117/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 531491.8750 - mean_squared_error: 531491.8750\n",
      "Epoch 1118/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 536291.6875 - mean_squared_error: 536291.6875\n",
      "Epoch 1119/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 533776.3750 - mean_squared_error: 533776.3750\n",
      "Epoch 1120/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 532358.0625 - mean_squared_error: 532358.0625\n",
      "Epoch 1121/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 537081.6250 - mean_squared_error: 537081.6250\n",
      "Epoch 1122/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 530026.1250 - mean_squared_error: 530026.1250\n",
      "Epoch 1123/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 531758.8750 - mean_squared_error: 531758.8750\n",
      "Epoch 1124/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 533379.2500 - mean_squared_error: 533379.2500\n",
      "Epoch 1125/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 525762.6875 - mean_squared_error: 525762.6875\n",
      "Epoch 1126/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 529662.3125 - mean_squared_error: 529662.3125\n",
      "Epoch 1127/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 526327.5000 - mean_squared_error: 526327.5000\n",
      "Epoch 1128/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 531562.8750 - mean_squared_error: 531562.8750\n",
      "Epoch 1129/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 530989.9375 - mean_squared_error: 530989.9375\n",
      "Epoch 1130/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 533145.6250 - mean_squared_error: 533145.5625\n",
      "Epoch 1131/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 531650.1250 - mean_squared_error: 531650.1250\n",
      "Epoch 1132/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 523847.9062 - mean_squared_error: 523847.9062\n",
      "Epoch 1133/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 527103.4375 - mean_squared_error: 527103.4375\n",
      "Epoch 1134/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 526358.9375 - mean_squared_error: 526358.9375\n",
      "Epoch 1135/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 530053.0000 - mean_squared_error: 530053.0000\n",
      "Epoch 1136/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 526665.3750 - mean_squared_error: 526665.3750\n",
      "Epoch 1137/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 524764.0000 - mean_squared_error: 524764.0000\n",
      "Epoch 1138/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 524564.8125 - mean_squared_error: 524564.8125\n",
      "Epoch 1139/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 527002.5000 - mean_squared_error: 527002.5000\n",
      "Epoch 1140/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 527195.6250 - mean_squared_error: 527195.6250\n",
      "Epoch 1141/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 519169.9062 - mean_squared_error: 519169.9062\n",
      "Epoch 1142/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 527220.6875 - mean_squared_error: 527220.6875\n",
      "Epoch 1143/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 526357.6875 - mean_squared_error: 526357.6875\n",
      "Epoch 1144/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 520533.6562 - mean_squared_error: 520533.6562\n",
      "Epoch 1145/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 522753.2188 - mean_squared_error: 522753.2188\n",
      "Epoch 1146/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 525019.8750 - mean_squared_error: 525019.8750\n",
      "Epoch 1147/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 521832.3750 - mean_squared_error: 521832.3750\n",
      "Epoch 1148/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 526451.5625 - mean_squared_error: 526451.5625\n",
      "Epoch 1149/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 524194.2500 - mean_squared_error: 524194.2500\n",
      "Epoch 1150/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 514453.1562 - mean_squared_error: 514453.1562\n",
      "Epoch 1151/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 523996.6875 - mean_squared_error: 523996.6875\n",
      "Epoch 1152/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 517080.2188 - mean_squared_error: 517080.2188\n",
      "Epoch 1153/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 515263.9375 - mean_squared_error: 515263.9375\n",
      "Epoch 1154/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 521893.9375 - mean_squared_error: 521893.9375\n",
      "Epoch 1155/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 518884.2188 - mean_squared_error: 518884.2188\n",
      "Epoch 1156/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 515953.1250 - mean_squared_error: 515953.1250\n",
      "Epoch 1157/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 522169.9375 - mean_squared_error: 522169.9375\n",
      "Epoch 1158/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 517257.6562 - mean_squared_error: 517257.6562\n",
      "Epoch 1159/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 521235.4688 - mean_squared_error: 521235.4688\n",
      "Epoch 1160/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 518280.1875 - mean_squared_error: 518280.1875\n",
      "Epoch 1161/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 519803.7188 - mean_squared_error: 519803.7188\n",
      "Epoch 1162/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 514359.7500 - mean_squared_error: 514359.7500\n",
      "Epoch 1163/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 530003.9375 - mean_squared_error: 530003.9375\n",
      "Epoch 1164/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 514693.5000 - mean_squared_error: 514693.5000\n",
      "Epoch 1165/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 516525.7188 - mean_squared_error: 516525.7188\n",
      "Epoch 1166/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 513444.5938 - mean_squared_error: 513444.5938\n",
      "Epoch 1167/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 511936.3438 - mean_squared_error: 511936.3438\n",
      "Epoch 1168/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 509577.8438 - mean_squared_error: 509577.8438\n",
      "Epoch 1169/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 517302.0938 - mean_squared_error: 517302.0938\n",
      "Epoch 1170/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 517491.4062 - mean_squared_error: 517491.4062\n",
      "Epoch 1171/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 511555.8125 - mean_squared_error: 511555.8125\n",
      "Epoch 1172/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 508659.1250 - mean_squared_error: 508659.1250\n",
      "Epoch 1173/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 509637.7500 - mean_squared_error: 509637.7500\n",
      "Epoch 1174/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 512635.6250 - mean_squared_error: 512635.6250\n",
      "Epoch 1175/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 512705.4688 - mean_squared_error: 512705.4688\n",
      "Epoch 1176/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 514024.4375 - mean_squared_error: 514024.4375\n",
      "Epoch 1177/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 508361.6875 - mean_squared_error: 508361.6875\n",
      "Epoch 1178/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 510453.2188 - mean_squared_error: 510453.2188\n",
      "Epoch 1179/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 504502.1875 - mean_squared_error: 504502.2188\n",
      "Epoch 1180/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 500400.4688 - mean_squared_error: 500400.4688\n",
      "Epoch 1181/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 510700.9062 - mean_squared_error: 510700.9062\n",
      "Epoch 1182/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 506573.2188 - mean_squared_error: 506573.2188\n",
      "Epoch 1183/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 506291.8125 - mean_squared_error: 506291.8125\n",
      "Epoch 1184/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 14ms/step - loss: 506705.9062 - mean_squared_error: 506705.9062\n",
      "Epoch 1185/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 506599.9062 - mean_squared_error: 506599.9062\n",
      "Epoch 1186/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 509496.6562 - mean_squared_error: 509496.6562\n",
      "Epoch 1187/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 504773.5625 - mean_squared_error: 504773.5625\n",
      "Epoch 1188/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 504912.7500 - mean_squared_error: 504912.7500\n",
      "Epoch 1189/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 507980.2500 - mean_squared_error: 507980.2500\n",
      "Epoch 1190/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 511441.8750 - mean_squared_error: 511441.8750\n",
      "Epoch 1191/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 503126.3750 - mean_squared_error: 503126.3750\n",
      "Epoch 1192/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 506125.5000 - mean_squared_error: 506125.5000\n",
      "Epoch 1193/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 504991.3125 - mean_squared_error: 504991.3125\n",
      "Epoch 1194/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 499848.1875 - mean_squared_error: 499848.1875\n",
      "Epoch 1195/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 496541.1250 - mean_squared_error: 496541.1250\n",
      "Epoch 1196/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 502412.7812 - mean_squared_error: 502412.7812\n",
      "Epoch 1197/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 501659.0312 - mean_squared_error: 501659.0312\n",
      "Epoch 1198/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 503201.3750 - mean_squared_error: 503201.3125\n",
      "Epoch 1199/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 498757.5000 - mean_squared_error: 498757.5000\n",
      "Epoch 1200/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 502089.3750 - mean_squared_error: 502089.3750\n",
      "Epoch 1201/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 500423.4688 - mean_squared_error: 500423.4688\n",
      "Epoch 1202/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 500552.8125 - mean_squared_error: 500552.8125\n",
      "Epoch 1203/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 502711.2812 - mean_squared_error: 502711.3125\n",
      "Epoch 1204/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 502200.3125 - mean_squared_error: 502200.3125\n",
      "Epoch 1205/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 502283.4375 - mean_squared_error: 502283.4375\n",
      "Epoch 1206/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 494324.4375 - mean_squared_error: 494324.4375\n",
      "Epoch 1207/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 498582.6250 - mean_squared_error: 498582.6250\n",
      "Epoch 1208/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 498447.8438 - mean_squared_error: 498447.8438\n",
      "Epoch 1209/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 493862.3125 - mean_squared_error: 493862.3125\n",
      "Epoch 1210/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 500474.2188 - mean_squared_error: 500474.2188\n",
      "Epoch 1211/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 494701.0312 - mean_squared_error: 494701.0312\n",
      "Epoch 1212/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 497061.5312 - mean_squared_error: 497061.5312\n",
      "Epoch 1213/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 496798.5938 - mean_squared_error: 496798.5938\n",
      "Epoch 1214/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 499117.7812 - mean_squared_error: 499117.7500\n",
      "Epoch 1215/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 494221.3438 - mean_squared_error: 494221.3438\n",
      "Epoch 1216/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 504350.6562 - mean_squared_error: 504350.6562\n",
      "Epoch 1217/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 498542.3750 - mean_squared_error: 498542.3750\n",
      "Epoch 1218/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 492297.4062 - mean_squared_error: 492297.4062\n",
      "Epoch 1219/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 497436.2812 - mean_squared_error: 497436.2812\n",
      "Epoch 1220/5000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 494931.5625 - mean_squared_error: 494931.5625\n",
      "Epoch 1221/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 503553.1250 - mean_squared_error: 503553.1250\n",
      "Epoch 1222/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 491279.5312 - mean_squared_error: 491279.4688\n",
      "Epoch 1223/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 493023.7188 - mean_squared_error: 493023.7188\n",
      "Epoch 1224/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 499127.1562 - mean_squared_error: 499127.1562\n",
      "Epoch 1225/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 489186.6875 - mean_squared_error: 489186.6875\n",
      "Epoch 1226/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 495137.7188 - mean_squared_error: 495137.7188\n",
      "Epoch 1227/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 491913.7188 - mean_squared_error: 491913.7188\n",
      "Epoch 1228/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 496553.4375 - mean_squared_error: 496553.4688\n",
      "Epoch 1229/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 487934.1562 - mean_squared_error: 487934.1562\n",
      "Epoch 1230/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 493356.3125 - mean_squared_error: 493356.3125\n",
      "Epoch 1231/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 492104.3750 - mean_squared_error: 492104.3438\n",
      "Epoch 1232/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 488953.0000 - mean_squared_error: 488953.0000\n",
      "Epoch 1233/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 490158.2500 - mean_squared_error: 490158.2500\n",
      "Epoch 1234/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 493228.6875 - mean_squared_error: 493228.6875\n",
      "Epoch 1235/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 488630.8125 - mean_squared_error: 488630.8125\n",
      "Epoch 1236/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 487014.0938 - mean_squared_error: 487014.0938\n",
      "Epoch 1237/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 492176.4375 - mean_squared_error: 492176.4375\n",
      "Epoch 1238/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 488836.0312 - mean_squared_error: 488836.0312\n",
      "Epoch 1239/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 491362.5625 - mean_squared_error: 491362.5625\n",
      "Epoch 1240/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 482564.5938 - mean_squared_error: 482564.5938\n",
      "Epoch 1241/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 488933.5938 - mean_squared_error: 488933.5938\n",
      "Epoch 1242/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 493111.2812 - mean_squared_error: 493111.2812\n",
      "Epoch 1243/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 488139.7500 - mean_squared_error: 488139.7500\n",
      "Epoch 1244/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 486148.7812 - mean_squared_error: 486148.7812\n",
      "Epoch 1245/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 480179.4062 - mean_squared_error: 480179.3438\n",
      "Epoch 1246/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 485772.8750 - mean_squared_error: 485772.8750\n",
      "Epoch 1247/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 487790.0312 - mean_squared_error: 487790.0312\n",
      "Epoch 1248/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 487039.7500 - mean_squared_error: 487039.7500\n",
      "Epoch 1249/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 481409.6562 - mean_squared_error: 481409.6562\n",
      "Epoch 1250/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 482758.5312 - mean_squared_error: 482758.5625\n",
      "Epoch 1251/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 479928.0312 - mean_squared_error: 479928.0312\n",
      "Epoch 1252/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 489879.5938 - mean_squared_error: 489879.5938\n",
      "Epoch 1253/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 486286.2812 - mean_squared_error: 486286.2812\n",
      "Epoch 1254/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 485132.8125 - mean_squared_error: 485132.8125\n",
      "Epoch 1255/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 481471.6250 - mean_squared_error: 481471.6250\n",
      "Epoch 1256/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 479734.6562 - mean_squared_error: 479734.6562\n",
      "Epoch 1257/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 479849.2500 - mean_squared_error: 479849.2500\n",
      "Epoch 1258/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 480464.5000 - mean_squared_error: 480464.5000\n",
      "Epoch 1259/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 475573.4062 - mean_squared_error: 475573.4062\n",
      "Epoch 1260/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 480207.6875 - mean_squared_error: 480207.6875\n",
      "Epoch 1261/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 482805.2188 - mean_squared_error: 482805.1562\n",
      "Epoch 1262/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 481268.3438 - mean_squared_error: 481268.3438\n",
      "Epoch 1263/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 470104.5312 - mean_squared_error: 470104.5312\n",
      "Epoch 1264/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 478651.7188 - mean_squared_error: 478651.7188\n",
      "Epoch 1265/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 476012.8125 - mean_squared_error: 476012.8125\n",
      "Epoch 1266/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 484050.4375 - mean_squared_error: 484050.4375\n",
      "Epoch 1267/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 480539.6562 - mean_squared_error: 480539.6562\n",
      "Epoch 1268/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 480726.1250 - mean_squared_error: 480726.1250\n",
      "Epoch 1269/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 476335.5000 - mean_squared_error: 476335.5000\n",
      "Epoch 1270/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 479954.0938 - mean_squared_error: 479954.0938\n",
      "Epoch 1271/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 475166.1562 - mean_squared_error: 475166.1562\n",
      "Epoch 1272/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 476042.3750 - mean_squared_error: 476042.4062\n",
      "Epoch 1273/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 474559.6250 - mean_squared_error: 474559.6250\n",
      "Epoch 1274/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 474782.1875 - mean_squared_error: 474782.1875\n",
      "Epoch 1275/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 468503.1250 - mean_squared_error: 468503.1250\n",
      "Epoch 1276/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 479133.0938 - mean_squared_error: 479133.1250\n",
      "Epoch 1277/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 476110.9688 - mean_squared_error: 476110.9688\n",
      "Epoch 1278/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 467736.5625 - mean_squared_error: 467736.5625\n",
      "Epoch 1279/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 477388.5312 - mean_squared_error: 477388.5312\n",
      "Epoch 1280/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 475486.2812 - mean_squared_error: 475486.2812\n",
      "Epoch 1281/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 470755.6250 - mean_squared_error: 470755.6250\n",
      "Epoch 1282/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 467663.6562 - mean_squared_error: 467663.6562\n",
      "Epoch 1283/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 474961.2500 - mean_squared_error: 474961.2500\n",
      "Epoch 1284/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 470696.5938 - mean_squared_error: 470696.5938\n",
      "Epoch 1285/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 472010.1875 - mean_squared_error: 472010.1875\n",
      "Epoch 1286/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 475696.7188 - mean_squared_error: 475696.7188\n",
      "Epoch 1287/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 468664.4688 - mean_squared_error: 468664.4688\n",
      "Epoch 1288/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 469648.0312 - mean_squared_error: 469648.0312\n",
      "Epoch 1289/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 466438.8438 - mean_squared_error: 466438.8438\n",
      "Epoch 1290/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 466672.3438 - mean_squared_error: 466672.3438\n",
      "Epoch 1291/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 472665.4062 - mean_squared_error: 472665.4062\n",
      "Epoch 1292/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 478102.6250 - mean_squared_error: 478102.6250\n",
      "Epoch 1293/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 474357.6562 - mean_squared_error: 474357.6562\n",
      "Epoch 1294/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 465307.8125 - mean_squared_error: 465307.8125\n",
      "Epoch 1295/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 478406.7812 - mean_squared_error: 478406.7500\n",
      "Epoch 1296/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 467832.1875 - mean_squared_error: 467832.1875\n",
      "Epoch 1297/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 473678.1250 - mean_squared_error: 473678.1250\n",
      "Epoch 1298/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 470804.7500 - mean_squared_error: 470804.7500\n",
      "Epoch 1299/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 470767.2500 - mean_squared_error: 470767.2500\n",
      "Epoch 1300/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 469600.5000 - mean_squared_error: 469600.5000\n",
      "Epoch 1301/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 468355.5000 - mean_squared_error: 468355.5000\n",
      "Epoch 1302/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 471538.0000 - mean_squared_error: 471538.0000\n",
      "Epoch 1303/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 466670.5625 - mean_squared_error: 466670.5625\n",
      "Epoch 1304/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 468538.5312 - mean_squared_error: 468538.5312\n",
      "Epoch 1305/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 463278.9375 - mean_squared_error: 463278.9375\n",
      "Epoch 1306/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 463138.7188 - mean_squared_error: 463138.7188\n",
      "Epoch 1307/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 464308.4062 - mean_squared_error: 464308.4062\n",
      "Epoch 1308/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 461987.0938 - mean_squared_error: 461987.0938\n",
      "Epoch 1309/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 461949.5625 - mean_squared_error: 461949.5625\n",
      "Epoch 1310/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 461743.3438 - mean_squared_error: 461743.3438\n",
      "Epoch 1311/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 458812.2812 - mean_squared_error: 458812.2812\n",
      "Epoch 1312/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 466690.9688 - mean_squared_error: 466690.9688\n",
      "Epoch 1313/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 457394.3125 - mean_squared_error: 457394.3125\n",
      "Epoch 1314/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 459214.8125 - mean_squared_error: 459214.8125\n",
      "Epoch 1315/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 462631.5938 - mean_squared_error: 462631.5938\n",
      "Epoch 1316/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 458556.6875 - mean_squared_error: 458556.6875\n",
      "Epoch 1317/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 462781.6250 - mean_squared_error: 462781.6250\n",
      "Epoch 1318/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 462630.6562 - mean_squared_error: 462630.6562\n",
      "Epoch 1319/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 465171.3125 - mean_squared_error: 465171.3125\n",
      "Epoch 1320/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 454613.0312 - mean_squared_error: 454613.0312\n",
      "Epoch 1321/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 460605.5312 - mean_squared_error: 460605.5312\n",
      "Epoch 1322/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 457673.3750 - mean_squared_error: 457673.3750\n",
      "Epoch 1323/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 455322.3125 - mean_squared_error: 455322.3125\n",
      "Epoch 1324/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 462354.4375 - mean_squared_error: 462354.4375\n",
      "Epoch 1325/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 457670.7500 - mean_squared_error: 457670.7500\n",
      "Epoch 1326/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 462652.8125 - mean_squared_error: 462652.8125\n",
      "Epoch 1327/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 454181.8438 - mean_squared_error: 454181.8438\n",
      "Epoch 1328/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 454980.9062 - mean_squared_error: 454980.9062\n",
      "Epoch 1329/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 447657.5312 - mean_squared_error: 447657.5312\n",
      "Epoch 1330/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 452374.8750 - mean_squared_error: 452374.8750\n",
      "Epoch 1331/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 455600.3438 - mean_squared_error: 455600.3438\n",
      "Epoch 1332/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 452519.5312 - mean_squared_error: 452519.5312\n",
      "Epoch 1333/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 444961.9062 - mean_squared_error: 444961.9062\n",
      "Epoch 1334/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 449472.7812 - mean_squared_error: 449472.7812\n",
      "Epoch 1335/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 447128.4062 - mean_squared_error: 447128.4062\n",
      "Epoch 1336/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 451472.9688 - mean_squared_error: 451472.9688\n",
      "Epoch 1337/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 441067.4375 - mean_squared_error: 441067.4375\n",
      "Epoch 1338/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 431728.5000 - mean_squared_error: 431728.5000\n",
      "Epoch 1339/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 467582.8438 - mean_squared_error: 467582.8438\n",
      "Epoch 1340/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 441409.5312 - mean_squared_error: 441409.5312\n",
      "Epoch 1341/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 452734.8438 - mean_squared_error: 452734.8438\n",
      "Epoch 1342/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 449837.8125 - mean_squared_error: 449837.8125\n",
      "Epoch 1343/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 449483.1875 - mean_squared_error: 449483.1875\n",
      "Epoch 1344/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 454542.8438 - mean_squared_error: 454542.8438\n",
      "Epoch 1345/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 458613.0625 - mean_squared_error: 458613.0625\n",
      "Epoch 1346/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 447454.0938 - mean_squared_error: 447454.0938\n",
      "Epoch 1347/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 448843.6875 - mean_squared_error: 448843.6875\n",
      "Epoch 1348/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 451185.2812 - mean_squared_error: 451185.2812\n",
      "Epoch 1349/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 453304.0938 - mean_squared_error: 453304.0938\n",
      "Epoch 1350/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 454570.7812 - mean_squared_error: 454570.7812\n",
      "Epoch 1351/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 452462.3750 - mean_squared_error: 452462.3750\n",
      "Epoch 1352/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 447177.7188 - mean_squared_error: 447177.7188\n",
      "Epoch 1353/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 449468.4375 - mean_squared_error: 449468.4375\n",
      "Epoch 1354/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 445487.7188 - mean_squared_error: 445487.7812\n",
      "Epoch 1355/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 447029.8438 - mean_squared_error: 447029.8438\n",
      "Epoch 1356/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 447766.1562 - mean_squared_error: 447766.1875\n",
      "Epoch 1357/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 447725.2812 - mean_squared_error: 447725.2812\n",
      "Epoch 1358/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 450685.4688 - mean_squared_error: 450685.4688\n",
      "Epoch 1359/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 439871.9375 - mean_squared_error: 439871.9375\n",
      "Epoch 1360/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 445912.9688 - mean_squared_error: 445912.9688\n",
      "Epoch 1361/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 438075.2188 - mean_squared_error: 438075.2188\n",
      "Epoch 1362/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 450228.5000 - mean_squared_error: 450228.5000\n",
      "Epoch 1363/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 451992.9062 - mean_squared_error: 451992.9062\n",
      "Epoch 1364/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 448001.8125 - mean_squared_error: 448001.8125\n",
      "Epoch 1365/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 445663.6250 - mean_squared_error: 445663.6250\n",
      "Epoch 1366/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 449383.7188 - mean_squared_error: 449383.7188\n",
      "Epoch 1367/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 438929.1250 - mean_squared_error: 438929.1250\n",
      "Epoch 1368/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 439610.9688 - mean_squared_error: 439610.9688\n",
      "Epoch 1369/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 445932.3750 - mean_squared_error: 445932.3750\n",
      "Epoch 1370/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 446120.3125 - mean_squared_error: 446120.3125\n",
      "Epoch 1371/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 442002.7812 - mean_squared_error: 442002.7812\n",
      "Epoch 1372/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 442548.3438 - mean_squared_error: 442548.3438\n",
      "Epoch 1373/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 440527.1250 - mean_squared_error: 440527.1250\n",
      "Epoch 1374/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 440486.1250 - mean_squared_error: 440486.1250\n",
      "Epoch 1375/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 450716.9375 - mean_squared_error: 450716.9375\n",
      "Epoch 1376/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 443609.7188 - mean_squared_error: 443609.7188\n",
      "Epoch 1377/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 444703.0312 - mean_squared_error: 444703.0312\n",
      "Epoch 1378/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 438728.8438 - mean_squared_error: 438728.8438\n",
      "Epoch 1379/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 445789.5625 - mean_squared_error: 445789.5625\n",
      "Epoch 1380/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 445915.4062 - mean_squared_error: 445915.4062\n",
      "Epoch 1381/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 438194.1250 - mean_squared_error: 438194.1250\n",
      "Epoch 1382/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 433374.1875 - mean_squared_error: 433374.1875\n",
      "Epoch 1383/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 435662.5938 - mean_squared_error: 435662.5938\n",
      "Epoch 1384/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 435994.4688 - mean_squared_error: 435994.4688\n",
      "Epoch 1385/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 441913.3438 - mean_squared_error: 441913.3438\n",
      "Epoch 1386/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 435644.6875 - mean_squared_error: 435644.6875\n",
      "Epoch 1387/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 439071.5625 - mean_squared_error: 439071.5000\n",
      "Epoch 1388/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 440038.1562 - mean_squared_error: 440038.1562\n",
      "Epoch 1389/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 442023.1875 - mean_squared_error: 442023.1875\n",
      "Epoch 1390/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 439134.6250 - mean_squared_error: 439134.6250\n",
      "Epoch 1391/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 434386.8125 - mean_squared_error: 434386.8125\n",
      "Epoch 1392/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 439007.7500 - mean_squared_error: 439007.7500\n",
      "Epoch 1393/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 441913.1562 - mean_squared_error: 441913.1562\n",
      "Epoch 1394/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 441828.5625 - mean_squared_error: 441828.5625\n",
      "Epoch 1395/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 435147.1875 - mean_squared_error: 435147.1875\n",
      "Epoch 1396/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 434469.7812 - mean_squared_error: 434469.7812\n",
      "Epoch 1397/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 437420.2812 - mean_squared_error: 437420.2812\n",
      "Epoch 1398/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 435724.9375 - mean_squared_error: 435724.9375\n",
      "Epoch 1399/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 440305.8750 - mean_squared_error: 440305.8750\n",
      "Epoch 1400/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 435000.5000 - mean_squared_error: 435000.5000\n",
      "Epoch 1401/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 430597.2188 - mean_squared_error: 430597.2188\n",
      "Epoch 1402/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 429990.5312 - mean_squared_error: 429990.5312\n",
      "Epoch 1403/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 436801.6875 - mean_squared_error: 436801.6875\n",
      "Epoch 1404/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 435776.3750 - mean_squared_error: 435776.3750\n",
      "Epoch 1405/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 434603.1250 - mean_squared_error: 434603.1250\n",
      "Epoch 1406/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 431883.3438 - mean_squared_error: 431883.3438\n",
      "Epoch 1407/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 438059.6562 - mean_squared_error: 438059.6562\n",
      "Epoch 1408/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 434007.6562 - mean_squared_error: 434007.6562\n",
      "Epoch 1409/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 430912.1875 - mean_squared_error: 430912.1875\n",
      "Epoch 1410/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 429702.0938 - mean_squared_error: 429702.0938\n",
      "Epoch 1411/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 432964.2188 - mean_squared_error: 432964.2188\n",
      "Epoch 1412/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 430982.0000 - mean_squared_error: 430982.0000\n",
      "Epoch 1413/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 434471.6562 - mean_squared_error: 434471.6562\n",
      "Epoch 1414/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 428037.0938 - mean_squared_error: 428037.0938\n",
      "Epoch 1415/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 435080.4375 - mean_squared_error: 435080.4375\n",
      "Epoch 1416/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 438405.0938 - mean_squared_error: 438405.0938\n",
      "Epoch 1417/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 429993.4688 - mean_squared_error: 429993.4688\n",
      "Epoch 1418/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 426714.5000 - mean_squared_error: 426714.5000\n",
      "Epoch 1419/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 434646.8438 - mean_squared_error: 434646.8750\n",
      "Epoch 1420/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 433762.6562 - mean_squared_error: 433762.6562\n",
      "Epoch 1421/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 435864.4062 - mean_squared_error: 435864.4062\n",
      "Epoch 1422/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 427985.3438 - mean_squared_error: 427985.3125\n",
      "Epoch 1423/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 428704.9688 - mean_squared_error: 428704.9688\n",
      "Epoch 1424/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 429144.5000 - mean_squared_error: 429144.5000\n",
      "Epoch 1425/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 427414.9062 - mean_squared_error: 427414.9062\n",
      "Epoch 1426/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 423569.9375 - mean_squared_error: 423569.9375\n",
      "Epoch 1427/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 426963.4688 - mean_squared_error: 426963.4688\n",
      "Epoch 1428/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 432195.6250 - mean_squared_error: 432195.6250\n",
      "Epoch 1429/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 426426.1875 - mean_squared_error: 426426.1875\n",
      "Epoch 1430/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 431384.7500 - mean_squared_error: 431384.7500\n",
      "Epoch 1431/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 427624.7500 - mean_squared_error: 427624.7500\n",
      "Epoch 1432/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 425491.2188 - mean_squared_error: 425491.2188\n",
      "Epoch 1433/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 423337.5312 - mean_squared_error: 423337.5312\n",
      "Epoch 1434/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 425854.6875 - mean_squared_error: 425854.6875\n",
      "Epoch 1435/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 428870.9688 - mean_squared_error: 428870.9688\n",
      "Epoch 1436/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 417079.5938 - mean_squared_error: 417079.5938\n",
      "Epoch 1437/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 426848.7500 - mean_squared_error: 426848.7500\n",
      "Epoch 1438/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 421507.8750 - mean_squared_error: 421507.8750\n",
      "Epoch 1439/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 423041.9375 - mean_squared_error: 423041.9375\n",
      "Epoch 1440/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 425042.2188 - mean_squared_error: 425042.2500\n",
      "Epoch 1441/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 421939.4688 - mean_squared_error: 421939.4688\n",
      "Epoch 1442/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 422476.1562 - mean_squared_error: 422476.1562\n",
      "Epoch 1443/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 428146.0625 - mean_squared_error: 428146.0625\n",
      "Epoch 1444/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 425797.0312 - mean_squared_error: 425797.0312\n",
      "Epoch 1445/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 419871.9062 - mean_squared_error: 419871.9062\n",
      "Epoch 1446/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 427283.0938 - mean_squared_error: 427283.0938\n",
      "Epoch 1447/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 422782.2188 - mean_squared_error: 422782.2188\n",
      "Epoch 1448/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 428636.2188 - mean_squared_error: 428636.2188\n",
      "Epoch 1449/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 425542.7188 - mean_squared_error: 425542.7188\n",
      "Epoch 1450/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 422116.6562 - mean_squared_error: 422116.6562\n",
      "Epoch 1451/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 420592.1250 - mean_squared_error: 420592.1250\n",
      "Epoch 1452/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 420669.5625 - mean_squared_error: 420669.5938\n",
      "Epoch 1453/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 430741.2812 - mean_squared_error: 430741.2812\n",
      "Epoch 1454/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 417692.1875 - mean_squared_error: 417692.1875\n",
      "Epoch 1455/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 423026.7812 - mean_squared_error: 423026.7500\n",
      "Epoch 1456/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 422003.5312 - mean_squared_error: 422003.5312\n",
      "Epoch 1457/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 415797.9688 - mean_squared_error: 415797.9688\n",
      "Epoch 1458/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 421544.9375 - mean_squared_error: 421544.9375\n",
      "Epoch 1459/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 422435.4375 - mean_squared_error: 422435.4375\n",
      "Epoch 1460/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 412997.0938 - mean_squared_error: 412997.0938\n",
      "Epoch 1461/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 419430.7188 - mean_squared_error: 419430.7188\n",
      "Epoch 1462/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 415238.7812 - mean_squared_error: 415238.7812\n",
      "Epoch 1463/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 414774.8125 - mean_squared_error: 414774.7812\n",
      "Epoch 1464/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 416525.3125 - mean_squared_error: 416525.3125\n",
      "Epoch 1465/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 418699.7188 - mean_squared_error: 418699.7188\n",
      "Epoch 1466/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 414487.2500 - mean_squared_error: 414487.2500\n",
      "Epoch 1467/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 422883.8125 - mean_squared_error: 422883.8125\n",
      "Epoch 1468/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 423024.1250 - mean_squared_error: 423024.1250\n",
      "Epoch 1469/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 411172.0312 - mean_squared_error: 411172.0312\n",
      "Epoch 1470/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 416542.0938 - mean_squared_error: 416542.0938\n",
      "Epoch 1471/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 418364.4688 - mean_squared_error: 418364.4688\n",
      "Epoch 1472/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 414781.5938 - mean_squared_error: 414781.5938\n",
      "Epoch 1473/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 413222.9688 - mean_squared_error: 413222.9688\n",
      "Epoch 1474/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 414774.5625 - mean_squared_error: 414774.5625\n",
      "Epoch 1475/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 412885.0312 - mean_squared_error: 412885.0312\n",
      "Epoch 1476/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 410693.5000 - mean_squared_error: 410693.5000\n",
      "Epoch 1477/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 417045.1562 - mean_squared_error: 417045.1562\n",
      "Epoch 1478/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 418627.8438 - mean_squared_error: 418627.8438\n",
      "Epoch 1479/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 413836.4062 - mean_squared_error: 413836.4062\n",
      "Epoch 1480/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 415443.0625 - mean_squared_error: 415443.0625\n",
      "Epoch 1481/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 414610.2500 - mean_squared_error: 414610.2500\n",
      "Epoch 1482/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 416462.0625 - mean_squared_error: 416462.0625\n",
      "Epoch 1483/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 415828.8438 - mean_squared_error: 415828.8438\n",
      "Epoch 1484/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 417890.4375 - mean_squared_error: 417890.4375\n",
      "Epoch 1485/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 415162.6250 - mean_squared_error: 415162.6250\n",
      "Epoch 1486/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 419879.0312 - mean_squared_error: 419879.0312\n",
      "Epoch 1487/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 416939.4375 - mean_squared_error: 416939.4375\n",
      "Epoch 1488/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 416913.8438 - mean_squared_error: 416913.8438\n",
      "Epoch 1489/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 415067.7812 - mean_squared_error: 415067.7812\n",
      "Epoch 1490/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 411423.3750 - mean_squared_error: 411423.3750\n",
      "Epoch 1491/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 415848.9375 - mean_squared_error: 415848.9375\n",
      "Epoch 1492/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 410152.1875 - mean_squared_error: 410152.1875\n",
      "Epoch 1493/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 413583.1562 - mean_squared_error: 413583.1562\n",
      "Epoch 1494/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 410915.5625 - mean_squared_error: 410915.5625\n",
      "Epoch 1495/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 414546.5000 - mean_squared_error: 414546.5000\n",
      "Epoch 1496/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 410964.5625 - mean_squared_error: 410964.5625\n",
      "Epoch 1497/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 410030.3750 - mean_squared_error: 410030.3750\n",
      "Epoch 1498/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 405760.9688 - mean_squared_error: 405760.9688\n",
      "Epoch 1499/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 408062.0938 - mean_squared_error: 408062.0938\n",
      "Epoch 1500/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 414376.8750 - mean_squared_error: 414376.8750\n",
      "Epoch 1501/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 415758.3750 - mean_squared_error: 415758.3750\n",
      "Epoch 1502/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 416472.0000 - mean_squared_error: 416472.0000\n",
      "Epoch 1503/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 411709.8438 - mean_squared_error: 411709.8438\n",
      "Epoch 1504/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 413735.4688 - mean_squared_error: 413735.4688\n",
      "Epoch 1505/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 405728.9688 - mean_squared_error: 405728.9688\n",
      "Epoch 1506/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 413128.8750 - mean_squared_error: 413128.8750\n",
      "Epoch 1507/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 410313.9375 - mean_squared_error: 410313.9375\n",
      "Epoch 1508/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 410745.3750 - mean_squared_error: 410745.3750\n",
      "Epoch 1509/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 407111.5000 - mean_squared_error: 407111.5000\n",
      "Epoch 1510/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 408769.5000 - mean_squared_error: 408769.5000\n",
      "Epoch 1511/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 402881.5312 - mean_squared_error: 402881.5312\n",
      "Epoch 1512/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 413087.6250 - mean_squared_error: 413087.6250\n",
      "Epoch 1513/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 403818.5625 - mean_squared_error: 403818.5625\n",
      "Epoch 1514/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 409700.0938 - mean_squared_error: 409700.0938\n",
      "Epoch 1515/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 404182.3438 - mean_squared_error: 404182.3438\n",
      "Epoch 1516/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 411913.5938 - mean_squared_error: 411913.5625\n",
      "Epoch 1517/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 404365.5625 - mean_squared_error: 404365.5625\n",
      "Epoch 1518/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 407125.4688 - mean_squared_error: 407125.4688\n",
      "Epoch 1519/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 402695.8750 - mean_squared_error: 402695.8750\n",
      "Epoch 1520/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 407089.4688 - mean_squared_error: 407089.4688\n",
      "Epoch 1521/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 404109.5938 - mean_squared_error: 404109.5938\n",
      "Epoch 1522/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 401742.4375 - mean_squared_error: 401742.4375\n",
      "Epoch 1523/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 404489.5625 - mean_squared_error: 404489.5625\n",
      "Epoch 1524/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 397712.7500 - mean_squared_error: 397712.7500\n",
      "Epoch 1525/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 409673.5000 - mean_squared_error: 409673.5000\n",
      "Epoch 1526/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 400819.3750 - mean_squared_error: 400819.3750\n",
      "Epoch 1527/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 395847.3438 - mean_squared_error: 395847.3438\n",
      "Epoch 1528/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 396541.5000 - mean_squared_error: 396541.5000\n",
      "Epoch 1529/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 401749.3750 - mean_squared_error: 401749.3750\n",
      "Epoch 1530/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 404839.7812 - mean_squared_error: 404839.7812\n",
      "Epoch 1531/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 405280.9375 - mean_squared_error: 405280.9375\n",
      "Epoch 1532/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 402370.2500 - mean_squared_error: 402370.2500\n",
      "Epoch 1533/5000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 397522.1875 - mean_squared_error: 397522.1562\n",
      "Epoch 1534/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 394898.0625 - mean_squared_error: 394898.0625\n",
      "Epoch 1535/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 398552.1562 - mean_squared_error: 398552.1562\n",
      "Epoch 1536/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 394373.4375 - mean_squared_error: 394373.4375\n",
      "Epoch 1537/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 387530.4375 - mean_squared_error: 387530.4062\n",
      "Epoch 1538/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 395991.1562 - mean_squared_error: 395991.1562\n",
      "Epoch 1539/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 408062.5312 - mean_squared_error: 408062.5312\n",
      "Epoch 1540/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 399414.1562 - mean_squared_error: 399414.1562\n",
      "Epoch 1541/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 402826.8750 - mean_squared_error: 402826.8438\n",
      "Epoch 1542/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 399956.9375 - mean_squared_error: 399956.9062\n",
      "Epoch 1543/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 408681.4375 - mean_squared_error: 408681.4375\n",
      "Epoch 1544/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 394910.6562 - mean_squared_error: 394910.6250\n",
      "Epoch 1545/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 396127.0625 - mean_squared_error: 396127.0625\n",
      "Epoch 1546/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 400665.9375 - mean_squared_error: 400665.9375\n",
      "Epoch 1547/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 397887.4688 - mean_squared_error: 397887.4688\n",
      "Epoch 1548/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 396203.5000 - mean_squared_error: 396203.5000\n",
      "Epoch 1549/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 394653.0938 - mean_squared_error: 394653.0938\n",
      "Epoch 1550/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 396109.5938 - mean_squared_error: 396109.5938\n",
      "Epoch 1551/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 404221.8125 - mean_squared_error: 404221.8125\n",
      "Epoch 1552/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 395357.4375 - mean_squared_error: 395357.4375\n",
      "Epoch 1553/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 395839.8750 - mean_squared_error: 395839.8750\n",
      "Epoch 1554/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 403050.6250 - mean_squared_error: 403050.6250\n",
      "Epoch 1555/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 393615.2812 - mean_squared_error: 393615.2812\n",
      "Epoch 1556/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 405586.1250 - mean_squared_error: 405586.1250\n",
      "Epoch 1557/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 397957.8125 - mean_squared_error: 397957.8125\n",
      "Epoch 1558/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 397049.0312 - mean_squared_error: 397049.0312\n",
      "Epoch 1559/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 393641.3125 - mean_squared_error: 393641.3125\n",
      "Epoch 1560/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 393199.0938 - mean_squared_error: 393199.0938\n",
      "Epoch 1561/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 393486.0000 - mean_squared_error: 393486.0000\n",
      "Epoch 1562/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 390065.5938 - mean_squared_error: 390065.6250\n",
      "Epoch 1563/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 395749.6562 - mean_squared_error: 395749.6562\n",
      "Epoch 1564/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 401724.5000 - mean_squared_error: 401724.5000\n",
      "Epoch 1565/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 396754.2500 - mean_squared_error: 396754.2500\n",
      "Epoch 1566/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 398573.0312 - mean_squared_error: 398573.0312\n",
      "Epoch 1567/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 390576.1250 - mean_squared_error: 390576.1250\n",
      "Epoch 1568/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 393238.5000 - mean_squared_error: 393238.5000\n",
      "Epoch 1569/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 393906.0625 - mean_squared_error: 393906.0625\n",
      "Epoch 1570/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 396183.6250 - mean_squared_error: 396183.6250\n",
      "Epoch 1571/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 393005.3438 - mean_squared_error: 393005.3438\n",
      "Epoch 1572/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 397038.1562 - mean_squared_error: 397038.1562\n",
      "Epoch 1573/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 394481.0000 - mean_squared_error: 394481.0000\n",
      "Epoch 1574/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 392501.0000 - mean_squared_error: 392501.0000\n",
      "Epoch 1575/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 389015.9688 - mean_squared_error: 389015.9688\n",
      "Epoch 1576/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 396265.3438 - mean_squared_error: 396265.3438\n",
      "Epoch 1577/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 397472.9375 - mean_squared_error: 397472.9375\n",
      "Epoch 1578/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 393471.4688 - mean_squared_error: 393471.4688\n",
      "Epoch 1579/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 396287.8750 - mean_squared_error: 396287.8750\n",
      "Epoch 1580/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 391788.5000 - mean_squared_error: 391788.5000\n",
      "Epoch 1581/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 387831.5312 - mean_squared_error: 387831.5312\n",
      "Epoch 1582/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 389783.1875 - mean_squared_error: 389783.1875\n",
      "Epoch 1583/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 379430.3125 - mean_squared_error: 379430.3125\n",
      "Epoch 1584/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 391721.0938 - mean_squared_error: 391721.0938\n",
      "Epoch 1585/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 390398.0000 - mean_squared_error: 390398.0000\n",
      "Epoch 1586/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 390108.0625 - mean_squared_error: 390108.0625\n",
      "Epoch 1587/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 379444.4688 - mean_squared_error: 379444.4688\n",
      "Epoch 1588/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 390978.7188 - mean_squared_error: 390978.7188\n",
      "Epoch 1589/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 389867.2812 - mean_squared_error: 389867.2812\n",
      "Epoch 1590/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 394256.5000 - mean_squared_error: 394256.5312\n",
      "Epoch 1591/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 389587.6250 - mean_squared_error: 389587.6562\n",
      "Epoch 1592/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 383922.6562 - mean_squared_error: 383922.6562\n",
      "Epoch 1593/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 394776.5312 - mean_squared_error: 394776.5938\n",
      "Epoch 1594/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 390758.5312 - mean_squared_error: 390758.5312\n",
      "Epoch 1595/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 388146.2812 - mean_squared_error: 388146.2812\n",
      "Epoch 1596/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 383300.4062 - mean_squared_error: 383300.4062\n",
      "Epoch 1597/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 392165.3438 - mean_squared_error: 392165.3438\n",
      "Epoch 1598/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 396160.6250 - mean_squared_error: 396160.6250\n",
      "Epoch 1599/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 381303.1250 - mean_squared_error: 381303.1562\n",
      "Epoch 1600/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 389357.0312 - mean_squared_error: 389357.0312\n",
      "Epoch 1601/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 386869.3438 - mean_squared_error: 386869.3750\n",
      "Epoch 1602/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 386895.2500 - mean_squared_error: 386895.2500\n",
      "Epoch 1603/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 382649.7812 - mean_squared_error: 382649.7812\n",
      "Epoch 1604/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 382595.3125 - mean_squared_error: 382595.3125\n",
      "Epoch 1605/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 386156.9062 - mean_squared_error: 386156.9062\n",
      "Epoch 1606/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 383158.3125 - mean_squared_error: 383158.3438\n",
      "Epoch 1607/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 385363.7812 - mean_squared_error: 385363.7812\n",
      "Epoch 1608/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 380940.5938 - mean_squared_error: 380940.5938\n",
      "Epoch 1609/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 382168.3750 - mean_squared_error: 382168.3750\n",
      "Epoch 1610/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 378361.5938 - mean_squared_error: 378361.6250\n",
      "Epoch 1611/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 382750.0625 - mean_squared_error: 382750.0625\n",
      "Epoch 1612/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 384786.2812 - mean_squared_error: 384786.2812\n",
      "Epoch 1613/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 376644.2812 - mean_squared_error: 376644.2812\n",
      "Epoch 1614/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 378693.0000 - mean_squared_error: 378693.0000\n",
      "Epoch 1615/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 382486.4688 - mean_squared_error: 382486.4688\n",
      "Epoch 1616/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 385634.3438 - mean_squared_error: 385634.3438\n",
      "Epoch 1617/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 385326.6875 - mean_squared_error: 385326.6875\n",
      "Epoch 1618/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 386614.7500 - mean_squared_error: 386614.7500\n",
      "Epoch 1619/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 380983.8438 - mean_squared_error: 380983.8438\n",
      "Epoch 1620/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 383691.1875 - mean_squared_error: 383691.1875\n",
      "Epoch 1621/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 380840.1875 - mean_squared_error: 380840.2188\n",
      "Epoch 1622/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 380707.2500 - mean_squared_error: 380707.2500\n",
      "Epoch 1623/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 386848.4375 - mean_squared_error: 386848.4375\n",
      "Epoch 1624/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 383580.8438 - mean_squared_error: 383580.8438\n",
      "Epoch 1625/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 391375.3438 - mean_squared_error: 391375.3438\n",
      "Epoch 1626/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 383139.6875 - mean_squared_error: 383139.6875\n",
      "Epoch 1627/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 381071.5312 - mean_squared_error: 381071.5312\n",
      "Epoch 1628/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 382148.3438 - mean_squared_error: 382148.3438\n",
      "Epoch 1629/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 377790.0938 - mean_squared_error: 377790.0938\n",
      "Epoch 1630/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 382124.4688 - mean_squared_error: 382124.4688\n",
      "Epoch 1631/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 380154.8125 - mean_squared_error: 380154.8125\n",
      "Epoch 1632/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 378369.4062 - mean_squared_error: 378369.3750\n",
      "Epoch 1633/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 380649.0000 - mean_squared_error: 380649.0000\n",
      "Epoch 1634/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 373418.3750 - mean_squared_error: 373418.3750\n",
      "Epoch 1635/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 378732.6562 - mean_squared_error: 378732.6562\n",
      "Epoch 1636/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 371389.7500 - mean_squared_error: 371389.7500\n",
      "Epoch 1637/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 374197.2188 - mean_squared_error: 374197.2188\n",
      "Epoch 1638/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 374801.7500 - mean_squared_error: 374801.7500\n",
      "Epoch 1639/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 379285.3438 - mean_squared_error: 379285.3438\n",
      "Epoch 1640/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 383040.1250 - mean_squared_error: 383040.1250\n",
      "Epoch 1641/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 381594.0938 - mean_squared_error: 381594.1250\n",
      "Epoch 1642/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 380396.3750 - mean_squared_error: 380396.3750\n",
      "Epoch 1643/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 373746.0000 - mean_squared_error: 373746.0000\n",
      "Epoch 1644/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 382434.1875 - mean_squared_error: 382434.1875\n",
      "Epoch 1645/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 377978.9062 - mean_squared_error: 377978.8750\n",
      "Epoch 1646/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 377988.5938 - mean_squared_error: 377988.5938\n",
      "Epoch 1647/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 374826.2188 - mean_squared_error: 374826.2188\n",
      "Epoch 1648/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 371494.7500 - mean_squared_error: 371494.7500\n",
      "Epoch 1649/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 377344.2500 - mean_squared_error: 377344.2500\n",
      "Epoch 1650/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 378996.3438 - mean_squared_error: 378996.3438\n",
      "Epoch 1651/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 378229.5625 - mean_squared_error: 378229.5625\n",
      "Epoch 1652/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 373795.5625 - mean_squared_error: 373795.5625\n",
      "Epoch 1653/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 373075.8125 - mean_squared_error: 373075.8125\n",
      "Epoch 1654/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 374444.8125 - mean_squared_error: 374444.8125\n",
      "Epoch 1655/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 369239.0000 - mean_squared_error: 369239.0000\n",
      "Epoch 1656/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 378988.6250 - mean_squared_error: 378988.6250\n",
      "Epoch 1657/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 374840.3125 - mean_squared_error: 374840.3125\n",
      "Epoch 1658/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 377380.6562 - mean_squared_error: 377380.6562\n",
      "Epoch 1659/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 371383.2812 - mean_squared_error: 371383.2812\n",
      "Epoch 1660/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 373583.3438 - mean_squared_error: 373583.3125\n",
      "Epoch 1661/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 374783.3750 - mean_squared_error: 374783.3438\n",
      "Epoch 1662/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 372589.9062 - mean_squared_error: 372589.9062\n",
      "Epoch 1663/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 374799.7812 - mean_squared_error: 374799.7812\n",
      "Epoch 1664/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 380525.2188 - mean_squared_error: 380525.2188\n",
      "Epoch 1665/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 372245.7188 - mean_squared_error: 372245.7188\n",
      "Epoch 1666/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 367958.2188 - mean_squared_error: 367958.2188\n",
      "Epoch 1667/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 362247.0625 - mean_squared_error: 362247.0625\n",
      "Epoch 1668/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 380259.9375 - mean_squared_error: 380259.9375\n",
      "Epoch 1669/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 371601.8125 - mean_squared_error: 371601.8125\n",
      "Epoch 1670/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 372656.9688 - mean_squared_error: 372656.9688\n",
      "Epoch 1671/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 366010.5938 - mean_squared_error: 366010.5938\n",
      "Epoch 1672/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 373175.1250 - mean_squared_error: 373175.1250\n",
      "Epoch 1673/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 372366.2812 - mean_squared_error: 372366.2812\n",
      "Epoch 1674/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 366525.3125 - mean_squared_error: 366525.3125\n",
      "Epoch 1675/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 368903.3750 - mean_squared_error: 368903.3750\n",
      "Epoch 1676/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 371640.0938 - mean_squared_error: 371640.0938\n",
      "Epoch 1677/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 372763.5312 - mean_squared_error: 372763.5312\n",
      "Epoch 1678/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 373890.8750 - mean_squared_error: 373890.8750\n",
      "Epoch 1679/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 366960.2812 - mean_squared_error: 366960.2812\n",
      "Epoch 1680/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 375638.6250 - mean_squared_error: 375638.6250\n",
      "Epoch 1681/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 369730.9375 - mean_squared_error: 369730.9375\n",
      "Epoch 1682/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 371904.5625 - mean_squared_error: 371904.5625\n",
      "Epoch 1683/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 374567.6562 - mean_squared_error: 374567.6562\n",
      "Epoch 1684/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 372300.3438 - mean_squared_error: 372300.3438\n",
      "Epoch 1685/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 367198.1875 - mean_squared_error: 367198.1562\n",
      "Epoch 1686/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 364990.2812 - mean_squared_error: 364990.2812\n",
      "Epoch 1687/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 375148.2812 - mean_squared_error: 375148.2812\n",
      "Epoch 1688/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 366552.7500 - mean_squared_error: 366552.7500\n",
      "Epoch 1689/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 368528.1250 - mean_squared_error: 368528.1250\n",
      "Epoch 1690/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 375522.9688 - mean_squared_error: 375522.9688\n",
      "Epoch 1691/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 369256.4688 - mean_squared_error: 369256.4688\n",
      "Epoch 1692/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 373128.0625 - mean_squared_error: 373128.0625\n",
      "Epoch 1693/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 374411.4688 - mean_squared_error: 374411.4688\n",
      "Epoch 1694/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 365209.2500 - mean_squared_error: 365209.2500\n",
      "Epoch 1695/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 366764.0938 - mean_squared_error: 366764.0938\n",
      "Epoch 1696/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 365056.4688 - mean_squared_error: 365056.4688\n",
      "Epoch 1697/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 368196.2812 - mean_squared_error: 368196.2812\n",
      "Epoch 1698/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 367764.3125 - mean_squared_error: 367764.3125\n",
      "Epoch 1699/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 365332.2500 - mean_squared_error: 365332.2500\n",
      "Epoch 1700/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 367983.5938 - mean_squared_error: 367983.5938\n",
      "Epoch 1701/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 368460.3438 - mean_squared_error: 368460.3438\n",
      "Epoch 1702/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 373873.5312 - mean_squared_error: 373873.5625\n",
      "Epoch 1703/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 367783.8125 - mean_squared_error: 367783.8125\n",
      "Epoch 1704/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 367976.7188 - mean_squared_error: 367976.7188\n",
      "Epoch 1705/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 367703.3438 - mean_squared_error: 367703.3438\n",
      "Epoch 1706/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 359850.0938 - mean_squared_error: 359850.0938\n",
      "Epoch 1707/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 365573.1875 - mean_squared_error: 365573.1875\n",
      "Epoch 1708/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 369353.1875 - mean_squared_error: 369353.1875\n",
      "Epoch 1709/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 371335.2812 - mean_squared_error: 371335.2812\n",
      "Epoch 1710/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 369735.3125 - mean_squared_error: 369735.3125\n",
      "Epoch 1711/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 368751.8438 - mean_squared_error: 368751.8438\n",
      "Epoch 1712/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 362788.2188 - mean_squared_error: 362788.2188\n",
      "Epoch 1713/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 363845.7188 - mean_squared_error: 363845.7500\n",
      "Epoch 1714/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 371429.8125 - mean_squared_error: 371429.8125\n",
      "Epoch 1715/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 362865.6875 - mean_squared_error: 362865.6875\n",
      "Epoch 1716/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 364148.0938 - mean_squared_error: 364148.0938\n",
      "Epoch 1717/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 365839.7500 - mean_squared_error: 365839.7500\n",
      "Epoch 1718/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 370919.2188 - mean_squared_error: 370919.2188\n",
      "Epoch 1719/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 363478.6562 - mean_squared_error: 363478.6562\n",
      "Epoch 1720/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 364307.0312 - mean_squared_error: 364307.0312\n",
      "Epoch 1721/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 366626.1562 - mean_squared_error: 366626.1562\n",
      "Epoch 1722/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 365623.7188 - mean_squared_error: 365623.7188\n",
      "Epoch 1723/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 362488.0938 - mean_squared_error: 362488.1250\n",
      "Epoch 1724/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353290.3125 - mean_squared_error: 353290.3125\n",
      "Epoch 1725/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 367041.0312 - mean_squared_error: 367041.0312\n",
      "Epoch 1726/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 366376.1875 - mean_squared_error: 366376.1875\n",
      "Epoch 1727/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 361798.8125 - mean_squared_error: 361798.8125\n",
      "Epoch 1728/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 368705.0938 - mean_squared_error: 368705.0938\n",
      "Epoch 1729/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 361449.7500 - mean_squared_error: 361449.7500\n",
      "Epoch 1730/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 363353.9688 - mean_squared_error: 363353.9688\n",
      "Epoch 1731/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 366534.3750 - mean_squared_error: 366534.3750\n",
      "Epoch 1732/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 365584.8438 - mean_squared_error: 365584.8438\n",
      "Epoch 1733/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 360141.5625 - mean_squared_error: 360141.5625\n",
      "Epoch 1734/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 365009.9688 - mean_squared_error: 365009.9688\n",
      "Epoch 1735/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 363968.6875 - mean_squared_error: 363968.6875\n",
      "Epoch 1736/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 365408.6875 - mean_squared_error: 365408.6562\n",
      "Epoch 1737/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 367542.8438 - mean_squared_error: 367542.8125\n",
      "Epoch 1738/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 364449.5938 - mean_squared_error: 364449.5938\n",
      "Epoch 1739/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 370649.2188 - mean_squared_error: 370649.2188\n",
      "Epoch 1740/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 359071.7812 - mean_squared_error: 359071.7812\n",
      "Epoch 1741/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 361360.8750 - mean_squared_error: 361360.8750\n",
      "Epoch 1742/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 358471.8750 - mean_squared_error: 358471.8750\n",
      "Epoch 1743/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352534.0938 - mean_squared_error: 352534.0938\n",
      "Epoch 1744/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 360005.3750 - mean_squared_error: 360005.3750\n",
      "Epoch 1745/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 359604.4375 - mean_squared_error: 359604.4375\n",
      "Epoch 1746/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 361665.4062 - mean_squared_error: 361665.4375\n",
      "Epoch 1747/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 362010.9062 - mean_squared_error: 362010.9062\n",
      "Epoch 1748/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 366332.0000 - mean_squared_error: 366332.0000\n",
      "Epoch 1749/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 364331.5625 - mean_squared_error: 364331.5625\n",
      "Epoch 1750/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357827.1250 - mean_squared_error: 357827.1250\n",
      "Epoch 1751/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 355015.7188 - mean_squared_error: 355015.7188\n",
      "Epoch 1752/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 359662.6875 - mean_squared_error: 359662.6875\n",
      "Epoch 1753/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 360008.7812 - mean_squared_error: 360008.7812\n",
      "Epoch 1754/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 366931.1562 - mean_squared_error: 366931.1562\n",
      "Epoch 1755/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 360053.0000 - mean_squared_error: 360053.0000\n",
      "Epoch 1756/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 357747.5312 - mean_squared_error: 357747.5312\n",
      "Epoch 1757/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 356776.5000 - mean_squared_error: 356776.5000\n",
      "Epoch 1758/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 359475.8750 - mean_squared_error: 359475.8750\n",
      "Epoch 1759/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 355778.2188 - mean_squared_error: 355778.2188\n",
      "Epoch 1760/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 359844.4688 - mean_squared_error: 359844.4688\n",
      "Epoch 1761/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356557.4375 - mean_squared_error: 356557.4375\n",
      "Epoch 1762/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354189.3438 - mean_squared_error: 354189.3438\n",
      "Epoch 1763/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 355064.0312 - mean_squared_error: 355064.0312\n",
      "Epoch 1764/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 359707.0938 - mean_squared_error: 359707.0938\n",
      "Epoch 1765/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356768.4375 - mean_squared_error: 356768.4375\n",
      "Epoch 1766/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356588.0625 - mean_squared_error: 356588.0625\n",
      "Epoch 1767/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357209.2188 - mean_squared_error: 357209.2188\n",
      "Epoch 1768/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 358522.9062 - mean_squared_error: 358522.9062\n",
      "Epoch 1769/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 349002.3438 - mean_squared_error: 349002.3438\n",
      "Epoch 1770/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 359062.8438 - mean_squared_error: 359062.8438\n",
      "Epoch 1771/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 355750.5312 - mean_squared_error: 355750.5312\n",
      "Epoch 1772/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 359653.1250 - mean_squared_error: 359653.1250\n",
      "Epoch 1773/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 360684.7188 - mean_squared_error: 360684.7188\n",
      "Epoch 1774/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356949.8125 - mean_squared_error: 356949.8125\n",
      "Epoch 1775/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 361441.5000 - mean_squared_error: 361441.5000\n",
      "Epoch 1776/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 357683.8438 - mean_squared_error: 357683.8750\n",
      "Epoch 1777/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353626.9062 - mean_squared_error: 353626.9062\n",
      "Epoch 1778/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351015.5938 - mean_squared_error: 351015.5938\n",
      "Epoch 1779/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356799.1250 - mean_squared_error: 356799.1250\n",
      "Epoch 1780/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354039.8438 - mean_squared_error: 354039.8438\n",
      "Epoch 1781/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353504.7812 - mean_squared_error: 353504.7812\n",
      "Epoch 1782/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354451.6250 - mean_squared_error: 354451.6250\n",
      "Epoch 1783/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352762.5000 - mean_squared_error: 352762.5000\n",
      "Epoch 1784/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356436.9062 - mean_squared_error: 356436.9062\n",
      "Epoch 1785/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356317.4375 - mean_squared_error: 356317.4375\n",
      "Epoch 1786/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357856.5625 - mean_squared_error: 357856.5625\n",
      "Epoch 1787/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 355663.9062 - mean_squared_error: 355663.9062\n",
      "Epoch 1788/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 353956.8125 - mean_squared_error: 353956.7812\n",
      "Epoch 1789/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356345.7500 - mean_squared_error: 356345.7812\n",
      "Epoch 1790/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352174.8125 - mean_squared_error: 352174.8438\n",
      "Epoch 1791/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356825.4688 - mean_squared_error: 356825.4688\n",
      "Epoch 1792/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 360988.1562 - mean_squared_error: 360988.1562\n",
      "Epoch 1793/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352456.8125 - mean_squared_error: 352456.8125\n",
      "Epoch 1794/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 361376.9375 - mean_squared_error: 361376.9375\n",
      "Epoch 1795/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350312.7812 - mean_squared_error: 350312.7812\n",
      "Epoch 1796/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 355405.3438 - mean_squared_error: 355405.3750\n",
      "Epoch 1797/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356418.0312 - mean_squared_error: 356418.0312\n",
      "Epoch 1798/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352749.7188 - mean_squared_error: 352749.7188\n",
      "Epoch 1799/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349302.8125 - mean_squared_error: 349302.8125\n",
      "Epoch 1800/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346911.3125 - mean_squared_error: 346911.3125\n",
      "Epoch 1801/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352836.1250 - mean_squared_error: 352836.1250\n",
      "Epoch 1802/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354727.5625 - mean_squared_error: 354727.5625\n",
      "Epoch 1803/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347446.7500 - mean_squared_error: 347446.7500\n",
      "Epoch 1804/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 354169.9375 - mean_squared_error: 354169.9375\n",
      "Epoch 1805/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351209.3438 - mean_squared_error: 351209.3750\n",
      "Epoch 1806/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 355315.9062 - mean_squared_error: 355315.9062\n",
      "Epoch 1807/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 355336.1875 - mean_squared_error: 355336.1875\n",
      "Epoch 1808/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353233.2500 - mean_squared_error: 353233.2500\n",
      "Epoch 1809/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345849.9375 - mean_squared_error: 345849.9375\n",
      "Epoch 1810/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352886.3438 - mean_squared_error: 352886.3438\n",
      "Epoch 1811/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349510.0000 - mean_squared_error: 349510.0000\n",
      "Epoch 1812/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343008.0000 - mean_squared_error: 343008.0000\n",
      "Epoch 1813/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356548.3125 - mean_squared_error: 356548.3125\n",
      "Epoch 1814/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345446.3750 - mean_squared_error: 345446.4062\n",
      "Epoch 1815/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344813.6562 - mean_squared_error: 344813.6562\n",
      "Epoch 1816/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 355120.5312 - mean_squared_error: 355120.5312\n",
      "Epoch 1817/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343655.4688 - mean_squared_error: 343655.4688\n",
      "Epoch 1818/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353296.5312 - mean_squared_error: 353296.5312\n",
      "Epoch 1819/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345637.4688 - mean_squared_error: 345637.4688\n",
      "Epoch 1820/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351326.4062 - mean_squared_error: 351326.4062\n",
      "Epoch 1821/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353099.6250 - mean_squared_error: 353099.6250\n",
      "Epoch 1822/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348662.0938 - mean_squared_error: 348662.1250\n",
      "Epoch 1823/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348801.3125 - mean_squared_error: 348801.3125\n",
      "Epoch 1824/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350854.0312 - mean_squared_error: 350854.0312\n",
      "Epoch 1825/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351446.8125 - mean_squared_error: 351446.8125\n",
      "Epoch 1826/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351390.9688 - mean_squared_error: 351390.9688\n",
      "Epoch 1827/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352615.2188 - mean_squared_error: 352615.2188\n",
      "Epoch 1828/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351213.8125 - mean_squared_error: 351213.8125\n",
      "Epoch 1829/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353753.0312 - mean_squared_error: 353753.0312\n",
      "Epoch 1830/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352120.2188 - mean_squared_error: 352120.2188\n",
      "Epoch 1831/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350284.3750 - mean_squared_error: 350284.3750\n",
      "Epoch 1832/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348574.1250 - mean_squared_error: 348574.1562\n",
      "Epoch 1833/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340262.9062 - mean_squared_error: 340262.9062\n",
      "Epoch 1834/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348811.6875 - mean_squared_error: 348811.6875\n",
      "Epoch 1835/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350285.0312 - mean_squared_error: 350285.0312\n",
      "Epoch 1836/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344878.3438 - mean_squared_error: 344878.3125\n",
      "Epoch 1837/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344029.5312 - mean_squared_error: 344029.5312\n",
      "Epoch 1838/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343547.0000 - mean_squared_error: 343547.0000\n",
      "Epoch 1839/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343242.0312 - mean_squared_error: 343242.0312\n",
      "Epoch 1840/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348382.6562 - mean_squared_error: 348382.6562\n",
      "Epoch 1841/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338214.0938 - mean_squared_error: 338214.0938\n",
      "Epoch 1842/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350938.5938 - mean_squared_error: 350938.5938\n",
      "Epoch 1843/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347792.0000 - mean_squared_error: 347792.0000\n",
      "Epoch 1844/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 350447.7188 - mean_squared_error: 350447.7188\n",
      "Epoch 1845/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349496.8750 - mean_squared_error: 349496.8750\n",
      "Epoch 1846/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351771.2188 - mean_squared_error: 351771.2188\n",
      "Epoch 1847/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350634.9062 - mean_squared_error: 350634.9062\n",
      "Epoch 1848/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346042.6250 - mean_squared_error: 346042.6250\n",
      "Epoch 1849/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339118.4375 - mean_squared_error: 339118.4375\n",
      "Epoch 1850/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340772.5938 - mean_squared_error: 340772.5938\n",
      "Epoch 1851/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344214.7812 - mean_squared_error: 344214.7812\n",
      "Epoch 1852/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340883.4688 - mean_squared_error: 340883.4688\n",
      "Epoch 1853/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348648.5938 - mean_squared_error: 348648.5938\n",
      "Epoch 1854/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342709.9062 - mean_squared_error: 342709.9062\n",
      "Epoch 1855/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341352.3438 - mean_squared_error: 341352.3438\n",
      "Epoch 1856/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341565.8438 - mean_squared_error: 341565.8438\n",
      "Epoch 1857/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348267.7812 - mean_squared_error: 348267.8125\n",
      "Epoch 1858/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347872.7812 - mean_squared_error: 347872.7812\n",
      "Epoch 1859/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345558.2812 - mean_squared_error: 345558.2500\n",
      "Epoch 1860/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350020.0938 - mean_squared_error: 350020.0938\n",
      "Epoch 1861/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345973.4688 - mean_squared_error: 345973.4688\n",
      "Epoch 1862/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346173.5000 - mean_squared_error: 346173.5000\n",
      "Epoch 1863/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340161.3438 - mean_squared_error: 340161.3438\n",
      "Epoch 1864/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342477.9375 - mean_squared_error: 342477.9375\n",
      "Epoch 1865/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344334.7500 - mean_squared_error: 344334.7500\n",
      "Epoch 1866/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349241.6562 - mean_squared_error: 349241.6250\n",
      "Epoch 1867/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341201.5625 - mean_squared_error: 341201.5625\n",
      "Epoch 1868/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341835.9688 - mean_squared_error: 341835.9688\n",
      "Epoch 1869/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 332245.0938 - mean_squared_error: 332245.0938\n",
      "Epoch 1870/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347609.6875 - mean_squared_error: 347609.6875\n",
      "Epoch 1871/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341994.3438 - mean_squared_error: 341994.3438\n",
      "Epoch 1872/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337443.8438 - mean_squared_error: 337443.8438\n",
      "Epoch 1873/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346320.9062 - mean_squared_error: 346320.9062\n",
      "Epoch 1874/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343280.6250 - mean_squared_error: 343280.6250\n",
      "Epoch 1875/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345934.5000 - mean_squared_error: 345934.5312\n",
      "Epoch 1876/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346283.0312 - mean_squared_error: 346283.0312\n",
      "Epoch 1877/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343786.6562 - mean_squared_error: 343786.6562\n",
      "Epoch 1878/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343008.9062 - mean_squared_error: 343008.9062\n",
      "Epoch 1879/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336781.2812 - mean_squared_error: 336781.2812\n",
      "Epoch 1880/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337234.6562 - mean_squared_error: 337234.6562\n",
      "Epoch 1881/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338745.4062 - mean_squared_error: 338745.4062\n",
      "Epoch 1882/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339148.7812 - mean_squared_error: 339148.7812\n",
      "Epoch 1883/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340348.8438 - mean_squared_error: 340348.8438\n",
      "Epoch 1884/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339676.0000 - mean_squared_error: 339676.0000\n",
      "Epoch 1885/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340615.8438 - mean_squared_error: 340615.8438\n",
      "Epoch 1886/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343892.5312 - mean_squared_error: 343892.5312\n",
      "Epoch 1887/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339551.7188 - mean_squared_error: 339551.7188\n",
      "Epoch 1888/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341161.9375 - mean_squared_error: 341161.9375\n",
      "Epoch 1889/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349941.9062 - mean_squared_error: 349941.9062\n",
      "Epoch 1890/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343806.7500 - mean_squared_error: 343806.7500\n",
      "Epoch 1891/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348016.7188 - mean_squared_error: 348016.7188\n",
      "Epoch 1892/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342226.3750 - mean_squared_error: 342226.3438\n",
      "Epoch 1893/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343603.8438 - mean_squared_error: 343603.8438\n",
      "Epoch 1894/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337083.8750 - mean_squared_error: 337083.8750\n",
      "Epoch 1895/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338320.2500 - mean_squared_error: 338320.2500\n",
      "Epoch 1896/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345590.5625 - mean_squared_error: 345590.5625\n",
      "Epoch 1897/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341148.6875 - mean_squared_error: 341148.6875\n",
      "Epoch 1898/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337153.5938 - mean_squared_error: 337153.5938\n",
      "Epoch 1899/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345332.8438 - mean_squared_error: 345332.8438\n",
      "Epoch 1900/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336072.0000 - mean_squared_error: 336072.0000\n",
      "Epoch 1901/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335907.6250 - mean_squared_error: 335907.6250\n",
      "Epoch 1902/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340604.8750 - mean_squared_error: 340604.8750\n",
      "Epoch 1903/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340408.4688 - mean_squared_error: 340408.5312\n",
      "Epoch 1904/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337334.4375 - mean_squared_error: 337334.4375\n",
      "Epoch 1905/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336343.4062 - mean_squared_error: 336343.4062\n",
      "Epoch 1906/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339398.8125 - mean_squared_error: 339398.8125\n",
      "Epoch 1907/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 332608.1562 - mean_squared_error: 332608.1562\n",
      "Epoch 1908/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 328353.4688 - mean_squared_error: 328353.4688\n",
      "Epoch 1909/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338262.1562 - mean_squared_error: 338262.1562\n",
      "Epoch 1910/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 335351.1562 - mean_squared_error: 335351.1250\n",
      "Epoch 1911/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336104.9062 - mean_squared_error: 336104.9062\n",
      "Epoch 1912/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338432.7812 - mean_squared_error: 338432.7812\n",
      "Epoch 1913/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 333476.8438 - mean_squared_error: 333476.8438\n",
      "Epoch 1914/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 331152.5625 - mean_squared_error: 331152.5625\n",
      "Epoch 1915/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338155.7500 - mean_squared_error: 338155.7500\n",
      "Epoch 1916/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 333701.6562 - mean_squared_error: 333701.6562\n",
      "Epoch 1917/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337913.0000 - mean_squared_error: 337912.9688\n",
      "Epoch 1918/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 331562.2188 - mean_squared_error: 331562.2188\n",
      "Epoch 1919/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336612.6562 - mean_squared_error: 336612.6562\n",
      "Epoch 1920/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 340117.0938 - mean_squared_error: 340117.0938\n",
      "Epoch 1921/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340636.4688 - mean_squared_error: 340636.4375\n",
      "Epoch 1922/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 335832.1562 - mean_squared_error: 335832.1562\n",
      "Epoch 1923/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 333613.7188 - mean_squared_error: 333613.7188\n",
      "Epoch 1924/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338570.6250 - mean_squared_error: 338570.6250\n",
      "Epoch 1925/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342378.0938 - mean_squared_error: 342378.0938\n",
      "Epoch 1926/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339462.1250 - mean_squared_error: 339462.1250\n",
      "Epoch 1927/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345228.8750 - mean_squared_error: 345228.8750\n",
      "Epoch 1928/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 333378.7812 - mean_squared_error: 333378.7812\n",
      "Epoch 1929/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 330977.7188 - mean_squared_error: 330977.7188\n",
      "Epoch 1930/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 324468.3125 - mean_squared_error: 324468.2812\n",
      "Epoch 1931/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 334242.4375 - mean_squared_error: 334242.4375\n",
      "Epoch 1932/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 332085.9375 - mean_squared_error: 332085.9375\n",
      "Epoch 1933/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 334264.9688 - mean_squared_error: 334264.9688\n",
      "Epoch 1934/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338247.1250 - mean_squared_error: 338247.1250\n",
      "Epoch 1935/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 334522.6250 - mean_squared_error: 334522.6250\n",
      "Epoch 1936/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339438.8125 - mean_squared_error: 339438.8125\n",
      "Epoch 1937/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 331217.8125 - mean_squared_error: 331217.8125\n",
      "Epoch 1938/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340558.1250 - mean_squared_error: 340558.1250\n",
      "Epoch 1939/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 332122.7500 - mean_squared_error: 332122.7500\n",
      "Epoch 1940/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337400.0312 - mean_squared_error: 337400.0312\n",
      "Epoch 1941/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 333971.3750 - mean_squared_error: 333971.3750\n",
      "Epoch 1942/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 335727.0312 - mean_squared_error: 335727.0312\n",
      "Epoch 1943/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341978.2500 - mean_squared_error: 341978.2500\n",
      "Epoch 1944/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337592.8750 - mean_squared_error: 337592.8750\n",
      "Epoch 1945/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 329741.8438 - mean_squared_error: 329741.8438\n",
      "Epoch 1946/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 331433.2188 - mean_squared_error: 331433.2188\n",
      "Epoch 1947/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340260.0000 - mean_squared_error: 340260.0000\n",
      "Epoch 1948/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338050.5312 - mean_squared_error: 338050.5312\n",
      "Epoch 1949/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 331608.2188 - mean_squared_error: 331608.2188\n",
      "Epoch 1950/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 328115.8125 - mean_squared_error: 328115.8125\n",
      "Epoch 1951/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339498.1562 - mean_squared_error: 339498.1562\n",
      "Epoch 1952/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335945.0312 - mean_squared_error: 335945.0312\n",
      "Epoch 1953/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 334155.3438 - mean_squared_error: 334155.3438\n",
      "Epoch 1954/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335527.6250 - mean_squared_error: 335527.6250\n",
      "Epoch 1955/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 328594.2188 - mean_squared_error: 328594.2188\n",
      "Epoch 1956/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 334633.2188 - mean_squared_error: 334633.2188\n",
      "Epoch 1957/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 330851.5938 - mean_squared_error: 330851.5938\n",
      "Epoch 1958/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335690.1562 - mean_squared_error: 335690.1875\n",
      "Epoch 1959/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336778.9688 - mean_squared_error: 336778.9375\n",
      "Epoch 1960/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 332179.9375 - mean_squared_error: 332179.9375\n",
      "Epoch 1961/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 334449.9375 - mean_squared_error: 334449.9375\n",
      "Epoch 1962/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 332841.9688 - mean_squared_error: 332841.9688\n",
      "Epoch 1963/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 331073.9688 - mean_squared_error: 331073.9688\n",
      "Epoch 1964/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337743.0625 - mean_squared_error: 337743.0625\n",
      "Epoch 1965/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 333021.7500 - mean_squared_error: 333021.7500\n",
      "Epoch 1966/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 333211.6562 - mean_squared_error: 333211.6562\n",
      "Epoch 1967/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337117.5625 - mean_squared_error: 337117.5625\n",
      "Epoch 1968/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 330143.4688 - mean_squared_error: 330143.4688\n",
      "Epoch 1969/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 379749.4375 - mean_squared_error: 379749.4375\n",
      "Epoch 1970/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 329281.3750 - mean_squared_error: 329281.3750\n",
      "Epoch 1971/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 308988.3438 - mean_squared_error: 308988.3438\n",
      "Epoch 1972/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 989130.8125 - mean_squared_error: 989130.8125\n",
      "Epoch 1973/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 858674.4375 - mean_squared_error: 858674.4375\n",
      "Epoch 1974/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 795968.6250 - mean_squared_error: 795968.6250\n",
      "Epoch 1975/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 736515.7500 - mean_squared_error: 736515.7500\n",
      "Epoch 1976/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 14ms/step - loss: 631457.3750 - mean_squared_error: 631457.3750\n",
      "Epoch 1977/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 415403.3750 - mean_squared_error: 415403.3750\n",
      "Epoch 1978/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 377595.7188 - mean_squared_error: 377595.6875\n",
      "Epoch 1979/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 382881.1250 - mean_squared_error: 382881.1250\n",
      "Epoch 1980/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 383177.8125 - mean_squared_error: 383177.8438\n",
      "Epoch 1981/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 375830.2812 - mean_squared_error: 375830.2812\n",
      "Epoch 1982/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 387760.5312 - mean_squared_error: 387760.5312\n",
      "Epoch 1983/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 385792.9062 - mean_squared_error: 385792.9062\n",
      "Epoch 1984/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 378917.3125 - mean_squared_error: 378917.3125\n",
      "Epoch 1985/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 381900.1875 - mean_squared_error: 381900.1875\n",
      "Epoch 1986/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 373306.4062 - mean_squared_error: 373306.4062\n",
      "Epoch 1987/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 378104.9062 - mean_squared_error: 378104.9375\n",
      "Epoch 1988/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 386429.8438 - mean_squared_error: 386429.8438\n",
      "Epoch 1989/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 383671.7500 - mean_squared_error: 383671.7500\n",
      "Epoch 1990/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 376472.7500 - mean_squared_error: 376472.7500\n",
      "Epoch 1991/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 375541.2188 - mean_squared_error: 375541.2188\n",
      "Epoch 1992/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 373581.4375 - mean_squared_error: 373581.4375\n",
      "Epoch 1993/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 377725.9375 - mean_squared_error: 377725.9375\n",
      "Epoch 1994/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 375985.6250 - mean_squared_error: 375985.6250\n",
      "Epoch 1995/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 373900.0625 - mean_squared_error: 373900.0625\n",
      "Epoch 1996/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 375570.7812 - mean_squared_error: 375570.7812\n",
      "Epoch 1997/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 376509.7500 - mean_squared_error: 376509.7500\n",
      "Epoch 1998/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 376626.9688 - mean_squared_error: 376626.9688\n",
      "Epoch 1999/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 377774.3125 - mean_squared_error: 377774.3125\n",
      "Epoch 2000/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 369922.7500 - mean_squared_error: 369922.7500\n",
      "Epoch 2001/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 377699.7500 - mean_squared_error: 377699.7500\n",
      "Epoch 2002/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 372608.0312 - mean_squared_error: 372608.0625\n",
      "Epoch 2003/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 375536.1250 - mean_squared_error: 375536.1250\n",
      "Epoch 2004/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 376968.9688 - mean_squared_error: 376969.0000\n",
      "Epoch 2005/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 364618.7812 - mean_squared_error: 364618.7812\n",
      "Epoch 2006/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 374645.5625 - mean_squared_error: 374645.5625\n",
      "Epoch 2007/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 372925.5000 - mean_squared_error: 372925.5000\n",
      "Epoch 2008/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 366979.1250 - mean_squared_error: 366979.1250\n",
      "Epoch 2009/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 367617.2188 - mean_squared_error: 367617.2188\n",
      "Epoch 2010/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 366533.8438 - mean_squared_error: 366533.8438\n",
      "Epoch 2011/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 376674.1875 - mean_squared_error: 376674.1875\n",
      "Epoch 2012/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 370848.4375 - mean_squared_error: 370848.4375\n",
      "Epoch 2013/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 366415.2812 - mean_squared_error: 366415.2812\n",
      "Epoch 2014/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 373128.6875 - mean_squared_error: 373128.6875\n",
      "Epoch 2015/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 374699.3438 - mean_squared_error: 374699.3125\n",
      "Epoch 2016/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 375502.6562 - mean_squared_error: 375502.6562\n",
      "Epoch 2017/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 371851.2500 - mean_squared_error: 371851.2500\n",
      "Epoch 2018/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 366830.2188 - mean_squared_error: 366830.2188\n",
      "Epoch 2019/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 372724.3125 - mean_squared_error: 372724.3125\n",
      "Epoch 2020/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 371809.7812 - mean_squared_error: 371809.8125\n",
      "Epoch 2021/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 360756.3125 - mean_squared_error: 360756.3125\n",
      "Epoch 2022/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 368550.4375 - mean_squared_error: 368550.4375\n",
      "Epoch 2023/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 373409.4375 - mean_squared_error: 373409.4375\n",
      "Epoch 2024/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 364973.3438 - mean_squared_error: 364973.3438\n",
      "Epoch 2025/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 370867.1875 - mean_squared_error: 370867.1875\n",
      "Epoch 2026/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 362994.3750 - mean_squared_error: 362994.4062\n",
      "Epoch 2027/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 366174.6875 - mean_squared_error: 366174.6875\n",
      "Epoch 2028/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 370414.5000 - mean_squared_error: 370414.5000\n",
      "Epoch 2029/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 374822.4062 - mean_squared_error: 374822.4062\n",
      "Epoch 2030/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 369198.2188 - mean_squared_error: 369198.2188\n",
      "Epoch 2031/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 372138.0625 - mean_squared_error: 372138.0625\n",
      "Epoch 2032/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 366214.4375 - mean_squared_error: 366214.4375\n",
      "Epoch 2033/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 369292.6562 - mean_squared_error: 369292.6875\n",
      "Epoch 2034/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 373260.6875 - mean_squared_error: 373260.6875\n",
      "Epoch 2035/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 371894.6562 - mean_squared_error: 371894.6562\n",
      "Epoch 2036/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 366355.0000 - mean_squared_error: 366355.0000\n",
      "Epoch 2037/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 365419.9062 - mean_squared_error: 365419.9062\n",
      "Epoch 2038/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 367995.5938 - mean_squared_error: 367995.5938\n",
      "Epoch 2039/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 368040.9062 - mean_squared_error: 368040.9062\n",
      "Epoch 2040/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 370371.8750 - mean_squared_error: 370371.8750\n",
      "Epoch 2041/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 370568.8125 - mean_squared_error: 370568.8125\n",
      "Epoch 2042/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 363465.3125 - mean_squared_error: 363465.3125\n",
      "Epoch 2043/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 364118.6562 - mean_squared_error: 364118.6562\n",
      "Epoch 2044/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 362490.4375 - mean_squared_error: 362490.4375\n",
      "Epoch 2045/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 371390.6250 - mean_squared_error: 371390.5938\n",
      "Epoch 2046/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 365373.2188 - mean_squared_error: 365373.2188\n",
      "Epoch 2047/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 363477.6250 - mean_squared_error: 363477.6250\n",
      "Epoch 2048/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 370980.2812 - mean_squared_error: 370980.2812\n",
      "Epoch 2049/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 360575.5625 - mean_squared_error: 360575.5625\n",
      "Epoch 2050/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 355534.2188 - mean_squared_error: 355534.2188\n",
      "Epoch 2051/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 360672.1250 - mean_squared_error: 360672.1250\n",
      "Epoch 2052/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 363112.6250 - mean_squared_error: 363112.6250\n",
      "Epoch 2053/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 362607.7812 - mean_squared_error: 362607.7812\n",
      "Epoch 2054/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 368051.0312 - mean_squared_error: 368051.0312\n",
      "Epoch 2055/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 366392.2500 - mean_squared_error: 366392.2500\n",
      "Epoch 2056/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 361009.8125 - mean_squared_error: 361009.8125\n",
      "Epoch 2057/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 362099.8125 - mean_squared_error: 362099.8125\n",
      "Epoch 2058/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 361283.2188 - mean_squared_error: 361283.2188\n",
      "Epoch 2059/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 373923.2500 - mean_squared_error: 373923.2812\n",
      "Epoch 2060/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 359005.5625 - mean_squared_error: 359005.5625\n",
      "Epoch 2061/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 365128.4062 - mean_squared_error: 365128.4062\n",
      "Epoch 2062/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 365767.9062 - mean_squared_error: 365767.9062\n",
      "Epoch 2063/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 370508.7188 - mean_squared_error: 370508.7188\n",
      "Epoch 2064/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 367425.3125 - mean_squared_error: 367425.3125\n",
      "Epoch 2065/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 379240.1875 - mean_squared_error: 379240.1875\n",
      "Epoch 2066/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 366865.8438 - mean_squared_error: 366865.8125\n",
      "Epoch 2067/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 363067.7812 - mean_squared_error: 363067.7812\n",
      "Epoch 2068/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 361994.7812 - mean_squared_error: 361994.7812\n",
      "Epoch 2069/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 359867.8750 - mean_squared_error: 359867.8750\n",
      "Epoch 2070/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 360085.4688 - mean_squared_error: 360085.4688\n",
      "Epoch 2071/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 369074.8438 - mean_squared_error: 369074.8438\n",
      "Epoch 2072/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357719.6875 - mean_squared_error: 357719.6875\n",
      "Epoch 2073/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 363951.0312 - mean_squared_error: 363951.0312\n",
      "Epoch 2074/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 359356.4375 - mean_squared_error: 359356.4375\n",
      "Epoch 2075/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354642.6562 - mean_squared_error: 354642.6562\n",
      "Epoch 2076/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 359820.6562 - mean_squared_error: 359820.6562\n",
      "Epoch 2077/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 365515.9375 - mean_squared_error: 365515.9375\n",
      "Epoch 2078/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 365801.9688 - mean_squared_error: 365801.9688\n",
      "Epoch 2079/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 361841.3125 - mean_squared_error: 361841.3125\n",
      "Epoch 2080/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 365966.9688 - mean_squared_error: 365966.9688\n",
      "Epoch 2081/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 360016.3750 - mean_squared_error: 360016.3750\n",
      "Epoch 2082/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 360816.7188 - mean_squared_error: 360816.7188\n",
      "Epoch 2083/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 361084.5625 - mean_squared_error: 361084.6250\n",
      "Epoch 2084/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 364579.5625 - mean_squared_error: 364579.5625\n",
      "Epoch 2085/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 365474.0938 - mean_squared_error: 365474.0938\n",
      "Epoch 2086/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 373651.1250 - mean_squared_error: 373651.0938\n",
      "Epoch 2087/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 359686.1875 - mean_squared_error: 359686.1875\n",
      "Epoch 2088/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 361307.0938 - mean_squared_error: 361307.0938\n",
      "Epoch 2089/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 366656.1875 - mean_squared_error: 366656.1875\n",
      "Epoch 2090/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 358418.2500 - mean_squared_error: 358418.2500\n",
      "Epoch 2091/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 369508.8438 - mean_squared_error: 369508.8438\n",
      "Epoch 2092/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 363328.2812 - mean_squared_error: 363328.2812\n",
      "Epoch 2093/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 362808.1562 - mean_squared_error: 362808.1562\n",
      "Epoch 2094/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 363229.1875 - mean_squared_error: 363229.1875\n",
      "Epoch 2095/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 359477.6562 - mean_squared_error: 359477.6562\n",
      "Epoch 2096/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 355465.0938 - mean_squared_error: 355465.0938\n",
      "Epoch 2097/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 363482.9688 - mean_squared_error: 363482.9688\n",
      "Epoch 2098/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 364963.0000 - mean_squared_error: 364963.0312\n",
      "Epoch 2099/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 359418.5312 - mean_squared_error: 359418.5312\n",
      "Epoch 2100/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 354823.1250 - mean_squared_error: 354823.1250\n",
      "Epoch 2101/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 365112.9375 - mean_squared_error: 365112.9375\n",
      "Epoch 2102/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 359308.2812 - mean_squared_error: 359308.2812\n",
      "Epoch 2103/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 362051.8125 - mean_squared_error: 362051.8125\n",
      "Epoch 2104/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357058.3125 - mean_squared_error: 357058.3125\n",
      "Epoch 2105/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 354121.6250 - mean_squared_error: 354121.6250\n",
      "Epoch 2106/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 365862.1562 - mean_squared_error: 365862.1562\n",
      "Epoch 2107/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 356941.4688 - mean_squared_error: 356941.4688\n",
      "Epoch 2108/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 14ms/step - loss: 360219.3438 - mean_squared_error: 360219.3438\n",
      "Epoch 2109/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 364466.0938 - mean_squared_error: 364466.0938\n",
      "Epoch 2110/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 362283.8438 - mean_squared_error: 362283.8438\n",
      "Epoch 2111/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 367000.0625 - mean_squared_error: 367000.0625\n",
      "Epoch 2112/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 361793.7812 - mean_squared_error: 361793.7812\n",
      "Epoch 2113/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357749.8750 - mean_squared_error: 357749.8750\n",
      "Epoch 2114/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354590.6250 - mean_squared_error: 354590.6250\n",
      "Epoch 2115/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 365139.8750 - mean_squared_error: 365139.8750\n",
      "Epoch 2116/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 358912.7500 - mean_squared_error: 358912.7500\n",
      "Epoch 2117/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 361149.0625 - mean_squared_error: 361149.0625\n",
      "Epoch 2118/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 361126.7500 - mean_squared_error: 361126.7500\n",
      "Epoch 2119/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356184.3125 - mean_squared_error: 356184.3125\n",
      "Epoch 2120/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 362118.5938 - mean_squared_error: 362118.5938\n",
      "Epoch 2121/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 360100.9062 - mean_squared_error: 360100.9062\n",
      "Epoch 2122/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 359842.5000 - mean_squared_error: 359842.5000\n",
      "Epoch 2123/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357100.3750 - mean_squared_error: 357100.3750\n",
      "Epoch 2124/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 362939.0312 - mean_squared_error: 362939.0312\n",
      "Epoch 2125/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353262.9375 - mean_squared_error: 353262.9375\n",
      "Epoch 2126/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 354724.5312 - mean_squared_error: 354724.5312\n",
      "Epoch 2127/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 360942.3750 - mean_squared_error: 360942.3438\n",
      "Epoch 2128/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356841.5312 - mean_squared_error: 356841.5312\n",
      "Epoch 2129/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353557.0938 - mean_squared_error: 353557.0938\n",
      "Epoch 2130/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356929.6250 - mean_squared_error: 356929.5938\n",
      "Epoch 2131/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 355516.4062 - mean_squared_error: 355516.4062\n",
      "Epoch 2132/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356171.6250 - mean_squared_error: 356171.6250\n",
      "Epoch 2133/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357328.9688 - mean_squared_error: 357328.9688\n",
      "Epoch 2134/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 357007.9375 - mean_squared_error: 357007.9375\n",
      "Epoch 2135/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 358208.5625 - mean_squared_error: 358208.5625\n",
      "Epoch 2136/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 358456.3750 - mean_squared_error: 358456.3750\n",
      "Epoch 2137/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356666.4688 - mean_squared_error: 356666.5000\n",
      "Epoch 2138/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 353514.1562 - mean_squared_error: 353514.1562\n",
      "Epoch 2139/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 354945.2188 - mean_squared_error: 354945.2188\n",
      "Epoch 2140/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356906.5938 - mean_squared_error: 356906.5938\n",
      "Epoch 2141/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 359836.9688 - mean_squared_error: 359836.9688\n",
      "Epoch 2142/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 361258.5000 - mean_squared_error: 361258.5000\n",
      "Epoch 2143/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 358307.4375 - mean_squared_error: 358307.4375\n",
      "Epoch 2144/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 355532.4062 - mean_squared_error: 355532.4062\n",
      "Epoch 2145/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 357408.1875 - mean_squared_error: 357408.1875\n",
      "Epoch 2146/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 360470.2188 - mean_squared_error: 360470.2188\n",
      "Epoch 2147/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 358293.8750 - mean_squared_error: 358293.8750\n",
      "Epoch 2148/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 355932.8750 - mean_squared_error: 355932.8750\n",
      "Epoch 2149/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354552.0000 - mean_squared_error: 354552.0000\n",
      "Epoch 2150/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356942.3750 - mean_squared_error: 356942.3438\n",
      "Epoch 2151/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357194.1875 - mean_squared_error: 357194.2188\n",
      "Epoch 2152/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350942.8750 - mean_squared_error: 350942.8750\n",
      "Epoch 2153/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 360195.0938 - mean_squared_error: 360195.0938\n",
      "Epoch 2154/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354939.6250 - mean_squared_error: 354939.6250\n",
      "Epoch 2155/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 364401.7500 - mean_squared_error: 364401.7500\n",
      "Epoch 2156/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 359719.4375 - mean_squared_error: 359719.4375\n",
      "Epoch 2157/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 358280.5625 - mean_squared_error: 358280.5312\n",
      "Epoch 2158/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351839.9375 - mean_squared_error: 351839.9688\n",
      "Epoch 2159/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 368244.0312 - mean_squared_error: 368244.0312\n",
      "Epoch 2160/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356930.9688 - mean_squared_error: 356930.9375\n",
      "Epoch 2161/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 355191.8438 - mean_squared_error: 355191.8438\n",
      "Epoch 2162/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 354323.9688 - mean_squared_error: 354323.9688\n",
      "Epoch 2163/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353645.5938 - mean_squared_error: 353645.5938\n",
      "Epoch 2164/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356327.4375 - mean_squared_error: 356327.4375\n",
      "Epoch 2165/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 353564.7188 - mean_squared_error: 353564.7188\n",
      "Epoch 2166/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352475.6562 - mean_squared_error: 352475.6562\n",
      "Epoch 2167/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350068.8750 - mean_squared_error: 350068.8750\n",
      "Epoch 2168/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353470.7812 - mean_squared_error: 353470.8125\n",
      "Epoch 2169/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351317.9688 - mean_squared_error: 351317.9688\n",
      "Epoch 2170/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357247.8438 - mean_squared_error: 357247.8125\n",
      "Epoch 2171/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 354602.8750 - mean_squared_error: 354602.8750\n",
      "Epoch 2172/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356309.9688 - mean_squared_error: 356309.9688\n",
      "Epoch 2173/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356859.2188 - mean_squared_error: 356859.2188\n",
      "Epoch 2174/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354284.6875 - mean_squared_error: 354284.7188\n",
      "Epoch 2175/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 358142.4688 - mean_squared_error: 358142.4688\n",
      "Epoch 2176/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 355130.6250 - mean_squared_error: 355130.6250\n",
      "Epoch 2177/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 355950.5625 - mean_squared_error: 355950.5625\n",
      "Epoch 2178/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354187.1875 - mean_squared_error: 354187.1875\n",
      "Epoch 2179/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 366171.3438 - mean_squared_error: 366171.3438\n",
      "Epoch 2180/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 358975.8750 - mean_squared_error: 358975.8750\n",
      "Epoch 2181/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356632.3750 - mean_squared_error: 356632.3750\n",
      "Epoch 2182/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352738.4062 - mean_squared_error: 352738.4062\n",
      "Epoch 2183/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349688.4688 - mean_squared_error: 349688.4688\n",
      "Epoch 2184/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 350987.6250 - mean_squared_error: 350987.6250\n",
      "Epoch 2185/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356806.0000 - mean_squared_error: 356806.0000\n",
      "Epoch 2186/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 354784.7812 - mean_squared_error: 354784.8125\n",
      "Epoch 2187/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354958.0938 - mean_squared_error: 354958.0625\n",
      "Epoch 2188/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352421.6875 - mean_squared_error: 352421.6875\n",
      "Epoch 2189/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 358459.0938 - mean_squared_error: 358459.0938\n",
      "Epoch 2190/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356360.4375 - mean_squared_error: 356360.4375\n",
      "Epoch 2191/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 351966.7188 - mean_squared_error: 351966.7188\n",
      "Epoch 2192/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349365.4062 - mean_squared_error: 349365.4062\n",
      "Epoch 2193/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 353025.3125 - mean_squared_error: 353025.3125\n",
      "Epoch 2194/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 360945.2500 - mean_squared_error: 360945.2500\n",
      "Epoch 2195/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354730.0312 - mean_squared_error: 354730.0312\n",
      "Epoch 2196/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 363899.5625 - mean_squared_error: 363899.5625\n",
      "Epoch 2197/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354904.5312 - mean_squared_error: 354904.5312\n",
      "Epoch 2198/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354588.4375 - mean_squared_error: 354588.4375\n",
      "Epoch 2199/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353700.3125 - mean_squared_error: 353700.3125\n",
      "Epoch 2200/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351950.6250 - mean_squared_error: 351950.6250\n",
      "Epoch 2201/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357294.8750 - mean_squared_error: 357294.8750\n",
      "Epoch 2202/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 357483.8750 - mean_squared_error: 357483.8750\n",
      "Epoch 2203/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 358390.2188 - mean_squared_error: 358390.2188\n",
      "Epoch 2204/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 355322.9375 - mean_squared_error: 355322.9375\n",
      "Epoch 2205/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 357108.8750 - mean_squared_error: 357108.8750\n",
      "Epoch 2206/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353856.8125 - mean_squared_error: 353856.7812\n",
      "Epoch 2207/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 363975.9062 - mean_squared_error: 363975.9062\n",
      "Epoch 2208/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350424.2500 - mean_squared_error: 350424.2500\n",
      "Epoch 2209/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350023.3438 - mean_squared_error: 350023.3438\n",
      "Epoch 2210/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356213.5312 - mean_squared_error: 356213.5312\n",
      "Epoch 2211/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 355254.3125 - mean_squared_error: 355254.3125\n",
      "Epoch 2212/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347579.4375 - mean_squared_error: 347579.4375\n",
      "Epoch 2213/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350205.5625 - mean_squared_error: 350205.5625\n",
      "Epoch 2214/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349837.0938 - mean_squared_error: 349837.0938\n",
      "Epoch 2215/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 355130.8750 - mean_squared_error: 355130.8750\n",
      "Epoch 2216/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346305.4688 - mean_squared_error: 346305.4688\n",
      "Epoch 2217/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351991.4375 - mean_squared_error: 351991.4375\n",
      "Epoch 2218/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345222.0000 - mean_squared_error: 345222.0000\n",
      "Epoch 2219/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351384.4688 - mean_squared_error: 351384.4688\n",
      "Epoch 2220/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356390.4062 - mean_squared_error: 356390.4062\n",
      "Epoch 2221/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350210.0938 - mean_squared_error: 350210.0938\n",
      "Epoch 2222/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 358522.8125 - mean_squared_error: 358522.8125\n",
      "Epoch 2223/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347005.8438 - mean_squared_error: 347005.8438\n",
      "Epoch 2224/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354819.9062 - mean_squared_error: 354819.9062\n",
      "Epoch 2225/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 358531.4375 - mean_squared_error: 358531.4375\n",
      "Epoch 2226/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351607.5312 - mean_squared_error: 351607.5312\n",
      "Epoch 2227/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 358002.1250 - mean_squared_error: 358002.1562\n",
      "Epoch 2228/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343458.2188 - mean_squared_error: 343458.2188\n",
      "Epoch 2229/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353808.0312 - mean_squared_error: 353808.0312\n",
      "Epoch 2230/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 358119.5938 - mean_squared_error: 358119.5938\n",
      "Epoch 2231/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356623.6562 - mean_squared_error: 356623.6562\n",
      "Epoch 2232/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 353341.8750 - mean_squared_error: 353341.8750\n",
      "Epoch 2233/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346776.2188 - mean_squared_error: 346776.2188\n",
      "Epoch 2234/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346291.6250 - mean_squared_error: 346291.6250\n",
      "Epoch 2235/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346735.7188 - mean_squared_error: 346735.7812\n",
      "Epoch 2236/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 355081.4375 - mean_squared_error: 355081.4062\n",
      "Epoch 2237/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349863.3125 - mean_squared_error: 349863.3125\n",
      "Epoch 2238/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354574.2188 - mean_squared_error: 354574.2188\n",
      "Epoch 2239/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356681.9062 - mean_squared_error: 356681.9375\n",
      "Epoch 2240/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 14ms/step - loss: 353805.5625 - mean_squared_error: 353805.5625\n",
      "Epoch 2241/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 357086.4688 - mean_squared_error: 357086.4688\n",
      "Epoch 2242/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 354889.8750 - mean_squared_error: 354889.8750\n",
      "Epoch 2243/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352821.7500 - mean_squared_error: 352821.7500\n",
      "Epoch 2244/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353077.0312 - mean_squared_error: 353077.0000\n",
      "Epoch 2245/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349624.7500 - mean_squared_error: 349624.7500\n",
      "Epoch 2246/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348068.9688 - mean_squared_error: 348068.9688\n",
      "Epoch 2247/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351806.3438 - mean_squared_error: 351806.3438\n",
      "Epoch 2248/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 354102.1250 - mean_squared_error: 354102.1250\n",
      "Epoch 2249/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 358543.5625 - mean_squared_error: 358543.5625\n",
      "Epoch 2250/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351833.8438 - mean_squared_error: 351833.8438\n",
      "Epoch 2251/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352686.9375 - mean_squared_error: 352686.9375\n",
      "Epoch 2252/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352655.4062 - mean_squared_error: 352655.4062\n",
      "Epoch 2253/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353581.2500 - mean_squared_error: 353581.2500\n",
      "Epoch 2254/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 358794.7500 - mean_squared_error: 358794.7500\n",
      "Epoch 2255/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351754.0938 - mean_squared_error: 351754.0938\n",
      "Epoch 2256/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348815.7188 - mean_squared_error: 348815.7188\n",
      "Epoch 2257/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 358944.5000 - mean_squared_error: 358944.5000\n",
      "Epoch 2258/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350406.2500 - mean_squared_error: 350406.2500\n",
      "Epoch 2259/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346699.0625 - mean_squared_error: 346699.0625\n",
      "Epoch 2260/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352314.4375 - mean_squared_error: 352314.4375\n",
      "Epoch 2261/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347710.2188 - mean_squared_error: 347710.2188\n",
      "Epoch 2262/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346272.8438 - mean_squared_error: 346272.8438\n",
      "Epoch 2263/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 360835.4375 - mean_squared_error: 360835.4375\n",
      "Epoch 2264/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356858.4688 - mean_squared_error: 356858.4688\n",
      "Epoch 2265/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347523.6562 - mean_squared_error: 347523.6562\n",
      "Epoch 2266/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 353911.2812 - mean_squared_error: 353911.2812\n",
      "Epoch 2267/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 357811.8750 - mean_squared_error: 357811.9062\n",
      "Epoch 2268/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349489.2500 - mean_squared_error: 349489.2500\n",
      "Epoch 2269/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350941.4375 - mean_squared_error: 350941.4375\n",
      "Epoch 2270/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349753.1562 - mean_squared_error: 349753.1562\n",
      "Epoch 2271/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352051.9062 - mean_squared_error: 352051.9062\n",
      "Epoch 2272/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345601.6562 - mean_squared_error: 345601.6562\n",
      "Epoch 2273/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351603.7500 - mean_squared_error: 351603.7500\n",
      "Epoch 2274/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350051.5000 - mean_squared_error: 350051.5000\n",
      "Epoch 2275/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343770.1875 - mean_squared_error: 343770.1875\n",
      "Epoch 2276/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 359484.6250 - mean_squared_error: 359484.6250\n",
      "Epoch 2277/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348947.2188 - mean_squared_error: 348947.2188\n",
      "Epoch 2278/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346318.7188 - mean_squared_error: 346318.7188\n",
      "Epoch 2279/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 353975.9375 - mean_squared_error: 353975.9375\n",
      "Epoch 2280/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 353507.0938 - mean_squared_error: 353507.0938\n",
      "Epoch 2281/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353439.6250 - mean_squared_error: 353439.5938\n",
      "Epoch 2282/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348452.5938 - mean_squared_error: 348452.5938\n",
      "Epoch 2283/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346425.2812 - mean_squared_error: 346425.2812\n",
      "Epoch 2284/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356969.5625 - mean_squared_error: 356969.5625\n",
      "Epoch 2285/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350030.9688 - mean_squared_error: 350030.9688\n",
      "Epoch 2286/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354238.7500 - mean_squared_error: 354238.7812\n",
      "Epoch 2287/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353882.3125 - mean_squared_error: 353882.3125\n",
      "Epoch 2288/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347936.7812 - mean_squared_error: 347936.7812\n",
      "Epoch 2289/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340096.5625 - mean_squared_error: 340096.5625\n",
      "Epoch 2290/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349216.6250 - mean_squared_error: 349216.6250\n",
      "Epoch 2291/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344922.4688 - mean_squared_error: 344922.4688\n",
      "Epoch 2292/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350965.0000 - mean_squared_error: 350965.0000\n",
      "Epoch 2293/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349457.6250 - mean_squared_error: 349457.6250\n",
      "Epoch 2294/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349079.5938 - mean_squared_error: 349079.5938\n",
      "Epoch 2295/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 345647.0938 - mean_squared_error: 345647.0938\n",
      "Epoch 2296/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351439.6250 - mean_squared_error: 351439.6250\n",
      "Epoch 2297/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344262.2188 - mean_squared_error: 344262.2188\n",
      "Epoch 2298/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350218.1250 - mean_squared_error: 350218.1250\n",
      "Epoch 2299/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346225.6562 - mean_squared_error: 346225.6562\n",
      "Epoch 2300/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 357159.1562 - mean_squared_error: 357159.1562\n",
      "Epoch 2301/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345982.0312 - mean_squared_error: 345982.0625\n",
      "Epoch 2302/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347356.6562 - mean_squared_error: 347356.6562\n",
      "Epoch 2303/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350215.0625 - mean_squared_error: 350215.0625\n",
      "Epoch 2304/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 354544.0000 - mean_squared_error: 354544.0000\n",
      "Epoch 2305/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349903.0938 - mean_squared_error: 349903.0938\n",
      "Epoch 2306/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352841.9062 - mean_squared_error: 352841.9062\n",
      "Epoch 2307/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357773.5625 - mean_squared_error: 357773.5625\n",
      "Epoch 2308/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352350.5312 - mean_squared_error: 352350.5312\n",
      "Epoch 2309/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352270.2500 - mean_squared_error: 352270.2500\n",
      "Epoch 2310/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 355738.5312 - mean_squared_error: 355738.5312\n",
      "Epoch 2311/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337372.5938 - mean_squared_error: 337372.5938\n",
      "Epoch 2312/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350516.9375 - mean_squared_error: 350516.9375\n",
      "Epoch 2313/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349565.5312 - mean_squared_error: 349565.5625\n",
      "Epoch 2314/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 349035.1562 - mean_squared_error: 349035.1562\n",
      "Epoch 2315/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 353622.4375 - mean_squared_error: 353622.4375\n",
      "Epoch 2316/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346210.6250 - mean_squared_error: 346210.6250\n",
      "Epoch 2317/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346707.7188 - mean_squared_error: 346707.7188\n",
      "Epoch 2318/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348145.1250 - mean_squared_error: 348145.1250\n",
      "Epoch 2319/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 358325.6562 - mean_squared_error: 358325.6562\n",
      "Epoch 2320/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345216.2500 - mean_squared_error: 345216.2500\n",
      "Epoch 2321/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352105.1250 - mean_squared_error: 352105.1250\n",
      "Epoch 2322/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348759.0312 - mean_squared_error: 348759.0000\n",
      "Epoch 2323/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352490.6250 - mean_squared_error: 352490.6250\n",
      "Epoch 2324/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352045.5312 - mean_squared_error: 352045.5312\n",
      "Epoch 2325/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351298.0938 - mean_squared_error: 351298.0938\n",
      "Epoch 2326/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346089.8438 - mean_squared_error: 346089.8438\n",
      "Epoch 2327/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343851.1875 - mean_squared_error: 343851.1875\n",
      "Epoch 2328/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344946.5938 - mean_squared_error: 344946.5938\n",
      "Epoch 2329/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350756.5938 - mean_squared_error: 350756.5938\n",
      "Epoch 2330/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348731.0312 - mean_squared_error: 348731.0312\n",
      "Epoch 2331/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351267.9062 - mean_squared_error: 351267.9062\n",
      "Epoch 2332/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348949.3438 - mean_squared_error: 348949.3750\n",
      "Epoch 2333/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356812.3125 - mean_squared_error: 356812.3125\n",
      "Epoch 2334/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352182.7500 - mean_squared_error: 352182.7500\n",
      "Epoch 2335/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352409.8750 - mean_squared_error: 352409.8750\n",
      "Epoch 2336/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342800.2188 - mean_squared_error: 342800.2188\n",
      "Epoch 2337/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348436.7500 - mean_squared_error: 348436.7188\n",
      "Epoch 2338/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345264.8438 - mean_squared_error: 345264.8438\n",
      "Epoch 2339/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 341326.1875 - mean_squared_error: 341326.1875\n",
      "Epoch 2340/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 357680.4688 - mean_squared_error: 357680.4688\n",
      "Epoch 2341/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 355131.7188 - mean_squared_error: 355131.7188\n",
      "Epoch 2342/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344307.0625 - mean_squared_error: 344307.0625\n",
      "Epoch 2343/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348530.7188 - mean_squared_error: 348530.7188\n",
      "Epoch 2344/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357375.4062 - mean_squared_error: 357375.4062\n",
      "Epoch 2345/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349554.3125 - mean_squared_error: 349554.3125\n",
      "Epoch 2346/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349000.9688 - mean_squared_error: 349000.9375\n",
      "Epoch 2347/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 355176.4062 - mean_squared_error: 355176.4062\n",
      "Epoch 2348/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345963.5938 - mean_squared_error: 345963.5938\n",
      "Epoch 2349/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351001.3750 - mean_squared_error: 351001.3750\n",
      "Epoch 2350/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342635.3750 - mean_squared_error: 342635.3750\n",
      "Epoch 2351/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354333.2812 - mean_squared_error: 354333.2812\n",
      "Epoch 2352/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 358187.6562 - mean_squared_error: 358187.6562\n",
      "Epoch 2353/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352267.0625 - mean_squared_error: 352267.0625\n",
      "Epoch 2354/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 357355.9062 - mean_squared_error: 357355.9062\n",
      "Epoch 2355/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 345853.6250 - mean_squared_error: 345853.6250\n",
      "Epoch 2356/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342421.0938 - mean_squared_error: 342421.0938\n",
      "Epoch 2357/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337021.3438 - mean_squared_error: 337021.3438\n",
      "Epoch 2358/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351952.9375 - mean_squared_error: 351952.9375\n",
      "Epoch 2359/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343108.6250 - mean_squared_error: 343108.6250\n",
      "Epoch 2360/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353931.0312 - mean_squared_error: 353931.0312\n",
      "Epoch 2361/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342299.1875 - mean_squared_error: 342299.1562\n",
      "Epoch 2362/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348246.2812 - mean_squared_error: 348246.2812\n",
      "Epoch 2363/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352265.6250 - mean_squared_error: 352265.6562\n",
      "Epoch 2364/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348304.4062 - mean_squared_error: 348304.4062\n",
      "Epoch 2365/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342270.5625 - mean_squared_error: 342270.5625\n",
      "Epoch 2366/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343669.7812 - mean_squared_error: 343669.7812\n",
      "Epoch 2367/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346998.0312 - mean_squared_error: 346998.0312\n",
      "Epoch 2368/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352922.7188 - mean_squared_error: 352922.7188\n",
      "Epoch 2369/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344858.3750 - mean_squared_error: 344858.3750\n",
      "Epoch 2370/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344071.6250 - mean_squared_error: 344071.6250\n",
      "Epoch 2371/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351070.1562 - mean_squared_error: 351070.1250\n",
      "Epoch 2372/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 346840.8750 - mean_squared_error: 346840.8750\n",
      "Epoch 2373/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343602.7188 - mean_squared_error: 343602.7188\n",
      "Epoch 2374/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352301.7812 - mean_squared_error: 352301.7812\n",
      "Epoch 2375/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346421.0625 - mean_squared_error: 346421.0938\n",
      "Epoch 2376/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342949.2500 - mean_squared_error: 342949.2500\n",
      "Epoch 2377/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346422.7188 - mean_squared_error: 346422.7188\n",
      "Epoch 2378/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346033.8438 - mean_squared_error: 346033.8438\n",
      "Epoch 2379/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348624.6562 - mean_squared_error: 348624.6562\n",
      "Epoch 2380/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348026.5625 - mean_squared_error: 348026.5625\n",
      "Epoch 2381/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348646.8438 - mean_squared_error: 348646.8438\n",
      "Epoch 2382/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352777.7500 - mean_squared_error: 352777.7500\n",
      "Epoch 2383/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354762.3438 - mean_squared_error: 354762.3438\n",
      "Epoch 2384/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351959.1875 - mean_squared_error: 351959.1875\n",
      "Epoch 2385/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351400.6875 - mean_squared_error: 351400.6875\n",
      "Epoch 2386/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352473.3438 - mean_squared_error: 352473.3438\n",
      "Epoch 2387/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351209.9688 - mean_squared_error: 351209.9688\n",
      "Epoch 2388/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348557.4062 - mean_squared_error: 348557.4062\n",
      "Epoch 2389/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348464.6562 - mean_squared_error: 348464.6562\n",
      "Epoch 2390/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344324.7188 - mean_squared_error: 344324.7188\n",
      "Epoch 2391/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347529.2500 - mean_squared_error: 347529.2500\n",
      "Epoch 2392/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343882.5625 - mean_squared_error: 343882.5625\n",
      "Epoch 2393/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343146.9062 - mean_squared_error: 343146.9062\n",
      "Epoch 2394/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 359569.9688 - mean_squared_error: 359569.9688\n",
      "Epoch 2395/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348624.0625 - mean_squared_error: 348624.0625\n",
      "Epoch 2396/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343269.3125 - mean_squared_error: 343269.3125\n",
      "Epoch 2397/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349313.4062 - mean_squared_error: 349313.4062\n",
      "Epoch 2398/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346162.7188 - mean_squared_error: 346162.7188\n",
      "Epoch 2399/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340597.5000 - mean_squared_error: 340597.5000\n",
      "Epoch 2400/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 359648.7188 - mean_squared_error: 359648.7188\n",
      "Epoch 2401/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345072.7188 - mean_squared_error: 345072.7188\n",
      "Epoch 2402/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346199.6250 - mean_squared_error: 346199.6250\n",
      "Epoch 2403/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356664.5938 - mean_squared_error: 356664.5938\n",
      "Epoch 2404/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352361.0625 - mean_squared_error: 352361.0625\n",
      "Epoch 2405/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349417.9688 - mean_squared_error: 349417.9688\n",
      "Epoch 2406/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352815.0312 - mean_squared_error: 352815.0312\n",
      "Epoch 2407/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342795.6250 - mean_squared_error: 342795.6250\n",
      "Epoch 2408/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345996.1250 - mean_squared_error: 345996.1250\n",
      "Epoch 2409/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338363.0000 - mean_squared_error: 338363.0000\n",
      "Epoch 2410/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352599.3438 - mean_squared_error: 352599.3438\n",
      "Epoch 2411/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351183.4688 - mean_squared_error: 351183.4688\n",
      "Epoch 2412/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350364.3750 - mean_squared_error: 350364.3750\n",
      "Epoch 2413/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346608.1250 - mean_squared_error: 346608.1250\n",
      "Epoch 2414/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344401.7812 - mean_squared_error: 344401.7812\n",
      "Epoch 2415/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352706.3125 - mean_squared_error: 352706.3125\n",
      "Epoch 2416/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 345865.0938 - mean_squared_error: 345865.0938\n",
      "Epoch 2417/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335675.6250 - mean_squared_error: 335675.6250\n",
      "Epoch 2418/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352323.1562 - mean_squared_error: 352323.1562\n",
      "Epoch 2419/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 354815.2812 - mean_squared_error: 354815.3125\n",
      "Epoch 2420/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350174.0312 - mean_squared_error: 350174.0312\n",
      "Epoch 2421/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346487.0938 - mean_squared_error: 346487.0938\n",
      "Epoch 2422/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343319.8750 - mean_squared_error: 343319.8750\n",
      "Epoch 2423/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353344.7500 - mean_squared_error: 353344.7500\n",
      "Epoch 2424/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348667.6562 - mean_squared_error: 348667.6562\n",
      "Epoch 2425/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353545.1250 - mean_squared_error: 353545.1250\n",
      "Epoch 2426/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351569.9375 - mean_squared_error: 351569.9688\n",
      "Epoch 2427/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347149.1875 - mean_squared_error: 347149.1875\n",
      "Epoch 2428/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352350.4062 - mean_squared_error: 352350.4062\n",
      "Epoch 2429/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 354009.7812 - mean_squared_error: 354009.7812\n",
      "Epoch 2430/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343246.7812 - mean_squared_error: 343246.7812\n",
      "Epoch 2431/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340101.6875 - mean_squared_error: 340101.6875\n",
      "Epoch 2432/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345103.9062 - mean_squared_error: 345103.9375\n",
      "Epoch 2433/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344029.5938 - mean_squared_error: 344029.5938\n",
      "Epoch 2434/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340357.6562 - mean_squared_error: 340357.6562\n",
      "Epoch 2435/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347550.3438 - mean_squared_error: 347550.4062\n",
      "Epoch 2436/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343587.5312 - mean_squared_error: 343587.5312\n",
      "Epoch 2437/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 354102.3750 - mean_squared_error: 354102.3750\n",
      "Epoch 2438/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349612.0312 - mean_squared_error: 349612.0312\n",
      "Epoch 2439/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348343.2500 - mean_squared_error: 348343.2500\n",
      "Epoch 2440/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345704.6875 - mean_squared_error: 345704.6875\n",
      "Epoch 2441/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352109.9375 - mean_squared_error: 352109.9375\n",
      "Epoch 2442/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345641.2188 - mean_squared_error: 345641.2188\n",
      "Epoch 2443/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348368.2188 - mean_squared_error: 348368.2188\n",
      "Epoch 2444/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344089.1875 - mean_squared_error: 344089.1875\n",
      "Epoch 2445/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349319.4375 - mean_squared_error: 349319.4375\n",
      "Epoch 2446/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350657.0000 - mean_squared_error: 350657.0000\n",
      "Epoch 2447/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347997.8750 - mean_squared_error: 347997.8750\n",
      "Epoch 2448/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350856.5625 - mean_squared_error: 350856.5625\n",
      "Epoch 2449/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348441.1875 - mean_squared_error: 348441.1875\n",
      "Epoch 2450/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349085.8438 - mean_squared_error: 349085.8438\n",
      "Epoch 2451/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351013.0938 - mean_squared_error: 351013.0938\n",
      "Epoch 2452/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352309.3750 - mean_squared_error: 352309.3750\n",
      "Epoch 2453/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347976.4062 - mean_squared_error: 347976.4062\n",
      "Epoch 2454/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345557.8438 - mean_squared_error: 345557.8438\n",
      "Epoch 2455/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354168.2188 - mean_squared_error: 354168.2188\n",
      "Epoch 2456/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348740.7500 - mean_squared_error: 348740.7500\n",
      "Epoch 2457/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347874.3438 - mean_squared_error: 347874.3438\n",
      "Epoch 2458/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344272.5625 - mean_squared_error: 344272.5625\n",
      "Epoch 2459/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340267.1562 - mean_squared_error: 340267.1562\n",
      "Epoch 2460/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347774.8438 - mean_squared_error: 347774.8438\n",
      "Epoch 2461/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351113.3750 - mean_squared_error: 351113.3750\n",
      "Epoch 2462/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340341.7188 - mean_squared_error: 340341.7188\n",
      "Epoch 2463/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343463.7188 - mean_squared_error: 343463.7188\n",
      "Epoch 2464/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343923.7812 - mean_squared_error: 343923.7812\n",
      "Epoch 2465/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345881.0312 - mean_squared_error: 345881.0312\n",
      "Epoch 2466/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353202.5000 - mean_squared_error: 353202.5000\n",
      "Epoch 2467/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346242.2188 - mean_squared_error: 346242.2188\n",
      "Epoch 2468/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345883.6562 - mean_squared_error: 345883.6562\n",
      "Epoch 2469/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348733.7500 - mean_squared_error: 348733.7500\n",
      "Epoch 2470/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350189.9062 - mean_squared_error: 350189.9062\n",
      "Epoch 2471/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345546.0312 - mean_squared_error: 345546.0312\n",
      "Epoch 2472/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349588.3438 - mean_squared_error: 349588.3438\n",
      "Epoch 2473/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353183.5000 - mean_squared_error: 353183.5000\n",
      "Epoch 2474/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350778.4688 - mean_squared_error: 350778.4688\n",
      "Epoch 2475/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345135.9062 - mean_squared_error: 345135.9062\n",
      "Epoch 2476/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347044.7812 - mean_squared_error: 347044.7812\n",
      "Epoch 2477/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344878.0312 - mean_squared_error: 344878.0000\n",
      "Epoch 2478/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349374.0000 - mean_squared_error: 349374.0000\n",
      "Epoch 2479/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347051.9062 - mean_squared_error: 347051.9062\n",
      "Epoch 2480/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341550.6250 - mean_squared_error: 341550.6250\n",
      "Epoch 2481/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357065.9062 - mean_squared_error: 357065.9062\n",
      "Epoch 2482/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357284.2812 - mean_squared_error: 357284.2812\n",
      "Epoch 2483/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337289.3750 - mean_squared_error: 337289.3750\n",
      "Epoch 2484/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349406.5938 - mean_squared_error: 349406.5938\n",
      "Epoch 2485/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347455.9062 - mean_squared_error: 347455.9062\n",
      "Epoch 2486/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347773.5000 - mean_squared_error: 347773.5312\n",
      "Epoch 2487/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350123.8750 - mean_squared_error: 350123.8750\n",
      "Epoch 2488/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339437.2812 - mean_squared_error: 339437.2812\n",
      "Epoch 2489/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345660.4688 - mean_squared_error: 345660.4375\n",
      "Epoch 2490/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339566.5938 - mean_squared_error: 339566.5938\n",
      "Epoch 2491/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346189.3438 - mean_squared_error: 346189.3438\n",
      "Epoch 2492/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342708.4375 - mean_squared_error: 342708.4375\n",
      "Epoch 2493/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347124.0625 - mean_squared_error: 347124.0625\n",
      "Epoch 2494/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349170.4062 - mean_squared_error: 349170.4062\n",
      "Epoch 2495/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346854.6875 - mean_squared_error: 346854.6875\n",
      "Epoch 2496/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346119.3438 - mean_squared_error: 346119.3438\n",
      "Epoch 2497/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341854.5000 - mean_squared_error: 341854.5000\n",
      "Epoch 2498/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346617.6250 - mean_squared_error: 346617.6250\n",
      "Epoch 2499/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341175.8438 - mean_squared_error: 341175.8438\n",
      "Epoch 2500/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336923.1250 - mean_squared_error: 336923.1250\n",
      "Epoch 2501/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343230.0938 - mean_squared_error: 343230.0938\n",
      "Epoch 2502/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 345908.1562 - mean_squared_error: 345908.1562\n",
      "Epoch 2503/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346780.0312 - mean_squared_error: 346780.0312\n",
      "Epoch 2504/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 348226.2188 - mean_squared_error: 348226.2188\n",
      "Epoch 2505/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350919.9062 - mean_squared_error: 350919.9062\n",
      "Epoch 2506/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342168.1250 - mean_squared_error: 342168.1250\n",
      "Epoch 2507/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342087.4062 - mean_squared_error: 342087.4062\n",
      "Epoch 2508/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348214.5312 - mean_squared_error: 348214.5312\n",
      "Epoch 2509/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348602.1875 - mean_squared_error: 348602.2188\n",
      "Epoch 2510/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347670.1250 - mean_squared_error: 347670.1250\n",
      "Epoch 2511/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340642.9688 - mean_squared_error: 340642.9688\n",
      "Epoch 2512/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344029.4688 - mean_squared_error: 344029.4688\n",
      "Epoch 2513/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342526.6562 - mean_squared_error: 342526.6562\n",
      "Epoch 2514/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348055.4375 - mean_squared_error: 348055.4375\n",
      "Epoch 2515/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347112.9062 - mean_squared_error: 347112.9062\n",
      "Epoch 2516/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347853.9375 - mean_squared_error: 347853.9375\n",
      "Epoch 2517/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344068.0938 - mean_squared_error: 344068.0938\n",
      "Epoch 2518/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 353937.7188 - mean_squared_error: 353937.7188\n",
      "Epoch 2519/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343632.0625 - mean_squared_error: 343632.0625\n",
      "Epoch 2520/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338140.0312 - mean_squared_error: 338140.0625\n",
      "Epoch 2521/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347264.3750 - mean_squared_error: 347264.3750\n",
      "Epoch 2522/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352338.6250 - mean_squared_error: 352338.6250\n",
      "Epoch 2523/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338651.4062 - mean_squared_error: 338651.4062\n",
      "Epoch 2524/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344084.1562 - mean_squared_error: 344084.1562\n",
      "Epoch 2525/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 356071.8750 - mean_squared_error: 356071.8750\n",
      "Epoch 2526/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351653.0938 - mean_squared_error: 351653.0938\n",
      "Epoch 2527/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351544.5938 - mean_squared_error: 351544.5938\n",
      "Epoch 2528/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338326.5312 - mean_squared_error: 338326.5312\n",
      "Epoch 2529/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344204.2812 - mean_squared_error: 344204.2812\n",
      "Epoch 2530/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349257.3750 - mean_squared_error: 349257.3750\n",
      "Epoch 2531/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344204.0938 - mean_squared_error: 344204.0938\n",
      "Epoch 2532/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344206.8438 - mean_squared_error: 344206.8438\n",
      "Epoch 2533/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348477.3750 - mean_squared_error: 348477.3750\n",
      "Epoch 2534/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345129.6875 - mean_squared_error: 345129.6875\n",
      "Epoch 2535/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343306.3125 - mean_squared_error: 343306.3125\n",
      "Epoch 2536/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337625.2812 - mean_squared_error: 337625.2812\n",
      "Epoch 2537/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352176.5000 - mean_squared_error: 352176.5000\n",
      "Epoch 2538/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342535.0312 - mean_squared_error: 342535.0312\n",
      "Epoch 2539/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344589.8750 - mean_squared_error: 344589.8750\n",
      "Epoch 2540/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346058.9062 - mean_squared_error: 346058.9062\n",
      "Epoch 2541/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347691.8438 - mean_squared_error: 347691.8438\n",
      "Epoch 2542/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347504.6250 - mean_squared_error: 347504.6250\n",
      "Epoch 2543/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348101.0938 - mean_squared_error: 348101.0938\n",
      "Epoch 2544/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 334904.8438 - mean_squared_error: 334904.8438\n",
      "Epoch 2545/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350156.1875 - mean_squared_error: 350156.1562\n",
      "Epoch 2546/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348236.1250 - mean_squared_error: 348236.1250\n",
      "Epoch 2547/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348461.8125 - mean_squared_error: 348461.8125\n",
      "Epoch 2548/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339061.6562 - mean_squared_error: 339061.6562\n",
      "Epoch 2549/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340460.8750 - mean_squared_error: 340460.8750\n",
      "Epoch 2550/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337781.6250 - mean_squared_error: 337781.5938\n",
      "Epoch 2551/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342942.5000 - mean_squared_error: 342942.5000\n",
      "Epoch 2552/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345381.1875 - mean_squared_error: 345381.1875\n",
      "Epoch 2553/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345887.4375 - mean_squared_error: 345887.4375\n",
      "Epoch 2554/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348592.8750 - mean_squared_error: 348592.8750\n",
      "Epoch 2555/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347130.8125 - mean_squared_error: 347130.8125\n",
      "Epoch 2556/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342682.2188 - mean_squared_error: 342682.2500\n",
      "Epoch 2557/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343565.6562 - mean_squared_error: 343565.6562\n",
      "Epoch 2558/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343523.3125 - mean_squared_error: 343523.2812\n",
      "Epoch 2559/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351431.8750 - mean_squared_error: 351431.8750\n",
      "Epoch 2560/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345213.8438 - mean_squared_error: 345213.8438\n",
      "Epoch 2561/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341315.6250 - mean_squared_error: 341315.6250\n",
      "Epoch 2562/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 356277.5625 - mean_squared_error: 356277.5938\n",
      "Epoch 2563/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346999.2812 - mean_squared_error: 346999.2812\n",
      "Epoch 2564/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349728.0000 - mean_squared_error: 349728.0000\n",
      "Epoch 2565/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343169.8438 - mean_squared_error: 343169.8438\n",
      "Epoch 2566/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343366.8438 - mean_squared_error: 343366.8438\n",
      "Epoch 2567/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344617.0000 - mean_squared_error: 344617.0000\n",
      "Epoch 2568/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342563.1250 - mean_squared_error: 342563.1250\n",
      "Epoch 2569/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348966.5312 - mean_squared_error: 348966.5312\n",
      "Epoch 2570/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336509.3125 - mean_squared_error: 336509.3125\n",
      "Epoch 2571/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347298.8750 - mean_squared_error: 347298.8750\n",
      "Epoch 2572/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 349160.5312 - mean_squared_error: 349160.5312\n",
      "Epoch 2573/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350689.9062 - mean_squared_error: 350689.9062\n",
      "Epoch 2574/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345820.0000 - mean_squared_error: 345820.0000\n",
      "Epoch 2575/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347180.7812 - mean_squared_error: 347180.7812\n",
      "Epoch 2576/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341457.0625 - mean_squared_error: 341457.0625\n",
      "Epoch 2577/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346800.8750 - mean_squared_error: 346800.8750\n",
      "Epoch 2578/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341629.2188 - mean_squared_error: 341629.2188\n",
      "Epoch 2579/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353094.0938 - mean_squared_error: 353094.0938\n",
      "Epoch 2580/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336991.5625 - mean_squared_error: 336991.5625\n",
      "Epoch 2581/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337583.5312 - mean_squared_error: 337583.5312\n",
      "Epoch 2582/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340848.4375 - mean_squared_error: 340848.4062\n",
      "Epoch 2583/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341240.2812 - mean_squared_error: 341240.2812\n",
      "Epoch 2584/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340511.5625 - mean_squared_error: 340511.5625\n",
      "Epoch 2585/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348081.9375 - mean_squared_error: 348081.9375\n",
      "Epoch 2586/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348922.7812 - mean_squared_error: 348922.7812\n",
      "Epoch 2587/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343233.9375 - mean_squared_error: 343233.9375\n",
      "Epoch 2588/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343843.1562 - mean_squared_error: 343843.1875\n",
      "Epoch 2589/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345495.1562 - mean_squared_error: 345495.1562\n",
      "Epoch 2590/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341776.7500 - mean_squared_error: 341776.7500\n",
      "Epoch 2591/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342393.8125 - mean_squared_error: 342393.8125\n",
      "Epoch 2592/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 334639.0312 - mean_squared_error: 334639.0312\n",
      "Epoch 2593/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348491.8750 - mean_squared_error: 348491.8750\n",
      "Epoch 2594/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 337943.0625 - mean_squared_error: 337943.0625\n",
      "Epoch 2595/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346586.5312 - mean_squared_error: 346586.5312\n",
      "Epoch 2596/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352262.4375 - mean_squared_error: 352262.4375\n",
      "Epoch 2597/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335668.3438 - mean_squared_error: 335668.3438\n",
      "Epoch 2598/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349343.3438 - mean_squared_error: 349343.3750\n",
      "Epoch 2599/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 353412.6562 - mean_squared_error: 353412.6562\n",
      "Epoch 2600/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344306.5625 - mean_squared_error: 344306.5625\n",
      "Epoch 2601/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348816.4375 - mean_squared_error: 348816.4375\n",
      "Epoch 2602/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337748.2812 - mean_squared_error: 337748.2812\n",
      "Epoch 2603/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341597.4062 - mean_squared_error: 341597.4062\n",
      "Epoch 2604/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345252.6875 - mean_squared_error: 345252.7188\n",
      "Epoch 2605/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337553.4375 - mean_squared_error: 337553.4375\n",
      "Epoch 2606/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336549.0312 - mean_squared_error: 336549.0312\n",
      "Epoch 2607/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347679.4688 - mean_squared_error: 347679.4688\n",
      "Epoch 2608/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341227.0625 - mean_squared_error: 341227.0625\n",
      "Epoch 2609/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342541.9375 - mean_squared_error: 342541.9375\n",
      "Epoch 2610/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342124.9375 - mean_squared_error: 342124.9375\n",
      "Epoch 2611/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341624.7500 - mean_squared_error: 341624.7500\n",
      "Epoch 2612/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340391.4688 - mean_squared_error: 340391.4688\n",
      "Epoch 2613/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345445.9375 - mean_squared_error: 345445.9375\n",
      "Epoch 2614/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349820.1875 - mean_squared_error: 349820.1875\n",
      "Epoch 2615/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344659.1250 - mean_squared_error: 344659.1250\n",
      "Epoch 2616/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340306.9375 - mean_squared_error: 340306.9375\n",
      "Epoch 2617/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350375.0625 - mean_squared_error: 350375.0625\n",
      "Epoch 2618/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351269.8438 - mean_squared_error: 351269.8438\n",
      "Epoch 2619/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341894.4375 - mean_squared_error: 341894.4375\n",
      "Epoch 2620/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351120.6562 - mean_squared_error: 351120.6562\n",
      "Epoch 2621/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343024.8125 - mean_squared_error: 343024.8125\n",
      "Epoch 2622/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 333504.5938 - mean_squared_error: 333504.5938\n",
      "Epoch 2623/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349907.1875 - mean_squared_error: 349907.1875\n",
      "Epoch 2624/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351114.9688 - mean_squared_error: 351114.9688\n",
      "Epoch 2625/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340703.5000 - mean_squared_error: 340703.5000\n",
      "Epoch 2626/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343867.2812 - mean_squared_error: 343867.2812\n",
      "Epoch 2627/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346130.9062 - mean_squared_error: 346130.9062\n",
      "Epoch 2628/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339356.5312 - mean_squared_error: 339356.5625\n",
      "Epoch 2629/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346517.4062 - mean_squared_error: 346517.4062\n",
      "Epoch 2630/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346767.7188 - mean_squared_error: 346767.7188\n",
      "Epoch 2631/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344713.3750 - mean_squared_error: 344713.3750\n",
      "Epoch 2632/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352137.5938 - mean_squared_error: 352137.5938\n",
      "Epoch 2633/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342720.0000 - mean_squared_error: 342720.0000\n",
      "Epoch 2634/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340452.0625 - mean_squared_error: 340452.0625\n",
      "Epoch 2635/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343456.4062 - mean_squared_error: 343456.4062\n",
      "Epoch 2636/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 342756.1875 - mean_squared_error: 342756.1875\n",
      "Epoch 2637/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350075.8750 - mean_squared_error: 350075.8750\n",
      "Epoch 2638/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344442.6250 - mean_squared_error: 344442.6250\n",
      "Epoch 2639/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341867.0000 - mean_squared_error: 341867.0000\n",
      "Epoch 2640/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 335546.9062 - mean_squared_error: 335546.9062\n",
      "Epoch 2641/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 352574.7188 - mean_squared_error: 352574.7188\n",
      "Epoch 2642/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343503.8750 - mean_squared_error: 343503.8750\n",
      "Epoch 2643/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351475.5000 - mean_squared_error: 351475.5000\n",
      "Epoch 2644/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345172.0000 - mean_squared_error: 345172.0000\n",
      "Epoch 2645/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349695.5625 - mean_squared_error: 349695.5938\n",
      "Epoch 2646/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345670.4375 - mean_squared_error: 345670.4375\n",
      "Epoch 2647/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341201.3125 - mean_squared_error: 341201.3125\n",
      "Epoch 2648/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345713.8750 - mean_squared_error: 345713.8750\n",
      "Epoch 2649/5000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 344488.5000 - mean_squared_error: 344488.5000\n",
      "Epoch 2650/5000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 340163.5938 - mean_squared_error: 340163.5938\n",
      "Epoch 2651/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 350377.5938 - mean_squared_error: 350377.5938\n",
      "Epoch 2652/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 344327.1250 - mean_squared_error: 344327.1250\n",
      "Epoch 2653/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 339942.5312 - mean_squared_error: 339942.5312\n",
      "Epoch 2654/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 339656.6562 - mean_squared_error: 339656.6562\n",
      "Epoch 2655/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343786.0000 - mean_squared_error: 343786.0000\n",
      "Epoch 2656/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346349.9375 - mean_squared_error: 346349.9375\n",
      "Epoch 2657/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343038.8750 - mean_squared_error: 343038.8750\n",
      "Epoch 2658/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347692.0938 - mean_squared_error: 347692.0938\n",
      "Epoch 2659/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348109.9062 - mean_squared_error: 348109.9375\n",
      "Epoch 2660/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347887.6562 - mean_squared_error: 347887.6562\n",
      "Epoch 2661/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345316.1250 - mean_squared_error: 345316.1250\n",
      "Epoch 2662/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342641.4375 - mean_squared_error: 342641.4375\n",
      "Epoch 2663/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342998.4062 - mean_squared_error: 342998.4062\n",
      "Epoch 2664/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342267.2500 - mean_squared_error: 342267.2500\n",
      "Epoch 2665/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340151.5000 - mean_squared_error: 340151.5000\n",
      "Epoch 2666/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346522.2812 - mean_squared_error: 346522.2812\n",
      "Epoch 2667/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346726.2500 - mean_squared_error: 346726.2500\n",
      "Epoch 2668/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344325.0625 - mean_squared_error: 344325.0625\n",
      "Epoch 2669/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340097.0312 - mean_squared_error: 340097.0000\n",
      "Epoch 2670/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343620.9688 - mean_squared_error: 343620.9688\n",
      "Epoch 2671/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347268.2188 - mean_squared_error: 347268.2188\n",
      "Epoch 2672/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336360.4062 - mean_squared_error: 336360.4062\n",
      "Epoch 2673/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 354524.9062 - mean_squared_error: 354524.9062\n",
      "Epoch 2674/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342321.0938 - mean_squared_error: 342321.0938\n",
      "Epoch 2675/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346518.6250 - mean_squared_error: 346518.6250\n",
      "Epoch 2676/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344612.2500 - mean_squared_error: 344612.2500\n",
      "Epoch 2677/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345979.8750 - mean_squared_error: 345979.8750\n",
      "Epoch 2678/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346144.7188 - mean_squared_error: 346144.7188\n",
      "Epoch 2679/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342652.9688 - mean_squared_error: 342652.9688\n",
      "Epoch 2680/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350074.0312 - mean_squared_error: 350074.0312\n",
      "Epoch 2681/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 349545.3438 - mean_squared_error: 349545.3438\n",
      "Epoch 2682/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349265.5938 - mean_squared_error: 349265.5938\n",
      "Epoch 2683/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344530.1562 - mean_squared_error: 344530.1562\n",
      "Epoch 2684/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 357436.1562 - mean_squared_error: 357436.1562\n",
      "Epoch 2685/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 348905.1562 - mean_squared_error: 348905.1562\n",
      "Epoch 2686/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345965.7188 - mean_squared_error: 345965.7188\n",
      "Epoch 2687/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346060.8125 - mean_squared_error: 346060.8125\n",
      "Epoch 2688/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341378.1875 - mean_squared_error: 341378.1875\n",
      "Epoch 2689/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348239.2188 - mean_squared_error: 348239.2188\n",
      "Epoch 2690/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344189.5625 - mean_squared_error: 344189.5625\n",
      "Epoch 2691/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343265.3438 - mean_squared_error: 343265.3438\n",
      "Epoch 2692/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337445.0938 - mean_squared_error: 337445.0938\n",
      "Epoch 2693/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340966.9062 - mean_squared_error: 340966.9062\n",
      "Epoch 2694/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351358.6250 - mean_squared_error: 351358.6250\n",
      "Epoch 2695/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342183.7500 - mean_squared_error: 342183.7500\n",
      "Epoch 2696/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341501.4688 - mean_squared_error: 341501.4688\n",
      "Epoch 2697/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341987.0312 - mean_squared_error: 341987.0312\n",
      "Epoch 2698/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341604.9375 - mean_squared_error: 341604.9375\n",
      "Epoch 2699/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341202.9062 - mean_squared_error: 341202.9062\n",
      "Epoch 2700/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336411.7188 - mean_squared_error: 336411.7188\n",
      "Epoch 2701/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344878.8125 - mean_squared_error: 344878.8125\n",
      "Epoch 2702/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340948.9062 - mean_squared_error: 340948.9062\n",
      "Epoch 2703/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341567.9062 - mean_squared_error: 341567.9062\n",
      "Epoch 2704/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340314.3750 - mean_squared_error: 340314.3750\n",
      "Epoch 2705/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 341438.5000 - mean_squared_error: 341438.5000\n",
      "Epoch 2706/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344767.8438 - mean_squared_error: 344767.8438\n",
      "Epoch 2707/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339208.7188 - mean_squared_error: 339208.7188\n",
      "Epoch 2708/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341694.8438 - mean_squared_error: 341694.8438\n",
      "Epoch 2709/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342186.8438 - mean_squared_error: 342186.8438\n",
      "Epoch 2710/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341816.7500 - mean_squared_error: 341816.7500\n",
      "Epoch 2711/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345050.3125 - mean_squared_error: 345050.3125\n",
      "Epoch 2712/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341450.0000 - mean_squared_error: 341450.0312\n",
      "Epoch 2713/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349987.0000 - mean_squared_error: 349987.0000\n",
      "Epoch 2714/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343596.3438 - mean_squared_error: 343596.3438\n",
      "Epoch 2715/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346302.9062 - mean_squared_error: 346302.9062\n",
      "Epoch 2716/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339372.9062 - mean_squared_error: 339372.9062\n",
      "Epoch 2717/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 343310.4062 - mean_squared_error: 343310.4062\n",
      "Epoch 2718/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344354.9688 - mean_squared_error: 344354.9688\n",
      "Epoch 2719/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344352.6250 - mean_squared_error: 344352.6250\n",
      "Epoch 2720/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345819.2188 - mean_squared_error: 345819.2188\n",
      "Epoch 2721/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338763.5000 - mean_squared_error: 338763.5000\n",
      "Epoch 2722/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348647.6562 - mean_squared_error: 348647.6562\n",
      "Epoch 2723/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339985.6250 - mean_squared_error: 339985.6250\n",
      "Epoch 2724/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341155.0625 - mean_squared_error: 341155.0625\n",
      "Epoch 2725/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343883.6250 - mean_squared_error: 343883.6250\n",
      "Epoch 2726/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347408.6562 - mean_squared_error: 347408.6562\n",
      "Epoch 2727/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349177.0312 - mean_squared_error: 349177.0312\n",
      "Epoch 2728/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339984.7812 - mean_squared_error: 339984.7812\n",
      "Epoch 2729/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341527.2500 - mean_squared_error: 341527.2500\n",
      "Epoch 2730/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347863.5312 - mean_squared_error: 347863.5312\n",
      "Epoch 2731/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342117.0312 - mean_squared_error: 342117.0312\n",
      "Epoch 2732/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352725.9688 - mean_squared_error: 352725.9688\n",
      "Epoch 2733/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 355797.4062 - mean_squared_error: 355797.4062\n",
      "Epoch 2734/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343938.4375 - mean_squared_error: 343938.4375\n",
      "Epoch 2735/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347206.7188 - mean_squared_error: 347206.7188\n",
      "Epoch 2736/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347621.7500 - mean_squared_error: 347621.7500\n",
      "Epoch 2737/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346429.8125 - mean_squared_error: 346429.8125\n",
      "Epoch 2738/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 343515.2188 - mean_squared_error: 343515.2188\n",
      "Epoch 2739/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343981.6562 - mean_squared_error: 343981.6562\n",
      "Epoch 2740/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341563.5312 - mean_squared_error: 341563.5312\n",
      "Epoch 2741/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348748.6875 - mean_squared_error: 348748.6875\n",
      "Epoch 2742/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346412.2500 - mean_squared_error: 346412.2500\n",
      "Epoch 2743/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340990.7500 - mean_squared_error: 340990.7500\n",
      "Epoch 2744/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342786.1875 - mean_squared_error: 342786.1875\n",
      "Epoch 2745/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341954.7500 - mean_squared_error: 341954.7500\n",
      "Epoch 2746/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344880.7500 - mean_squared_error: 344880.8125\n",
      "Epoch 2747/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344911.9688 - mean_squared_error: 344911.9688\n",
      "Epoch 2748/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336987.4062 - mean_squared_error: 336987.4062\n",
      "Epoch 2749/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339949.3750 - mean_squared_error: 339949.3750\n",
      "Epoch 2750/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337871.2812 - mean_squared_error: 337871.2812\n",
      "Epoch 2751/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340208.3438 - mean_squared_error: 340208.2812\n",
      "Epoch 2752/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342603.1875 - mean_squared_error: 342603.1875\n",
      "Epoch 2753/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346287.2812 - mean_squared_error: 346287.2812\n",
      "Epoch 2754/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346917.7812 - mean_squared_error: 346917.7812\n",
      "Epoch 2755/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346346.7188 - mean_squared_error: 346346.7188\n",
      "Epoch 2756/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346080.8125 - mean_squared_error: 346080.8125\n",
      "Epoch 2757/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 348446.7500 - mean_squared_error: 348446.7500\n",
      "Epoch 2758/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338500.7812 - mean_squared_error: 338500.7812\n",
      "Epoch 2759/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345386.5625 - mean_squared_error: 345386.5625\n",
      "Epoch 2760/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342336.3750 - mean_squared_error: 342336.3750\n",
      "Epoch 2761/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345251.2188 - mean_squared_error: 345251.2188\n",
      "Epoch 2762/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343834.9062 - mean_squared_error: 343834.9062\n",
      "Epoch 2763/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349312.3125 - mean_squared_error: 349312.3125\n",
      "Epoch 2764/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339118.2812 - mean_squared_error: 339118.2812\n",
      "Epoch 2765/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340970.2500 - mean_squared_error: 340970.2500\n",
      "Epoch 2766/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339127.0938 - mean_squared_error: 339127.0938\n",
      "Epoch 2767/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337196.4688 - mean_squared_error: 337196.4688\n",
      "Epoch 2768/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 12ms/step - loss: 345759.7812 - mean_squared_error: 345759.7812\n",
      "Epoch 2769/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339208.0312 - mean_squared_error: 339208.0312\n",
      "Epoch 2770/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345824.0000 - mean_squared_error: 345824.0000\n",
      "Epoch 2771/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345630.8438 - mean_squared_error: 345630.8438\n",
      "Epoch 2772/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336201.0938 - mean_squared_error: 336201.0938\n",
      "Epoch 2773/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 333760.9688 - mean_squared_error: 333760.9688\n",
      "Epoch 2774/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345959.1562 - mean_squared_error: 345959.1562\n",
      "Epoch 2775/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351210.0312 - mean_squared_error: 351210.0312\n",
      "Epoch 2776/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341282.5312 - mean_squared_error: 341282.5312\n",
      "Epoch 2777/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339000.8125 - mean_squared_error: 339000.8125\n",
      "Epoch 2778/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341118.5938 - mean_squared_error: 341118.5938\n",
      "Epoch 2779/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342959.0312 - mean_squared_error: 342959.0312\n",
      "Epoch 2780/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335718.2188 - mean_squared_error: 335718.2188\n",
      "Epoch 2781/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348306.5938 - mean_squared_error: 348306.5938\n",
      "Epoch 2782/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342266.5625 - mean_squared_error: 342266.5938\n",
      "Epoch 2783/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348353.5625 - mean_squared_error: 348353.5625\n",
      "Epoch 2784/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337501.0938 - mean_squared_error: 337501.0938\n",
      "Epoch 2785/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341603.2500 - mean_squared_error: 341603.2500\n",
      "Epoch 2786/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337675.0938 - mean_squared_error: 337675.0938\n",
      "Epoch 2787/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347418.4688 - mean_squared_error: 347418.4688\n",
      "Epoch 2788/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345492.1562 - mean_squared_error: 345492.1562\n",
      "Epoch 2789/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340595.9375 - mean_squared_error: 340595.9375\n",
      "Epoch 2790/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342797.7500 - mean_squared_error: 342797.7500\n",
      "Epoch 2791/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345303.5000 - mean_squared_error: 345303.5000\n",
      "Epoch 2792/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347504.2188 - mean_squared_error: 347504.2188\n",
      "Epoch 2793/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341818.9688 - mean_squared_error: 341818.9688\n",
      "Epoch 2794/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344709.4375 - mean_squared_error: 344709.4375\n",
      "Epoch 2795/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336650.7812 - mean_squared_error: 336650.7812\n",
      "Epoch 2796/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340263.9062 - mean_squared_error: 340263.9062\n",
      "Epoch 2797/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346502.3125 - mean_squared_error: 346502.3125\n",
      "Epoch 2798/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346284.4688 - mean_squared_error: 346284.4688\n",
      "Epoch 2799/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341895.3125 - mean_squared_error: 341895.3125\n",
      "Epoch 2800/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345886.5000 - mean_squared_error: 345886.5000\n",
      "Epoch 2801/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336060.8125 - mean_squared_error: 336060.8125\n",
      "Epoch 2802/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340861.5312 - mean_squared_error: 340861.5312\n",
      "Epoch 2803/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345431.2500 - mean_squared_error: 345431.2500\n",
      "Epoch 2804/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346452.4062 - mean_squared_error: 346452.4062\n",
      "Epoch 2805/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343314.8438 - mean_squared_error: 343314.8438\n",
      "Epoch 2806/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 343464.0938 - mean_squared_error: 343464.0938\n",
      "Epoch 2807/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351364.7188 - mean_squared_error: 351364.7188\n",
      "Epoch 2808/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340656.2188 - mean_squared_error: 340656.2188\n",
      "Epoch 2809/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347568.2188 - mean_squared_error: 347568.2188\n",
      "Epoch 2810/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346610.2188 - mean_squared_error: 346610.2188\n",
      "Epoch 2811/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337624.3438 - mean_squared_error: 337624.3438\n",
      "Epoch 2812/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345033.0000 - mean_squared_error: 345033.0000\n",
      "Epoch 2813/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348764.2500 - mean_squared_error: 348764.2500\n",
      "Epoch 2814/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344864.5000 - mean_squared_error: 344864.5000\n",
      "Epoch 2815/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343486.9688 - mean_squared_error: 343486.9688\n",
      "Epoch 2816/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340313.6875 - mean_squared_error: 340313.6250\n",
      "Epoch 2817/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338943.2500 - mean_squared_error: 338943.2500\n",
      "Epoch 2818/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345216.4688 - mean_squared_error: 345216.4688\n",
      "Epoch 2819/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347012.8750 - mean_squared_error: 347012.8750\n",
      "Epoch 2820/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342942.1562 - mean_squared_error: 342942.1562\n",
      "Epoch 2821/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348247.4062 - mean_squared_error: 348247.4062\n",
      "Epoch 2822/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338514.5312 - mean_squared_error: 338514.5312\n",
      "Epoch 2823/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342624.7188 - mean_squared_error: 342624.7188\n",
      "Epoch 2824/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338010.2812 - mean_squared_error: 338010.2812\n",
      "Epoch 2825/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343235.8125 - mean_squared_error: 343235.8125\n",
      "Epoch 2826/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345311.2812 - mean_squared_error: 345311.2812\n",
      "Epoch 2827/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344250.6250 - mean_squared_error: 344250.6250\n",
      "Epoch 2828/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339243.5625 - mean_squared_error: 339243.5625\n",
      "Epoch 2829/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342247.9688 - mean_squared_error: 342247.9688\n",
      "Epoch 2830/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350258.5938 - mean_squared_error: 350258.5938\n",
      "Epoch 2831/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345006.8750 - mean_squared_error: 345006.8750\n",
      "Epoch 2832/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336894.6875 - mean_squared_error: 336894.6875\n",
      "Epoch 2833/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 334981.0625 - mean_squared_error: 334981.0625\n",
      "Epoch 2834/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344234.4688 - mean_squared_error: 344234.4688\n",
      "Epoch 2835/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339257.0000 - mean_squared_error: 339257.0000\n",
      "Epoch 2836/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340039.4688 - mean_squared_error: 340039.4688\n",
      "Epoch 2837/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342726.7188 - mean_squared_error: 342726.7188\n",
      "Epoch 2838/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341875.9375 - mean_squared_error: 341875.9375\n",
      "Epoch 2839/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347175.3750 - mean_squared_error: 347175.3750\n",
      "Epoch 2840/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349686.9062 - mean_squared_error: 349686.9062\n",
      "Epoch 2841/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340526.6250 - mean_squared_error: 340526.6250\n",
      "Epoch 2842/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340078.5625 - mean_squared_error: 340078.5625\n",
      "Epoch 2843/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341141.1875 - mean_squared_error: 341141.1875\n",
      "Epoch 2844/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335797.1250 - mean_squared_error: 335797.1250\n",
      "Epoch 2845/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343824.8750 - mean_squared_error: 343824.8750\n",
      "Epoch 2846/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336534.6562 - mean_squared_error: 336534.6562\n",
      "Epoch 2847/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346167.7812 - mean_squared_error: 346167.7812\n",
      "Epoch 2848/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341477.6250 - mean_squared_error: 341477.6250\n",
      "Epoch 2849/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345091.5625 - mean_squared_error: 345091.5625\n",
      "Epoch 2850/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336200.7500 - mean_squared_error: 336200.7500\n",
      "Epoch 2851/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339513.4062 - mean_squared_error: 339513.4062\n",
      "Epoch 2852/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336461.6562 - mean_squared_error: 336461.6562\n",
      "Epoch 2853/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345327.2188 - mean_squared_error: 345327.2188\n",
      "Epoch 2854/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341207.4688 - mean_squared_error: 341207.4688\n",
      "Epoch 2855/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339495.2500 - mean_squared_error: 339495.2500\n",
      "Epoch 2856/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344266.8438 - mean_squared_error: 344266.8438\n",
      "Epoch 2857/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344658.8438 - mean_squared_error: 344658.8438\n",
      "Epoch 2858/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346827.6875 - mean_squared_error: 346827.6875\n",
      "Epoch 2859/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338401.6562 - mean_squared_error: 338401.6562\n",
      "Epoch 2860/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343661.2500 - mean_squared_error: 343661.2500\n",
      "Epoch 2861/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340226.7188 - mean_squared_error: 340226.7188\n",
      "Epoch 2862/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338409.1250 - mean_squared_error: 338409.1250\n",
      "Epoch 2863/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346329.4062 - mean_squared_error: 346329.4062\n",
      "Epoch 2864/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339178.8438 - mean_squared_error: 339178.8438\n",
      "Epoch 2865/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345155.9062 - mean_squared_error: 345155.9062\n",
      "Epoch 2866/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346126.2812 - mean_squared_error: 346126.2812\n",
      "Epoch 2867/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339182.9688 - mean_squared_error: 339182.9688\n",
      "Epoch 2868/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352101.3750 - mean_squared_error: 352101.3750\n",
      "Epoch 2869/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341860.9688 - mean_squared_error: 341860.9688\n",
      "Epoch 2870/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338091.1875 - mean_squared_error: 338091.1875\n",
      "Epoch 2871/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346549.0625 - mean_squared_error: 346549.0625\n",
      "Epoch 2872/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340623.2500 - mean_squared_error: 340623.2500\n",
      "Epoch 2873/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335712.9688 - mean_squared_error: 335712.9688\n",
      "Epoch 2874/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344468.8438 - mean_squared_error: 344468.8438\n",
      "Epoch 2875/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 355219.6250 - mean_squared_error: 355219.6250\n",
      "Epoch 2876/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 334603.2500 - mean_squared_error: 334603.2500\n",
      "Epoch 2877/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347288.9375 - mean_squared_error: 347288.9375\n",
      "Epoch 2878/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339678.9688 - mean_squared_error: 339678.9688\n",
      "Epoch 2879/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343939.7500 - mean_squared_error: 343939.7500\n",
      "Epoch 2880/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345402.9688 - mean_squared_error: 345403.0000\n",
      "Epoch 2881/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344804.4375 - mean_squared_error: 344804.4375\n",
      "Epoch 2882/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345928.9688 - mean_squared_error: 345928.9375\n",
      "Epoch 2883/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343376.9375 - mean_squared_error: 343376.9375\n",
      "Epoch 2884/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 335480.9375 - mean_squared_error: 335480.9375\n",
      "Epoch 2885/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348170.9375 - mean_squared_error: 348170.9375\n",
      "Epoch 2886/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341768.0938 - mean_squared_error: 341768.0938\n",
      "Epoch 2887/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 334703.4688 - mean_squared_error: 334703.4688\n",
      "Epoch 2888/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340366.5625 - mean_squared_error: 340366.5312\n",
      "Epoch 2889/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 353005.0938 - mean_squared_error: 353005.0938\n",
      "Epoch 2890/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 334172.3125 - mean_squared_error: 334172.3125\n",
      "Epoch 2891/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340883.0000 - mean_squared_error: 340883.0000\n",
      "Epoch 2892/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341198.2500 - mean_squared_error: 341198.2500\n",
      "Epoch 2893/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339090.5312 - mean_squared_error: 339090.5000\n",
      "Epoch 2894/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341138.2812 - mean_squared_error: 341138.2812\n",
      "Epoch 2895/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346573.8438 - mean_squared_error: 346573.8438\n",
      "Epoch 2896/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344895.0312 - mean_squared_error: 344895.0312\n",
      "Epoch 2897/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343574.6250 - mean_squared_error: 343574.6250\n",
      "Epoch 2898/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339618.8750 - mean_squared_error: 339618.8750\n",
      "Epoch 2899/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340560.8125 - mean_squared_error: 340560.8125\n",
      "Epoch 2900/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 338057.7812 - mean_squared_error: 338057.7812\n",
      "Epoch 2901/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338966.6875 - mean_squared_error: 338966.6875\n",
      "Epoch 2902/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340594.8125 - mean_squared_error: 340594.8125\n",
      "Epoch 2903/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 333944.4062 - mean_squared_error: 333944.4062\n",
      "Epoch 2904/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342260.8750 - mean_squared_error: 342260.9062\n",
      "Epoch 2905/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341881.3125 - mean_squared_error: 341881.3125\n",
      "Epoch 2906/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343431.2812 - mean_squared_error: 343431.2812\n",
      "Epoch 2907/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345799.8750 - mean_squared_error: 345799.8750\n",
      "Epoch 2908/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342665.2500 - mean_squared_error: 342665.2500\n",
      "Epoch 2909/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339865.8750 - mean_squared_error: 339865.8750\n",
      "Epoch 2910/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337417.3750 - mean_squared_error: 337417.4062\n",
      "Epoch 2911/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343352.9375 - mean_squared_error: 343352.9375\n",
      "Epoch 2912/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347190.8125 - mean_squared_error: 347190.8125\n",
      "Epoch 2913/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342513.3750 - mean_squared_error: 342513.3750\n",
      "Epoch 2914/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348419.3438 - mean_squared_error: 348419.3438\n",
      "Epoch 2915/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 334671.1562 - mean_squared_error: 334671.1562\n",
      "Epoch 2916/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339488.1875 - mean_squared_error: 339488.1875\n",
      "Epoch 2917/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 333341.8438 - mean_squared_error: 333341.8438\n",
      "Epoch 2918/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 346373.5000 - mean_squared_error: 346373.5000\n",
      "Epoch 2919/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344010.1875 - mean_squared_error: 344010.1875\n",
      "Epoch 2920/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335722.2500 - mean_squared_error: 335722.2500\n",
      "Epoch 2921/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335679.0000 - mean_squared_error: 335679.0000\n",
      "Epoch 2922/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342531.7812 - mean_squared_error: 342531.7812\n",
      "Epoch 2923/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338987.9688 - mean_squared_error: 338987.9688\n",
      "Epoch 2924/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343132.0312 - mean_squared_error: 343132.0312\n",
      "Epoch 2925/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342085.4062 - mean_squared_error: 342085.4062\n",
      "Epoch 2926/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339356.8438 - mean_squared_error: 339356.8438\n",
      "Epoch 2927/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344291.3750 - mean_squared_error: 344291.3750\n",
      "Epoch 2928/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342626.5000 - mean_squared_error: 342626.4375\n",
      "Epoch 2929/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347504.6562 - mean_squared_error: 347504.6562\n",
      "Epoch 2930/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340305.9375 - mean_squared_error: 340305.9375\n",
      "Epoch 2931/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342858.1250 - mean_squared_error: 342858.1250\n",
      "Epoch 2932/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339461.8125 - mean_squared_error: 339461.8125\n",
      "Epoch 2933/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344059.7812 - mean_squared_error: 344059.7812\n",
      "Epoch 2934/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345076.8750 - mean_squared_error: 345076.8750\n",
      "Epoch 2935/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339728.5312 - mean_squared_error: 339728.5312\n",
      "Epoch 2936/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 353462.3750 - mean_squared_error: 353462.3750\n",
      "Epoch 2937/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344263.9375 - mean_squared_error: 344263.9375\n",
      "Epoch 2938/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346316.2500 - mean_squared_error: 346316.2500\n",
      "Epoch 2939/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343120.0312 - mean_squared_error: 343120.0312\n",
      "Epoch 2940/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347832.0312 - mean_squared_error: 347832.0312\n",
      "Epoch 2941/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348750.3125 - mean_squared_error: 348750.3125\n",
      "Epoch 2942/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342213.0938 - mean_squared_error: 342213.0938\n",
      "Epoch 2943/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337217.9688 - mean_squared_error: 337217.9688\n",
      "Epoch 2944/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344278.4688 - mean_squared_error: 344278.4688\n",
      "Epoch 2945/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342611.5625 - mean_squared_error: 342611.5625\n",
      "Epoch 2946/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340675.0000 - mean_squared_error: 340675.0000\n",
      "Epoch 2947/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341538.0938 - mean_squared_error: 341538.0938\n",
      "Epoch 2948/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345660.9062 - mean_squared_error: 345660.9062\n",
      "Epoch 2949/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343622.0000 - mean_squared_error: 343622.0000\n",
      "Epoch 2950/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346147.2188 - mean_squared_error: 346147.2188\n",
      "Epoch 2951/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345587.0000 - mean_squared_error: 345587.0000\n",
      "Epoch 2952/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344648.7500 - mean_squared_error: 344648.7500\n",
      "Epoch 2953/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 334102.7812 - mean_squared_error: 334102.7812\n",
      "Epoch 2954/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344850.4062 - mean_squared_error: 344850.4062\n",
      "Epoch 2955/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347170.1562 - mean_squared_error: 347170.1562\n",
      "Epoch 2956/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347355.2812 - mean_squared_error: 347355.2812\n",
      "Epoch 2957/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342831.3438 - mean_squared_error: 342831.3438\n",
      "Epoch 2958/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339421.8125 - mean_squared_error: 339421.8125\n",
      "Epoch 2959/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338464.0625 - mean_squared_error: 338464.0625\n",
      "Epoch 2960/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 334205.8125 - mean_squared_error: 334205.8125\n",
      "Epoch 2961/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340902.5312 - mean_squared_error: 340902.5312\n",
      "Epoch 2962/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345168.8750 - mean_squared_error: 345168.8750\n",
      "Epoch 2963/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347891.3750 - mean_squared_error: 347891.3750\n",
      "Epoch 2964/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 334358.4062 - mean_squared_error: 334358.4062\n",
      "Epoch 2965/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346532.9062 - mean_squared_error: 346532.9062\n",
      "Epoch 2966/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341356.3750 - mean_squared_error: 341356.3750\n",
      "Epoch 2967/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345965.2812 - mean_squared_error: 345965.2812\n",
      "Epoch 2968/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340161.5000 - mean_squared_error: 340161.5000\n",
      "Epoch 2969/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 335759.7812 - mean_squared_error: 335759.7812\n",
      "Epoch 2970/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338236.3438 - mean_squared_error: 338236.3438\n",
      "Epoch 2971/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341526.6875 - mean_squared_error: 341526.6875\n",
      "Epoch 2972/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346548.1562 - mean_squared_error: 346548.1562\n",
      "Epoch 2973/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345288.0312 - mean_squared_error: 345288.0312\n",
      "Epoch 2974/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341928.3125 - mean_squared_error: 341928.3125\n",
      "Epoch 2975/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337785.0938 - mean_squared_error: 337785.0938\n",
      "Epoch 2976/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344988.5000 - mean_squared_error: 344988.5000\n",
      "Epoch 2977/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 330266.3438 - mean_squared_error: 330266.3438\n",
      "Epoch 2978/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342477.6250 - mean_squared_error: 342477.6250\n",
      "Epoch 2979/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343991.6875 - mean_squared_error: 343991.6875\n",
      "Epoch 2980/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342674.6250 - mean_squared_error: 342674.6250\n",
      "Epoch 2981/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344914.2812 - mean_squared_error: 344914.2812\n",
      "Epoch 2982/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335792.1875 - mean_squared_error: 335792.1875\n",
      "Epoch 2983/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346075.7188 - mean_squared_error: 346075.7188\n",
      "Epoch 2984/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344249.4688 - mean_squared_error: 344249.4688\n",
      "Epoch 2985/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345197.6562 - mean_squared_error: 345197.6562\n",
      "Epoch 2986/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336446.4375 - mean_squared_error: 336446.4375\n",
      "Epoch 2987/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339680.5000 - mean_squared_error: 339680.5000\n",
      "Epoch 2988/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 332336.7812 - mean_squared_error: 332336.7812\n",
      "Epoch 2989/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346931.0312 - mean_squared_error: 346931.0312\n",
      "Epoch 2990/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 334076.1875 - mean_squared_error: 334076.1875\n",
      "Epoch 2991/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336921.4688 - mean_squared_error: 336921.4688\n",
      "Epoch 2992/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345477.5938 - mean_squared_error: 345477.5938\n",
      "Epoch 2993/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340455.0938 - mean_squared_error: 340455.0938\n",
      "Epoch 2994/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346931.1250 - mean_squared_error: 346931.1250\n",
      "Epoch 2995/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340608.3750 - mean_squared_error: 340608.3750\n",
      "Epoch 2996/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343553.5625 - mean_squared_error: 343553.5625\n",
      "Epoch 2997/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343455.7500 - mean_squared_error: 343455.7500\n",
      "Epoch 2998/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351042.9375 - mean_squared_error: 351042.9375\n",
      "Epoch 2999/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344326.3750 - mean_squared_error: 344326.3750\n",
      "Epoch 3000/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342584.1562 - mean_squared_error: 342584.1562\n",
      "Epoch 3001/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337697.5938 - mean_squared_error: 337697.5938\n",
      "Epoch 3002/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346244.0938 - mean_squared_error: 346244.0938\n",
      "Epoch 3003/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 333339.4062 - mean_squared_error: 333339.4062\n",
      "Epoch 3004/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343294.3438 - mean_squared_error: 343294.3438\n",
      "Epoch 3005/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337443.0312 - mean_squared_error: 337443.0312\n",
      "Epoch 3006/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345940.4062 - mean_squared_error: 345940.4062\n",
      "Epoch 3007/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 335528.2812 - mean_squared_error: 335528.2812\n",
      "Epoch 3008/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 336397.5938 - mean_squared_error: 336397.5938\n",
      "Epoch 3009/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337365.1562 - mean_squared_error: 337365.1562\n",
      "Epoch 3010/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347614.6250 - mean_squared_error: 347614.6250\n",
      "Epoch 3011/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342396.8125 - mean_squared_error: 342396.8125\n",
      "Epoch 3012/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346479.7812 - mean_squared_error: 346479.7812\n",
      "Epoch 3013/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343869.8438 - mean_squared_error: 343869.8438\n",
      "Epoch 3014/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345864.8438 - mean_squared_error: 345864.8438\n",
      "Epoch 3015/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339167.3125 - mean_squared_error: 339167.3125\n",
      "Epoch 3016/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341327.0625 - mean_squared_error: 341327.0625\n",
      "Epoch 3017/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343723.3750 - mean_squared_error: 343723.3750\n",
      "Epoch 3018/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342899.2188 - mean_squared_error: 342899.2188\n",
      "Epoch 3019/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342253.0000 - mean_squared_error: 342253.0000\n",
      "Epoch 3020/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343612.3750 - mean_squared_error: 343612.3750\n",
      "Epoch 3021/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344110.1250 - mean_squared_error: 344110.1250\n",
      "Epoch 3022/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345148.0312 - mean_squared_error: 345148.0312\n",
      "Epoch 3023/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344816.3438 - mean_squared_error: 344816.3438\n",
      "Epoch 3024/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341166.7500 - mean_squared_error: 341166.7500\n",
      "Epoch 3025/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343996.1250 - mean_squared_error: 343996.1250\n",
      "Epoch 3026/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342092.1250 - mean_squared_error: 342092.1250\n",
      "Epoch 3027/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341667.8750 - mean_squared_error: 341667.8750\n",
      "Epoch 3028/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347987.0625 - mean_squared_error: 347987.0625\n",
      "Epoch 3029/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341630.5312 - mean_squared_error: 341630.5312\n",
      "Epoch 3030/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344218.2188 - mean_squared_error: 344218.2188\n",
      "Epoch 3031/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341152.2812 - mean_squared_error: 341152.2812\n",
      "Epoch 3032/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 14ms/step - loss: 340605.7500 - mean_squared_error: 340605.7500\n",
      "Epoch 3033/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347664.6875 - mean_squared_error: 347664.6875\n",
      "Epoch 3034/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340307.3750 - mean_squared_error: 340307.3750\n",
      "Epoch 3035/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339876.8750 - mean_squared_error: 339876.8750\n",
      "Epoch 3036/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347129.9688 - mean_squared_error: 347129.9688\n",
      "Epoch 3037/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343547.7812 - mean_squared_error: 343547.7812\n",
      "Epoch 3038/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338226.4688 - mean_squared_error: 338226.4688\n",
      "Epoch 3039/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 332768.2500 - mean_squared_error: 332768.2500\n",
      "Epoch 3040/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338511.2812 - mean_squared_error: 338511.2812\n",
      "Epoch 3041/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344493.0000 - mean_squared_error: 344493.0000\n",
      "Epoch 3042/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342915.9062 - mean_squared_error: 342915.9062\n",
      "Epoch 3043/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350195.9688 - mean_squared_error: 350195.9688\n",
      "Epoch 3044/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 355021.9375 - mean_squared_error: 355021.9375\n",
      "Epoch 3045/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348792.2188 - mean_squared_error: 348792.2188\n",
      "Epoch 3046/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 336933.5938 - mean_squared_error: 336933.5938\n",
      "Epoch 3047/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338452.2812 - mean_squared_error: 338452.2812\n",
      "Epoch 3048/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340211.4375 - mean_squared_error: 340211.4375\n",
      "Epoch 3049/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346483.1875 - mean_squared_error: 346483.1875\n",
      "Epoch 3050/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345996.1875 - mean_squared_error: 345996.1875\n",
      "Epoch 3051/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349248.6250 - mean_squared_error: 349248.6250\n",
      "Epoch 3052/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340331.2500 - mean_squared_error: 340331.2500\n",
      "Epoch 3053/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351255.5000 - mean_squared_error: 351255.5000\n",
      "Epoch 3054/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345989.6875 - mean_squared_error: 345989.6875\n",
      "Epoch 3055/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341930.5938 - mean_squared_error: 341930.5938\n",
      "Epoch 3056/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346505.1250 - mean_squared_error: 346505.1250\n",
      "Epoch 3057/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342411.0312 - mean_squared_error: 342411.0312\n",
      "Epoch 3058/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345390.3125 - mean_squared_error: 345390.3125\n",
      "Epoch 3059/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336193.9062 - mean_squared_error: 336193.9062\n",
      "Epoch 3060/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338458.1250 - mean_squared_error: 338458.1250\n",
      "Epoch 3061/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348850.6562 - mean_squared_error: 348850.6562\n",
      "Epoch 3062/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345935.5938 - mean_squared_error: 345935.5938\n",
      "Epoch 3063/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353259.6875 - mean_squared_error: 353259.6875\n",
      "Epoch 3064/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340283.7500 - mean_squared_error: 340283.7500\n",
      "Epoch 3065/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 334946.3750 - mean_squared_error: 334946.3750\n",
      "Epoch 3066/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 339675.5625 - mean_squared_error: 339675.5625\n",
      "Epoch 3067/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336433.9375 - mean_squared_error: 336433.9375\n",
      "Epoch 3068/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346317.7500 - mean_squared_error: 346317.7500\n",
      "Epoch 3069/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342423.3125 - mean_squared_error: 342423.3125\n",
      "Epoch 3070/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344190.8125 - mean_squared_error: 344190.8125\n",
      "Epoch 3071/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349361.9062 - mean_squared_error: 349361.9062\n",
      "Epoch 3072/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342095.4375 - mean_squared_error: 342095.4375\n",
      "Epoch 3073/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342042.5000 - mean_squared_error: 342042.5000\n",
      "Epoch 3074/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337464.6250 - mean_squared_error: 337464.6250\n",
      "Epoch 3075/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340186.2500 - mean_squared_error: 340186.2500\n",
      "Epoch 3076/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337927.1562 - mean_squared_error: 337927.1562\n",
      "Epoch 3077/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341737.2500 - mean_squared_error: 341737.2500\n",
      "Epoch 3078/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341540.9375 - mean_squared_error: 341540.9375\n",
      "Epoch 3079/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341365.6562 - mean_squared_error: 341365.6562\n",
      "Epoch 3080/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341045.6562 - mean_squared_error: 341045.6562\n",
      "Epoch 3081/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342471.0000 - mean_squared_error: 342471.0000\n",
      "Epoch 3082/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340223.5000 - mean_squared_error: 340223.5000\n",
      "Epoch 3083/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 341488.0938 - mean_squared_error: 341488.0938\n",
      "Epoch 3084/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340567.3750 - mean_squared_error: 340567.3750\n",
      "Epoch 3085/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 339494.9062 - mean_squared_error: 339494.9062\n",
      "Epoch 3086/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338450.2188 - mean_squared_error: 338450.2188\n",
      "Epoch 3087/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 338130.0312 - mean_squared_error: 338130.0312\n",
      "Epoch 3088/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345509.3438 - mean_squared_error: 345509.3438\n",
      "Epoch 3089/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341289.6562 - mean_squared_error: 341289.6562\n",
      "Epoch 3090/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 334589.9375 - mean_squared_error: 334589.9375\n",
      "Epoch 3091/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341422.0000 - mean_squared_error: 341422.0000\n",
      "Epoch 3092/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 338491.4375 - mean_squared_error: 338491.4375\n",
      "Epoch 3093/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 345932.5938 - mean_squared_error: 345932.5938\n",
      "Epoch 3094/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345161.1562 - mean_squared_error: 345161.1562\n",
      "Epoch 3095/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337266.0000 - mean_squared_error: 337266.0000\n",
      "Epoch 3096/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340125.2500 - mean_squared_error: 340125.2500\n",
      "Epoch 3097/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338202.0625 - mean_squared_error: 338202.0625\n",
      "Epoch 3098/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340420.6250 - mean_squared_error: 340420.6250\n",
      "Epoch 3099/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346217.2188 - mean_squared_error: 346217.2188\n",
      "Epoch 3100/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344399.1875 - mean_squared_error: 344399.1875\n",
      "Epoch 3101/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342143.1875 - mean_squared_error: 342143.1875\n",
      "Epoch 3102/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339593.7812 - mean_squared_error: 339593.7812\n",
      "Epoch 3103/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 333397.7188 - mean_squared_error: 333397.7188\n",
      "Epoch 3104/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342566.7188 - mean_squared_error: 342566.7188\n",
      "Epoch 3105/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349007.2812 - mean_squared_error: 349007.2812\n",
      "Epoch 3106/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342895.3438 - mean_squared_error: 342895.3438\n",
      "Epoch 3107/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345939.2812 - mean_squared_error: 345939.2812\n",
      "Epoch 3108/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340074.4688 - mean_squared_error: 340074.4688\n",
      "Epoch 3109/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343300.5312 - mean_squared_error: 343300.5312\n",
      "Epoch 3110/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338331.0000 - mean_squared_error: 338331.0000\n",
      "Epoch 3111/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341161.3438 - mean_squared_error: 341161.3438\n",
      "Epoch 3112/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341258.1562 - mean_squared_error: 341258.1562\n",
      "Epoch 3113/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352974.0625 - mean_squared_error: 352974.0625\n",
      "Epoch 3114/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344049.2500 - mean_squared_error: 344049.2500\n",
      "Epoch 3115/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339167.4062 - mean_squared_error: 339167.4062\n",
      "Epoch 3116/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340614.4062 - mean_squared_error: 340614.4062\n",
      "Epoch 3117/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337682.0625 - mean_squared_error: 337682.0625\n",
      "Epoch 3118/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344997.1562 - mean_squared_error: 344997.1562\n",
      "Epoch 3119/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337138.7188 - mean_squared_error: 337138.7188\n",
      "Epoch 3120/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 334649.6250 - mean_squared_error: 334649.6250\n",
      "Epoch 3121/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337280.4062 - mean_squared_error: 337280.4062\n",
      "Epoch 3122/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347767.5312 - mean_squared_error: 347767.5312\n",
      "Epoch 3123/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337190.4688 - mean_squared_error: 337190.4688\n",
      "Epoch 3124/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340072.7188 - mean_squared_error: 340072.7188\n",
      "Epoch 3125/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336133.5625 - mean_squared_error: 336133.5625\n",
      "Epoch 3126/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341209.7500 - mean_squared_error: 341209.7500\n",
      "Epoch 3127/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 342735.7188 - mean_squared_error: 342735.7188\n",
      "Epoch 3128/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344137.5000 - mean_squared_error: 344137.5000\n",
      "Epoch 3129/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340302.1875 - mean_squared_error: 340302.1875\n",
      "Epoch 3130/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343055.0000 - mean_squared_error: 343055.0000\n",
      "Epoch 3131/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339178.0312 - mean_squared_error: 339178.0312\n",
      "Epoch 3132/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344365.8750 - mean_squared_error: 344365.8750\n",
      "Epoch 3133/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339522.8750 - mean_squared_error: 339522.8750\n",
      "Epoch 3134/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345148.3438 - mean_squared_error: 345148.3438\n",
      "Epoch 3135/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344098.5312 - mean_squared_error: 344098.5312\n",
      "Epoch 3136/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351301.0312 - mean_squared_error: 351301.0312\n",
      "Epoch 3137/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344589.7500 - mean_squared_error: 344589.7500\n",
      "Epoch 3138/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344516.0625 - mean_squared_error: 344516.0625\n",
      "Epoch 3139/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349300.0000 - mean_squared_error: 349300.0000\n",
      "Epoch 3140/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345394.2812 - mean_squared_error: 345394.2812\n",
      "Epoch 3141/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 336524.7500 - mean_squared_error: 336524.7500\n",
      "Epoch 3142/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339172.6250 - mean_squared_error: 339172.6250\n",
      "Epoch 3143/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344472.0000 - mean_squared_error: 344472.0000\n",
      "Epoch 3144/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335621.9062 - mean_squared_error: 335621.9062\n",
      "Epoch 3145/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338214.9375 - mean_squared_error: 338214.9375\n",
      "Epoch 3146/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340349.0625 - mean_squared_error: 340349.0625\n",
      "Epoch 3147/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340838.1250 - mean_squared_error: 340838.1250\n",
      "Epoch 3148/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347267.0938 - mean_squared_error: 347267.0938\n",
      "Epoch 3149/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350106.4062 - mean_squared_error: 350106.4062\n",
      "Epoch 3150/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342117.5625 - mean_squared_error: 342117.5625\n",
      "Epoch 3151/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341523.2188 - mean_squared_error: 341523.2188\n",
      "Epoch 3152/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346634.5938 - mean_squared_error: 346634.5938\n",
      "Epoch 3153/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342983.8438 - mean_squared_error: 342983.8438\n",
      "Epoch 3154/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336170.9062 - mean_squared_error: 336170.9062\n",
      "Epoch 3155/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344240.4375 - mean_squared_error: 344240.4375\n",
      "Epoch 3156/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344608.9062 - mean_squared_error: 344608.9062\n",
      "Epoch 3157/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 333797.6875 - mean_squared_error: 333797.6875\n",
      "Epoch 3158/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337032.6562 - mean_squared_error: 337032.6562\n",
      "Epoch 3159/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347272.0625 - mean_squared_error: 347272.0625\n",
      "Epoch 3160/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 335944.6562 - mean_squared_error: 335944.6562\n",
      "Epoch 3161/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343795.9688 - mean_squared_error: 343795.9688\n",
      "Epoch 3162/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342436.6250 - mean_squared_error: 342436.6250\n",
      "Epoch 3163/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 335343.7500 - mean_squared_error: 335343.7500\n",
      "Epoch 3164/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 14ms/step - loss: 349231.0625 - mean_squared_error: 349231.0625\n",
      "Epoch 3165/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340370.7188 - mean_squared_error: 340370.6875\n",
      "Epoch 3166/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340248.8125 - mean_squared_error: 340248.8125\n",
      "Epoch 3167/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337428.2188 - mean_squared_error: 337428.2188\n",
      "Epoch 3168/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 352714.5938 - mean_squared_error: 352714.5938\n",
      "Epoch 3169/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345049.9688 - mean_squared_error: 345049.9688\n",
      "Epoch 3170/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339987.0625 - mean_squared_error: 339987.0625\n",
      "Epoch 3171/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342744.2188 - mean_squared_error: 342744.2188\n",
      "Epoch 3172/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338286.6250 - mean_squared_error: 338286.6250\n",
      "Epoch 3173/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348058.5000 - mean_squared_error: 348058.5312\n",
      "Epoch 3174/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342813.8125 - mean_squared_error: 342813.8125\n",
      "Epoch 3175/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346729.8750 - mean_squared_error: 346729.8750\n",
      "Epoch 3176/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346640.7188 - mean_squared_error: 346640.7188\n",
      "Epoch 3177/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343656.6250 - mean_squared_error: 343656.6250\n",
      "Epoch 3178/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351176.0625 - mean_squared_error: 351176.0625\n",
      "Epoch 3179/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350711.8438 - mean_squared_error: 350711.8438\n",
      "Epoch 3180/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349951.5312 - mean_squared_error: 349951.5312\n",
      "Epoch 3181/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344012.9688 - mean_squared_error: 344012.9688\n",
      "Epoch 3182/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341056.6875 - mean_squared_error: 341056.6875\n",
      "Epoch 3183/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341620.4688 - mean_squared_error: 341620.4688\n",
      "Epoch 3184/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349497.5312 - mean_squared_error: 349497.5312\n",
      "Epoch 3185/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345642.6250 - mean_squared_error: 345642.6250\n",
      "Epoch 3186/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345538.8125 - mean_squared_error: 345538.8125\n",
      "Epoch 3187/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 344380.9688 - mean_squared_error: 344380.9688\n",
      "Epoch 3188/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346068.3750 - mean_squared_error: 346068.3750\n",
      "Epoch 3189/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350289.6875 - mean_squared_error: 350289.6875\n",
      "Epoch 3190/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344625.4375 - mean_squared_error: 344625.4375\n",
      "Epoch 3191/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 335058.3750 - mean_squared_error: 335058.3750\n",
      "Epoch 3192/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 353986.3750 - mean_squared_error: 353986.3750\n",
      "Epoch 3193/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341020.5000 - mean_squared_error: 341020.5000\n",
      "Epoch 3194/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342314.5000 - mean_squared_error: 342314.5000\n",
      "Epoch 3195/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340572.4375 - mean_squared_error: 340572.4375\n",
      "Epoch 3196/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341995.8750 - mean_squared_error: 341995.8750\n",
      "Epoch 3197/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338346.5938 - mean_squared_error: 338346.5938\n",
      "Epoch 3198/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340583.5312 - mean_squared_error: 340583.5312\n",
      "Epoch 3199/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341435.2188 - mean_squared_error: 341435.2188\n",
      "Epoch 3200/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337952.4062 - mean_squared_error: 337952.4062\n",
      "Epoch 3201/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348704.7188 - mean_squared_error: 348704.7188\n",
      "Epoch 3202/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336110.9062 - mean_squared_error: 336110.9062\n",
      "Epoch 3203/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339502.1875 - mean_squared_error: 339502.1875\n",
      "Epoch 3204/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345542.3750 - mean_squared_error: 345542.3750\n",
      "Epoch 3205/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343410.4688 - mean_squared_error: 343410.4688\n",
      "Epoch 3206/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343674.8438 - mean_squared_error: 343674.8438\n",
      "Epoch 3207/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345174.6250 - mean_squared_error: 345174.6250\n",
      "Epoch 3208/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341821.0938 - mean_squared_error: 341821.0938\n",
      "Epoch 3209/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345228.4688 - mean_squared_error: 345228.4375\n",
      "Epoch 3210/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338767.9375 - mean_squared_error: 338767.9375\n",
      "Epoch 3211/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339740.4062 - mean_squared_error: 339740.4062\n",
      "Epoch 3212/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343905.4062 - mean_squared_error: 343905.3750\n",
      "Epoch 3213/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 334394.7188 - mean_squared_error: 334394.7188\n",
      "Epoch 3214/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346826.2812 - mean_squared_error: 346826.2812\n",
      "Epoch 3215/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349376.9688 - mean_squared_error: 349376.9688\n",
      "Epoch 3216/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345950.6250 - mean_squared_error: 345950.6250\n",
      "Epoch 3217/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342794.3438 - mean_squared_error: 342794.3438\n",
      "Epoch 3218/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 334454.9062 - mean_squared_error: 334454.9062\n",
      "Epoch 3219/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351126.8438 - mean_squared_error: 351126.8438\n",
      "Epoch 3220/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342057.6562 - mean_squared_error: 342057.6562\n",
      "Epoch 3221/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349396.0000 - mean_squared_error: 349396.0000\n",
      "Epoch 3222/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346398.2812 - mean_squared_error: 346398.2812\n",
      "Epoch 3223/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341210.2812 - mean_squared_error: 341210.2812\n",
      "Epoch 3224/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346395.7188 - mean_squared_error: 346395.7188\n",
      "Epoch 3225/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338955.7500 - mean_squared_error: 338955.7500\n",
      "Epoch 3226/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341945.6875 - mean_squared_error: 341945.6875\n",
      "Epoch 3227/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339848.0000 - mean_squared_error: 339848.0000\n",
      "Epoch 3228/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 349165.0625 - mean_squared_error: 349165.0625\n",
      "Epoch 3229/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336342.4375 - mean_squared_error: 336342.4375\n",
      "Epoch 3230/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344469.3125 - mean_squared_error: 344469.3125\n",
      "Epoch 3231/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340676.4688 - mean_squared_error: 340676.4688\n",
      "Epoch 3232/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343601.8125 - mean_squared_error: 343601.8125\n",
      "Epoch 3233/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343029.4688 - mean_squared_error: 343029.4688\n",
      "Epoch 3234/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350201.3750 - mean_squared_error: 350201.3750\n",
      "Epoch 3235/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344304.5625 - mean_squared_error: 344304.5625\n",
      "Epoch 3236/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335509.6562 - mean_squared_error: 335509.6562\n",
      "Epoch 3237/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351346.9062 - mean_squared_error: 351346.9062\n",
      "Epoch 3238/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343250.8438 - mean_squared_error: 343250.8438\n",
      "Epoch 3239/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347931.3438 - mean_squared_error: 347931.3438\n",
      "Epoch 3240/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343543.4062 - mean_squared_error: 343543.4062\n",
      "Epoch 3241/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344033.0000 - mean_squared_error: 344033.0000\n",
      "Epoch 3242/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341861.6562 - mean_squared_error: 341861.6562\n",
      "Epoch 3243/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343651.9688 - mean_squared_error: 343651.9688\n",
      "Epoch 3244/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341059.2500 - mean_squared_error: 341059.2500\n",
      "Epoch 3245/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 331573.7188 - mean_squared_error: 331573.7500\n",
      "Epoch 3246/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351039.1875 - mean_squared_error: 351039.1875\n",
      "Epoch 3247/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348051.5938 - mean_squared_error: 348051.5938\n",
      "Epoch 3248/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351836.0938 - mean_squared_error: 351836.0938\n",
      "Epoch 3249/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340377.0938 - mean_squared_error: 340377.0938\n",
      "Epoch 3250/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341996.1875 - mean_squared_error: 341996.1875\n",
      "Epoch 3251/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347255.2812 - mean_squared_error: 347255.2812\n",
      "Epoch 3252/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341416.4062 - mean_squared_error: 341416.4062\n",
      "Epoch 3253/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341585.5625 - mean_squared_error: 341585.5625\n",
      "Epoch 3254/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343473.5000 - mean_squared_error: 343473.5000\n",
      "Epoch 3255/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340189.9688 - mean_squared_error: 340189.9688\n",
      "Epoch 3256/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348351.8750 - mean_squared_error: 348351.8750\n",
      "Epoch 3257/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337664.6250 - mean_squared_error: 337664.6250\n",
      "Epoch 3258/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338496.4375 - mean_squared_error: 338496.4375\n",
      "Epoch 3259/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351358.5312 - mean_squared_error: 351358.5312\n",
      "Epoch 3260/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344648.7500 - mean_squared_error: 344648.7500\n",
      "Epoch 3261/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339392.9688 - mean_squared_error: 339392.9688\n",
      "Epoch 3262/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339871.5312 - mean_squared_error: 339871.5312\n",
      "Epoch 3263/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344439.8750 - mean_squared_error: 344439.8750\n",
      "Epoch 3264/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 332575.0312 - mean_squared_error: 332575.0312\n",
      "Epoch 3265/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339913.5000 - mean_squared_error: 339913.5000\n",
      "Epoch 3266/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340928.5938 - mean_squared_error: 340928.5938\n",
      "Epoch 3267/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338838.7188 - mean_squared_error: 338838.7188\n",
      "Epoch 3268/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339362.5000 - mean_squared_error: 339362.5000\n",
      "Epoch 3269/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341679.7500 - mean_squared_error: 341679.7812\n",
      "Epoch 3270/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346437.5625 - mean_squared_error: 346437.5625\n",
      "Epoch 3271/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340681.0312 - mean_squared_error: 340681.0312\n",
      "Epoch 3272/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 333206.9375 - mean_squared_error: 333206.9375\n",
      "Epoch 3273/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342589.5938 - mean_squared_error: 342589.5938\n",
      "Epoch 3274/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340720.7812 - mean_squared_error: 340720.7812\n",
      "Epoch 3275/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343026.0938 - mean_squared_error: 343026.0938\n",
      "Epoch 3276/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339242.6250 - mean_squared_error: 339242.6250\n",
      "Epoch 3277/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341461.4062 - mean_squared_error: 341461.4062\n",
      "Epoch 3278/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339113.6250 - mean_squared_error: 339113.6250\n",
      "Epoch 3279/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342974.4688 - mean_squared_error: 342974.4688\n",
      "Epoch 3280/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347615.8438 - mean_squared_error: 347615.8438\n",
      "Epoch 3281/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346485.0938 - mean_squared_error: 346485.0938\n",
      "Epoch 3282/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344621.0312 - mean_squared_error: 344621.0312\n",
      "Epoch 3283/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343032.0625 - mean_squared_error: 343032.0625\n",
      "Epoch 3284/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337746.4375 - mean_squared_error: 337746.4375\n",
      "Epoch 3285/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342878.8438 - mean_squared_error: 342878.8438\n",
      "Epoch 3286/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345375.5000 - mean_squared_error: 345375.5000\n",
      "Epoch 3287/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338493.7500 - mean_squared_error: 338493.7500\n",
      "Epoch 3288/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347030.3125 - mean_squared_error: 347030.3125\n",
      "Epoch 3289/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341532.2812 - mean_squared_error: 341532.2812\n",
      "Epoch 3290/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349430.4062 - mean_squared_error: 349430.4062\n",
      "Epoch 3291/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345724.2188 - mean_squared_error: 345724.2188\n",
      "Epoch 3292/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 335886.6875 - mean_squared_error: 335886.6875\n",
      "Epoch 3293/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336191.7500 - mean_squared_error: 336191.7500\n",
      "Epoch 3294/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 333832.1250 - mean_squared_error: 333832.1250\n",
      "Epoch 3295/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347654.7812 - mean_squared_error: 347654.7812\n",
      "Epoch 3296/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 339993.5938 - mean_squared_error: 339993.5938\n",
      "Epoch 3297/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338202.8750 - mean_squared_error: 338202.8750\n",
      "Epoch 3298/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337190.8750 - mean_squared_error: 337190.8750\n",
      "Epoch 3299/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348481.3750 - mean_squared_error: 348481.3750\n",
      "Epoch 3300/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341894.9375 - mean_squared_error: 341894.9375\n",
      "Epoch 3301/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341399.9375 - mean_squared_error: 341399.9375\n",
      "Epoch 3302/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340820.6250 - mean_squared_error: 340820.6250\n",
      "Epoch 3303/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341257.0625 - mean_squared_error: 341257.0625\n",
      "Epoch 3304/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350052.8750 - mean_squared_error: 350052.9062\n",
      "Epoch 3305/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339524.5312 - mean_squared_error: 339524.5312\n",
      "Epoch 3306/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340250.6250 - mean_squared_error: 340250.6250\n",
      "Epoch 3307/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347963.5312 - mean_squared_error: 347963.5312\n",
      "Epoch 3308/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338530.3750 - mean_squared_error: 338530.3750\n",
      "Epoch 3309/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345254.7500 - mean_squared_error: 345254.7500\n",
      "Epoch 3310/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346620.8125 - mean_squared_error: 346620.7812\n",
      "Epoch 3311/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344766.3750 - mean_squared_error: 344766.3750\n",
      "Epoch 3312/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346279.2188 - mean_squared_error: 346279.2188\n",
      "Epoch 3313/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342436.3438 - mean_squared_error: 342436.3438\n",
      "Epoch 3314/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346398.7812 - mean_squared_error: 346398.7812\n",
      "Epoch 3315/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341331.6250 - mean_squared_error: 341331.6562\n",
      "Epoch 3316/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 338485.2812 - mean_squared_error: 338485.2812\n",
      "Epoch 3317/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345970.6562 - mean_squared_error: 345970.6562\n",
      "Epoch 3318/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343886.0000 - mean_squared_error: 343886.0000\n",
      "Epoch 3319/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337536.0000 - mean_squared_error: 337536.0000\n",
      "Epoch 3320/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338329.3438 - mean_squared_error: 338329.3438\n",
      "Epoch 3321/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350025.6875 - mean_squared_error: 350025.6875\n",
      "Epoch 3322/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343364.0625 - mean_squared_error: 343364.0625\n",
      "Epoch 3323/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348271.3750 - mean_squared_error: 348271.3750\n",
      "Epoch 3324/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346065.3438 - mean_squared_error: 346065.3438\n",
      "Epoch 3325/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341424.6875 - mean_squared_error: 341424.6875\n",
      "Epoch 3326/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340677.0938 - mean_squared_error: 340677.0938\n",
      "Epoch 3327/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338871.3125 - mean_squared_error: 338871.3125\n",
      "Epoch 3328/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340251.6875 - mean_squared_error: 340251.6875\n",
      "Epoch 3329/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 333286.5000 - mean_squared_error: 333286.5000\n",
      "Epoch 3330/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351846.6875 - mean_squared_error: 351846.6875\n",
      "Epoch 3331/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340587.2500 - mean_squared_error: 340587.2500\n",
      "Epoch 3332/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342923.0312 - mean_squared_error: 342923.0312\n",
      "Epoch 3333/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345112.8438 - mean_squared_error: 345112.8438\n",
      "Epoch 3334/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349342.4062 - mean_squared_error: 349342.4062\n",
      "Epoch 3335/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338232.8750 - mean_squared_error: 338232.8750\n",
      "Epoch 3336/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349558.8438 - mean_squared_error: 349558.8438\n",
      "Epoch 3337/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342814.5312 - mean_squared_error: 342814.5312\n",
      "Epoch 3338/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346909.9062 - mean_squared_error: 346909.9062\n",
      "Epoch 3339/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337846.4375 - mean_squared_error: 337846.4375\n",
      "Epoch 3340/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339848.4688 - mean_squared_error: 339848.4688\n",
      "Epoch 3341/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345847.8750 - mean_squared_error: 345847.8750\n",
      "Epoch 3342/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343560.3750 - mean_squared_error: 343560.3750\n",
      "Epoch 3343/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338151.1562 - mean_squared_error: 338151.1562\n",
      "Epoch 3344/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342498.9688 - mean_squared_error: 342498.9688\n",
      "Epoch 3345/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348290.8438 - mean_squared_error: 348290.8438\n",
      "Epoch 3346/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341925.5000 - mean_squared_error: 341925.5000\n",
      "Epoch 3347/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341789.9688 - mean_squared_error: 341789.9688\n",
      "Epoch 3348/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342926.6875 - mean_squared_error: 342926.6875\n",
      "Epoch 3349/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340326.9688 - mean_squared_error: 340326.9688\n",
      "Epoch 3350/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338386.8125 - mean_squared_error: 338386.8125\n",
      "Epoch 3351/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339069.9375 - mean_squared_error: 339069.9375\n",
      "Epoch 3352/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 335846.5625 - mean_squared_error: 335846.5625\n",
      "Epoch 3353/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 350938.8438 - mean_squared_error: 350938.8438\n",
      "Epoch 3354/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 354678.0625 - mean_squared_error: 354678.0625\n",
      "Epoch 3355/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 345365.0938 - mean_squared_error: 345365.0938\n",
      "Epoch 3356/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 339127.0625 - mean_squared_error: 339127.0625\n",
      "Epoch 3357/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 345632.7500 - mean_squared_error: 345632.7500\n",
      "Epoch 3358/5000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 350443.8750 - mean_squared_error: 350443.8750\n",
      "Epoch 3359/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342559.5625 - mean_squared_error: 342559.5625\n",
      "Epoch 3360/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345801.7812 - mean_squared_error: 345801.7812\n",
      "Epoch 3361/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347522.3750 - mean_squared_error: 347522.3750\n",
      "Epoch 3362/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343408.0938 - mean_squared_error: 343408.0938\n",
      "Epoch 3363/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350070.2188 - mean_squared_error: 350070.2188\n",
      "Epoch 3364/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345885.3125 - mean_squared_error: 345885.3125\n",
      "Epoch 3365/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347291.5938 - mean_squared_error: 347291.5938\n",
      "Epoch 3366/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353018.3750 - mean_squared_error: 353018.3750\n",
      "Epoch 3367/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 353565.4375 - mean_squared_error: 353565.4375\n",
      "Epoch 3368/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340493.6875 - mean_squared_error: 340493.6875\n",
      "Epoch 3369/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345891.8438 - mean_squared_error: 345891.8438\n",
      "Epoch 3370/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347214.3125 - mean_squared_error: 347214.3125\n",
      "Epoch 3371/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336591.0000 - mean_squared_error: 336591.0000\n",
      "Epoch 3372/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342376.6250 - mean_squared_error: 342376.6250\n",
      "Epoch 3373/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342084.4062 - mean_squared_error: 342084.4062\n",
      "Epoch 3374/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342262.4062 - mean_squared_error: 342262.4062\n",
      "Epoch 3375/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 334212.3438 - mean_squared_error: 334212.3438\n",
      "Epoch 3376/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342961.8750 - mean_squared_error: 342961.8750\n",
      "Epoch 3377/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342907.0000 - mean_squared_error: 342907.0000\n",
      "Epoch 3378/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339873.9688 - mean_squared_error: 339873.9688\n",
      "Epoch 3379/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340954.5000 - mean_squared_error: 340954.5000\n",
      "Epoch 3380/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348523.3750 - mean_squared_error: 348523.3750\n",
      "Epoch 3381/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347807.9688 - mean_squared_error: 347807.9688\n",
      "Epoch 3382/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341063.6875 - mean_squared_error: 341063.6875\n",
      "Epoch 3383/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350436.5312 - mean_squared_error: 350436.5625\n",
      "Epoch 3384/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344079.5938 - mean_squared_error: 344079.5938\n",
      "Epoch 3385/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 352369.0000 - mean_squared_error: 352369.0000\n",
      "Epoch 3386/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339612.2812 - mean_squared_error: 339612.2812\n",
      "Epoch 3387/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351600.0938 - mean_squared_error: 351600.0938\n",
      "Epoch 3388/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345853.0625 - mean_squared_error: 345853.0625\n",
      "Epoch 3389/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 332298.7812 - mean_squared_error: 332298.7812\n",
      "Epoch 3390/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344548.0938 - mean_squared_error: 344548.0938\n",
      "Epoch 3391/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342822.1562 - mean_squared_error: 342822.1562\n",
      "Epoch 3392/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339136.2812 - mean_squared_error: 339136.2812\n",
      "Epoch 3393/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351048.4062 - mean_squared_error: 351048.4062\n",
      "Epoch 3394/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347780.6250 - mean_squared_error: 347780.6250\n",
      "Epoch 3395/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 345181.5938 - mean_squared_error: 345181.5938\n",
      "Epoch 3396/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339046.0625 - mean_squared_error: 339046.0625\n",
      "Epoch 3397/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340485.1250 - mean_squared_error: 340485.1250\n",
      "Epoch 3398/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338721.6562 - mean_squared_error: 338721.6562\n",
      "Epoch 3399/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345333.5938 - mean_squared_error: 345333.5938\n",
      "Epoch 3400/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347026.7188 - mean_squared_error: 347026.7188\n",
      "Epoch 3401/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338328.2500 - mean_squared_error: 338328.2500\n",
      "Epoch 3402/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350730.4688 - mean_squared_error: 350730.4688\n",
      "Epoch 3403/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346885.3750 - mean_squared_error: 346885.3750\n",
      "Epoch 3404/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349058.0625 - mean_squared_error: 349058.0625\n",
      "Epoch 3405/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342678.7188 - mean_squared_error: 342678.7188\n",
      "Epoch 3406/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348292.6875 - mean_squared_error: 348292.6875\n",
      "Epoch 3407/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342529.9062 - mean_squared_error: 342529.9062\n",
      "Epoch 3408/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339565.8750 - mean_squared_error: 339565.8750\n",
      "Epoch 3409/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336625.0000 - mean_squared_error: 336625.0000\n",
      "Epoch 3410/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347141.9375 - mean_squared_error: 347141.9375\n",
      "Epoch 3411/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 346819.2500 - mean_squared_error: 346819.2500\n",
      "Epoch 3412/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 334173.9375 - mean_squared_error: 334173.9375\n",
      "Epoch 3413/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340555.7812 - mean_squared_error: 340555.7812\n",
      "Epoch 3414/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348693.6875 - mean_squared_error: 348693.6875\n",
      "Epoch 3415/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337515.5625 - mean_squared_error: 337515.5625\n",
      "Epoch 3416/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335633.8750 - mean_squared_error: 335633.8750\n",
      "Epoch 3417/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 347249.8438 - mean_squared_error: 347249.8438\n",
      "Epoch 3418/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338495.5000 - mean_squared_error: 338495.5000\n",
      "Epoch 3419/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 335569.7188 - mean_squared_error: 335569.7188\n",
      "Epoch 3420/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336729.1562 - mean_squared_error: 336729.1562\n",
      "Epoch 3421/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346482.9062 - mean_squared_error: 346482.9062\n",
      "Epoch 3422/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339850.6875 - mean_squared_error: 339850.6875\n",
      "Epoch 3423/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343207.1250 - mean_squared_error: 343207.1250\n",
      "Epoch 3424/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 331597.3438 - mean_squared_error: 331597.3438\n",
      "Epoch 3425/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 350599.6250 - mean_squared_error: 350599.6250\n",
      "Epoch 3426/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342344.5625 - mean_squared_error: 342344.5625\n",
      "Epoch 3427/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342779.0938 - mean_squared_error: 342779.0938\n",
      "Epoch 3428/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 347634.3438 - mean_squared_error: 347634.3438\n",
      "Epoch 3429/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336424.8125 - mean_squared_error: 336424.8125\n",
      "Epoch 3430/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340110.2500 - mean_squared_error: 340110.2500\n",
      "Epoch 3431/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338951.0938 - mean_squared_error: 338951.0938\n",
      "Epoch 3432/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336288.9062 - mean_squared_error: 336288.9062\n",
      "Epoch 3433/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 336694.1562 - mean_squared_error: 336694.1562\n",
      "Epoch 3434/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343411.5938 - mean_squared_error: 343411.5938\n",
      "Epoch 3435/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339537.5938 - mean_squared_error: 339537.5938\n",
      "Epoch 3436/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343789.5938 - mean_squared_error: 343789.5938\n",
      "Epoch 3437/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 350719.9062 - mean_squared_error: 350719.9062\n",
      "Epoch 3438/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340520.6250 - mean_squared_error: 340520.6250\n",
      "Epoch 3439/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339466.6562 - mean_squared_error: 339466.7188\n",
      "Epoch 3440/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 346088.7500 - mean_squared_error: 346088.7500\n",
      "Epoch 3441/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 332306.6875 - mean_squared_error: 332306.6875\n",
      "Epoch 3442/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344244.2812 - mean_squared_error: 344244.2812\n",
      "Epoch 3443/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338043.4375 - mean_squared_error: 338043.4688\n",
      "Epoch 3444/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341931.0938 - mean_squared_error: 341931.0938\n",
      "Epoch 3445/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342624.4375 - mean_squared_error: 342624.4375\n",
      "Epoch 3446/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342654.5312 - mean_squared_error: 342654.5312\n",
      "Epoch 3447/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343125.3438 - mean_squared_error: 343125.2812\n",
      "Epoch 3448/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344685.7812 - mean_squared_error: 344685.7812\n",
      "Epoch 3449/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343977.9375 - mean_squared_error: 343977.9375\n",
      "Epoch 3450/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 346165.3125 - mean_squared_error: 346165.3125\n",
      "Epoch 3451/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337981.1562 - mean_squared_error: 337981.1875\n",
      "Epoch 3452/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 334802.0312 - mean_squared_error: 334802.0312\n",
      "Epoch 3453/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 349010.6875 - mean_squared_error: 349010.6875\n",
      "Epoch 3454/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338106.9375 - mean_squared_error: 338106.9062\n",
      "Epoch 3455/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347230.7812 - mean_squared_error: 347230.7812\n",
      "Epoch 3456/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 344044.2500 - mean_squared_error: 344044.2500\n",
      "Epoch 3457/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336314.3750 - mean_squared_error: 336314.3750\n",
      "Epoch 3458/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348914.0938 - mean_squared_error: 348914.0938\n",
      "Epoch 3459/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348767.7500 - mean_squared_error: 348767.7500\n",
      "Epoch 3460/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344409.4688 - mean_squared_error: 344409.4688\n",
      "Epoch 3461/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337833.3750 - mean_squared_error: 337833.3750\n",
      "Epoch 3462/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344792.1250 - mean_squared_error: 344792.1250\n",
      "Epoch 3463/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339356.8125 - mean_squared_error: 339356.8125\n",
      "Epoch 3464/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344262.9688 - mean_squared_error: 344262.9688\n",
      "Epoch 3465/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339537.1250 - mean_squared_error: 339537.1250\n",
      "Epoch 3466/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348094.0625 - mean_squared_error: 348094.0625\n",
      "Epoch 3467/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345310.5312 - mean_squared_error: 345310.5312\n",
      "Epoch 3468/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 339566.4062 - mean_squared_error: 339566.4062\n",
      "Epoch 3469/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337426.9688 - mean_squared_error: 337426.9688\n",
      "Epoch 3470/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343864.7812 - mean_squared_error: 343864.7812\n",
      "Epoch 3471/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 335750.0625 - mean_squared_error: 335750.0625\n",
      "Epoch 3472/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337054.8750 - mean_squared_error: 337054.8750\n",
      "Epoch 3473/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347606.8750 - mean_squared_error: 347606.8750\n",
      "Epoch 3474/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348492.5312 - mean_squared_error: 348492.5312\n",
      "Epoch 3475/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 342651.3438 - mean_squared_error: 342651.3438\n",
      "Epoch 3476/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340352.2188 - mean_squared_error: 340352.1875\n",
      "Epoch 3477/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341807.5312 - mean_squared_error: 341807.5312\n",
      "Epoch 3478/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 325923.2812 - mean_squared_error: 325923.2812\n",
      "Epoch 3479/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344617.8125 - mean_squared_error: 344617.8125\n",
      "Epoch 3480/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341158.6562 - mean_squared_error: 341158.6562\n",
      "Epoch 3481/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338999.0000 - mean_squared_error: 338999.0000\n",
      "Epoch 3482/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341867.3750 - mean_squared_error: 341867.3750\n",
      "Epoch 3483/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342403.3125 - mean_squared_error: 342403.3125\n",
      "Epoch 3484/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343096.4375 - mean_squared_error: 343096.4375\n",
      "Epoch 3485/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 340662.8125 - mean_squared_error: 340662.8125\n",
      "Epoch 3486/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348143.3438 - mean_squared_error: 348143.3438\n",
      "Epoch 3487/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344121.0000 - mean_squared_error: 344121.0000\n",
      "Epoch 3488/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 343625.6250 - mean_squared_error: 343625.6250\n",
      "Epoch 3489/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339116.4375 - mean_squared_error: 339116.4375\n",
      "Epoch 3490/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344850.0312 - mean_squared_error: 344850.0312\n",
      "Epoch 3491/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343114.5938 - mean_squared_error: 343114.5938\n",
      "Epoch 3492/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340595.2500 - mean_squared_error: 340595.2500\n",
      "Epoch 3493/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347981.9688 - mean_squared_error: 347981.9688\n",
      "Epoch 3494/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341613.0625 - mean_squared_error: 341613.0625\n",
      "Epoch 3495/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345572.2500 - mean_squared_error: 345572.2500\n",
      "Epoch 3496/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337791.5625 - mean_squared_error: 337791.5625\n",
      "Epoch 3497/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345671.3750 - mean_squared_error: 345671.3750\n",
      "Epoch 3498/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342006.5625 - mean_squared_error: 342006.5625\n",
      "Epoch 3499/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338569.6562 - mean_squared_error: 338569.6562\n",
      "Epoch 3500/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336608.7500 - mean_squared_error: 336608.7500\n",
      "Epoch 3501/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337592.5000 - mean_squared_error: 337592.5000\n",
      "Epoch 3502/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336606.1562 - mean_squared_error: 336606.1562\n",
      "Epoch 3503/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 347030.8750 - mean_squared_error: 347030.8750\n",
      "Epoch 3504/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 347701.5312 - mean_squared_error: 347701.5312\n",
      "Epoch 3505/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341472.4375 - mean_squared_error: 341472.4375\n",
      "Epoch 3506/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 348623.6875 - mean_squared_error: 348623.6875\n",
      "Epoch 3507/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 341399.9375 - mean_squared_error: 341399.9375\n",
      "Epoch 3508/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 339805.8438 - mean_squared_error: 339805.8438\n",
      "Epoch 3509/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338211.3750 - mean_squared_error: 338211.3750\n",
      "Epoch 3510/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 339360.8125 - mean_squared_error: 339360.8125\n",
      "Epoch 3511/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344334.0625 - mean_squared_error: 344334.0625\n",
      "Epoch 3512/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343285.3750 - mean_squared_error: 343285.3750\n",
      "Epoch 3513/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 351686.5312 - mean_squared_error: 351686.5312\n",
      "Epoch 3514/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 345311.0000 - mean_squared_error: 345311.0000\n",
      "Epoch 3515/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 353189.9062 - mean_squared_error: 353189.9062\n",
      "Epoch 3516/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 353091.9062 - mean_squared_error: 353091.9062\n",
      "Epoch 3517/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 336913.8438 - mean_squared_error: 336913.8438\n",
      "Epoch 3518/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343076.2188 - mean_squared_error: 343076.2188\n",
      "Epoch 3519/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 343823.6250 - mean_squared_error: 343823.5938\n",
      "Epoch 3520/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 335245.7500 - mean_squared_error: 335245.7500\n",
      "Epoch 3521/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 336776.0000 - mean_squared_error: 336776.0000\n",
      "Epoch 3522/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345574.4062 - mean_squared_error: 345574.4062\n",
      "Epoch 3523/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 338940.2188 - mean_squared_error: 338940.2188\n",
      "Epoch 3524/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 348996.0938 - mean_squared_error: 348996.0938\n",
      "Epoch 3525/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340631.6562 - mean_squared_error: 340631.6562\n",
      "Epoch 3526/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 337849.8750 - mean_squared_error: 337849.8750\n",
      "Epoch 3527/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 335932.6562 - mean_squared_error: 335932.6562\n",
      "Epoch 3528/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344752.0625 - mean_squared_error: 344752.0625\n",
      "Epoch 3529/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 341592.8750 - mean_squared_error: 341592.8750\n",
      "Epoch 3530/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 340460.6250 - mean_squared_error: 340460.6250\n",
      "Epoch 3531/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 332065.3750 - mean_squared_error: 332065.3750\n",
      "Epoch 3532/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 354011.2188 - mean_squared_error: 354011.2188\n",
      "Epoch 3533/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 335558.1875 - mean_squared_error: 335558.1875\n",
      "Epoch 3534/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 333991.5625 - mean_squared_error: 333991.5625\n",
      "Epoch 3535/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344602.9062 - mean_squared_error: 344602.9062\n",
      "Epoch 3536/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 340850.6250 - mean_squared_error: 340850.6250\n",
      "Epoch 3537/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342627.0938 - mean_squared_error: 342627.0938\n",
      "Epoch 3538/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 344388.4375 - mean_squared_error: 344388.4375\n",
      "Epoch 3539/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342510.6562 - mean_squared_error: 342510.6562\n",
      "Epoch 3540/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 331255.3750 - mean_squared_error: 331255.3750\n",
      "Epoch 3541/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 345652.4062 - mean_squared_error: 345652.4062\n",
      "Epoch 3542/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 334589.3125 - mean_squared_error: 334589.3125\n",
      "Epoch 3543/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 334999.1875 - mean_squared_error: 334999.1875\n",
      "Epoch 3544/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350317.8750 - mean_squared_error: 350317.8750\n",
      "Epoch 3545/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 331961.8438 - mean_squared_error: 331961.8438\n",
      "Epoch 3546/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 341972.8125 - mean_squared_error: 341972.8125\n",
      "Epoch 3547/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 334290.7500 - mean_squared_error: 334290.7500\n",
      "Epoch 3548/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 332213.8750 - mean_squared_error: 332213.8750\n",
      "Epoch 3549/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 351840.3125 - mean_squared_error: 351840.3125\n",
      "Epoch 3550/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 342639.7812 - mean_squared_error: 342639.7812\n",
      "Epoch 3551/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338549.0000 - mean_squared_error: 338549.0000\n",
      "Epoch 3552/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 349112.5938 - mean_squared_error: 349112.5938\n",
      "Epoch 3553/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 344727.7812 - mean_squared_error: 344727.7812\n",
      "Epoch 3554/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 334309.6875 - mean_squared_error: 334309.6875\n",
      "Epoch 3555/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338066.0000 - mean_squared_error: 338066.0000\n",
      "Epoch 3556/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 338651.9375 - mean_squared_error: 338651.9375\n",
      "Epoch 3557/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337256.2812 - mean_squared_error: 337256.2812\n",
      "Epoch 3558/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 337928.6562 - mean_squared_error: 337928.6562\n",
      "Epoch 3559/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 331258.6875 - mean_squared_error: 331258.6875\n",
      "Epoch 3560/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 318466.1562 - mean_squared_error: 318466.1562\n",
      "Epoch 3561/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 324513.5000 - mean_squared_error: 324513.5000\n",
      "Epoch 3562/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 318756.9062 - mean_squared_error: 318756.9062\n",
      "Epoch 3563/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 319295.8125 - mean_squared_error: 319295.8125\n",
      "Epoch 3564/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 316136.2500 - mean_squared_error: 316136.2500\n",
      "Epoch 3565/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 318718.8438 - mean_squared_error: 318718.8438\n",
      "Epoch 3566/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 317037.7812 - mean_squared_error: 317037.7812\n",
      "Epoch 3567/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 316612.8438 - mean_squared_error: 316612.8438\n",
      "Epoch 3568/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 312543.0625 - mean_squared_error: 312543.0625\n",
      "Epoch 3569/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 305615.4062 - mean_squared_error: 305615.4062\n",
      "Epoch 3570/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 315821.9062 - mean_squared_error: 315821.9062\n",
      "Epoch 3571/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 307753.4688 - mean_squared_error: 307753.4688\n",
      "Epoch 3572/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 314048.8750 - mean_squared_error: 314048.8750\n",
      "Epoch 3573/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 319533.5312 - mean_squared_error: 319533.5312\n",
      "Epoch 3574/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 315656.1250 - mean_squared_error: 315656.1250\n",
      "Epoch 3575/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 309382.1250 - mean_squared_error: 309382.1250\n",
      "Epoch 3576/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 310662.0312 - mean_squared_error: 310662.0312\n",
      "Epoch 3577/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 306016.9375 - mean_squared_error: 306016.9375\n",
      "Epoch 3578/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 312120.8438 - mean_squared_error: 312120.8438\n",
      "Epoch 3579/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295265.0312 - mean_squared_error: 295265.0312\n",
      "Epoch 3580/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297428.7812 - mean_squared_error: 297428.7812\n",
      "Epoch 3581/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297783.3438 - mean_squared_error: 297783.3438\n",
      "Epoch 3582/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292095.1562 - mean_squared_error: 292095.1562\n",
      "Epoch 3583/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297752.0000 - mean_squared_error: 297752.0000\n",
      "Epoch 3584/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297731.0000 - mean_squared_error: 297731.0312\n",
      "Epoch 3585/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300208.6562 - mean_squared_error: 300208.6562\n",
      "Epoch 3586/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303046.6250 - mean_squared_error: 303046.6250\n",
      "Epoch 3587/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301278.3438 - mean_squared_error: 301278.3438\n",
      "Epoch 3588/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297435.1562 - mean_squared_error: 297435.1562\n",
      "Epoch 3589/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 304554.3125 - mean_squared_error: 304554.3125\n",
      "Epoch 3590/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 305375.2812 - mean_squared_error: 305375.2812\n",
      "Epoch 3591/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299223.9062 - mean_squared_error: 299223.9062\n",
      "Epoch 3592/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 306132.1562 - mean_squared_error: 306132.1562\n",
      "Epoch 3593/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303155.5938 - mean_squared_error: 303155.5938\n",
      "Epoch 3594/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 306126.3750 - mean_squared_error: 306126.3750\n",
      "Epoch 3595/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300187.8438 - mean_squared_error: 300187.8438\n",
      "Epoch 3596/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299252.0625 - mean_squared_error: 299252.0625\n",
      "Epoch 3597/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297550.8125 - mean_squared_error: 297550.8125\n",
      "Epoch 3598/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299389.1562 - mean_squared_error: 299389.1562\n",
      "Epoch 3599/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293038.9688 - mean_squared_error: 293038.9688\n",
      "Epoch 3600/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 307092.0312 - mean_squared_error: 307092.0312\n",
      "Epoch 3601/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 298117.6250 - mean_squared_error: 298117.6250\n",
      "Epoch 3602/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299974.9688 - mean_squared_error: 299974.9688\n",
      "Epoch 3603/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 305999.5000 - mean_squared_error: 305999.5000\n",
      "Epoch 3604/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 307788.4375 - mean_squared_error: 307788.4375\n",
      "Epoch 3605/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 294899.9062 - mean_squared_error: 294899.9062\n",
      "Epoch 3606/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295863.8438 - mean_squared_error: 295863.8438\n",
      "Epoch 3607/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 286845.9688 - mean_squared_error: 286845.9688\n",
      "Epoch 3608/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 304003.6250 - mean_squared_error: 304003.6250\n",
      "Epoch 3609/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294551.7188 - mean_squared_error: 294551.7188\n",
      "Epoch 3610/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296947.2500 - mean_squared_error: 296947.2500\n",
      "Epoch 3611/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296610.2188 - mean_squared_error: 296610.2188\n",
      "Epoch 3612/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294079.2812 - mean_squared_error: 294079.2812\n",
      "Epoch 3613/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296957.8438 - mean_squared_error: 296957.8438\n",
      "Epoch 3614/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299464.5000 - mean_squared_error: 299464.5000\n",
      "Epoch 3615/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 304674.4688 - mean_squared_error: 304674.4688\n",
      "Epoch 3616/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299240.9375 - mean_squared_error: 299240.9375\n",
      "Epoch 3617/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292279.3750 - mean_squared_error: 292279.3750\n",
      "Epoch 3618/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299429.0312 - mean_squared_error: 299429.0312\n",
      "Epoch 3619/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 283898.8125 - mean_squared_error: 283898.8125\n",
      "Epoch 3620/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299060.8750 - mean_squared_error: 299060.8750\n",
      "Epoch 3621/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296346.9375 - mean_squared_error: 296346.9375\n",
      "Epoch 3622/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 304558.4375 - mean_squared_error: 304558.4375\n",
      "Epoch 3623/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 303379.5312 - mean_squared_error: 303379.5312\n",
      "Epoch 3624/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303759.2500 - mean_squared_error: 303759.2500\n",
      "Epoch 3625/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294246.7812 - mean_squared_error: 294246.7812\n",
      "Epoch 3626/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290443.8125 - mean_squared_error: 290443.8125\n",
      "Epoch 3627/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290179.9688 - mean_squared_error: 290179.9688\n",
      "Epoch 3628/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301517.8125 - mean_squared_error: 301517.8125\n",
      "Epoch 3629/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302559.0000 - mean_squared_error: 302559.0000\n",
      "Epoch 3630/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297748.4375 - mean_squared_error: 297748.4375\n",
      "Epoch 3631/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 283451.1875 - mean_squared_error: 283451.1875\n",
      "Epoch 3632/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290146.1250 - mean_squared_error: 290146.1250\n",
      "Epoch 3633/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 248713.5469 - mean_squared_error: 248713.5469\n",
      "Epoch 3634/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 195732.7812 - mean_squared_error: 195732.7812\n",
      "Epoch 3635/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 223834.0781 - mean_squared_error: 223834.0781\n",
      "Epoch 3636/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 228853.8281 - mean_squared_error: 228853.8281\n",
      "Epoch 3637/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 315035.2812 - mean_squared_error: 315035.2812\n",
      "Epoch 3638/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 229072.6719 - mean_squared_error: 229072.6719\n",
      "Epoch 3639/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299925.5312 - mean_squared_error: 299925.5312\n",
      "Epoch 3640/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 305901.1250 - mean_squared_error: 305901.1250\n",
      "Epoch 3641/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293010.3750 - mean_squared_error: 293010.3750\n",
      "Epoch 3642/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297086.5000 - mean_squared_error: 297086.4688\n",
      "Epoch 3643/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301338.0312 - mean_squared_error: 301338.0312\n",
      "Epoch 3644/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 296279.2812 - mean_squared_error: 296279.2812\n",
      "Epoch 3645/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303715.3125 - mean_squared_error: 303715.3125\n",
      "Epoch 3646/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 293604.2500 - mean_squared_error: 293604.2500\n",
      "Epoch 3647/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303531.6250 - mean_squared_error: 303531.6562\n",
      "Epoch 3648/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293835.3438 - mean_squared_error: 293835.3438\n",
      "Epoch 3649/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294861.9688 - mean_squared_error: 294861.9688\n",
      "Epoch 3650/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303462.1875 - mean_squared_error: 303462.1875\n",
      "Epoch 3651/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291363.7812 - mean_squared_error: 291363.7812\n",
      "Epoch 3652/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295000.9688 - mean_squared_error: 295000.9688\n",
      "Epoch 3653/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 289865.3125 - mean_squared_error: 289865.3125\n",
      "Epoch 3654/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296073.4062 - mean_squared_error: 296073.4062\n",
      "Epoch 3655/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292213.0625 - mean_squared_error: 292213.0625\n",
      "Epoch 3656/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297789.0000 - mean_squared_error: 297789.0000\n",
      "Epoch 3657/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297321.8125 - mean_squared_error: 297321.8125\n",
      "Epoch 3658/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300947.0312 - mean_squared_error: 300947.0312\n",
      "Epoch 3659/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290371.6562 - mean_squared_error: 290371.6562\n",
      "Epoch 3660/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295396.8750 - mean_squared_error: 295396.8750\n",
      "Epoch 3661/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299029.5312 - mean_squared_error: 299029.5312\n",
      "Epoch 3662/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301273.2188 - mean_squared_error: 301273.2188\n",
      "Epoch 3663/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294851.2812 - mean_squared_error: 294851.2812\n",
      "Epoch 3664/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298215.5000 - mean_squared_error: 298215.5000\n",
      "Epoch 3665/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299000.9062 - mean_squared_error: 299000.9062\n",
      "Epoch 3666/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299188.4688 - mean_squared_error: 299188.4688\n",
      "Epoch 3667/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297640.3125 - mean_squared_error: 297640.3125\n",
      "Epoch 3668/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290127.3750 - mean_squared_error: 290127.3750\n",
      "Epoch 3669/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294682.4688 - mean_squared_error: 294682.4688\n",
      "Epoch 3670/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295820.0938 - mean_squared_error: 295820.0938\n",
      "Epoch 3671/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299425.8750 - mean_squared_error: 299425.8750\n",
      "Epoch 3672/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292319.8125 - mean_squared_error: 292319.8125\n",
      "Epoch 3673/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291185.3750 - mean_squared_error: 291185.3750\n",
      "Epoch 3674/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296732.0625 - mean_squared_error: 296732.0625\n",
      "Epoch 3675/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297020.9375 - mean_squared_error: 297020.9375\n",
      "Epoch 3676/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293302.2812 - mean_squared_error: 293302.2812\n",
      "Epoch 3677/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298237.1562 - mean_squared_error: 298237.1562\n",
      "Epoch 3678/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292072.1250 - mean_squared_error: 292072.1250\n",
      "Epoch 3679/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298360.6562 - mean_squared_error: 298360.6562\n",
      "Epoch 3680/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294406.1875 - mean_squared_error: 294406.1875\n",
      "Epoch 3681/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 304014.6250 - mean_squared_error: 304014.5938\n",
      "Epoch 3682/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291752.2812 - mean_squared_error: 291752.2812\n",
      "Epoch 3683/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288668.3750 - mean_squared_error: 288668.3438\n",
      "Epoch 3684/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297094.2500 - mean_squared_error: 297094.2500\n",
      "Epoch 3685/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298791.0625 - mean_squared_error: 298791.0625\n",
      "Epoch 3686/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 281199.0312 - mean_squared_error: 281199.0312\n",
      "Epoch 3687/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296062.0625 - mean_squared_error: 296062.0625\n",
      "Epoch 3688/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292391.4688 - mean_squared_error: 292391.4688\n",
      "Epoch 3689/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299762.3750 - mean_squared_error: 299762.3750\n",
      "Epoch 3690/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299924.8750 - mean_squared_error: 299924.8750\n",
      "Epoch 3691/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293186.0938 - mean_squared_error: 293186.0625\n",
      "Epoch 3692/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 298835.8125 - mean_squared_error: 298835.8125\n",
      "Epoch 3693/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291227.4062 - mean_squared_error: 291227.4062\n",
      "Epoch 3694/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293806.5625 - mean_squared_error: 293806.5625\n",
      "Epoch 3695/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294567.6875 - mean_squared_error: 294567.6875\n",
      "Epoch 3696/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299503.1250 - mean_squared_error: 299503.1250\n",
      "Epoch 3697/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 289500.3125 - mean_squared_error: 289500.3125\n",
      "Epoch 3698/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296117.8438 - mean_squared_error: 296117.7812\n",
      "Epoch 3699/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292724.2500 - mean_squared_error: 292724.2500\n",
      "Epoch 3700/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294462.2500 - mean_squared_error: 294462.2500\n",
      "Epoch 3701/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297509.5312 - mean_squared_error: 297509.5312\n",
      "Epoch 3702/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296007.9375 - mean_squared_error: 296007.9375\n",
      "Epoch 3703/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293221.7188 - mean_squared_error: 293221.7188\n",
      "Epoch 3704/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302769.5625 - mean_squared_error: 302769.5625\n",
      "Epoch 3705/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296307.8750 - mean_squared_error: 296307.8750\n",
      "Epoch 3706/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 287232.0312 - mean_squared_error: 287232.0312\n",
      "Epoch 3707/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296294.0938 - mean_squared_error: 296294.0938\n",
      "Epoch 3708/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303374.6875 - mean_squared_error: 303374.6875\n",
      "Epoch 3709/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301039.4688 - mean_squared_error: 301039.4688\n",
      "Epoch 3710/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292261.0312 - mean_squared_error: 292261.0312\n",
      "Epoch 3711/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291691.5625 - mean_squared_error: 291691.5625\n",
      "Epoch 3712/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298280.8438 - mean_squared_error: 298280.8438\n",
      "Epoch 3713/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296814.2188 - mean_squared_error: 296814.2188\n",
      "Epoch 3714/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297549.9062 - mean_squared_error: 297549.9062\n",
      "Epoch 3715/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291338.7812 - mean_squared_error: 291338.7812\n",
      "Epoch 3716/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 293883.0000 - mean_squared_error: 293883.0000\n",
      "Epoch 3717/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 298593.5000 - mean_squared_error: 298593.5000\n",
      "Epoch 3718/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292998.0312 - mean_squared_error: 292998.0312\n",
      "Epoch 3719/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297567.0312 - mean_squared_error: 297567.0312\n",
      "Epoch 3720/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290920.0000 - mean_squared_error: 290920.0000\n",
      "Epoch 3721/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291089.0938 - mean_squared_error: 291089.0938\n",
      "Epoch 3722/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292816.6875 - mean_squared_error: 292816.6875\n",
      "Epoch 3723/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 297044.3750 - mean_squared_error: 297044.3750\n",
      "Epoch 3724/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291338.3750 - mean_squared_error: 291338.4062\n",
      "Epoch 3725/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302818.8750 - mean_squared_error: 302818.8750\n",
      "Epoch 3726/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303823.6562 - mean_squared_error: 303823.6562\n",
      "Epoch 3727/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297131.0312 - mean_squared_error: 297131.0000\n",
      "Epoch 3728/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292713.2812 - mean_squared_error: 292713.2812\n",
      "Epoch 3729/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288088.7812 - mean_squared_error: 288088.7812\n",
      "Epoch 3730/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303351.2188 - mean_squared_error: 303351.2500\n",
      "Epoch 3731/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293354.9062 - mean_squared_error: 293354.9062\n",
      "Epoch 3732/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292692.9688 - mean_squared_error: 292692.9375\n",
      "Epoch 3733/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 284484.3125 - mean_squared_error: 284484.3125\n",
      "Epoch 3734/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294686.6875 - mean_squared_error: 294686.7188\n",
      "Epoch 3735/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298038.8438 - mean_squared_error: 298038.8438\n",
      "Epoch 3736/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296328.4688 - mean_squared_error: 296328.4688\n",
      "Epoch 3737/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296447.3438 - mean_squared_error: 296447.3438\n",
      "Epoch 3738/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303568.5625 - mean_squared_error: 303568.5625\n",
      "Epoch 3739/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290707.2812 - mean_squared_error: 290707.2812\n",
      "Epoch 3740/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 285097.0625 - mean_squared_error: 285097.0625\n",
      "Epoch 3741/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293957.3125 - mean_squared_error: 293957.3125\n",
      "Epoch 3742/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299956.7500 - mean_squared_error: 299956.7500\n",
      "Epoch 3743/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295571.3750 - mean_squared_error: 295571.3750\n",
      "Epoch 3744/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290863.0938 - mean_squared_error: 290863.0938\n",
      "Epoch 3745/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292674.3750 - mean_squared_error: 292674.3750\n",
      "Epoch 3746/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289822.0312 - mean_squared_error: 289822.0312\n",
      "Epoch 3747/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295848.5938 - mean_squared_error: 295848.5938\n",
      "Epoch 3748/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301406.2812 - mean_squared_error: 301406.2812\n",
      "Epoch 3749/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300055.9062 - mean_squared_error: 300055.9062\n",
      "Epoch 3750/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292677.7812 - mean_squared_error: 292677.7812\n",
      "Epoch 3751/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301937.6562 - mean_squared_error: 301937.6562\n",
      "Epoch 3752/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 294342.8438 - mean_squared_error: 294342.8125\n",
      "Epoch 3753/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293187.5000 - mean_squared_error: 293187.5000\n",
      "Epoch 3754/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298431.2188 - mean_squared_error: 298431.2188\n",
      "Epoch 3755/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295839.2188 - mean_squared_error: 295839.2188\n",
      "Epoch 3756/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294565.6250 - mean_squared_error: 294565.6250\n",
      "Epoch 3757/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300494.5938 - mean_squared_error: 300494.5938\n",
      "Epoch 3758/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299998.4688 - mean_squared_error: 299998.4688\n",
      "Epoch 3759/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296714.3750 - mean_squared_error: 296714.3750\n",
      "Epoch 3760/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297054.1250 - mean_squared_error: 297054.1250\n",
      "Epoch 3761/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292360.8438 - mean_squared_error: 292360.8438\n",
      "Epoch 3762/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296877.6562 - mean_squared_error: 296877.6562\n",
      "Epoch 3763/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293219.4688 - mean_squared_error: 293219.4688\n",
      "Epoch 3764/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293009.8125 - mean_squared_error: 293009.8125\n",
      "Epoch 3765/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292237.9375 - mean_squared_error: 292237.9375\n",
      "Epoch 3766/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302554.4688 - mean_squared_error: 302554.4688\n",
      "Epoch 3767/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291754.9062 - mean_squared_error: 291754.9062\n",
      "Epoch 3768/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299386.0625 - mean_squared_error: 299386.0625\n",
      "Epoch 3769/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291417.9375 - mean_squared_error: 291417.9375\n",
      "Epoch 3770/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294512.0000 - mean_squared_error: 294512.0312\n",
      "Epoch 3771/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295043.3125 - mean_squared_error: 295043.3125\n",
      "Epoch 3772/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293659.9688 - mean_squared_error: 293659.9688\n",
      "Epoch 3773/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 292803.3125 - mean_squared_error: 292803.2500\n",
      "Epoch 3774/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291184.2500 - mean_squared_error: 291184.2812\n",
      "Epoch 3775/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295334.2500 - mean_squared_error: 295334.2500\n",
      "Epoch 3776/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299630.5625 - mean_squared_error: 299630.5625\n",
      "Epoch 3777/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 285896.5938 - mean_squared_error: 285896.5938\n",
      "Epoch 3778/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296250.4375 - mean_squared_error: 296250.4375\n",
      "Epoch 3779/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291754.4688 - mean_squared_error: 291754.4688\n",
      "Epoch 3780/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293433.5938 - mean_squared_error: 293433.5938\n",
      "Epoch 3781/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288015.9062 - mean_squared_error: 288015.9062\n",
      "Epoch 3782/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 288736.5938 - mean_squared_error: 288736.5938\n",
      "Epoch 3783/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293622.3750 - mean_squared_error: 293622.3750\n",
      "Epoch 3784/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293982.1562 - mean_squared_error: 293982.1875\n",
      "Epoch 3785/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290279.1875 - mean_squared_error: 290279.1562\n",
      "Epoch 3786/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303570.0625 - mean_squared_error: 303570.0625\n",
      "Epoch 3787/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290792.4062 - mean_squared_error: 290792.4062\n",
      "Epoch 3788/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293482.7188 - mean_squared_error: 293482.6875\n",
      "Epoch 3789/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295630.5312 - mean_squared_error: 295630.5312\n",
      "Epoch 3790/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301980.1562 - mean_squared_error: 301980.1562\n",
      "Epoch 3791/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291760.0312 - mean_squared_error: 291760.0312\n",
      "Epoch 3792/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299741.0938 - mean_squared_error: 299741.0938\n",
      "Epoch 3793/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297203.6250 - mean_squared_error: 297203.6250\n",
      "Epoch 3794/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290906.1250 - mean_squared_error: 290906.1250\n",
      "Epoch 3795/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292304.1562 - mean_squared_error: 292304.1562\n",
      "Epoch 3796/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295242.8125 - mean_squared_error: 295242.8125\n",
      "Epoch 3797/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290793.4688 - mean_squared_error: 290793.4688\n",
      "Epoch 3798/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 285114.3750 - mean_squared_error: 285114.3750\n",
      "Epoch 3799/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300501.9375 - mean_squared_error: 300501.9375\n",
      "Epoch 3800/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291612.1875 - mean_squared_error: 291612.1875\n",
      "Epoch 3801/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293213.8750 - mean_squared_error: 293213.8750\n",
      "Epoch 3802/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292554.5312 - mean_squared_error: 292554.5312\n",
      "Epoch 3803/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295878.1875 - mean_squared_error: 295878.1875\n",
      "Epoch 3804/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 288429.3750 - mean_squared_error: 288429.3750\n",
      "Epoch 3805/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293377.5312 - mean_squared_error: 293377.5312\n",
      "Epoch 3806/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296648.0625 - mean_squared_error: 296648.0625\n",
      "Epoch 3807/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296360.0938 - mean_squared_error: 296360.0938\n",
      "Epoch 3808/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 287984.4062 - mean_squared_error: 287984.4062\n",
      "Epoch 3809/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292656.5312 - mean_squared_error: 292656.5312\n",
      "Epoch 3810/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292093.0000 - mean_squared_error: 292093.0000\n",
      "Epoch 3811/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298129.3750 - mean_squared_error: 298129.3438\n",
      "Epoch 3812/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288318.5000 - mean_squared_error: 288318.5000\n",
      "Epoch 3813/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295950.2812 - mean_squared_error: 295950.2812\n",
      "Epoch 3814/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292929.8438 - mean_squared_error: 292929.8438\n",
      "Epoch 3815/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294776.2500 - mean_squared_error: 294776.2500\n",
      "Epoch 3816/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297045.4062 - mean_squared_error: 297045.4062\n",
      "Epoch 3817/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298894.3438 - mean_squared_error: 298894.3438\n",
      "Epoch 3818/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295230.4375 - mean_squared_error: 295230.4375\n",
      "Epoch 3819/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 296732.3750 - mean_squared_error: 296732.3750\n",
      "Epoch 3820/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296108.9062 - mean_squared_error: 296108.9062\n",
      "Epoch 3821/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294970.4375 - mean_squared_error: 294970.4375\n",
      "Epoch 3822/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293778.5000 - mean_squared_error: 293778.5000\n",
      "Epoch 3823/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300963.4688 - mean_squared_error: 300963.4688\n",
      "Epoch 3824/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 292668.1562 - mean_squared_error: 292668.1562\n",
      "Epoch 3825/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292293.1562 - mean_squared_error: 292293.1562\n",
      "Epoch 3826/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 289841.3438 - mean_squared_error: 289841.3125\n",
      "Epoch 3827/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 285466.6562 - mean_squared_error: 285466.6562\n",
      "Epoch 3828/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295551.0938 - mean_squared_error: 295551.1562\n",
      "Epoch 3829/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 304944.9688 - mean_squared_error: 304944.9688\n",
      "Epoch 3830/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 285526.6250 - mean_squared_error: 285526.6250\n",
      "Epoch 3831/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297758.6875 - mean_squared_error: 297758.6875\n",
      "Epoch 3832/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295687.1875 - mean_squared_error: 295687.1875\n",
      "Epoch 3833/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295715.6250 - mean_squared_error: 295715.6250\n",
      "Epoch 3834/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297263.5312 - mean_squared_error: 297263.5312\n",
      "Epoch 3835/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 292632.0000 - mean_squared_error: 292632.0000\n",
      "Epoch 3836/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302586.3750 - mean_squared_error: 302586.3750\n",
      "Epoch 3837/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 290118.6562 - mean_squared_error: 290118.6562\n",
      "Epoch 3838/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290323.8750 - mean_squared_error: 290323.8750\n",
      "Epoch 3839/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294241.8438 - mean_squared_error: 294241.8438\n",
      "Epoch 3840/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299302.3125 - mean_squared_error: 299302.3125\n",
      "Epoch 3841/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296408.9688 - mean_squared_error: 296408.9688\n",
      "Epoch 3842/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302754.1562 - mean_squared_error: 302754.1562\n",
      "Epoch 3843/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294530.8125 - mean_squared_error: 294530.8125\n",
      "Epoch 3844/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298095.7812 - mean_squared_error: 298095.7812\n",
      "Epoch 3845/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295249.5938 - mean_squared_error: 295249.5938\n",
      "Epoch 3846/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292290.7500 - mean_squared_error: 292290.7500\n",
      "Epoch 3847/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296921.8750 - mean_squared_error: 296921.8750\n",
      "Epoch 3848/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303925.7812 - mean_squared_error: 303925.7812\n",
      "Epoch 3849/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292681.0000 - mean_squared_error: 292681.0000\n",
      "Epoch 3850/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295470.4375 - mean_squared_error: 295470.4375\n",
      "Epoch 3851/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299987.4688 - mean_squared_error: 299987.4688\n",
      "Epoch 3852/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293862.9062 - mean_squared_error: 293862.9062\n",
      "Epoch 3853/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295119.9062 - mean_squared_error: 295119.9062\n",
      "Epoch 3854/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291321.8750 - mean_squared_error: 291321.8750\n",
      "Epoch 3855/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294184.1562 - mean_squared_error: 294184.1562\n",
      "Epoch 3856/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291791.2188 - mean_squared_error: 291791.2188\n",
      "Epoch 3857/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 289870.8438 - mean_squared_error: 289870.8438\n",
      "Epoch 3858/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 294924.4375 - mean_squared_error: 294924.4375\n",
      "Epoch 3859/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293777.0938 - mean_squared_error: 293777.0938\n",
      "Epoch 3860/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294605.4688 - mean_squared_error: 294605.4688\n",
      "Epoch 3861/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296835.7500 - mean_squared_error: 296835.7812\n",
      "Epoch 3862/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291755.3125 - mean_squared_error: 291755.3125\n",
      "Epoch 3863/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293087.3125 - mean_squared_error: 293087.3125\n",
      "Epoch 3864/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295798.2188 - mean_squared_error: 295798.2188\n",
      "Epoch 3865/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295185.4688 - mean_squared_error: 295185.4688\n",
      "Epoch 3866/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296540.3750 - mean_squared_error: 296540.3750\n",
      "Epoch 3867/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294763.6562 - mean_squared_error: 294763.6562\n",
      "Epoch 3868/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296485.2500 - mean_squared_error: 296485.2500\n",
      "Epoch 3869/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295752.5938 - mean_squared_error: 295752.5938\n",
      "Epoch 3870/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293224.7188 - mean_squared_error: 293224.7188\n",
      "Epoch 3871/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293750.2812 - mean_squared_error: 293750.2812\n",
      "Epoch 3872/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292615.8438 - mean_squared_error: 292615.8438\n",
      "Epoch 3873/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296921.9375 - mean_squared_error: 296921.9375\n",
      "Epoch 3874/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296977.8125 - mean_squared_error: 296977.8125\n",
      "Epoch 3875/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290572.3750 - mean_squared_error: 290572.3750\n",
      "Epoch 3876/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297631.4375 - mean_squared_error: 297631.4375\n",
      "Epoch 3877/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291579.7812 - mean_squared_error: 291579.8438\n",
      "Epoch 3878/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290456.5000 - mean_squared_error: 290456.4688\n",
      "Epoch 3879/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291293.8438 - mean_squared_error: 291293.8438\n",
      "Epoch 3880/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297249.9062 - mean_squared_error: 297249.9062\n",
      "Epoch 3881/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292344.4062 - mean_squared_error: 292344.4062\n",
      "Epoch 3882/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295623.6875 - mean_squared_error: 295623.6875\n",
      "Epoch 3883/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295818.1250 - mean_squared_error: 295818.1250\n",
      "Epoch 3884/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296966.0312 - mean_squared_error: 296966.0312\n",
      "Epoch 3885/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291360.5000 - mean_squared_error: 291360.5000\n",
      "Epoch 3886/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290276.4375 - mean_squared_error: 290276.4375\n",
      "Epoch 3887/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295366.5625 - mean_squared_error: 295366.5625\n",
      "Epoch 3888/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289779.4688 - mean_squared_error: 289779.4688\n",
      "Epoch 3889/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288589.2500 - mean_squared_error: 288589.2500\n",
      "Epoch 3890/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297824.7188 - mean_squared_error: 297824.7188\n",
      "Epoch 3891/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 296422.6875 - mean_squared_error: 296422.6875\n",
      "Epoch 3892/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300377.5625 - mean_squared_error: 300377.5625\n",
      "Epoch 3893/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291708.3125 - mean_squared_error: 291708.3125\n",
      "Epoch 3894/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302354.4062 - mean_squared_error: 302354.4062\n",
      "Epoch 3895/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296098.5312 - mean_squared_error: 296098.5625\n",
      "Epoch 3896/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288776.0312 - mean_squared_error: 288776.0312\n",
      "Epoch 3897/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298123.3125 - mean_squared_error: 298123.3125\n",
      "Epoch 3898/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298343.6562 - mean_squared_error: 298343.6250\n",
      "Epoch 3899/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292560.1875 - mean_squared_error: 292560.1875\n",
      "Epoch 3900/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 296526.0625 - mean_squared_error: 296526.0625\n",
      "Epoch 3901/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298700.6250 - mean_squared_error: 298700.6250\n",
      "Epoch 3902/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296030.8438 - mean_squared_error: 296030.8438\n",
      "Epoch 3903/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292677.1250 - mean_squared_error: 292677.1250\n",
      "Epoch 3904/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289660.7188 - mean_squared_error: 289660.7188\n",
      "Epoch 3905/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289114.3750 - mean_squared_error: 289114.3750\n",
      "Epoch 3906/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294868.9062 - mean_squared_error: 294868.9062\n",
      "Epoch 3907/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297867.8125 - mean_squared_error: 297867.8125\n",
      "Epoch 3908/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292136.1250 - mean_squared_error: 292136.1250\n",
      "Epoch 3909/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291135.7500 - mean_squared_error: 291135.7500\n",
      "Epoch 3910/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289029.3750 - mean_squared_error: 289029.3750\n",
      "Epoch 3911/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287700.9688 - mean_squared_error: 287700.9688\n",
      "Epoch 3912/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298565.7188 - mean_squared_error: 298565.7188\n",
      "Epoch 3913/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302041.7188 - mean_squared_error: 302041.7188\n",
      "Epoch 3914/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291673.8125 - mean_squared_error: 291673.8125\n",
      "Epoch 3915/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287951.0938 - mean_squared_error: 287951.0938\n",
      "Epoch 3916/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 299391.2188 - mean_squared_error: 299391.2188\n",
      "Epoch 3917/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290635.4062 - mean_squared_error: 290635.4062\n",
      "Epoch 3918/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294493.4688 - mean_squared_error: 294493.4688\n",
      "Epoch 3919/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 297739.9688 - mean_squared_error: 297739.9688\n",
      "Epoch 3920/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295661.4375 - mean_squared_error: 295661.4375\n",
      "Epoch 3921/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 283067.7500 - mean_squared_error: 283067.7500\n",
      "Epoch 3922/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300280.5938 - mean_squared_error: 300280.5938\n",
      "Epoch 3923/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 288527.4688 - mean_squared_error: 288527.4688\n",
      "Epoch 3924/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 284820.4062 - mean_squared_error: 284820.3750\n",
      "Epoch 3925/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 283012.9688 - mean_squared_error: 283012.9688\n",
      "Epoch 3926/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 282158.5312 - mean_squared_error: 282158.5312\n",
      "Epoch 3927/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 281603.9375 - mean_squared_error: 281603.9688\n",
      "Epoch 3928/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 283916.6250 - mean_squared_error: 283916.6250\n",
      "Epoch 3929/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 308965.9375 - mean_squared_error: 308965.9375\n",
      "Epoch 3930/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290108.1562 - mean_squared_error: 290108.1562\n",
      "Epoch 3931/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295114.5625 - mean_squared_error: 295114.5625\n",
      "Epoch 3932/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294905.1875 - mean_squared_error: 294905.1875\n",
      "Epoch 3933/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301207.9688 - mean_squared_error: 301207.9688\n",
      "Epoch 3934/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300771.8438 - mean_squared_error: 300771.9062\n",
      "Epoch 3935/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 306432.9062 - mean_squared_error: 306432.9062\n",
      "Epoch 3936/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295019.0625 - mean_squared_error: 295019.0625\n",
      "Epoch 3937/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300204.3125 - mean_squared_error: 300204.3125\n",
      "Epoch 3938/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299773.7188 - mean_squared_error: 299773.6875\n",
      "Epoch 3939/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296068.4375 - mean_squared_error: 296068.4375\n",
      "Epoch 3940/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293970.8125 - mean_squared_error: 293970.7812\n",
      "Epoch 3941/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290894.8438 - mean_squared_error: 290894.8438\n",
      "Epoch 3942/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 295454.3750 - mean_squared_error: 295454.3750\n",
      "Epoch 3943/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298012.7812 - mean_squared_error: 298012.7812\n",
      "Epoch 3944/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295210.8125 - mean_squared_error: 295210.8125\n",
      "Epoch 3945/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293666.0000 - mean_squared_error: 293666.0000\n",
      "Epoch 3946/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298453.6250 - mean_squared_error: 298453.6250\n",
      "Epoch 3947/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288991.5625 - mean_squared_error: 288991.5625\n",
      "Epoch 3948/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302730.5000 - mean_squared_error: 302730.5000\n",
      "Epoch 3949/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297470.6562 - mean_squared_error: 297470.6562\n",
      "Epoch 3950/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292811.2500 - mean_squared_error: 292811.2500\n",
      "Epoch 3951/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287224.1250 - mean_squared_error: 287224.1250\n",
      "Epoch 3952/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298962.7188 - mean_squared_error: 298962.7188\n",
      "Epoch 3953/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293338.3125 - mean_squared_error: 293338.3125\n",
      "Epoch 3954/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293872.1875 - mean_squared_error: 293872.1875\n",
      "Epoch 3955/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298859.4062 - mean_squared_error: 298859.4062\n",
      "Epoch 3956/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 296843.6562 - mean_squared_error: 296843.6562\n",
      "Epoch 3957/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297285.0938 - mean_squared_error: 297285.0938\n",
      "Epoch 3958/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 304095.2500 - mean_squared_error: 304095.2500\n",
      "Epoch 3959/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 305005.0625 - mean_squared_error: 305005.0625\n",
      "Epoch 3960/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296652.3438 - mean_squared_error: 296652.3438\n",
      "Epoch 3961/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299933.9062 - mean_squared_error: 299933.9062\n",
      "Epoch 3962/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299810.9688 - mean_squared_error: 299810.9688\n",
      "Epoch 3963/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300009.6875 - mean_squared_error: 300009.6875\n",
      "Epoch 3964/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 293398.5938 - mean_squared_error: 293398.5938\n",
      "Epoch 3965/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302673.0625 - mean_squared_error: 302673.0625\n",
      "Epoch 3966/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296051.6250 - mean_squared_error: 296051.6250\n",
      "Epoch 3967/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297671.4062 - mean_squared_error: 297671.4062\n",
      "Epoch 3968/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299125.3125 - mean_squared_error: 299125.3125\n",
      "Epoch 3969/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297215.5312 - mean_squared_error: 297215.5000\n",
      "Epoch 3970/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 289756.0000 - mean_squared_error: 289756.0000\n",
      "Epoch 3971/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291292.4062 - mean_squared_error: 291292.4062\n",
      "Epoch 3972/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296660.3750 - mean_squared_error: 296660.3750\n",
      "Epoch 3973/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295580.7812 - mean_squared_error: 295580.7812\n",
      "Epoch 3974/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292946.7188 - mean_squared_error: 292946.7188\n",
      "Epoch 3975/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293734.4062 - mean_squared_error: 293734.4062\n",
      "Epoch 3976/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301233.8438 - mean_squared_error: 301233.8750\n",
      "Epoch 3977/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298920.3750 - mean_squared_error: 298920.3750\n",
      "Epoch 3978/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298443.6250 - mean_squared_error: 298443.6250\n",
      "Epoch 3979/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 310407.7188 - mean_squared_error: 310407.7188\n",
      "Epoch 3980/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296328.6875 - mean_squared_error: 296328.6875\n",
      "Epoch 3981/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290723.3750 - mean_squared_error: 290723.3750\n",
      "Epoch 3982/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302162.0938 - mean_squared_error: 302162.0938\n",
      "Epoch 3983/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 305441.8125 - mean_squared_error: 305441.8125\n",
      "Epoch 3984/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297491.2500 - mean_squared_error: 297491.2500\n",
      "Epoch 3985/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290838.4688 - mean_squared_error: 290838.4688\n",
      "Epoch 3986/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 300814.6250 - mean_squared_error: 300814.5938\n",
      "Epoch 3987/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 304822.1875 - mean_squared_error: 304822.1875\n",
      "Epoch 3988/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293261.8125 - mean_squared_error: 293261.8125\n",
      "Epoch 3989/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294080.5000 - mean_squared_error: 294080.5000\n",
      "Epoch 3990/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 287839.0312 - mean_squared_error: 287839.0312\n",
      "Epoch 3991/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297745.5312 - mean_squared_error: 297745.5312\n",
      "Epoch 3992/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295807.8750 - mean_squared_error: 295807.8750\n",
      "Epoch 3993/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297252.1875 - mean_squared_error: 297252.1875\n",
      "Epoch 3994/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294626.6250 - mean_squared_error: 294626.6250\n",
      "Epoch 3995/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301711.2812 - mean_squared_error: 301711.2812\n",
      "Epoch 3996/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294801.8750 - mean_squared_error: 294801.8750\n",
      "Epoch 3997/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296495.1250 - mean_squared_error: 296495.1250\n",
      "Epoch 3998/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301763.1562 - mean_squared_error: 301763.1250\n",
      "Epoch 3999/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294370.5625 - mean_squared_error: 294370.5625\n",
      "Epoch 4000/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296677.3125 - mean_squared_error: 296677.3125\n",
      "Epoch 4001/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295849.2812 - mean_squared_error: 295849.2812\n",
      "Epoch 4002/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 295687.6562 - mean_squared_error: 295687.6562\n",
      "Epoch 4003/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299181.2812 - mean_squared_error: 299181.2812\n",
      "Epoch 4004/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294453.8438 - mean_squared_error: 294453.8438\n",
      "Epoch 4005/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290594.1875 - mean_squared_error: 290594.1875\n",
      "Epoch 4006/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297016.7812 - mean_squared_error: 297016.7812\n",
      "Epoch 4007/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291381.9688 - mean_squared_error: 291381.9688\n",
      "Epoch 4008/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298140.5000 - mean_squared_error: 298140.5000\n",
      "Epoch 4009/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298160.3750 - mean_squared_error: 298160.3750\n",
      "Epoch 4010/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298420.0312 - mean_squared_error: 298420.0312\n",
      "Epoch 4011/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299818.9062 - mean_squared_error: 299818.9062\n",
      "Epoch 4012/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294080.7188 - mean_squared_error: 294080.7188\n",
      "Epoch 4013/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297260.7500 - mean_squared_error: 297260.7500\n",
      "Epoch 4014/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293814.6875 - mean_squared_error: 293814.6875\n",
      "Epoch 4015/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 305678.1250 - mean_squared_error: 305678.1250\n",
      "Epoch 4016/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299658.7188 - mean_squared_error: 299658.7500\n",
      "Epoch 4017/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294458.0312 - mean_squared_error: 294458.0312\n",
      "Epoch 4018/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293728.9375 - mean_squared_error: 293728.9375\n",
      "Epoch 4019/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294449.0938 - mean_squared_error: 294449.0938\n",
      "Epoch 4020/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293484.9688 - mean_squared_error: 293484.9375\n",
      "Epoch 4021/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294481.3438 - mean_squared_error: 294481.3438\n",
      "Epoch 4022/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 306171.3750 - mean_squared_error: 306171.3750\n",
      "Epoch 4023/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299780.3750 - mean_squared_error: 299780.3750\n",
      "Epoch 4024/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298907.8750 - mean_squared_error: 298907.9062\n",
      "Epoch 4025/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 298155.5625 - mean_squared_error: 298155.5625\n",
      "Epoch 4026/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292785.3125 - mean_squared_error: 292785.3125\n",
      "Epoch 4027/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 304903.6250 - mean_squared_error: 304903.6250\n",
      "Epoch 4028/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299473.6562 - mean_squared_error: 299473.6562\n",
      "Epoch 4029/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299912.5938 - mean_squared_error: 299912.5938\n",
      "Epoch 4030/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299517.2188 - mean_squared_error: 299517.2188\n",
      "Epoch 4031/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299029.0000 - mean_squared_error: 299028.9688\n",
      "Epoch 4032/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303477.0938 - mean_squared_error: 303477.0938\n",
      "Epoch 4033/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300755.1250 - mean_squared_error: 300755.1250\n",
      "Epoch 4034/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301791.8125 - mean_squared_error: 301791.8125\n",
      "Epoch 4035/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302718.0938 - mean_squared_error: 302718.0938\n",
      "Epoch 4036/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293143.3750 - mean_squared_error: 293143.3750\n",
      "Epoch 4037/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296946.7188 - mean_squared_error: 296946.7188\n",
      "Epoch 4038/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291965.6562 - mean_squared_error: 291965.6562\n",
      "Epoch 4039/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302086.6875 - mean_squared_error: 302086.6875\n",
      "Epoch 4040/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 308787.3750 - mean_squared_error: 308787.3750\n",
      "Epoch 4041/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296185.0938 - mean_squared_error: 296185.0938\n",
      "Epoch 4042/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294098.5938 - mean_squared_error: 294098.5938\n",
      "Epoch 4043/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295692.7188 - mean_squared_error: 295692.7188\n",
      "Epoch 4044/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292177.3125 - mean_squared_error: 292177.3125\n",
      "Epoch 4045/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300982.3750 - mean_squared_error: 300982.3750\n",
      "Epoch 4046/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 287121.1875 - mean_squared_error: 287121.1875\n",
      "Epoch 4047/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 287526.3750 - mean_squared_error: 287526.3750\n",
      "Epoch 4048/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297212.0000 - mean_squared_error: 297212.0000\n",
      "Epoch 4049/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291340.0938 - mean_squared_error: 291340.1250\n",
      "Epoch 4050/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299980.7188 - mean_squared_error: 299980.7188\n",
      "Epoch 4051/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303855.7188 - mean_squared_error: 303855.7188\n",
      "Epoch 4052/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293956.5938 - mean_squared_error: 293956.5938\n",
      "Epoch 4053/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293199.5312 - mean_squared_error: 293199.5312\n",
      "Epoch 4054/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291904.2500 - mean_squared_error: 291904.2500\n",
      "Epoch 4055/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301524.3438 - mean_squared_error: 301524.3125\n",
      "Epoch 4056/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290131.4375 - mean_squared_error: 290131.4375\n",
      "Epoch 4057/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299103.6875 - mean_squared_error: 299103.6875\n",
      "Epoch 4058/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294800.0625 - mean_squared_error: 294800.0625\n",
      "Epoch 4059/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290709.7188 - mean_squared_error: 290709.7188\n",
      "Epoch 4060/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299467.4375 - mean_squared_error: 299467.4375\n",
      "Epoch 4061/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296514.4688 - mean_squared_error: 296514.4688\n",
      "Epoch 4062/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298748.5938 - mean_squared_error: 298748.5625\n",
      "Epoch 4063/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299814.6562 - mean_squared_error: 299814.6562\n",
      "Epoch 4064/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297239.8438 - mean_squared_error: 297239.8438\n",
      "Epoch 4065/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301262.1250 - mean_squared_error: 301262.1250\n",
      "Epoch 4066/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291203.2500 - mean_squared_error: 291203.2188\n",
      "Epoch 4067/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301920.4375 - mean_squared_error: 301920.4375\n",
      "Epoch 4068/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298312.1562 - mean_squared_error: 298312.1562\n",
      "Epoch 4069/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295415.0312 - mean_squared_error: 295415.0312\n",
      "Epoch 4070/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 283257.8125 - mean_squared_error: 283257.8125\n",
      "Epoch 4071/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291219.9062 - mean_squared_error: 291219.9062\n",
      "Epoch 4072/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301653.5000 - mean_squared_error: 301653.5000\n",
      "Epoch 4073/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 289544.1250 - mean_squared_error: 289544.1250\n",
      "Epoch 4074/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301546.7188 - mean_squared_error: 301546.7188\n",
      "Epoch 4075/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296058.5312 - mean_squared_error: 296058.5312\n",
      "Epoch 4076/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296126.5938 - mean_squared_error: 296126.5938\n",
      "Epoch 4077/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298803.9062 - mean_squared_error: 298803.9062\n",
      "Epoch 4078/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303238.4688 - mean_squared_error: 303238.4688\n",
      "Epoch 4079/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298929.0938 - mean_squared_error: 298929.0938\n",
      "Epoch 4080/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293685.4062 - mean_squared_error: 293685.4062\n",
      "Epoch 4081/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 288843.1875 - mean_squared_error: 288843.1875\n",
      "Epoch 4082/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 304826.9688 - mean_squared_error: 304826.9688\n",
      "Epoch 4083/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299774.3438 - mean_squared_error: 299774.3750\n",
      "Epoch 4084/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 305311.0938 - mean_squared_error: 305311.0938\n",
      "Epoch 4085/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291970.3750 - mean_squared_error: 291970.3750\n",
      "Epoch 4086/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292446.9375 - mean_squared_error: 292446.9375\n",
      "Epoch 4087/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301335.1562 - mean_squared_error: 301335.1875\n",
      "Epoch 4088/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 300380.3125 - mean_squared_error: 300380.3125\n",
      "Epoch 4089/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294535.7812 - mean_squared_error: 294535.7812\n",
      "Epoch 4090/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293103.1875 - mean_squared_error: 293103.1875\n",
      "Epoch 4091/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291807.7188 - mean_squared_error: 291807.7188\n",
      "Epoch 4092/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 298988.4062 - mean_squared_error: 298988.4062\n",
      "Epoch 4093/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301376.2500 - mean_squared_error: 301376.2500\n",
      "Epoch 4094/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300306.5938 - mean_squared_error: 300306.5938\n",
      "Epoch 4095/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292277.6875 - mean_squared_error: 292277.6875\n",
      "Epoch 4096/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298081.2500 - mean_squared_error: 298081.2500\n",
      "Epoch 4097/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292865.1875 - mean_squared_error: 292865.1875\n",
      "Epoch 4098/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297105.4688 - mean_squared_error: 297105.5000\n",
      "Epoch 4099/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295330.3438 - mean_squared_error: 295330.3438\n",
      "Epoch 4100/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 306555.5938 - mean_squared_error: 306555.5938\n",
      "Epoch 4101/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295296.6250 - mean_squared_error: 295296.6250\n",
      "Epoch 4102/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292245.3438 - mean_squared_error: 292245.3438\n",
      "Epoch 4103/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298293.3438 - mean_squared_error: 298293.3438\n",
      "Epoch 4104/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298652.0625 - mean_squared_error: 298652.0625\n",
      "Epoch 4105/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 285741.8438 - mean_squared_error: 285741.8438\n",
      "Epoch 4106/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295439.5312 - mean_squared_error: 295439.5312\n",
      "Epoch 4107/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 304817.2500 - mean_squared_error: 304817.2500\n",
      "Epoch 4108/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296927.7812 - mean_squared_error: 296927.7812\n",
      "Epoch 4109/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296733.3438 - mean_squared_error: 296733.3438\n",
      "Epoch 4110/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296431.5312 - mean_squared_error: 296431.5312\n",
      "Epoch 4111/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297460.1562 - mean_squared_error: 297460.1562\n",
      "Epoch 4112/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 287478.1562 - mean_squared_error: 287478.1562\n",
      "Epoch 4113/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302395.2812 - mean_squared_error: 302395.2812\n",
      "Epoch 4114/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297913.1875 - mean_squared_error: 297913.1875\n",
      "Epoch 4115/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291934.7188 - mean_squared_error: 291934.7188\n",
      "Epoch 4116/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303806.7812 - mean_squared_error: 303806.7812\n",
      "Epoch 4117/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299789.8125 - mean_squared_error: 299789.8125\n",
      "Epoch 4118/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293154.5938 - mean_squared_error: 293154.5938\n",
      "Epoch 4119/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295817.0938 - mean_squared_error: 295817.0938\n",
      "Epoch 4120/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295116.1250 - mean_squared_error: 295116.1250\n",
      "Epoch 4121/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 306454.4688 - mean_squared_error: 306454.4688\n",
      "Epoch 4122/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 306236.6875 - mean_squared_error: 306236.6875\n",
      "Epoch 4123/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301812.5000 - mean_squared_error: 301812.5000\n",
      "Epoch 4124/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 292130.5312 - mean_squared_error: 292130.5312\n",
      "Epoch 4125/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301627.6875 - mean_squared_error: 301627.6875\n",
      "Epoch 4126/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294344.6250 - mean_squared_error: 294344.6250\n",
      "Epoch 4127/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296344.2812 - mean_squared_error: 296344.2812\n",
      "Epoch 4128/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300596.4375 - mean_squared_error: 300596.4375\n",
      "Epoch 4129/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 297465.2812 - mean_squared_error: 297465.2812\n",
      "Epoch 4130/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300266.2188 - mean_squared_error: 300266.2188\n",
      "Epoch 4131/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303191.4375 - mean_squared_error: 303191.4375\n",
      "Epoch 4132/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287938.7188 - mean_squared_error: 287938.7188\n",
      "Epoch 4133/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301329.3750 - mean_squared_error: 301329.3750\n",
      "Epoch 4134/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303272.5312 - mean_squared_error: 303272.5000\n",
      "Epoch 4135/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291834.9062 - mean_squared_error: 291834.9062\n",
      "Epoch 4136/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293856.0000 - mean_squared_error: 293856.0000\n",
      "Epoch 4137/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295198.6562 - mean_squared_error: 295198.6562\n",
      "Epoch 4138/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293758.4688 - mean_squared_error: 293758.4688\n",
      "Epoch 4139/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301036.2500 - mean_squared_error: 301036.2500\n",
      "Epoch 4140/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301993.9688 - mean_squared_error: 301993.9688\n",
      "Epoch 4141/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292488.5625 - mean_squared_error: 292488.5625\n",
      "Epoch 4142/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301018.7812 - mean_squared_error: 301018.7812\n",
      "Epoch 4143/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300309.6875 - mean_squared_error: 300309.6562\n",
      "Epoch 4144/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294018.7188 - mean_squared_error: 294018.7188\n",
      "Epoch 4145/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301227.5000 - mean_squared_error: 301227.5000\n",
      "Epoch 4146/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300222.5625 - mean_squared_error: 300222.5625\n",
      "Epoch 4147/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301366.6875 - mean_squared_error: 301366.6875\n",
      "Epoch 4148/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301044.0312 - mean_squared_error: 301044.0312\n",
      "Epoch 4149/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297838.2500 - mean_squared_error: 297838.2500\n",
      "Epoch 4150/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303852.3750 - mean_squared_error: 303852.3750\n",
      "Epoch 4151/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298371.5938 - mean_squared_error: 298371.5938\n",
      "Epoch 4152/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296297.3125 - mean_squared_error: 296297.3125\n",
      "Epoch 4153/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296495.7188 - mean_squared_error: 296495.7188\n",
      "Epoch 4154/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296852.2812 - mean_squared_error: 296852.2812\n",
      "Epoch 4155/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290664.5625 - mean_squared_error: 290664.5625\n",
      "Epoch 4156/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302812.5938 - mean_squared_error: 302812.5938\n",
      "Epoch 4157/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289490.7812 - mean_squared_error: 289490.7812\n",
      "Epoch 4158/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301545.3438 - mean_squared_error: 301545.3438\n",
      "Epoch 4159/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293138.0000 - mean_squared_error: 293138.0000\n",
      "Epoch 4160/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299972.3750 - mean_squared_error: 299972.3750\n",
      "Epoch 4161/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296911.7188 - mean_squared_error: 296911.7188\n",
      "Epoch 4162/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293580.0312 - mean_squared_error: 293580.0312\n",
      "Epoch 4163/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294274.3438 - mean_squared_error: 294274.3438\n",
      "Epoch 4164/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 304263.4062 - mean_squared_error: 304263.4062\n",
      "Epoch 4165/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297624.0938 - mean_squared_error: 297624.0938\n",
      "Epoch 4166/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 287696.0312 - mean_squared_error: 287696.0312\n",
      "Epoch 4167/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300249.1250 - mean_squared_error: 300249.1250\n",
      "Epoch 4168/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293778.0000 - mean_squared_error: 293778.0000\n",
      "Epoch 4169/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299564.0938 - mean_squared_error: 299564.0938\n",
      "Epoch 4170/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300717.1875 - mean_squared_error: 300717.1875\n",
      "Epoch 4171/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 304528.0938 - mean_squared_error: 304528.0938\n",
      "Epoch 4172/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294927.7500 - mean_squared_error: 294927.7500\n",
      "Epoch 4173/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299076.1250 - mean_squared_error: 299076.1250\n",
      "Epoch 4174/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296395.7500 - mean_squared_error: 296395.7500\n",
      "Epoch 4175/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299968.8125 - mean_squared_error: 299968.8125\n",
      "Epoch 4176/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301211.0938 - mean_squared_error: 301211.0938\n",
      "Epoch 4177/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300741.8125 - mean_squared_error: 300741.8125\n",
      "Epoch 4178/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295620.2500 - mean_squared_error: 295620.2500\n",
      "Epoch 4179/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302226.5625 - mean_squared_error: 302226.5625\n",
      "Epoch 4180/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301017.1875 - mean_squared_error: 301017.1562\n",
      "Epoch 4181/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292820.2188 - mean_squared_error: 292820.2188\n",
      "Epoch 4182/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300009.4688 - mean_squared_error: 300009.4688\n",
      "Epoch 4183/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300898.8125 - mean_squared_error: 300898.8125\n",
      "Epoch 4184/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291686.6562 - mean_squared_error: 291686.6562\n",
      "Epoch 4185/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289394.7188 - mean_squared_error: 289394.7188\n",
      "Epoch 4186/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300177.9062 - mean_squared_error: 300177.9062\n",
      "Epoch 4187/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291981.8750 - mean_squared_error: 291981.9062\n",
      "Epoch 4188/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298354.8125 - mean_squared_error: 298354.8438\n",
      "Epoch 4189/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295122.9375 - mean_squared_error: 295122.9375\n",
      "Epoch 4190/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 306520.0938 - mean_squared_error: 306520.0938\n",
      "Epoch 4191/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295875.6250 - mean_squared_error: 295875.6250\n",
      "Epoch 4192/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302061.4375 - mean_squared_error: 302061.4375\n",
      "Epoch 4193/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295390.8750 - mean_squared_error: 295390.8750\n",
      "Epoch 4194/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299550.5938 - mean_squared_error: 299550.5938\n",
      "Epoch 4195/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299108.2812 - mean_squared_error: 299108.2812\n",
      "Epoch 4196/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 308457.6562 - mean_squared_error: 308457.6562\n",
      "Epoch 4197/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290015.1875 - mean_squared_error: 290015.1875\n",
      "Epoch 4198/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294777.2188 - mean_squared_error: 294777.2188\n",
      "Epoch 4199/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290200.1250 - mean_squared_error: 290200.1250\n",
      "Epoch 4200/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300155.3438 - mean_squared_error: 300155.3438\n",
      "Epoch 4201/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297276.6250 - mean_squared_error: 297276.6250\n",
      "Epoch 4202/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 304483.2188 - mean_squared_error: 304483.2188\n",
      "Epoch 4203/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 307402.4688 - mean_squared_error: 307402.4688\n",
      "Epoch 4204/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291083.6875 - mean_squared_error: 291083.6875\n",
      "Epoch 4205/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292111.4062 - mean_squared_error: 292111.4375\n",
      "Epoch 4206/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299269.8750 - mean_squared_error: 299269.8750\n",
      "Epoch 4207/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293378.7500 - mean_squared_error: 293378.7500\n",
      "Epoch 4208/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298926.8750 - mean_squared_error: 298926.8750\n",
      "Epoch 4209/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 306943.1875 - mean_squared_error: 306943.1875\n",
      "Epoch 4210/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301688.2188 - mean_squared_error: 301688.2188\n",
      "Epoch 4211/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303996.8125 - mean_squared_error: 303996.8125\n",
      "Epoch 4212/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 304162.7500 - mean_squared_error: 304162.7500\n",
      "Epoch 4213/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299353.1562 - mean_squared_error: 299353.1562\n",
      "Epoch 4214/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301883.9062 - mean_squared_error: 301883.9062\n",
      "Epoch 4215/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297848.6562 - mean_squared_error: 297848.6562\n",
      "Epoch 4216/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 306832.1250 - mean_squared_error: 306832.1250\n",
      "Epoch 4217/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293452.4688 - mean_squared_error: 293452.4688\n",
      "Epoch 4218/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302552.2812 - mean_squared_error: 302552.2812\n",
      "Epoch 4219/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303182.4688 - mean_squared_error: 303182.4688\n",
      "Epoch 4220/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 14ms/step - loss: 290585.0000 - mean_squared_error: 290585.0312\n",
      "Epoch 4221/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290699.3750 - mean_squared_error: 290699.3750\n",
      "Epoch 4222/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291199.9688 - mean_squared_error: 291199.9375\n",
      "Epoch 4223/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299238.8750 - mean_squared_error: 299238.8750\n",
      "Epoch 4224/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293623.4375 - mean_squared_error: 293623.4375\n",
      "Epoch 4225/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298198.1875 - mean_squared_error: 298198.1875\n",
      "Epoch 4226/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292143.3750 - mean_squared_error: 292143.3750\n",
      "Epoch 4227/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302639.1875 - mean_squared_error: 302639.1875\n",
      "Epoch 4228/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301241.1562 - mean_squared_error: 301241.1562\n",
      "Epoch 4229/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293497.4062 - mean_squared_error: 293497.4062\n",
      "Epoch 4230/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295157.3438 - mean_squared_error: 295157.3125\n",
      "Epoch 4231/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 301886.6562 - mean_squared_error: 301886.6562\n",
      "Epoch 4232/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303020.5938 - mean_squared_error: 303020.6250\n",
      "Epoch 4233/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293701.0938 - mean_squared_error: 293701.0938\n",
      "Epoch 4234/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299401.0625 - mean_squared_error: 299401.0625\n",
      "Epoch 4235/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303228.6875 - mean_squared_error: 303228.6875\n",
      "Epoch 4236/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292916.4062 - mean_squared_error: 292916.4062\n",
      "Epoch 4237/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294888.7500 - mean_squared_error: 294888.7500\n",
      "Epoch 4238/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292353.1875 - mean_squared_error: 292353.1875\n",
      "Epoch 4239/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 295202.5312 - mean_squared_error: 295202.5312\n",
      "Epoch 4240/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289861.6250 - mean_squared_error: 289861.6250\n",
      "Epoch 4241/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291459.9062 - mean_squared_error: 291459.9062\n",
      "Epoch 4242/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303473.0625 - mean_squared_error: 303473.0625\n",
      "Epoch 4243/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 304058.9062 - mean_squared_error: 304058.9062\n",
      "Epoch 4244/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302717.4375 - mean_squared_error: 302717.4375\n",
      "Epoch 4245/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293317.5938 - mean_squared_error: 293317.5938\n",
      "Epoch 4246/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 289520.3125 - mean_squared_error: 289520.3125\n",
      "Epoch 4247/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 288200.0938 - mean_squared_error: 288200.0938\n",
      "Epoch 4248/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294031.2500 - mean_squared_error: 294031.2500\n",
      "Epoch 4249/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299883.7812 - mean_squared_error: 299883.7812\n",
      "Epoch 4250/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294056.0625 - mean_squared_error: 294056.0625\n",
      "Epoch 4251/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295252.2500 - mean_squared_error: 295252.2500\n",
      "Epoch 4252/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295676.6250 - mean_squared_error: 295676.6250\n",
      "Epoch 4253/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299470.2188 - mean_squared_error: 299470.2188\n",
      "Epoch 4254/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 295413.2500 - mean_squared_error: 295413.2500\n",
      "Epoch 4255/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297408.4375 - mean_squared_error: 297408.4375\n",
      "Epoch 4256/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289905.5312 - mean_squared_error: 289905.5312\n",
      "Epoch 4257/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292235.2500 - mean_squared_error: 292235.2500\n",
      "Epoch 4258/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297851.9688 - mean_squared_error: 297851.9688\n",
      "Epoch 4259/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299002.0938 - mean_squared_error: 299002.0938\n",
      "Epoch 4260/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296106.0312 - mean_squared_error: 296106.0000\n",
      "Epoch 4261/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296785.8438 - mean_squared_error: 296785.8750\n",
      "Epoch 4262/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298771.0938 - mean_squared_error: 298771.0938\n",
      "Epoch 4263/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299181.2188 - mean_squared_error: 299181.1875\n",
      "Epoch 4264/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297215.5312 - mean_squared_error: 297215.5312\n",
      "Epoch 4265/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299188.3125 - mean_squared_error: 299188.3125\n",
      "Epoch 4266/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295350.5000 - mean_squared_error: 295350.5000\n",
      "Epoch 4267/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297666.9062 - mean_squared_error: 297666.9062\n",
      "Epoch 4268/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299512.2188 - mean_squared_error: 299512.2188\n",
      "Epoch 4269/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290965.9062 - mean_squared_error: 290965.9062\n",
      "Epoch 4270/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297628.1250 - mean_squared_error: 297628.1250\n",
      "Epoch 4271/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297865.6562 - mean_squared_error: 297865.6562\n",
      "Epoch 4272/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291801.6875 - mean_squared_error: 291801.6875\n",
      "Epoch 4273/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296846.7812 - mean_squared_error: 296846.7812\n",
      "Epoch 4274/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291075.3750 - mean_squared_error: 291075.3750\n",
      "Epoch 4275/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300265.3438 - mean_squared_error: 300265.3438\n",
      "Epoch 4276/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295397.0000 - mean_squared_error: 295397.0000\n",
      "Epoch 4277/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296122.4062 - mean_squared_error: 296122.4062\n",
      "Epoch 4278/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300809.1562 - mean_squared_error: 300809.1562\n",
      "Epoch 4279/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294145.3125 - mean_squared_error: 294145.3125\n",
      "Epoch 4280/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293729.0938 - mean_squared_error: 293729.0938\n",
      "Epoch 4281/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293862.1250 - mean_squared_error: 293862.1250\n",
      "Epoch 4282/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296738.1875 - mean_squared_error: 296738.1875\n",
      "Epoch 4283/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295468.7812 - mean_squared_error: 295468.7812\n",
      "Epoch 4284/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300026.5312 - mean_squared_error: 300026.5312\n",
      "Epoch 4285/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301504.2812 - mean_squared_error: 301504.2812\n",
      "Epoch 4286/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296873.3438 - mean_squared_error: 296873.3438\n",
      "Epoch 4287/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297845.0938 - mean_squared_error: 297845.0938\n",
      "Epoch 4288/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289826.1875 - mean_squared_error: 289826.1875\n",
      "Epoch 4289/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297037.0312 - mean_squared_error: 297037.0312\n",
      "Epoch 4290/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290177.5312 - mean_squared_error: 290177.5312\n",
      "Epoch 4291/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300319.6875 - mean_squared_error: 300319.6875\n",
      "Epoch 4292/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297171.0938 - mean_squared_error: 297171.0938\n",
      "Epoch 4293/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298093.5312 - mean_squared_error: 298093.5312\n",
      "Epoch 4294/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 301229.5938 - mean_squared_error: 301229.5938\n",
      "Epoch 4295/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298901.1250 - mean_squared_error: 298901.1250\n",
      "Epoch 4296/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291596.5000 - mean_squared_error: 291596.5000\n",
      "Epoch 4297/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 281081.1562 - mean_squared_error: 281081.1562\n",
      "Epoch 4298/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288996.0938 - mean_squared_error: 288996.0938\n",
      "Epoch 4299/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 294554.1875 - mean_squared_error: 294554.1875\n",
      "Epoch 4300/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298591.0000 - mean_squared_error: 298591.0000\n",
      "Epoch 4301/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297705.7188 - mean_squared_error: 297705.7188\n",
      "Epoch 4302/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294073.7500 - mean_squared_error: 294073.8125\n",
      "Epoch 4303/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292351.6875 - mean_squared_error: 292351.6875\n",
      "Epoch 4304/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291252.5000 - mean_squared_error: 291252.5000\n",
      "Epoch 4305/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300015.0000 - mean_squared_error: 300014.9688\n",
      "Epoch 4306/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290338.3125 - mean_squared_error: 290338.3125\n",
      "Epoch 4307/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293472.4062 - mean_squared_error: 293472.3750\n",
      "Epoch 4308/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287904.7812 - mean_squared_error: 287904.7812\n",
      "Epoch 4309/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287504.0938 - mean_squared_error: 287504.0938\n",
      "Epoch 4310/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298435.4375 - mean_squared_error: 298435.4375\n",
      "Epoch 4311/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293738.0938 - mean_squared_error: 293738.0938\n",
      "Epoch 4312/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293770.6562 - mean_squared_error: 293770.6562\n",
      "Epoch 4313/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290391.5938 - mean_squared_error: 290391.5938\n",
      "Epoch 4314/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 282754.0625 - mean_squared_error: 282754.0625\n",
      "Epoch 4315/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296698.5312 - mean_squared_error: 296698.5312\n",
      "Epoch 4316/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294657.0312 - mean_squared_error: 294657.0312\n",
      "Epoch 4317/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 302517.7812 - mean_squared_error: 302517.7812\n",
      "Epoch 4318/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303654.0000 - mean_squared_error: 303654.0000\n",
      "Epoch 4319/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293338.5625 - mean_squared_error: 293338.5625\n",
      "Epoch 4320/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301123.5000 - mean_squared_error: 301123.5000\n",
      "Epoch 4321/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301041.0625 - mean_squared_error: 301041.0625\n",
      "Epoch 4322/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294317.3438 - mean_squared_error: 294317.3438\n",
      "Epoch 4323/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299401.2812 - mean_squared_error: 299401.2812\n",
      "Epoch 4324/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292456.5000 - mean_squared_error: 292456.5000\n",
      "Epoch 4325/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296035.9062 - mean_squared_error: 296035.9062\n",
      "Epoch 4326/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293220.1562 - mean_squared_error: 293220.1562\n",
      "Epoch 4327/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299673.9375 - mean_squared_error: 299673.9375\n",
      "Epoch 4328/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 297844.6250 - mean_squared_error: 297844.6250\n",
      "Epoch 4329/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294476.9375 - mean_squared_error: 294476.9375\n",
      "Epoch 4330/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293595.4062 - mean_squared_error: 293595.4062\n",
      "Epoch 4331/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 304538.6875 - mean_squared_error: 304538.6875\n",
      "Epoch 4332/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291625.7500 - mean_squared_error: 291625.7500\n",
      "Epoch 4333/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300846.7500 - mean_squared_error: 300846.7500\n",
      "Epoch 4334/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299717.6250 - mean_squared_error: 299717.6250\n",
      "Epoch 4335/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293614.9375 - mean_squared_error: 293614.9375\n",
      "Epoch 4336/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 304355.1875 - mean_squared_error: 304355.1875\n",
      "Epoch 4337/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289305.2812 - mean_squared_error: 289305.2812\n",
      "Epoch 4338/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300613.0938 - mean_squared_error: 300613.0938\n",
      "Epoch 4339/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295385.2500 - mean_squared_error: 295385.2812\n",
      "Epoch 4340/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297711.5312 - mean_squared_error: 297711.5312\n",
      "Epoch 4341/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299479.1875 - mean_squared_error: 299479.1875\n",
      "Epoch 4342/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290967.9375 - mean_squared_error: 290967.9375\n",
      "Epoch 4343/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295274.0000 - mean_squared_error: 295274.0000\n",
      "Epoch 4344/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298264.9375 - mean_squared_error: 298264.9375\n",
      "Epoch 4345/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291845.3750 - mean_squared_error: 291845.3750\n",
      "Epoch 4346/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296624.1875 - mean_squared_error: 296624.1875\n",
      "Epoch 4347/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290260.4375 - mean_squared_error: 290260.4375\n",
      "Epoch 4348/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298988.0000 - mean_squared_error: 298988.0000\n",
      "Epoch 4349/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294757.9062 - mean_squared_error: 294757.9062\n",
      "Epoch 4350/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299386.6562 - mean_squared_error: 299386.6562\n",
      "Epoch 4351/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296753.8125 - mean_squared_error: 296753.8125\n",
      "Epoch 4352/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 298610.5312 - mean_squared_error: 298610.5312\n",
      "Epoch 4353/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299635.8750 - mean_squared_error: 299635.8750\n",
      "Epoch 4354/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297768.5625 - mean_squared_error: 297768.5625\n",
      "Epoch 4355/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 286325.4375 - mean_squared_error: 286325.4375\n",
      "Epoch 4356/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 309150.1875 - mean_squared_error: 309150.1875\n",
      "Epoch 4357/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295747.5000 - mean_squared_error: 295747.5000\n",
      "Epoch 4358/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292477.0312 - mean_squared_error: 292477.0312\n",
      "Epoch 4359/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 288869.8438 - mean_squared_error: 288869.8438\n",
      "Epoch 4360/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294656.0312 - mean_squared_error: 294656.0312\n",
      "Epoch 4361/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292862.1250 - mean_squared_error: 292862.1250\n",
      "Epoch 4362/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291844.9375 - mean_squared_error: 291844.9375\n",
      "Epoch 4363/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291777.7812 - mean_squared_error: 291777.7812\n",
      "Epoch 4364/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296465.9375 - mean_squared_error: 296465.9375\n",
      "Epoch 4365/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295341.8438 - mean_squared_error: 295341.8438\n",
      "Epoch 4366/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291319.1875 - mean_squared_error: 291319.1875\n",
      "Epoch 4367/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288696.6562 - mean_squared_error: 288696.6562\n",
      "Epoch 4368/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298817.4062 - mean_squared_error: 298817.4062\n",
      "Epoch 4369/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293087.8750 - mean_squared_error: 293087.8750\n",
      "Epoch 4370/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295426.0938 - mean_squared_error: 295426.1250\n",
      "Epoch 4371/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297790.7500 - mean_squared_error: 297790.7500\n",
      "Epoch 4372/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 307945.9375 - mean_squared_error: 307945.9375\n",
      "Epoch 4373/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299887.7188 - mean_squared_error: 299887.7188\n",
      "Epoch 4374/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293218.4375 - mean_squared_error: 293218.4688\n",
      "Epoch 4375/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294106.0000 - mean_squared_error: 294106.0000\n",
      "Epoch 4376/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303017.2188 - mean_squared_error: 303017.2188\n",
      "Epoch 4377/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291475.1562 - mean_squared_error: 291475.1562\n",
      "Epoch 4378/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295675.5312 - mean_squared_error: 295675.5312\n",
      "Epoch 4379/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294419.7188 - mean_squared_error: 294419.6875\n",
      "Epoch 4380/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295798.6562 - mean_squared_error: 295798.6562\n",
      "Epoch 4381/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292783.5312 - mean_squared_error: 292783.5312\n",
      "Epoch 4382/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303441.4062 - mean_squared_error: 303441.4062\n",
      "Epoch 4383/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295028.7188 - mean_squared_error: 295028.7188\n",
      "Epoch 4384/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300984.2812 - mean_squared_error: 300984.2812\n",
      "Epoch 4385/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289538.8125 - mean_squared_error: 289538.8125\n",
      "Epoch 4386/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297686.9375 - mean_squared_error: 297686.9375\n",
      "Epoch 4387/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294800.6562 - mean_squared_error: 294800.6562\n",
      "Epoch 4388/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295537.3750 - mean_squared_error: 295537.3750\n",
      "Epoch 4389/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298957.0625 - mean_squared_error: 298957.0625\n",
      "Epoch 4390/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296738.1875 - mean_squared_error: 296738.1875\n",
      "Epoch 4391/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290315.8438 - mean_squared_error: 290315.8438\n",
      "Epoch 4392/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288898.5000 - mean_squared_error: 288898.5000\n",
      "Epoch 4393/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 287501.8125 - mean_squared_error: 287501.8125\n",
      "Epoch 4394/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302544.4688 - mean_squared_error: 302544.4688\n",
      "Epoch 4395/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 283867.3438 - mean_squared_error: 283867.3438\n",
      "Epoch 4396/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293906.2500 - mean_squared_error: 293906.2500\n",
      "Epoch 4397/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 285457.0938 - mean_squared_error: 285457.0938\n",
      "Epoch 4398/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299133.8125 - mean_squared_error: 299133.8125\n",
      "Epoch 4399/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291442.8125 - mean_squared_error: 291442.7812\n",
      "Epoch 4400/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295735.5000 - mean_squared_error: 295735.5000\n",
      "Epoch 4401/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293851.7188 - mean_squared_error: 293851.6875\n",
      "Epoch 4402/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294871.9062 - mean_squared_error: 294871.9062\n",
      "Epoch 4403/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293470.0000 - mean_squared_error: 293470.0000\n",
      "Epoch 4404/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301345.7188 - mean_squared_error: 301345.7188\n",
      "Epoch 4405/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290060.1250 - mean_squared_error: 290060.1250\n",
      "Epoch 4406/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298146.0625 - mean_squared_error: 298146.0625\n",
      "Epoch 4407/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292041.2188 - mean_squared_error: 292041.2188\n",
      "Epoch 4408/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292687.1875 - mean_squared_error: 292687.1875\n",
      "Epoch 4409/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298396.5000 - mean_squared_error: 298396.5000\n",
      "Epoch 4410/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295148.2188 - mean_squared_error: 295148.2188\n",
      "Epoch 4411/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289924.2188 - mean_squared_error: 289924.2188\n",
      "Epoch 4412/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294915.6875 - mean_squared_error: 294915.6875\n",
      "Epoch 4413/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287340.8438 - mean_squared_error: 287340.8438\n",
      "Epoch 4414/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287110.9688 - mean_squared_error: 287110.9688\n",
      "Epoch 4415/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297066.2812 - mean_squared_error: 297066.2812\n",
      "Epoch 4416/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294542.4375 - mean_squared_error: 294542.4375\n",
      "Epoch 4417/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290980.0312 - mean_squared_error: 290980.0312\n",
      "Epoch 4418/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292317.7188 - mean_squared_error: 292317.7188\n",
      "Epoch 4419/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296195.6562 - mean_squared_error: 296195.6562\n",
      "Epoch 4420/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 296171.5312 - mean_squared_error: 296171.5312\n",
      "Epoch 4421/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300000.7188 - mean_squared_error: 300000.7188\n",
      "Epoch 4422/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 295888.1875 - mean_squared_error: 295888.1875\n",
      "Epoch 4423/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291089.3438 - mean_squared_error: 291089.3438\n",
      "Epoch 4424/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287034.3750 - mean_squared_error: 287034.3750\n",
      "Epoch 4425/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293901.4375 - mean_squared_error: 293901.4375\n",
      "Epoch 4426/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297001.5000 - mean_squared_error: 297001.5000\n",
      "Epoch 4427/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 279775.2188 - mean_squared_error: 279775.2188\n",
      "Epoch 4428/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291630.7500 - mean_squared_error: 291630.7500\n",
      "Epoch 4429/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289362.6875 - mean_squared_error: 289362.6875\n",
      "Epoch 4430/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293476.7500 - mean_squared_error: 293476.7500\n",
      "Epoch 4431/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291946.9375 - mean_squared_error: 291946.9375\n",
      "Epoch 4432/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298129.3438 - mean_squared_error: 298129.3750\n",
      "Epoch 4433/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298751.2500 - mean_squared_error: 298751.2500\n",
      "Epoch 4434/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295648.3750 - mean_squared_error: 295648.3750\n",
      "Epoch 4435/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295059.6875 - mean_squared_error: 295059.6875\n",
      "Epoch 4436/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 292805.9062 - mean_squared_error: 292805.9062\n",
      "Epoch 4437/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296188.5625 - mean_squared_error: 296188.5625\n",
      "Epoch 4438/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303395.7812 - mean_squared_error: 303395.7812\n",
      "Epoch 4439/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300421.9062 - mean_squared_error: 300421.9062\n",
      "Epoch 4440/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292676.4062 - mean_squared_error: 292676.4062\n",
      "Epoch 4441/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296704.9375 - mean_squared_error: 296704.9375\n",
      "Epoch 4442/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295422.0000 - mean_squared_error: 295422.0000\n",
      "Epoch 4443/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297454.4375 - mean_squared_error: 297454.4375\n",
      "Epoch 4444/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297586.3438 - mean_squared_error: 297586.3438\n",
      "Epoch 4445/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 304276.8750 - mean_squared_error: 304276.8750\n",
      "Epoch 4446/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297648.3438 - mean_squared_error: 297648.3438\n",
      "Epoch 4447/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301963.0000 - mean_squared_error: 301963.0000\n",
      "Epoch 4448/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296709.9688 - mean_squared_error: 296709.9688\n",
      "Epoch 4449/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296501.7500 - mean_squared_error: 296501.7500\n",
      "Epoch 4450/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299988.3125 - mean_squared_error: 299988.3125\n",
      "Epoch 4451/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302239.6562 - mean_squared_error: 302239.6562\n",
      "Epoch 4452/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 289027.7188 - mean_squared_error: 289027.7188\n",
      "Epoch 4453/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293041.6875 - mean_squared_error: 293041.6875\n",
      "Epoch 4454/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295943.2500 - mean_squared_error: 295943.2500\n",
      "Epoch 4455/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301342.0000 - mean_squared_error: 301342.0000\n",
      "Epoch 4456/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 304852.1562 - mean_squared_error: 304852.1562\n",
      "Epoch 4457/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301580.8438 - mean_squared_error: 301580.8438\n",
      "Epoch 4458/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 287372.1562 - mean_squared_error: 287372.1562\n",
      "Epoch 4459/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 299869.4062 - mean_squared_error: 299869.4062\n",
      "Epoch 4460/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291090.3750 - mean_squared_error: 291090.3750\n",
      "Epoch 4461/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294943.5000 - mean_squared_error: 294943.5000\n",
      "Epoch 4462/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296826.4375 - mean_squared_error: 296826.4375\n",
      "Epoch 4463/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296682.9688 - mean_squared_error: 296682.9688\n",
      "Epoch 4464/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295233.6875 - mean_squared_error: 295233.6875\n",
      "Epoch 4465/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300885.0312 - mean_squared_error: 300885.0000\n",
      "Epoch 4466/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293661.8750 - mean_squared_error: 293661.8438\n",
      "Epoch 4467/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 287122.6250 - mean_squared_error: 287122.6250\n",
      "Epoch 4468/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288123.1562 - mean_squared_error: 288123.1250\n",
      "Epoch 4469/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290638.8438 - mean_squared_error: 290638.8438\n",
      "Epoch 4470/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301501.6562 - mean_squared_error: 301501.6562\n",
      "Epoch 4471/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 296353.2188 - mean_squared_error: 296353.2188\n",
      "Epoch 4472/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 299865.7188 - mean_squared_error: 299865.7188\n",
      "Epoch 4473/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290497.8750 - mean_squared_error: 290497.8750\n",
      "Epoch 4474/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297579.5000 - mean_squared_error: 297579.5000\n",
      "Epoch 4475/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294762.7188 - mean_squared_error: 294762.7188\n",
      "Epoch 4476/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298954.2812 - mean_squared_error: 298954.2812\n",
      "Epoch 4477/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295819.7500 - mean_squared_error: 295819.7500\n",
      "Epoch 4478/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295517.3750 - mean_squared_error: 295517.3750\n",
      "Epoch 4479/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293677.9375 - mean_squared_error: 293677.9375\n",
      "Epoch 4480/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294668.7500 - mean_squared_error: 294668.7500\n",
      "Epoch 4481/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294335.7812 - mean_squared_error: 294335.8125\n",
      "Epoch 4482/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297465.3125 - mean_squared_error: 297465.3125\n",
      "Epoch 4483/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288996.4062 - mean_squared_error: 288996.4062\n",
      "Epoch 4484/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 299689.0938 - mean_squared_error: 299689.0938\n",
      "Epoch 4485/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290781.1562 - mean_squared_error: 290781.1875\n",
      "Epoch 4486/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298611.5000 - mean_squared_error: 298611.5000\n",
      "Epoch 4487/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 286390.4062 - mean_squared_error: 286390.4062\n",
      "Epoch 4488/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299360.8125 - mean_squared_error: 299360.8125\n",
      "Epoch 4489/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287553.0938 - mean_squared_error: 287553.0938\n",
      "Epoch 4490/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296782.7500 - mean_squared_error: 296782.7500\n",
      "Epoch 4491/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293800.1875 - mean_squared_error: 293800.1875\n",
      "Epoch 4492/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294214.5312 - mean_squared_error: 294214.5312\n",
      "Epoch 4493/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293749.8438 - mean_squared_error: 293749.8438\n",
      "Epoch 4494/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291141.4062 - mean_squared_error: 291141.4062\n",
      "Epoch 4495/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294334.4688 - mean_squared_error: 294334.4688\n",
      "Epoch 4496/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 296999.5000 - mean_squared_error: 296999.5000\n",
      "Epoch 4497/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 286424.1562 - mean_squared_error: 286424.1562\n",
      "Epoch 4498/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300135.2188 - mean_squared_error: 300135.2188\n",
      "Epoch 4499/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296619.7500 - mean_squared_error: 296619.7812\n",
      "Epoch 4500/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 290874.4688 - mean_squared_error: 290874.4688\n",
      "Epoch 4501/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294725.7188 - mean_squared_error: 294725.7188\n",
      "Epoch 4502/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291242.5938 - mean_squared_error: 291242.5938\n",
      "Epoch 4503/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 297742.5000 - mean_squared_error: 297742.5000\n",
      "Epoch 4504/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295333.3750 - mean_squared_error: 295333.3750\n",
      "Epoch 4505/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298102.3438 - mean_squared_error: 298102.3438\n",
      "Epoch 4506/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295526.1562 - mean_squared_error: 295526.1562\n",
      "Epoch 4507/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294478.3125 - mean_squared_error: 294478.3125\n",
      "Epoch 4508/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294579.6875 - mean_squared_error: 294579.6875\n",
      "Epoch 4509/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293833.3125 - mean_squared_error: 293833.3125\n",
      "Epoch 4510/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292295.7500 - mean_squared_error: 292295.7500\n",
      "Epoch 4511/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292073.6562 - mean_squared_error: 292073.6562\n",
      "Epoch 4512/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290946.6875 - mean_squared_error: 290946.6875\n",
      "Epoch 4513/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294232.7500 - mean_squared_error: 294232.7500\n",
      "Epoch 4514/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 298912.0625 - mean_squared_error: 298912.0312\n",
      "Epoch 4515/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297007.2500 - mean_squared_error: 297007.2500\n",
      "Epoch 4516/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 307214.7500 - mean_squared_error: 307214.7500\n",
      "Epoch 4517/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291443.6562 - mean_squared_error: 291443.6250\n",
      "Epoch 4518/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292135.0000 - mean_squared_error: 292135.0000\n",
      "Epoch 4519/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291669.5312 - mean_squared_error: 291669.5312\n",
      "Epoch 4520/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291962.7500 - mean_squared_error: 291962.7500\n",
      "Epoch 4521/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299040.0938 - mean_squared_error: 299040.0938\n",
      "Epoch 4522/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300280.4688 - mean_squared_error: 300280.4688\n",
      "Epoch 4523/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291505.5625 - mean_squared_error: 291505.5625\n",
      "Epoch 4524/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290357.9688 - mean_squared_error: 290357.9688\n",
      "Epoch 4525/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295459.3750 - mean_squared_error: 295459.3750\n",
      "Epoch 4526/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290489.6875 - mean_squared_error: 290489.6875\n",
      "Epoch 4527/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290481.3750 - mean_squared_error: 290481.3750\n",
      "Epoch 4528/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292391.1875 - mean_squared_error: 292391.1875\n",
      "Epoch 4529/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291880.8750 - mean_squared_error: 291880.9062\n",
      "Epoch 4530/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 285053.0625 - mean_squared_error: 285053.0625\n",
      "Epoch 4531/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292627.8750 - mean_squared_error: 292627.8750\n",
      "Epoch 4532/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 302061.9688 - mean_squared_error: 302062.0000\n",
      "Epoch 4533/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294709.4062 - mean_squared_error: 294709.4062\n",
      "Epoch 4534/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293286.1875 - mean_squared_error: 293286.1875\n",
      "Epoch 4535/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293960.7812 - mean_squared_error: 293960.7500\n",
      "Epoch 4536/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 288000.5938 - mean_squared_error: 288000.6250\n",
      "Epoch 4537/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294418.1250 - mean_squared_error: 294418.1250\n",
      "Epoch 4538/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 287792.0000 - mean_squared_error: 287792.0000\n",
      "Epoch 4539/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 304655.0312 - mean_squared_error: 304655.0312\n",
      "Epoch 4540/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299987.5000 - mean_squared_error: 299987.5000\n",
      "Epoch 4541/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297265.8438 - mean_squared_error: 297265.8438\n",
      "Epoch 4542/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 284879.3438 - mean_squared_error: 284879.3438\n",
      "Epoch 4543/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294584.1875 - mean_squared_error: 294584.1875\n",
      "Epoch 4544/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291533.2812 - mean_squared_error: 291533.2812\n",
      "Epoch 4545/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 295849.2188 - mean_squared_error: 295849.2188\n",
      "Epoch 4546/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295963.1250 - mean_squared_error: 295963.1250\n",
      "Epoch 4547/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295497.9375 - mean_squared_error: 295497.9375\n",
      "Epoch 4548/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291567.8438 - mean_squared_error: 291567.8438\n",
      "Epoch 4549/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 288343.0625 - mean_squared_error: 288343.0625\n",
      "Epoch 4550/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296295.6875 - mean_squared_error: 296295.6875\n",
      "Epoch 4551/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 293155.9688 - mean_squared_error: 293155.9688\n",
      "Epoch 4552/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295883.2812 - mean_squared_error: 295883.2812\n",
      "Epoch 4553/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 286092.9062 - mean_squared_error: 286092.9062\n",
      "Epoch 4554/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299607.6562 - mean_squared_error: 299607.6562\n",
      "Epoch 4555/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293452.5312 - mean_squared_error: 293452.5625\n",
      "Epoch 4556/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293779.4688 - mean_squared_error: 293779.4688\n",
      "Epoch 4557/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293819.5625 - mean_squared_error: 293819.5625\n",
      "Epoch 4558/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297255.5312 - mean_squared_error: 297255.5312\n",
      "Epoch 4559/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291285.0000 - mean_squared_error: 291285.0000\n",
      "Epoch 4560/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292040.2812 - mean_squared_error: 292040.2812\n",
      "Epoch 4561/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295859.7500 - mean_squared_error: 295859.7500\n",
      "Epoch 4562/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296608.8438 - mean_squared_error: 296608.8438\n",
      "Epoch 4563/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 291746.4375 - mean_squared_error: 291746.4375\n",
      "Epoch 4564/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294989.9062 - mean_squared_error: 294989.9062\n",
      "Epoch 4565/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297862.6875 - mean_squared_error: 297862.6875\n",
      "Epoch 4566/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300045.3750 - mean_squared_error: 300045.3750\n",
      "Epoch 4567/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299131.0938 - mean_squared_error: 299131.0938\n",
      "Epoch 4568/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290324.5000 - mean_squared_error: 290324.5000\n",
      "Epoch 4569/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298020.0625 - mean_squared_error: 298020.0625\n",
      "Epoch 4570/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 285030.5938 - mean_squared_error: 285030.5938\n",
      "Epoch 4571/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300809.7188 - mean_squared_error: 300809.6875\n",
      "Epoch 4572/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295184.9062 - mean_squared_error: 295184.9062\n",
      "Epoch 4573/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295970.3438 - mean_squared_error: 295970.3438\n",
      "Epoch 4574/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290294.5938 - mean_squared_error: 290294.5938\n",
      "Epoch 4575/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297896.8125 - mean_squared_error: 297896.8125\n",
      "Epoch 4576/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292193.3750 - mean_squared_error: 292193.3750\n",
      "Epoch 4577/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297020.0938 - mean_squared_error: 297020.0938\n",
      "Epoch 4578/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 289159.1562 - mean_squared_error: 289159.1562\n",
      "Epoch 4579/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 294385.5312 - mean_squared_error: 294385.5312\n",
      "Epoch 4580/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294407.5000 - mean_squared_error: 294407.5312\n",
      "Epoch 4581/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301152.6875 - mean_squared_error: 301152.6875\n",
      "Epoch 4582/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296724.6875 - mean_squared_error: 296724.6875\n",
      "Epoch 4583/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 294131.9688 - mean_squared_error: 294131.9688\n",
      "Epoch 4584/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302225.8125 - mean_squared_error: 302225.8125\n",
      "Epoch 4585/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292053.2812 - mean_squared_error: 292053.2500\n",
      "Epoch 4586/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296260.4688 - mean_squared_error: 296260.4688\n",
      "Epoch 4587/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290331.1250 - mean_squared_error: 290331.1250\n",
      "Epoch 4588/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296650.4062 - mean_squared_error: 296650.4062\n",
      "Epoch 4589/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296001.4688 - mean_squared_error: 296001.4688\n",
      "Epoch 4590/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299543.0000 - mean_squared_error: 299543.0000\n",
      "Epoch 4591/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287714.1875 - mean_squared_error: 287714.1875\n",
      "Epoch 4592/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292020.2500 - mean_squared_error: 292020.2500\n",
      "Epoch 4593/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297778.0625 - mean_squared_error: 297778.0625\n",
      "Epoch 4594/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294522.7812 - mean_squared_error: 294522.7812\n",
      "Epoch 4595/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 289240.4688 - mean_squared_error: 289240.5000\n",
      "Epoch 4596/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299031.6562 - mean_squared_error: 299031.6562\n",
      "Epoch 4597/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 287097.6250 - mean_squared_error: 287097.6250\n",
      "Epoch 4598/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301269.3438 - mean_squared_error: 301269.3438\n",
      "Epoch 4599/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296580.3125 - mean_squared_error: 296580.3125\n",
      "Epoch 4600/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 295371.9375 - mean_squared_error: 295371.9375\n",
      "Epoch 4601/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292689.7812 - mean_squared_error: 292689.7812\n",
      "Epoch 4602/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294499.1250 - mean_squared_error: 294499.1250\n",
      "Epoch 4603/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293367.2500 - mean_squared_error: 293367.2500\n",
      "Epoch 4604/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296411.8750 - mean_squared_error: 296411.8750\n",
      "Epoch 4605/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293535.6250 - mean_squared_error: 293535.6250\n",
      "Epoch 4606/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293872.8438 - mean_squared_error: 293872.8438\n",
      "Epoch 4607/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299050.7812 - mean_squared_error: 299050.7812\n",
      "Epoch 4608/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298739.6875 - mean_squared_error: 298739.6875\n",
      "Epoch 4609/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293923.8438 - mean_squared_error: 293923.8438\n",
      "Epoch 4610/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299373.1250 - mean_squared_error: 299373.1250\n",
      "Epoch 4611/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297128.1250 - mean_squared_error: 297128.1250\n",
      "Epoch 4612/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299220.1562 - mean_squared_error: 299220.1562\n",
      "Epoch 4613/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290765.0625 - mean_squared_error: 290765.0625\n",
      "Epoch 4614/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292265.4688 - mean_squared_error: 292265.5312\n",
      "Epoch 4615/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300592.9375 - mean_squared_error: 300592.9375\n",
      "Epoch 4616/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 298064.4688 - mean_squared_error: 298064.4688\n",
      "Epoch 4617/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298661.6250 - mean_squared_error: 298661.6875\n",
      "Epoch 4618/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 294171.8125 - mean_squared_error: 294171.8125\n",
      "Epoch 4619/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292427.1250 - mean_squared_error: 292427.1250\n",
      "Epoch 4620/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303092.0000 - mean_squared_error: 303092.0000\n",
      "Epoch 4621/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 306084.6250 - mean_squared_error: 306084.5938\n",
      "Epoch 4622/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 300118.2500 - mean_squared_error: 300118.2500\n",
      "Epoch 4623/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297645.2500 - mean_squared_error: 297645.2188\n",
      "Epoch 4624/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287777.4688 - mean_squared_error: 287777.4688\n",
      "Epoch 4625/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295798.6250 - mean_squared_error: 295798.6250\n",
      "Epoch 4626/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 294426.5938 - mean_squared_error: 294426.5625\n",
      "Epoch 4627/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 289987.8438 - mean_squared_error: 289987.8438\n",
      "Epoch 4628/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299578.3438 - mean_squared_error: 299578.3438\n",
      "Epoch 4629/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 308328.6875 - mean_squared_error: 308328.6875\n",
      "Epoch 4630/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298944.5312 - mean_squared_error: 298944.5312\n",
      "Epoch 4631/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296349.0000 - mean_squared_error: 296349.0000\n",
      "Epoch 4632/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302597.1562 - mean_squared_error: 302597.1562\n",
      "Epoch 4633/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301508.4375 - mean_squared_error: 301508.4375\n",
      "Epoch 4634/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 294927.5312 - mean_squared_error: 294927.5312\n",
      "Epoch 4635/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289773.9688 - mean_squared_error: 289773.9688\n",
      "Epoch 4636/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291984.6875 - mean_squared_error: 291984.6562\n",
      "Epoch 4637/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296780.4375 - mean_squared_error: 296780.4062\n",
      "Epoch 4638/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299238.6250 - mean_squared_error: 299238.6250\n",
      "Epoch 4639/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297618.0000 - mean_squared_error: 297618.0000\n",
      "Epoch 4640/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302503.7812 - mean_squared_error: 302503.7812\n",
      "Epoch 4641/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 284745.7812 - mean_squared_error: 284745.7812\n",
      "Epoch 4642/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295740.8125 - mean_squared_error: 295740.8125\n",
      "Epoch 4643/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 286717.3438 - mean_squared_error: 286717.3438\n",
      "Epoch 4644/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297274.0938 - mean_squared_error: 297274.0938\n",
      "Epoch 4645/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299219.5625 - mean_squared_error: 299219.5625\n",
      "Epoch 4646/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300290.5625 - mean_squared_error: 300290.5625\n",
      "Epoch 4647/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 288609.9375 - mean_squared_error: 288609.9375\n",
      "Epoch 4648/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287764.3750 - mean_squared_error: 287764.3438\n",
      "Epoch 4649/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296075.1875 - mean_squared_error: 296075.2188\n",
      "Epoch 4650/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293596.9375 - mean_squared_error: 293596.9375\n",
      "Epoch 4651/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294606.6875 - mean_squared_error: 294606.6875\n",
      "Epoch 4652/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294101.0625 - mean_squared_error: 294101.0625\n",
      "Epoch 4653/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 300610.1562 - mean_squared_error: 300610.1562\n",
      "Epoch 4654/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293116.8750 - mean_squared_error: 293116.8750\n",
      "Epoch 4655/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302263.0000 - mean_squared_error: 302263.0000\n",
      "Epoch 4656/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294820.1562 - mean_squared_error: 294820.1562\n",
      "Epoch 4657/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291090.7500 - mean_squared_error: 291090.7500\n",
      "Epoch 4658/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297018.0000 - mean_squared_error: 297018.0000\n",
      "Epoch 4659/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289412.9688 - mean_squared_error: 289412.9688\n",
      "Epoch 4660/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299157.2500 - mean_squared_error: 299157.2500\n",
      "Epoch 4661/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 288590.6250 - mean_squared_error: 288590.6250\n",
      "Epoch 4662/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296746.5000 - mean_squared_error: 296746.5000\n",
      "Epoch 4663/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294732.5312 - mean_squared_error: 294732.5312\n",
      "Epoch 4664/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 288601.1875 - mean_squared_error: 288601.1875\n",
      "Epoch 4665/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292739.0000 - mean_squared_error: 292739.0000\n",
      "Epoch 4666/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 286776.3125 - mean_squared_error: 286776.2812\n",
      "Epoch 4667/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302258.0000 - mean_squared_error: 302258.0000\n",
      "Epoch 4668/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291971.7812 - mean_squared_error: 291971.7500\n",
      "Epoch 4669/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295175.9688 - mean_squared_error: 295175.9688\n",
      "Epoch 4670/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295150.2188 - mean_squared_error: 295150.2188\n",
      "Epoch 4671/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 304164.7812 - mean_squared_error: 304164.7812\n",
      "Epoch 4672/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302613.0000 - mean_squared_error: 302613.0000\n",
      "Epoch 4673/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288915.6250 - mean_squared_error: 288915.6250\n",
      "Epoch 4674/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299098.0312 - mean_squared_error: 299098.0625\n",
      "Epoch 4675/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295696.5312 - mean_squared_error: 295696.5312\n",
      "Epoch 4676/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298570.1562 - mean_squared_error: 298570.1562\n",
      "Epoch 4677/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 292990.2812 - mean_squared_error: 292990.2812\n",
      "Epoch 4678/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 285441.5938 - mean_squared_error: 285441.5938\n",
      "Epoch 4679/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295690.8438 - mean_squared_error: 295690.8438\n",
      "Epoch 4680/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291289.9375 - mean_squared_error: 291289.9375\n",
      "Epoch 4681/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 304062.7812 - mean_squared_error: 304062.7812\n",
      "Epoch 4682/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298104.7188 - mean_squared_error: 298104.7188\n",
      "Epoch 4683/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294947.4688 - mean_squared_error: 294947.4688\n",
      "Epoch 4684/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295484.7812 - mean_squared_error: 295484.7812\n",
      "Epoch 4685/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295386.3750 - mean_squared_error: 295386.3750\n",
      "Epoch 4686/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292472.1562 - mean_squared_error: 292472.1562\n",
      "Epoch 4687/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296190.3750 - mean_squared_error: 296190.3750\n",
      "Epoch 4688/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299525.6250 - mean_squared_error: 299525.5938\n",
      "Epoch 4689/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294233.4688 - mean_squared_error: 294233.4688\n",
      "Epoch 4690/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293222.1562 - mean_squared_error: 293222.1562\n",
      "Epoch 4691/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295066.1875 - mean_squared_error: 295066.1875\n",
      "Epoch 4692/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290600.2500 - mean_squared_error: 290600.2500\n",
      "Epoch 4693/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292331.4062 - mean_squared_error: 292331.4062\n",
      "Epoch 4694/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301859.3125 - mean_squared_error: 301859.3125\n",
      "Epoch 4695/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 294064.1875 - mean_squared_error: 294064.1562\n",
      "Epoch 4696/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297744.7188 - mean_squared_error: 297744.7188\n",
      "Epoch 4697/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 283333.5312 - mean_squared_error: 283333.5312\n",
      "Epoch 4698/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296043.6562 - mean_squared_error: 296043.6562\n",
      "Epoch 4699/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299852.9375 - mean_squared_error: 299852.9375\n",
      "Epoch 4700/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300446.1875 - mean_squared_error: 300446.1875\n",
      "Epoch 4701/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295144.3750 - mean_squared_error: 295144.3750\n",
      "Epoch 4702/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288535.9688 - mean_squared_error: 288535.9688\n",
      "Epoch 4703/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296269.8125 - mean_squared_error: 296269.8125\n",
      "Epoch 4704/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301208.3438 - mean_squared_error: 301208.3438\n",
      "Epoch 4705/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288415.0312 - mean_squared_error: 288415.0312\n",
      "Epoch 4706/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 286918.7500 - mean_squared_error: 286918.7188\n",
      "Epoch 4707/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302191.2812 - mean_squared_error: 302191.2812\n",
      "Epoch 4708/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299409.5938 - mean_squared_error: 299409.5938\n",
      "Epoch 4709/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301810.6250 - mean_squared_error: 301810.6250\n",
      "Epoch 4710/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291812.4688 - mean_squared_error: 291812.5000\n",
      "Epoch 4711/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297243.2812 - mean_squared_error: 297243.2812\n",
      "Epoch 4712/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292491.2500 - mean_squared_error: 292491.2500\n",
      "Epoch 4713/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297357.7188 - mean_squared_error: 297357.7188\n",
      "Epoch 4714/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290217.7188 - mean_squared_error: 290217.6875\n",
      "Epoch 4715/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294197.8125 - mean_squared_error: 294197.8125\n",
      "Epoch 4716/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 300371.1562 - mean_squared_error: 300371.1562\n",
      "Epoch 4717/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301861.3750 - mean_squared_error: 301861.3750\n",
      "Epoch 4718/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297248.4688 - mean_squared_error: 297248.4375\n",
      "Epoch 4719/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 305123.5312 - mean_squared_error: 305123.5312\n",
      "Epoch 4720/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290282.5938 - mean_squared_error: 290282.5938\n",
      "Epoch 4721/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 281104.5312 - mean_squared_error: 281104.5312\n",
      "Epoch 4722/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292521.1875 - mean_squared_error: 292521.1875\n",
      "Epoch 4723/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290643.8438 - mean_squared_error: 290643.8438\n",
      "Epoch 4724/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300436.5938 - mean_squared_error: 300436.5625\n",
      "Epoch 4725/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296272.8125 - mean_squared_error: 296272.8125\n",
      "Epoch 4726/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 276997.9688 - mean_squared_error: 276997.9688\n",
      "Epoch 4727/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296030.2188 - mean_squared_error: 296030.2188\n",
      "Epoch 4728/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296600.0000 - mean_squared_error: 296600.0000\n",
      "Epoch 4729/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 289068.1875 - mean_squared_error: 289068.1875\n",
      "Epoch 4730/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296592.2188 - mean_squared_error: 296592.2188\n",
      "Epoch 4731/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 288830.6562 - mean_squared_error: 288830.6562\n",
      "Epoch 4732/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293537.5312 - mean_squared_error: 293537.5312\n",
      "Epoch 4733/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 311580.5625 - mean_squared_error: 311580.5625\n",
      "Epoch 4734/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299703.5000 - mean_squared_error: 299703.4688\n",
      "Epoch 4735/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292120.7500 - mean_squared_error: 292120.7500\n",
      "Epoch 4736/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292808.6562 - mean_squared_error: 292808.6562\n",
      "Epoch 4737/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291914.5000 - mean_squared_error: 291914.5000\n",
      "Epoch 4738/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 282539.8438 - mean_squared_error: 282539.8438\n",
      "Epoch 4739/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299260.4062 - mean_squared_error: 299260.4062\n",
      "Epoch 4740/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296190.5938 - mean_squared_error: 296190.5938\n",
      "Epoch 4741/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296629.7500 - mean_squared_error: 296629.7500\n",
      "Epoch 4742/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294918.9062 - mean_squared_error: 294918.9062\n",
      "Epoch 4743/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299462.0000 - mean_squared_error: 299462.0000\n",
      "Epoch 4744/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 306474.1250 - mean_squared_error: 306474.1250\n",
      "Epoch 4745/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291450.0625 - mean_squared_error: 291450.0625\n",
      "Epoch 4746/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 305805.4375 - mean_squared_error: 305805.4375\n",
      "Epoch 4747/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297823.0625 - mean_squared_error: 297823.0938\n",
      "Epoch 4748/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 14ms/step - loss: 298153.2500 - mean_squared_error: 298153.2812\n",
      "Epoch 4749/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299323.7812 - mean_squared_error: 299323.7812\n",
      "Epoch 4750/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 296236.9375 - mean_squared_error: 296236.9062\n",
      "Epoch 4751/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294764.5625 - mean_squared_error: 294764.5625\n",
      "Epoch 4752/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 292917.8750 - mean_squared_error: 292917.8750\n",
      "Epoch 4753/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298746.1875 - mean_squared_error: 298746.1875\n",
      "Epoch 4754/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291740.9375 - mean_squared_error: 291740.9375\n",
      "Epoch 4755/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302401.5312 - mean_squared_error: 302401.5312\n",
      "Epoch 4756/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296664.4375 - mean_squared_error: 296664.4375\n",
      "Epoch 4757/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 284913.4688 - mean_squared_error: 284913.4688\n",
      "Epoch 4758/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290623.7500 - mean_squared_error: 290623.7188\n",
      "Epoch 4759/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295746.3125 - mean_squared_error: 295746.3125\n",
      "Epoch 4760/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300997.0312 - mean_squared_error: 300997.0312\n",
      "Epoch 4761/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291243.1250 - mean_squared_error: 291243.1250\n",
      "Epoch 4762/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 302366.0312 - mean_squared_error: 302366.0312\n",
      "Epoch 4763/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294762.3438 - mean_squared_error: 294762.3750\n",
      "Epoch 4764/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297831.2188 - mean_squared_error: 297831.2188\n",
      "Epoch 4765/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297293.3438 - mean_squared_error: 297293.3438\n",
      "Epoch 4766/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291626.9688 - mean_squared_error: 291626.9688\n",
      "Epoch 4767/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297343.1562 - mean_squared_error: 297343.1562\n",
      "Epoch 4768/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 284460.3125 - mean_squared_error: 284460.3125\n",
      "Epoch 4769/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296018.8125 - mean_squared_error: 296018.8125\n",
      "Epoch 4770/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291701.5625 - mean_squared_error: 291701.5625\n",
      "Epoch 4771/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293883.5000 - mean_squared_error: 293883.5312\n",
      "Epoch 4772/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291551.5625 - mean_squared_error: 291551.5938\n",
      "Epoch 4773/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303908.4688 - mean_squared_error: 303908.4688\n",
      "Epoch 4774/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292065.9062 - mean_squared_error: 292065.9062\n",
      "Epoch 4775/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290763.9375 - mean_squared_error: 290763.9375\n",
      "Epoch 4776/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302240.2500 - mean_squared_error: 302240.2500\n",
      "Epoch 4777/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296925.5625 - mean_squared_error: 296925.5625\n",
      "Epoch 4778/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295547.0312 - mean_squared_error: 295547.0312\n",
      "Epoch 4779/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292134.0312 - mean_squared_error: 292134.0312\n",
      "Epoch 4780/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297909.6562 - mean_squared_error: 297909.6562\n",
      "Epoch 4781/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290322.6562 - mean_squared_error: 290322.6562\n",
      "Epoch 4782/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295179.0312 - mean_squared_error: 295179.0312\n",
      "Epoch 4783/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291085.3750 - mean_squared_error: 291085.3750\n",
      "Epoch 4784/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302076.0312 - mean_squared_error: 302076.0312\n",
      "Epoch 4785/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293661.5000 - mean_squared_error: 293661.5000\n",
      "Epoch 4786/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299641.5312 - mean_squared_error: 299641.5312\n",
      "Epoch 4787/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289638.7500 - mean_squared_error: 289638.7500\n",
      "Epoch 4788/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294456.5000 - mean_squared_error: 294456.5000\n",
      "Epoch 4789/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290853.6562 - mean_squared_error: 290853.6250\n",
      "Epoch 4790/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292702.7500 - mean_squared_error: 292702.7500\n",
      "Epoch 4791/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 289803.3750 - mean_squared_error: 289803.4062\n",
      "Epoch 4792/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295467.0625 - mean_squared_error: 295467.0625\n",
      "Epoch 4793/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294121.0625 - mean_squared_error: 294121.0625\n",
      "Epoch 4794/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292205.6562 - mean_squared_error: 292205.6562\n",
      "Epoch 4795/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299057.7812 - mean_squared_error: 299057.7812\n",
      "Epoch 4796/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299373.4062 - mean_squared_error: 299373.4062\n",
      "Epoch 4797/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295293.9375 - mean_squared_error: 295293.9375\n",
      "Epoch 4798/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 350879.1250 - mean_squared_error: 350879.1250\n",
      "Epoch 4799/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296228.0312 - mean_squared_error: 296228.0312\n",
      "Epoch 4800/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298194.3438 - mean_squared_error: 298194.3438\n",
      "Epoch 4801/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293446.4062 - mean_squared_error: 293446.3750\n",
      "Epoch 4802/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 300117.3750 - mean_squared_error: 300117.4062\n",
      "Epoch 4803/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299198.0000 - mean_squared_error: 299198.0000\n",
      "Epoch 4804/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 306113.1562 - mean_squared_error: 306113.1562\n",
      "Epoch 4805/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 291931.0312 - mean_squared_error: 291931.0000\n",
      "Epoch 4806/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297784.8750 - mean_squared_error: 297784.8750\n",
      "Epoch 4807/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 307583.8438 - mean_squared_error: 307583.8438\n",
      "Epoch 4808/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294983.6875 - mean_squared_error: 294983.6875\n",
      "Epoch 4809/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296075.9062 - mean_squared_error: 296075.9062\n",
      "Epoch 4810/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298142.4688 - mean_squared_error: 298142.4688\n",
      "Epoch 4811/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292452.0938 - mean_squared_error: 292452.0938\n",
      "Epoch 4812/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292889.8750 - mean_squared_error: 292889.8750\n",
      "Epoch 4813/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294064.6250 - mean_squared_error: 294064.6250\n",
      "Epoch 4814/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295710.2500 - mean_squared_error: 295710.2500\n",
      "Epoch 4815/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 305600.3125 - mean_squared_error: 305600.3125\n",
      "Epoch 4816/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291504.3438 - mean_squared_error: 291504.3125\n",
      "Epoch 4817/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287835.5312 - mean_squared_error: 287835.5312\n",
      "Epoch 4818/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302775.2500 - mean_squared_error: 302775.2500\n",
      "Epoch 4819/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297677.0000 - mean_squared_error: 297676.9688\n",
      "Epoch 4820/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290390.1562 - mean_squared_error: 290390.1562\n",
      "Epoch 4821/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 298826.0938 - mean_squared_error: 298826.0938\n",
      "Epoch 4822/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300247.0625 - mean_squared_error: 300247.0625\n",
      "Epoch 4823/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295129.4062 - mean_squared_error: 295129.4062\n",
      "Epoch 4824/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295186.1875 - mean_squared_error: 295186.1875\n",
      "Epoch 4825/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294303.3125 - mean_squared_error: 294303.3125\n",
      "Epoch 4826/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296189.1250 - mean_squared_error: 296189.1250\n",
      "Epoch 4827/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301322.6250 - mean_squared_error: 301322.6250\n",
      "Epoch 4828/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289675.7812 - mean_squared_error: 289675.7812\n",
      "Epoch 4829/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293736.1562 - mean_squared_error: 293736.1562\n",
      "Epoch 4830/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299248.9688 - mean_squared_error: 299248.9688\n",
      "Epoch 4831/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 298454.9375 - mean_squared_error: 298454.9375\n",
      "Epoch 4832/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296321.1875 - mean_squared_error: 296321.1875\n",
      "Epoch 4833/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301632.5000 - mean_squared_error: 301632.5000\n",
      "Epoch 4834/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295909.6875 - mean_squared_error: 295909.6875\n",
      "Epoch 4835/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291577.9375 - mean_squared_error: 291577.9375\n",
      "Epoch 4836/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296086.2812 - mean_squared_error: 296086.3125\n",
      "Epoch 4837/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 305976.6562 - mean_squared_error: 305976.6875\n",
      "Epoch 4838/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297446.0000 - mean_squared_error: 297446.0000\n",
      "Epoch 4839/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297809.8125 - mean_squared_error: 297809.8125\n",
      "Epoch 4840/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297987.9375 - mean_squared_error: 297987.9375\n",
      "Epoch 4841/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299106.9688 - mean_squared_error: 299106.9688\n",
      "Epoch 4842/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297215.2812 - mean_squared_error: 297215.2812\n",
      "Epoch 4843/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297053.0938 - mean_squared_error: 297053.0938\n",
      "Epoch 4844/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298523.2500 - mean_squared_error: 298523.2500\n",
      "Epoch 4845/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302284.1875 - mean_squared_error: 302284.1875\n",
      "Epoch 4846/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 308828.7812 - mean_squared_error: 308828.7812\n",
      "Epoch 4847/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296367.9375 - mean_squared_error: 296367.9375\n",
      "Epoch 4848/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299362.7188 - mean_squared_error: 299362.7188\n",
      "Epoch 4849/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297702.2188 - mean_squared_error: 297702.2188\n",
      "Epoch 4850/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291304.9688 - mean_squared_error: 291305.0000\n",
      "Epoch 4851/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 304432.9688 - mean_squared_error: 304432.9688\n",
      "Epoch 4852/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295832.7812 - mean_squared_error: 295832.7812\n",
      "Epoch 4853/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298425.4688 - mean_squared_error: 298425.4688\n",
      "Epoch 4854/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 306158.4688 - mean_squared_error: 306158.4688\n",
      "Epoch 4855/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303987.6250 - mean_squared_error: 303987.6250\n",
      "Epoch 4856/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303699.0625 - mean_squared_error: 303699.0312\n",
      "Epoch 4857/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293622.9375 - mean_squared_error: 293622.9375\n",
      "Epoch 4858/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294711.9688 - mean_squared_error: 294711.9688\n",
      "Epoch 4859/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290961.8438 - mean_squared_error: 290961.8438\n",
      "Epoch 4860/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290080.9062 - mean_squared_error: 290080.9062\n",
      "Epoch 4861/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290619.4375 - mean_squared_error: 290619.4375\n",
      "Epoch 4862/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291592.1562 - mean_squared_error: 291592.1875\n",
      "Epoch 4863/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294548.3438 - mean_squared_error: 294548.3438\n",
      "Epoch 4864/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297259.6562 - mean_squared_error: 297259.6562\n",
      "Epoch 4865/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296063.5312 - mean_squared_error: 296063.5312\n",
      "Epoch 4866/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300749.4375 - mean_squared_error: 300749.4375\n",
      "Epoch 4867/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291909.5312 - mean_squared_error: 291909.5312\n",
      "Epoch 4868/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298266.9062 - mean_squared_error: 298266.9062\n",
      "Epoch 4869/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294574.5938 - mean_squared_error: 294574.5938\n",
      "Epoch 4870/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297216.1250 - mean_squared_error: 297216.1250\n",
      "Epoch 4871/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299393.1875 - mean_squared_error: 299393.2188\n",
      "Epoch 4872/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294851.0312 - mean_squared_error: 294851.0625\n",
      "Epoch 4873/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295026.5938 - mean_squared_error: 295026.5938\n",
      "Epoch 4874/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300933.9688 - mean_squared_error: 300933.9688\n",
      "Epoch 4875/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300403.2188 - mean_squared_error: 300403.2188\n",
      "Epoch 4876/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300902.2500 - mean_squared_error: 300902.2500\n",
      "Epoch 4877/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295741.4688 - mean_squared_error: 295741.4688\n",
      "Epoch 4878/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 304987.9688 - mean_squared_error: 304987.9688\n",
      "Epoch 4879/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 309089.7500 - mean_squared_error: 309089.7812\n",
      "Epoch 4880/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 289601.9062 - mean_squared_error: 289601.9375\n",
      "Epoch 4881/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298650.0000 - mean_squared_error: 298650.0000\n",
      "Epoch 4882/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296169.2500 - mean_squared_error: 296169.2500\n",
      "Epoch 4883/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301180.0000 - mean_squared_error: 301180.0000\n",
      "Epoch 4884/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294901.4688 - mean_squared_error: 294901.5000\n",
      "Epoch 4885/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298293.7188 - mean_squared_error: 298293.7188\n",
      "Epoch 4886/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 306101.5938 - mean_squared_error: 306101.5938\n",
      "Epoch 4887/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290757.1875 - mean_squared_error: 290757.2188\n",
      "Epoch 4888/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 289615.4062 - mean_squared_error: 289615.4062\n",
      "Epoch 4889/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298330.3750 - mean_squared_error: 298330.3750\n",
      "Epoch 4890/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300184.2188 - mean_squared_error: 300184.2188\n",
      "Epoch 4891/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299360.7188 - mean_squared_error: 299360.6875\n",
      "Epoch 4892/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295107.4688 - mean_squared_error: 295107.4688\n",
      "Epoch 4893/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294347.6250 - mean_squared_error: 294347.6250\n",
      "Epoch 4894/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296736.1562 - mean_squared_error: 296736.1875\n",
      "Epoch 4895/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298084.3438 - mean_squared_error: 298084.3438\n",
      "Epoch 4896/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292812.3750 - mean_squared_error: 292812.3750\n",
      "Epoch 4897/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300418.2500 - mean_squared_error: 300418.2500\n",
      "Epoch 4898/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297893.6875 - mean_squared_error: 297893.6875\n",
      "Epoch 4899/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292482.1562 - mean_squared_error: 292482.1562\n",
      "Epoch 4900/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300337.0312 - mean_squared_error: 300337.0312\n",
      "Epoch 4901/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299262.7188 - mean_squared_error: 299262.7188\n",
      "Epoch 4902/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293260.6875 - mean_squared_error: 293260.6875\n",
      "Epoch 4903/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 306344.4062 - mean_squared_error: 306344.4062\n",
      "Epoch 4904/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296637.1875 - mean_squared_error: 296637.1562\n",
      "Epoch 4905/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301841.0625 - mean_squared_error: 301841.0625\n",
      "Epoch 4906/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295494.6250 - mean_squared_error: 295494.6250\n",
      "Epoch 4907/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289052.0000 - mean_squared_error: 289052.0000\n",
      "Epoch 4908/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 300095.3125 - mean_squared_error: 300095.3125\n",
      "Epoch 4909/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 304995.3438 - mean_squared_error: 304995.3438\n",
      "Epoch 4910/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 290169.4062 - mean_squared_error: 290169.4062\n",
      "Epoch 4911/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292156.4688 - mean_squared_error: 292156.4688\n",
      "Epoch 4912/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299829.1562 - mean_squared_error: 299829.1562\n",
      "Epoch 4913/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294820.1562 - mean_squared_error: 294820.1562\n",
      "Epoch 4914/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300867.5625 - mean_squared_error: 300867.5625\n",
      "Epoch 4915/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294887.1875 - mean_squared_error: 294887.1875\n",
      "Epoch 4916/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292848.5000 - mean_squared_error: 292848.5000\n",
      "Epoch 4917/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295538.0312 - mean_squared_error: 295538.0000\n",
      "Epoch 4918/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 290879.5938 - mean_squared_error: 290879.5938\n",
      "Epoch 4919/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293247.6875 - mean_squared_error: 293247.6875\n",
      "Epoch 4920/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296590.9375 - mean_squared_error: 296590.9375\n",
      "Epoch 4921/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287990.4375 - mean_squared_error: 287990.4375\n",
      "Epoch 4922/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 314410.6562 - mean_squared_error: 314410.6562\n",
      "Epoch 4923/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 305023.5000 - mean_squared_error: 305023.5000\n",
      "Epoch 4924/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295301.3750 - mean_squared_error: 295301.3750\n",
      "Epoch 4925/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295329.5625 - mean_squared_error: 295329.5625\n",
      "Epoch 4926/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303672.5938 - mean_squared_error: 303672.5938\n",
      "Epoch 4927/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 286358.3438 - mean_squared_error: 286358.3438\n",
      "Epoch 4928/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295556.4375 - mean_squared_error: 295556.4375\n",
      "Epoch 4929/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 292380.5312 - mean_squared_error: 292380.5312\n",
      "Epoch 4930/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 286673.6875 - mean_squared_error: 286673.6875\n",
      "Epoch 4931/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294085.1562 - mean_squared_error: 294085.1562\n",
      "Epoch 4932/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 303535.2500 - mean_squared_error: 303535.2500\n",
      "Epoch 4933/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295204.9375 - mean_squared_error: 295204.9375\n",
      "Epoch 4934/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299426.1875 - mean_squared_error: 299426.1875\n",
      "Epoch 4935/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299620.7500 - mean_squared_error: 299620.7500\n",
      "Epoch 4936/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296877.9062 - mean_squared_error: 296877.9375\n",
      "Epoch 4937/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303751.5000 - mean_squared_error: 303751.5000\n",
      "Epoch 4938/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296151.9688 - mean_squared_error: 296151.9688\n",
      "Epoch 4939/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 294584.8750 - mean_squared_error: 294584.8750\n",
      "Epoch 4940/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296236.3125 - mean_squared_error: 296236.3125\n",
      "Epoch 4941/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 301941.4375 - mean_squared_error: 301941.4375\n",
      "Epoch 4942/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294205.8750 - mean_squared_error: 294205.8750\n",
      "Epoch 4943/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295203.5625 - mean_squared_error: 295203.5312\n",
      "Epoch 4944/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 302010.3438 - mean_squared_error: 302010.3438\n",
      "Epoch 4945/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 304630.7812 - mean_squared_error: 304630.7812\n",
      "Epoch 4946/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291521.2812 - mean_squared_error: 291521.2812\n",
      "Epoch 4947/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290827.5625 - mean_squared_error: 290827.5625\n",
      "Epoch 4948/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299061.5000 - mean_squared_error: 299061.5000\n",
      "Epoch 4949/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 292957.3438 - mean_squared_error: 292957.3438\n",
      "Epoch 4950/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 302139.6250 - mean_squared_error: 302139.6250\n",
      "Epoch 4951/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 287296.4062 - mean_squared_error: 287296.4062\n",
      "Epoch 4952/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293488.7812 - mean_squared_error: 293488.7812\n",
      "Epoch 4953/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 301523.2812 - mean_squared_error: 301523.3125\n",
      "Epoch 4954/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 287545.3750 - mean_squared_error: 287545.3750\n",
      "Epoch 4955/5000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 295862.6250 - mean_squared_error: 295862.6250\n",
      "Epoch 4956/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 298829.8750 - mean_squared_error: 298829.8750\n",
      "Epoch 4957/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 295366.7500 - mean_squared_error: 295366.7500\n",
      "Epoch 4958/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300135.8438 - mean_squared_error: 300135.8438\n",
      "Epoch 4959/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 293710.0000 - mean_squared_error: 293709.9688\n",
      "Epoch 4960/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 297094.7812 - mean_squared_error: 297094.7812\n",
      "Epoch 4961/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299808.5312 - mean_squared_error: 299808.5312\n",
      "Epoch 4962/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296105.9062 - mean_squared_error: 296105.9062\n",
      "Epoch 4963/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288263.0000 - mean_squared_error: 288263.0000\n",
      "Epoch 4964/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297841.2188 - mean_squared_error: 297841.2500\n",
      "Epoch 4965/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 299463.6562 - mean_squared_error: 299463.6562\n",
      "Epoch 4966/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 284686.3438 - mean_squared_error: 284686.3438\n",
      "Epoch 4967/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289967.3750 - mean_squared_error: 289967.3750\n",
      "Epoch 4968/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297247.4062 - mean_squared_error: 297247.4062\n",
      "Epoch 4969/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297892.6562 - mean_squared_error: 297892.6562\n",
      "Epoch 4970/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 300605.4375 - mean_squared_error: 300605.4375\n",
      "Epoch 4971/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 303418.9062 - mean_squared_error: 303418.9062\n",
      "Epoch 4972/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 296857.5625 - mean_squared_error: 296857.5625\n",
      "Epoch 4973/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295116.8438 - mean_squared_error: 295116.8438\n",
      "Epoch 4974/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 291230.5938 - mean_squared_error: 291230.5938\n",
      "Epoch 4975/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293691.7500 - mean_squared_error: 293691.7500\n",
      "Epoch 4976/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 289749.3438 - mean_squared_error: 289749.3125\n",
      "Epoch 4977/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293706.5625 - mean_squared_error: 293706.5625\n",
      "Epoch 4978/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289440.8750 - mean_squared_error: 289440.8750\n",
      "Epoch 4979/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290854.0625 - mean_squared_error: 290854.0625\n",
      "Epoch 4980/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 286279.9062 - mean_squared_error: 286279.9062\n",
      "Epoch 4981/5000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 292104.9062 - mean_squared_error: 292104.9062\n",
      "Epoch 4982/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 299323.7500 - mean_squared_error: 299323.7500\n",
      "Epoch 4983/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 289523.0312 - mean_squared_error: 289523.0312\n",
      "Epoch 4984/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 283911.0312 - mean_squared_error: 283911.0312\n",
      "Epoch 4985/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 291844.2812 - mean_squared_error: 291844.2812\n",
      "Epoch 4986/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 293355.9688 - mean_squared_error: 293355.9688\n",
      "Epoch 4987/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296610.9062 - mean_squared_error: 296610.9062\n",
      "Epoch 4988/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 286162.5938 - mean_squared_error: 286162.5938\n",
      "Epoch 4989/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 297500.5312 - mean_squared_error: 297500.5312\n",
      "Epoch 4990/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295901.4375 - mean_squared_error: 295901.4688\n",
      "Epoch 4991/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 288791.2188 - mean_squared_error: 288791.2188\n",
      "Epoch 4992/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294305.0000 - mean_squared_error: 294305.0000\n",
      "Epoch 4993/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 296355.2812 - mean_squared_error: 296355.2812\n",
      "Epoch 4994/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 288313.4688 - mean_squared_error: 288313.4688\n",
      "Epoch 4995/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294479.5938 - mean_squared_error: 294479.5938\n",
      "Epoch 4996/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 287110.3750 - mean_squared_error: 287110.3750\n",
      "Epoch 4997/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 294565.1562 - mean_squared_error: 294565.1562\n",
      "Epoch 4998/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 295657.6562 - mean_squared_error: 295657.6562\n",
      "Epoch 4999/5000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298884.3438 - mean_squared_error: 298884.3438\n",
      "Epoch 5000/5000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 290288.2188 - mean_squared_error: 290288.2188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:07.807385: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:07.808179: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:07.808760: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:07.879042: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:07.903540: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:07.904173: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:07.904749: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:07.999377: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:08.000017: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:08.000629: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:08.072554: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:08.098000: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:08.098651: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:08.099253: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:08.194255: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:08.194901: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:08.195516: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:08.266865: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:08.292159: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:08.292981: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:08.293574: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:08.402153: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:08.402813: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:08.403419: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:08.500215: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:08.500990: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:08.501569: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:08.515533: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,1]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:08.519827: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,1]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:08.950140: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:08.975118: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:08.975750: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:08.976332: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:09.054395: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:09.079060: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:09.079702: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:09.080295: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:09.094209: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,1]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:09.101608: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,1]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:09.193569: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:09.194398: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:09.194988: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:09.207719: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:09.272096: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:09.296715: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:09.297351: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:09.297932: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:09.310450: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:09.400949: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:09.401778: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:09.402367: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:09.415768: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:09.482117: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:09.506743: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:09.507398: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:09.507975: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:09.520567: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:09.531915: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:09.538037: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:09.544823: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:09.548694: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:09.552969: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:09.556820: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:09.653575: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:09.654390: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:09.654980: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:09.751715: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:09.752367: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:09.752969: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:09.766389: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:09.772649: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:09.841502: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:09.866347: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:09.867128: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:09.867728: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:09.942186: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:09.966693: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:09.967331: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:09.967908: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:09.981533: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:09.985767: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:10.075966: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:10.076751: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:10.077330: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:10.090169: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:10.153520: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:10.178114: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:10.178739: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:10.179320: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:10.191777: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:10.282162: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:10.282960: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:10.283562: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:10.296486: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:10.360401: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:10.385094: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:10.385727: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:10.386305: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:10.398725: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:10.405997: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:10.412056: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:10.416388: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:10.420256: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:10.424551: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:10.428439: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:10.522967: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:10.523783: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:10.524368: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:10.620091: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:10.620736: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:10.621329: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:10.634702: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:10.641042: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:10.708278: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:10.733286: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:10.734065: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:10.734644: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:10.810746: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:10.836127: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:10.836764: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:10.837363: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:10.851200: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:10.855596: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:10.948167: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:10.948965: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:10.949560: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:10.962877: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:11.036185: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:11.061279: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:11.061918: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:11.062535: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:11.075538: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:11.167608: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:11.168413: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:11.169003: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:11.182246: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:11.247271: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:11.272357: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:11.273159: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:11.273958: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:11.292048: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:11.299625: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:11.305994: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:11.310547: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:11.314541: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:11.318946: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:11.322940: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:11.421307: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:11.422118: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:11.422720: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:11.495683: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:11.520774: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:11.521443: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:11.522045: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:11.629046: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:11.629869: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:11.630488: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:11.708749: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:11.733978: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:11.734651: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:11.735273: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:11.840897: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:11.841719: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:11.842324: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:11.915301: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:11.940561: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:11.941230: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:11.941817: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:12.062296: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:12.063101: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:12.063715: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:12.136847: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:12.161932: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:12.162571: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:12.163593: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:12.268552: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:12.269356: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:12.269947: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:12.343693: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:12.368900: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:12.369583: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:12.370190: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:12.475231: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:12.476039: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:12.476641: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:12.550322: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:12.578885: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:12.579560: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:12.580152: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:12.632797: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'input_1' with dtype double and shape [?,30,1]\n",
      "\t [[{{node input_1}}]]\n",
      "2023-07-13 02:24:12.664979: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'input_1' with dtype double and shape [?,30,1]\n",
      "\t [[{{node input_1}}]]\n",
      "2023-07-13 02:24:12.671982: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,30,1]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:12.678106: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,30,1]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:12.684964: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,30,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:12.690959: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,30,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:12.697485: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,30,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:12.703658: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,30,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:12.706463: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,30,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:12.728956: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype double and shape [?,30,1]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:12.743401: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype double and shape [?,30,1]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:12.796869: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'input_1' with dtype double and shape [?,30,1]\n",
      "\t [[{{node input_1}}]]\n",
      "2023-07-13 02:24:12.821625: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype double and shape [?,30,1]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:12.835583: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype double and shape [?,30,1]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:12.927819: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:12.928650: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:12.929253: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:13.004025: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:13.029014: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:13.029670: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:13.030272: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:13.126564: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:13.127227: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:13.127858: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:13.201804: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:13.226864: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:13.227514: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:13.228108: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:13.324543: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:13.325187: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:13.325791: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:13.397718: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:13.423734: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:13.424547: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:13.425136: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:13.849183: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:13.850002: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:13.850597: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:13.923456: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:13.948574: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:13.949206: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:13.949784: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:14.046370: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:14.047037: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:14.047655: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:14.121101: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:14.146675: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:14.147337: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:14.147936: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:14.252351: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:14.252990: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:14.253581: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:14.325936: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:14.350970: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:14.351795: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:14.352559: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:14.383633: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:14.390020: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:14.396069: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,30,1]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:14.402189: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,30,1]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:14.490539: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:14.491235: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:14.491841: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:14.565881: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:14.598531: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:14.599563: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:14.600477: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:14.704465: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:14.705101: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:14.705689: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:14.777886: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:14.804621: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:14.805244: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:14.805819: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:14.912388: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:14.913049: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:14.913640: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:14.988820: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:15.016189: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:15.016991: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:15.017712: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:15.121742: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:15.122386: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:15.122966: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:15.194909: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:15.220484: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:15.221108: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:15.221689: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:15.247407: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:15.253712: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:15.259760: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,30,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:15.265792: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,30,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:15.354263: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:15.354919: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:15.355529: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:15.428274: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:15.453247: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:15.453871: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:15.454455: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:15.554848: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:15.555515: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:15.556090: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:15.627498: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:15.655257: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:15.656102: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:15.656703: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:15.763666: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:15.764318: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:15.764900: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:15.836708: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:15.862110: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:15.862939: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:15.863535: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:15.966591: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:15.967245: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:15.967850: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:16.039382: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:16.064686: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:16.065454: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:16.066033: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:16.092057: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:16.098365: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:16.104983: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,30,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:16.111019: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,30,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:16.204692: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:16.205338: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:16.205939: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:16.278670: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:16.303865: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:16.304503: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:16.305104: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:16.417109: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:16.417767: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:16.418358: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:16.491333: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:16.516370: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:16.517037: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:16.517620: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:16.620266: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:16.620916: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:16.621501: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:16.693757: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:16.718837: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:16.719521: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:16.720121: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:16.820807: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:16.821466: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:16.822066: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:16.893607: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:16.923797: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:16.924713: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:16.925320: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:16.947534: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,30,2]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:16.972556: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,1]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:16.978786: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,1]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:16.983484: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:16.987642: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:17.075506: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:17.076158: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:17.076746: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:17.175136: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:17.175963: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:17.176559: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:17.276949: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:17.277588: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:17.278166: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:17.379711: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:17.380488: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:17.381070: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:17.401433: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,1]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:17.405740: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,1]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:17.410017: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:17.414216: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:17.479234: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:17.506638: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:17.507289: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:17.507865: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:17.582917: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:17.613097: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:17.613751: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:17.614337: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:17.689821: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:17.714746: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:17.715409: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:17.716027: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:17.793125: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:17.820715: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:17.821343: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:17.821917: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:17.841992: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:17.846783: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:17.850947: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:17.855116: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:17.945461: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:17.946100: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:17.946679: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:18.045447: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:18.046249: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:18.046830: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:18.149607: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:18.150266: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:18.150857: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:18.248649: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:18.249409: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:18.249999: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:18.270031: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:18.274301: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:18.278569: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:18.282760: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:18.347639: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:18.373147: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:18.373801: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:18.374380: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:18.448499: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:18.473980: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:18.474778: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:18.475394: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:18.551221: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:18.576216: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:18.576867: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:18.577437: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:18.651611: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:18.676821: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:18.677577: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:18.678164: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:18.698442: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:18.703220: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:18.707349: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:18.711495: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:18.799601: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:18.800261: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:18.800852: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:18.898998: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:18.899826: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:18.900426: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:19.000781: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:19.001428: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:19.002017: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:19.102607: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:19.103384: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:19.103980: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:19.124207: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:19.128539: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs_0' with dtype float and shape [?,?,2]\n",
      "\t [[{{node inputs_0}}]]\n",
      "2023-07-13 02:24:19.132962: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:19.137151: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-07-13 02:24:19.202637: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:19.228341: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:19.229048: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:19.229634: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 02:24:19.304815: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:19.330745: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:19.331411: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:19.332034: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:19.411918: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:19.436811: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:19.437431: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:19.438031: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 02:24:19.512731: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 02:24:19.537772: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 02:24:19.538398: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 02:24:19.538972: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "{\"levelname\": \"WARNING\", \"asctime\": \"2023-07-13 02:24:19,572\", \"filename\": \"save.py\", \"funcName\": \"__init__\", \"lineno\": 274, \"message\": \"Found untraced functions such as _update_step_xla, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 13). These functions will not be directly callable after loading.\"}\n",
      "2023-07-13 02:24:19.690781: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'serving_default_input_1' with dtype double and shape [?,30,1]\n",
      "\t [[{{node serving_default_input_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./auto_model/best_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"levelname\": \"INFO\", \"asctime\": \"2023-07-13 02:24:21,822\", \"filename\": \"builder_impl.py\", \"funcName\": \"copy_assets_to_destination_dir\", \"lineno\": 797, \"message\": \"Assets written to: ./auto_model/best_model/assets\"}\n"
     ]
    }
   ],
   "source": [
    "import autokeras as ak\n",
    "import numpy as np\n",
    "\n",
    "def generate_time_series_forecaster(window_size, n_forecast):\n",
    "    # Generate some dummy data\n",
    "    X_train = np.random.rand(1000, window_size, 1)\n",
    "    y_train = np.random.rand(1000, n_forecast, 1)\n",
    "\n",
    "    # Initialize the time series forecaster\n",
    "    input_node = ak.Input()\n",
    "    gru_node = ak.RNNBlock(return_sequences=True, layer_type=\"lstm\")(input_node)\n",
    "    output_node = ak.RegressionHead(output_dim=n_forecast)(gru_node)\n",
    "\n",
    "    auto_model = ak.AutoModel(\n",
    "    inputs=input_node,\n",
    "    outputs=output_node,\n",
    "    overwrite=True,\n",
    "    max_trials=5)\n",
    "\n",
    "#     predict_from = 1\n",
    "#     predict_until = n_forecast\n",
    "#     lookback = window_size\n",
    "#     auto_model = ak.TimeseriesForecaster(\n",
    "#         lookback=lookback,\n",
    "#         predict_from=predict_from,\n",
    "#         predict_until=predict_until,\n",
    "#         max_trials=1,\n",
    "#         objective=\"mse\",\n",
    "#     )\n",
    "    i = np.array(inputs)\n",
    "    t = np.array(targets)\n",
    "    print(t.shape)\n",
    "    # Search for the best model architecture\n",
    "    auto_model.fit(\n",
    "        np.reshape(\n",
    "            i, (-1,window_size, 1)\n",
    "        ),\n",
    "        np.reshape(\n",
    "            t, (-1, n_forecast)\n",
    "        ),\n",
    "        batch_size=100,\n",
    "        epochs=5000,\n",
    "        validation_split=0.1)\n",
    "    \n",
    "    return auto_model\n",
    "\n",
    "model = generate_time_series_forecaster(window_size, nforecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1889a9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea031d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:49:09.249254: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.249327: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.249375: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.291780: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.291855: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.291903: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.304700: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.304756: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.304804: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.316899: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:09.317644: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.317693: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.317740: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.358748: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:09.359549: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.359602: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.359649: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.387654: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.387717: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.387764: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.401781: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.401838: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.401886: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.444001: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.444075: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.444123: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:49:09.457401: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.457475: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.457526: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.505908: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:09.506683: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.506735: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.506781: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.556972: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:09.557748: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.557800: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.557847: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.630518: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.630588: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.630636: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.684318: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:09.685121: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.685176: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.685223: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.700022: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.700083: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.700132: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.730474: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:09.731235: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.731286: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.731332: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.751166: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.751236: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.751286: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.764421: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.764477: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.764525: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.812510: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.812578: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.812626: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.832551: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.832611: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.832658: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:49:09.916427: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.916501: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.916550: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.972373: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:09.973156: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.973210: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.973257: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:09.993023: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:09.993089: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:09.993137: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.005268: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:10.006041: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.006091: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.006139: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.018152: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:10.018903: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.018953: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.018998: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.038876: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.038937: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.038985: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.075823: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:10.076574: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.076623: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.076669: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:49:10.233695: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.233769: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.233817: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.247471: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:10.248222: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.248272: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.248317: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.329624: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:10.330400: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.330452: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.330499: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.349960: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:10.350836: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.350899: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.350950: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.411327: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:10.412071: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.412119: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.412164: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.466218: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.466288: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.466333: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.478163: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:10.478891: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.478938: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.478983: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.567933: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.568002: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.568048: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.589088: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:10.589840: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.589889: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.589934: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.616796: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:10.617541: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.617590: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.617635: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.634408: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:10.635135: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.635183: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.635235: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.648732: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:10.649461: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.649509: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.649554: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.661288: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:10.662129: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.662180: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.662227: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:49:10.724033: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.724101: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.724149: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.828855: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.828925: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.828972: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.937345: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:10.938109: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.938159: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.938204: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:10.976936: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:10.976997: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:10.977043: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.019872: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:11.020604: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.020652: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.020697: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.036224: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:11.036960: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.037010: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.037056: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.092045: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.092116: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.092165: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.110300: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:11.111038: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.111086: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.111131: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:49:11.157220: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:11.157967: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.158017: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.158063: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.209580: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.209654: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.209703: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.222027: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.222082: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.222128: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.246889: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.246996: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.247073: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.291493: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.291558: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.291606: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.466958: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:11.467737: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.467786: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.467831: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.493453: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:11.494207: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.494255: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.494301: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.506808: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.506862: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.506908: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.520574: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.520628: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.520674: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.588400: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:11.589158: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.589209: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.589254: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:49:11.675019: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.675091: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.675138: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.785599: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:11.786353: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.786401: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.786445: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.823275: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:11.824026: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.824075: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.824120: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:11.994246: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:11.994316: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:11.994362: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:12.108673: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:12.109424: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:12.109472: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:12.109517: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:12.213389: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:12.214188: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:12.214237: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:12.214282: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:12.255208: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:12.256472: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:12.256556: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:12.256633: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:12.315616: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:12.316357: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:12.316406: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:12.316451: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:12.328099: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:49:12.328824: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:12.328873: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:12.328918: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:49:12.341369: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:49:12.341424: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:49:12.341470: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:49:12.602232: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:49:12.603062: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:49:12.603640: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:49:12.674139: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 11:49:12.698368: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:49:12.698996: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:49:12.699577: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:49:12.795395: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:49:12.796021: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:49:12.796577: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:49:12.871049: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 11:49:12.895943: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:49:12.896580: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:49:12.897151: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:49:12.992017: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:49:12.992653: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:49:12.993217: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:49:13.068228: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 11:49:13.092926: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:49:13.093747: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:49:13.094324: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:49:13.208932: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [50,30,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-07-13 11:49:13.209060: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [50,30,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:49:13.308535: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:49:13.309361: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:49:13.309948: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:49:13.386298: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 11:49:13.413840: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:49:13.414487: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:49:13.415069: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:49:13.514617: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:49:13.515443: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:49:13.516079: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:49:13.593737: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 11:49:13.618898: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:49:13.619549: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:49:13.620142: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:49:13.722678: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:49:13.723493: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:49:13.724113: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:49:13.798155: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 11:49:13.823310: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:49:13.823950: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:49:13.824543: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "pre2 = model.predict(\n",
    "    np.reshape(\n",
    "        i[-50:], (-1,window_size, 1)\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ac21a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.reshape(\n",
    "        i, (-1,window_size, 1)\n",
    "    ),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbcf563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:41:26.623907: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:26.623979: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:26.624029: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:26.712966: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:26.713045: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:26.713094: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:26.771136: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:26.772045: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:26.772105: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:26.772153: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:26.792987: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:26.793048: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:26.793097: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:26.805519: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:26.806362: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:26.806416: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:26.806463: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:26.819304: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:26.820175: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:26.820231: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:26.820280: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:41:26.843103: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:26.843225: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:26.843314: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:26.882634: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:26.883499: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:26.883555: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:26.883602: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:27.072842: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:27.072917: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:27.072968: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:27.087558: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:27.088478: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:27.088543: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:27.088592: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:27.178564: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:27.180007: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:27.180103: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:27.180185: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:27.200369: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:27.201222: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:27.201278: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:27.201326: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:27.256924: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:27.257787: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:27.257850: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:27.257897: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:41:27.318496: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:27.318575: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:27.318625: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:27.331243: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:27.332100: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:27.332155: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:27.332201: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:27.426962: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:27.427041: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:27.427094: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:27.449541: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:27.450401: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:27.450456: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:27.450504: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:27.484955: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:27.486294: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:27.486361: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:27.486418: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:27.504471: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:27.505312: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:27.505367: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:27.505415: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:41:27.519751: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:27.520589: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:27.520647: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:27.520694: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:27.534157: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:27.534998: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:27.535052: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:27.535098: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:27.598067: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:27.598142: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:27.598190: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:28.732146: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:28.732257: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:28.732338: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:28.837986: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:28.838808: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:28.838859: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:28.838905: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:28.881796: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:28.881899: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:28.881981: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:28.927012: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:28.927824: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:28.927875: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:28.927922: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:41:28.944443: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:28.945244: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:28.945295: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:28.945340: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:29.006945: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.007019: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.007068: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:29.025908: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:29.026704: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.026756: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.026802: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:29.073774: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:29.074569: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.074621: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.074667: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:29.127997: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.128106: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.128186: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:29.142604: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.142662: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.142709: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:41:29.168073: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.168151: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.168201: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:29.215352: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.215422: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.215469: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:29.405394: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:29.406194: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.406244: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.406291: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:29.432974: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:29.434263: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.434347: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.434424: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:29.448116: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.448179: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.448226: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:29.462282: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.462338: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.462386: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:29.529194: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:29.530017: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.530068: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.530113: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:41:29.622522: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.622595: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.622643: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:29.737747: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:29.738541: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.738594: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.738640: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:29.777294: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:29.778083: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.778135: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.778181: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:29.955748: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:29.955825: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:29.955875: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:30.077563: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:30.078340: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:30.078392: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:30.078437: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:30.184457: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:30.185248: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:30.185303: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:30.185350: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:30.226448: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:30.227225: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:30.227278: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:30.227325: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:30.283295: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:30.284116: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:30.284173: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:30.284219: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:30.296422: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_reversev2_grad_reversev2_reversev2_axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients_reversev2_grad_reversev2_reversev2_axis}}]]\n",
      "2023-07-13 11:41:30.297179: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:30.297229: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:30.297274: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n",
      "2023-07-13 11:41:30.310024: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_2_grad_concat_split_2_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_2_grad_concat_split_2_split_dim}}]]\n",
      "2023-07-13 11:41:30.310080: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_grad_concat_split_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_grad_concat_split_split_dim}}]]\n",
      "2023-07-13 11:41:30.310127: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients_split_1_grad_concat_split_1_split_dim' with dtype int32\n",
      "\t [[{{node gradients_split_1_grad_concat_split_1_split_dim}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:41:30.585588: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:41:30.586454: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:41:30.587039: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:41:30.661067: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 11:41:30.685976: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:41:30.686633: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:41:30.687225: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:41:30.785651: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:41:30.786553: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:41:30.787492: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:41:30.860675: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 11:41:30.885474: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:41:30.886151: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:41:30.886755: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:41:30.984516: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:41:30.985153: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:41:30.985723: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:41:31.059834: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 11:41:31.084964: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:41:31.085584: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:41:31.086164: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:41:31.177799: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [1046,30,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-07-13 11:41:31.177947: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [1046,30,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:41:31.296413: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:41:31.297266: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:41:31.297868: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:41:31.377009: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 11:41:31.402810: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:41:31.403476: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:41:31.404076: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:41:31.508922: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:41:31.509786: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:41:31.510433: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:41:31.588465: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 11:41:31.613867: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:41:31.614520: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:41:31.615115: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:41:31.720732: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:41:31.721561: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:41:31.722999: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-13 11:41:31.798345: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-07-13 11:41:31.823776: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 11:41:31.824422: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 11:41:31.825016: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 1s 3ms/step\n",
      "33/33 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  -19.882252,   -19.408764,   -19.544458, ...,   -21.68626 ,\n",
       "          -21.57542 ,   -21.454838],\n",
       "       [   -8.788613,    -8.169158,    -8.277711, ...,   -10.239681,\n",
       "          -10.036526,    -9.943562],\n",
       "       [  -26.897713,   -26.462757,   -26.327469, ...,   -28.92686 ,\n",
       "          -28.935604,   -28.633213],\n",
       "       ...,\n",
       "       [-1101.9879  , -1103.6895  , -1105.4188  , ..., -1131.5598  ,\n",
       "        -1133.3997  , -1135.3241  ],\n",
       "       [-1101.9879  , -1103.6895  , -1105.4188  , ..., -1131.5598  ,\n",
       "        -1133.3997  , -1135.3241  ],\n",
       "       [-1101.9879  , -1103.6895  , -1105.4188  , ..., -1131.5598  ,\n",
       "        -1133.3997  , -1135.3241  ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = np.array(inputs)\n",
    "pre = model.predict(\n",
    "    np.reshape(\n",
    "        i, (-1,window_size, 1)\n",
    "    ),\n",
    ")\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a68777",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo = model.export_model()\n",
    "mo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf16b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97595e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564abbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = model.predict(\n",
    "    np.reshape(\n",
    "        i, (-1,window_size, 1)\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189c9591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre = mo.predict(np.array(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff74a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autokeras as ak\n",
    "import numpy as np\n",
    "\n",
    "def generate_time_series_forecaster(window_size, n_forecast):\n",
    "    # Generate some dummy data\n",
    "    X_train = np.random.rand(1000, window_size, 1)\n",
    "    y_train = np.random.rand(1000, n_forecast, 1)\n",
    "\n",
    "    # Initialize the time series forecaster\n",
    "    input_node = ak.Input()\n",
    "    gru_node = ak.RNNBlock(return_sequences=True, layer_type=\"lstm\")(input_node)\n",
    "    dense_node = ak.DenseBlock()(gru_node)\n",
    "    output_node = ak.RegressionHead(output_dim=n_forecast)(dense_node)\n",
    "\n",
    "    auto_model = ak.AutoModel(\n",
    "    inputs=input_node,\n",
    "    outputs=output_node,\n",
    "    overwrite=True,\n",
    "    max_trials=2)\n",
    "\n",
    "#     predict_from = 1\n",
    "#     predict_until = n_forecast\n",
    "#     lookback = window_size\n",
    "#     auto_model = ak.TimeseriesForecaster(\n",
    "#         lookback=lookback,\n",
    "#         predict_from=predict_from,\n",
    "#         predict_until=predict_until,\n",
    "#         max_trials=1,\n",
    "#         objective=\"mse\",\n",
    "#     )\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    inputs\n",
    "    # Search for the best model architecture\n",
    "    auto_model.fit(\n",
    "        np.reshape(\n",
    "            np.array(inputs), (-1,window_size)\n",
    "        ),\n",
    "        np.reshape(\n",
    "            np.array(targets), (-1,n_forecast, 1)\n",
    "        ), batch_size=100, epochs=1, validation_split=0.1)\n",
    "    \n",
    "    return auto_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1319b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cc91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(\n",
    "    inputs, (-1,window_size, 1)\n",
    "),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af42ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.reshape(\n",
    "    np.array(targets), (-1,nforecast,1)\n",
    ")\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a14ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(\n",
    "            np.array(inputs), (-1,window_size,1)\n",
    "        )\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa82699",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_time_series_forecaster(window_size, nforecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb69f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo = model.export_model() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e431e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65e23f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = mo.predict(\n",
    "    np.reshape(\n",
    "        np.array(inputs), (-1,window_size,1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1333ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d82680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_compra_comigo.data_handler import Visualizer\n",
    "\n",
    "visualizer = Visualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380dc1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = visualizer.create_gif(\n",
    "    time_data=time_data,\n",
    "    series=series,\n",
    "    forecast=pre,\n",
    "    batch_size=batch_size,\n",
    "    window_size=window_size,\n",
    "    nforecast=nforecast,\n",
    "    gif_window=70\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1540c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[0].save('./tmp/knnak2.gif',\n",
    "             save_all = True, append_images = plots[1:], \n",
    "             optimize = False, duration = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a26c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
